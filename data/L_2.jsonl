{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "type": "method", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/socketutils.py", "signature_position": [667, 667], "body_position": [668, 669], "dependency": {"intra_class": ["boltons.socketutils.NetstringSocket._calc_msgsize_maxsize", "boltons.socketutils.NetstringSocket._msgsize_maxsize", "boltons.socketutils.NetstringSocket.maxsize"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Set the maximum size for receiving netstrings in the NetstringSocket instance. It updates the maxsize of the instance and calculates the maximum size for a netstring message based on the new maxsize value.", "Arguments": ":param self: NetstringSocket. An instance of the NetstringSocket class.\n:param maxsize: The maximum size for receiving netstrings.\n:return: No return values."}, "tests": ["tests/test_socketutils.py::test_socketutils_netstring"], "indent": 4, "domain": "Utilities", "code": "    def setmaxsize(self, maxsize):\n        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n", "context": "class NetstringSocket(object):\n    \"\"\"\n    Reads and writes using the netstring protocol.\n\n    More info: https://en.wikipedia.org/wiki/Netstring\n    Even more info: http://cr.yp.to/proto/netstrings.txt\n    \"\"\"\n    def __init__(self, sock, timeout=DEFAULT_TIMEOUT, maxsize=DEFAULT_MAXSIZE):\n        self.bsock = BufferedSocket(sock)\n        self.timeout = timeout\n        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n\n    def fileno(self):\n        return self.bsock.fileno()\n\n    def settimeout(self, timeout):\n        self.timeout = timeout\n\n###The function: setmaxsize###\n    def _calc_msgsize_maxsize(self, maxsize):\n        return len(str(maxsize)) + 1  # len(str()) == log10\n\n    def read_ns(self, timeout=_UNSET, maxsize=_UNSET):\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n            msgsize_maxsize = self._msgsize_maxsize\n        else:\n            msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n\n        size_prefix = self.bsock.recv_until(b':',\n                                            timeout=timeout,\n                                            maxsize=msgsize_maxsize)\n        try:\n            size = int(size_prefix)\n        except ValueError:\n            raise NetstringInvalidSize('netstring message size must be valid'\n                                       ' integer, not %r' % size_prefix)\n\n        if size > maxsize:\n            raise NetstringMessageTooLong(size, maxsize)\n        payload = self.bsock.recv_size(size)\n        if self.bsock.recv(1) != b',':\n            raise NetstringProtocolError(\"expected trailing ',' after message\")\n\n        return payload\n\n    def write_ns(self, payload):\n        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        data = str(size).encode('ascii') + b':' + payload + b','\n        self.bsock.send(data)\n", "prompt": "Please write a python function called 'setmaxsize' base the context. Set the maximum size for receiving netstrings in the NetstringSocket instance. It updates the maxsize of the instance and calculates the maximum size for a netstring message based on the new maxsize value.:param self: NetstringSocket. An instance of the NetstringSocket class.\n:param maxsize: The maximum size for receiving netstrings.\n:return: No return values..\n        The context you need to refer to is as follows: class NetstringSocket(object):\n    \"\"\"\n    Reads and writes using the netstring protocol.\n\n    More info: https://en.wikipedia.org/wiki/Netstring\n    Even more info: http://cr.yp.to/proto/netstrings.txt\n    \"\"\"\n    def __init__(self, sock, timeout=DEFAULT_TIMEOUT, maxsize=DEFAULT_MAXSIZE):\n        self.bsock = BufferedSocket(sock)\n        self.timeout = timeout\n        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n\n    def fileno(self):\n        return self.bsock.fileno()\n\n    def settimeout(self, timeout):\n        self.timeout = timeout\n\n###The function: setmaxsize###\n    def _calc_msgsize_maxsize(self, maxsize):\n        return len(str(maxsize)) + 1  # len(str()) == log10\n\n    def read_ns(self, timeout=_UNSET, maxsize=_UNSET):\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n            msgsize_maxsize = self._msgsize_maxsize\n        else:\n            msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n\n        size_prefix = self.bsock.recv_until(b':',\n                                            timeout=timeout,\n                                            maxsize=msgsize_maxsize)\n        try:\n            size = int(size_prefix)\n        except ValueError:\n            raise NetstringInvalidSize('netstring message size must be valid'\n                                       ' integer, not %r' % size_prefix)\n\n        if size > maxsize:\n            raise NetstringMessageTooLong(size, maxsize)\n        payload = self.bsock.recv_size(size)\n        if self.bsock.recv(1) != b',':\n            raise NetstringProtocolError(\"expected trailing ',' after message\")\n\n        return payload\n\n    def write_ns(self, payload):\n        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        data = str(size).encode('ascii') + b':' + payload + b','\n        self.bsock.send(data)\n", "test_list": ["def test_socketutils_netstring():\n    \"\"\"A holistic feature test of BufferedSocket via the NetstringSocket\n    wrapper. Runs\n    \"\"\"\n    print('running self tests')\n    server_socket = socket.socket()\n    server_socket.bind(('127.0.0.1', 0))\n    server_socket.listen(100)\n    ip, port = server_socket.getsockname()\n    start_server = lambda: netstring_server(server_socket)\n    threading.Thread(target=start_server).start()\n\n    def client_connect():\n        clientsock = socket.create_connection((ip, port))\n        client = NetstringSocket(clientsock)\n        return client\n    client = client_connect()\n    client.write_ns(b'ping')\n    assert client.read_ns() == b'pong'\n    s = time.time()\n    for i in range(1000):\n        client.write_ns(b'ping')\n        assert client.read_ns() == b'pong'\n    dur = time.time() - s\n    print('netstring ping-pong latency', dur, 'ms')\n    s = time.time()\n    for i in range(1000):\n        client.write_ns(b'ping')\n    resps = []\n    for i in range(1000):\n        resps.append(client.read_ns())\n    e = time.time()\n    assert all([r == b'pong' for r in resps])\n    assert client.bsock.getrecvbuffer() == b''\n    dur = e - s\n    print('netstring pipelined ping-pong latency', dur, 'ms')\n    client.write_ns(b'close')\n    try:\n        client.read_ns()\n        raise Exception('read from closed socket')\n    except ConnectionClosed:\n        print('raised ConnectionClosed correctly')\n    client = client_connect()\n    client.setmaxsize(128 * 1024)\n    client.write_ns(b'reply128k')\n    res = client.read_ns()\n    assert len(res) == 128 * 1024\n    client.write_ns(b'close')\n    client = client_connect()\n    client.settimeout(0.1)\n    try:\n        client.read_ns()\n        raise Exception('did not timeout')\n    except Timeout:\n        print('read_ns raised timeout correctly')\n    client.write_ns(b'close')\n    client = client_connect()\n    client.setmaxsize(2048)\n    client.write_ns(b'reply4k')\n    try:\n        client.read_ns()\n        raise Exception('read more than maxsize')\n    except NetstringMessageTooLong:\n        print('raised MessageTooLong correctly')\n    try:\n        client.bsock.recv_until(b'b', maxsize=4096)\n        raise Exception('recv_until did not raise MessageTooLong')\n    except MessageTooLong:\n        print('raised MessageTooLong correctly')\n    assert client.bsock.recv_size(4097) == b'a' * 4096 + b','\n    print('correctly maintained buffer after exception raised')\n    client.bsock.settimeout(0.01)\n    try:\n        client.bsock.recv_until(b'a')\n        raise Exception('recv_until did not raise Timeout')\n    except Timeout:\n        print('recv_until correctly raised Timeout')\n    try:\n        client.bsock.recv_size(1)\n        raise Exception('recv_size did not raise Timeout')\n    except Timeout:\n        print('recv_size correctly raised Timeout')\n    client.write_ns(b'shutdown')\n    print('all passed')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'setmaxsize' function should accept an integer 'maxsize' parameter and update the instance's 'maxsize' attribute accordingly. It should also update '_msgsize_maxsize' using '_calc_msgsize_maxsize'.", "unit_test": ["def test_setmaxsize_updates_attributes():\n    client = client_connect()\n    new_maxsize = 1024\n    client.setmaxsize(new_maxsize)\n    assert client.maxsize == new_maxsize\n    assert client._msgsize_maxsize == len(str(new_maxsize)) + 1\n    client.write_ns(b'close')"], "test": "tests/test_socketutils.py::test_setmaxsize_updates_attributes"}, "Exception Handling": {"requirement": "The 'setmaxsize' function should raise a ValueError if the 'maxsize' parameter is not a positive integer or zero.", "unit_test": ["def test_setmaxsize_raises_valueerror_on_invalid_maxsize():\n    client = client_connect()\n    try:\n        client.setmaxsize(-1)\n        raise Exception('setmaxsize did not raise ValueError')\n    except ValueError:\n        print('setmaxsize correctly raised ValueError for negative maxsize')\n    client.write_ns(b'close')"], "test": "tests/test_socketutils.py::test_setmaxsize_raises_valueerror_on_invalid_maxsize"}, "Edge Case Handling": {"requirement": "The setmaxsize method should correctly handle setting the maximum size to zero and ensure that any non-empty netstring payloads cause a NetstringMessageTooLong exception.", "unit_test": ["def test_setmaxsize_zero_behavior():\n    import socket\n    from yourmodule import NetstringSocket, NetstringMessageTooLong\n\n    server_sock, client_sock = socket.socketpair()\n    ns = NetstringSocket(client_sock)\n    ns.setmaxsize(0)\n\n    try:\n        ns.write_ns(b'x')\n        raise AssertionError(\"Expected NetstringMessageTooLong was not raised\")\n    except NetstringMessageTooLong as e:\n        assert e.args[1] == 0, f\"Expected maxsize=0 in exception, got {e.args[1]}\""], "test": "tests/test_socketutils.py::test_setmaxsize_zero_behavior"}, "Functionality Extension": {"requirement": "Extend the 'setmaxsize' function to print a message: 'Maxsize set to {new_maxsize}' indicating the change in 'maxsize' for debugging purposes.", "unit_test": ["def test_setmaxsize_logs_message():\n    client = client_connect()\n    new_maxsize = 2048\n    with patch('builtins.print') as mocked_print:\n        client.setmaxsize(new_maxsize)\n        mocked_print.assert_called_with(f'Maxsize set to {new_maxsize}')\n    client.write_ns(b'close')"], "test": "tests/test_socketutils.py::test_setmaxsize_logs_message"}, "Annotation Coverage": {"requirement": "Ensure that the 'setmaxsize' function includes type annotations for its parameters and return type, including one parameters: 'maxsize': int, and a return type: None.", "unit_test": ["def test_setmaxsize_annotations():\n    annotations = NetstringSocket.setmaxsize.__annotations__\n    assert annotations['maxsize'] == int\n    assert annotations['return'] == None\n    client.write_ns(b'close')"], "test": "tests/test_socketutils.py::test_setmaxsize_annotations"}, "Code Complexity": {"requirement": "The 'setmaxsize' function should maintain a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": ["def test_setmaxsize_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(NetstringSocket.setmaxsize).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity==1"], "test": "tests/test_socketutils.py::test_setmaxsize_complexity"}, "Code Standard": {"requirement": "The 'setmaxsize' function should adhere to PEP 8 standards, including proper indentation, line length, and spacing.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    import os\n    import inspect\n    code_string = inspect.getsource(NetstringSocket.setmaxsize)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_socketutils.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'setmaxsize' function should utilize the '_calc_msgsize_maxsize' method to update '_msgsize_maxsize'.", "unit_test": ["def test_setmaxsize_uses_calc_msgsize_maxsize():\n    client = client_connect()\n    new_maxsize = 4096\n    with patch.object(NetstringSocket, '_calc_msgsize_maxsize', wraps=client._calc_msgsize_maxsize) as mocked_method:\n        client.setmaxsize(new_maxsize)\n        mocked_method.assert_called_once_with(new_maxsize)\n    client.write_ns(b'close')"], "test": "tests/test_socketutils.py::test_setmaxsize_uses_calc_msgsize_maxsize"}, "Context Usage Correctness Verification": {"requirement": "Verify that the '_msgsize_maxsize' is correctly updated based on the new 'maxsize' using '_calc_msgsize_maxsize'.", "unit_test": ["def test_setmaxsize_correct_msgsize_maxsize_update():\n    client = client_connect()\n    new_maxsize = 5120\n    client.setmaxsize(new_maxsize)\n    expected_msgsize_maxsize = len(str(new_maxsize)) + 1\n    assert client._msgsize_maxsize == expected_msgsize_maxsize\n    client.write_ns(b'close')"], "test": "tests/test_socketutils.py::test_setmaxsize_updates_attributes"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "gunicorn.config.Config.__str__", "type": "method", "project_path": "Utilities/gunicorn", "completion_path": "Utilities/gunicorn/gunicorn/config.py", "signature_position": [54, 54], "body_position": [55, 62], "dependency": {"intra_class": ["gunicorn.config.Config.settings"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a string representation of the Config instance. It iterates through the settings dictionary, format all callable values (\"<{qual_name}()>\"), then formats each key-value pair (\"{key:{key_max_length}} = {value}\"), and appends it to a list. Finally, it joins all the lines in the list with a newline character and returns the resulting string.", "Arguments": ":param self: Config. An instance of the Config class.\n:return: str. The string representation of the Config instance."}, "tests": ["tests/test_config.py::test_str"], "indent": 4, "domain": "Utilities", "code": "    def __str__(self):\n        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n", "context": "class Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n###The function: __str__###\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n    @property\n    def proc_name(self):\n        pn = self.settings['proc_name'].get()\n        if pn is not None:\n            return pn\n        else:\n            return self.settings['default_proc_name'].get()\n\n    @property\n    def logger_class(self):\n        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n\n    @property\n    def is_ssl(self):\n        return self.certfile or self.keyfile\n\n    @property\n    def ssl_options(self):\n        opts = {}\n        for name, value in self.settings.items():\n            if value.section == 'SSL':\n                opts[name] = value.get()\n        return opts\n\n    @property\n    def env(self):\n        raw_env = self.settings['raw_env'].get()\n        env = {}\n\n        if not raw_env:\n            return env\n\n        for e in raw_env:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = s.split('=', 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n\n            env[k] = v\n\n        return env\n\n    @property\n    def sendfile(self):\n        if self.settings['sendfile'].get() is not None:\n            return False\n\n        if 'SENDFILE' in os.environ:\n            sendfile = os.environ['SENDFILE'].lower()\n            return sendfile in ['y', '1', 'yes', 'true']\n\n        return True\n\n    @property\n    def reuse_port(self):\n        return self.settings['reuse_port'].get()\n\n    @property\n    def paste_global_conf(self):\n        raw_global_conf = self.settings['raw_paste_global_conf'].get()\n        if raw_global_conf is None:\n            return None\n\n        global_conf = {}\n        for e in raw_global_conf:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = re.split(r'(?<!\\\\)=', s, 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n            k = k.replace('\\\\=', '=')\n            v = v.replace('\\\\=', '=')\n            global_conf[k] = v\n\n        return global_conf\n", "prompt": "Please write a python function called '__str__' base the context. This function returns a string representation of the Config instance. It iterates through the settings dictionary, format all callable values (\"<{qual_name}()>\"), then formats each key-value pair (\"{key:{key_max_length}} = {value}\"), and appends it to a list. Finally, it joins all the lines in the list with a newline character and returns the resulting string.:param self: Config. An instance of the Config class.\n:return: str. The string representation of the Config instance..\n        The context you need to refer to is as follows: class Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n###The function: __str__###\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n    @property\n    def proc_name(self):\n        pn = self.settings['proc_name'].get()\n        if pn is not None:\n            return pn\n        else:\n            return self.settings['default_proc_name'].get()\n\n    @property\n    def logger_class(self):\n        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n\n    @property\n    def is_ssl(self):\n        return self.certfile or self.keyfile\n\n    @property\n    def ssl_options(self):\n        opts = {}\n        for name, value in self.settings.items():\n            if value.section == 'SSL':\n                opts[name] = value.get()\n        return opts\n\n    @property\n    def env(self):\n        raw_env = self.settings['raw_env'].get()\n        env = {}\n\n        if not raw_env:\n            return env\n\n        for e in raw_env:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = s.split('=', 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n\n            env[k] = v\n\n        return env\n\n    @property\n    def sendfile(self):\n        if self.settings['sendfile'].get() is not None:\n            return False\n\n        if 'SENDFILE' in os.environ:\n            sendfile = os.environ['SENDFILE'].lower()\n            return sendfile in ['y', '1', 'yes', 'true']\n\n        return True\n\n    @property\n    def reuse_port(self):\n        return self.settings['reuse_port'].get()\n\n    @property\n    def paste_global_conf(self):\n        raw_global_conf = self.settings['raw_paste_global_conf'].get()\n        if raw_global_conf is None:\n            return None\n\n        global_conf = {}\n        for e in raw_global_conf:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = re.split(r'(?<!\\\\)=', s, 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n            k = k.replace('\\\\=', '=')\n            v = v.replace('\\\\=', '=')\n            global_conf[k] = v\n\n        return global_conf\n", "test_list": ["def test_str():\n    c = config.Config()\n    o = str(c)\n    OUTPUT_MATCH = {'access_log_format': '%(h)s %(l)s %(u)s %(t)s \"%(r)s\" %(s)s %(b)s \"%(f)s\" \"%(a)s\"', 'accesslog': 'None', 'backlog': '2048', 'bind': \"['127.0.0.1:8000']\", 'capture_output': 'False', 'child_exit': '<ChildExit.child_exit()>'}\n    for i, line in enumerate(o.splitlines()):\n        m = re.match('^(\\\\w+)\\\\s+= ', line)\n        assert m, \"Line {} didn't match expected format: {!r}\".format(i, line)\n        key = m.group(1)\n        try:\n            s = OUTPUT_MATCH.pop(key)\n        except KeyError:\n            continue\n        line_re = '^{}\\\\s+= {}$'.format(key, re.escape(s))\n        assert re.match(line_re, line), '{!r} != {!r}'.format(line_re, line)\n        if not OUTPUT_MATCH:\n            break\n    else:\n        assert False, 'missing expected setting lines? {}'.format(OUTPUT_MATCH.keys())"], "requirements": {"Input-Output Conditions": {"requirement": "The __str__ method should return a string where each line represents a key-value pair from the settings dictionary, formatted as '{key:{key_max_length}} = {value}'. Callable values should be formatted as '<{qual_name}()>'.", "unit_test": ["def test_str_output_format():\n    c = config.Config()\n    output = str(c)\n    for line in output.splitlines():\n        assert re.match(r'^\\w+\\s+=\\s+.+$', line), f'Line format is incorrect: {line}'"], "test": "tests/test_config.py::test_str_output_format"}, "Exception Handling": {"requirement": "The __str__ method should handle cases where the settings dictionary contains non-callable objects that do not have a __str__ method, and should not raise an exception 'Error processing non-callable objects'.", "unit_test": ["def test_str_exception_handling():\n    c = config.Config()\n    c.settings['non_callable'] = object()\n    try:\n        str(c)\n    except Exception as e:\n        assert False, f'__str__ raised an exception: {e}'"], "test": "tests/test_config.py::test_str_exception_handling"}, "Edge Case Handling": {"requirement": "The __str__ method should correctly handle an empty settings dictionary and return an empty string.", "unit_test": ["def test_str_empty_settings():\n    c = config.Config()\n    c.settings = {}\n    output = str(c)\n    assert output == '', f'Expected empty string, got: {output}'"], "test": "tests/test_config.py::test_str_empty_settings"}, "Functionality Extension": {"requirement": "Extend the __str__ method to include a header line 'Config Settings:' at the beginning of the output string.", "unit_test": ["def test_str_with_header():\n    c = config.Config()\n    output = str(c)\n    assert output.startswith('Config Settings:\\n'), 'Output should start with header line'"], "test": "tests/test_config.py::test_str_with_header"}, "Annotation Coverage": {"requirement": "Ensure that the __str__ method is fully documented with a docstring explaining its purpose, parameters, and return value.", "unit_test": ["def test_str_docstring():\n    assert hasattr(config.Config.__str__, '__doc__'), '__str__ method should have a docstring'\n    assert config.Config.__str__.__doc__, '__str__ method docstring should not be empty'"], "test": "tests/test_config.py::test_str_docstring"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity less than 4, indicating a simple, linear function.", "unit_test": ["def test_code_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(config.Config).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 4"], "test": "tests/test_config.py::test_code_complexity"}, "Code Standard": {"requirement": "The __str__ method should adhere to PEP 8 standards, including proper indentation, line length, and spacing.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    import os\n    import inspect\n    code_string = inspect.getsource(config.Config)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_config.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The __str__ method should utilize the settings attribute from the Config class context.", "unit_test": ["def test_str_context_usage():\n    c = config.Config()\n    assert 'settings' in c.__dict__, 'Config instance should have a settings attribute'\n    output = str(c)\n    assert 'settings' in output, 'Output should reflect usage of settings attribute'"], "test": "tests/test_config.py::test_str_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The __str__ method should correctly access and format values from the settings dictionary, ensuring that callable values are formatted as '<{qual_name}()>' and non-callable values are represented as strings.", "unit_test": ["def test_str_context_correctness():\n    c = config.Config()\n    c.settings['callable'] = lambda: None\n    output = str(c)\n    assert '<lambda()>' in output, 'Callable values should be formatted correctly'\n    c.settings['non_callable'] = 'test_value'\n    output = str(c)\n    assert 'non_callable = test_value' in output, 'Non-callable values should be formatted correctly'"], "test": "tests/test_config.py::test_str_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pyramid.registry.Introspector.remove", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/registry.py", "signature_position": [163, 163], "body_position": [164, 173], "dependency": {"intra_class": ["pyramid.registry.Introspector._categories", "pyramid.registry.Introspector._refs", "pyramid.registry.Introspector.get"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Remove an introspection object from the Introspector instance. It first retrieves the introspection object based on the category name and discriminator. If the object is found, it removes all references to the object and deletes it from the category dictionary.", "Arguments": ":param self: Introspector. An instance of the Introspector class.\n:param category_name: str. The name of the category where the introspection object belongs.\n:param discriminator: The discriminator of the introspection object.\n:return: No return values."}, "tests": ["tests/test_registry.py::TestIntrospector::test_remove"], "indent": 4, "domain": "Internet", "code": "    def remove(self, category_name, discriminator):\n        intr = self.get(category_name, discriminator)\n        if intr is None:\n            return\n        L = self._refs.pop(intr, [])\n        for d in L:\n            L2 = self._refs[d]\n            L2.remove(intr)\n        category = self._categories[intr.category_name]\n        del category[intr.discriminator]\n        del category[intr.discriminator_hash]\n", "context": "class Introspector:\n    def __init__(self):\n        self._refs = {}\n        self._categories = {}\n        self._counter = 0\n\n    def add(self, intr):\n        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1\n\n    def get(self, category_name, discriminator, default=None):\n        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator, default)\n        return intr\n\n    def get_category(self, category_name, default=None, sort_key=None):\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        values = category.values()\n        values = sorted(set(values), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self.related(intr)}\n            for intr in values\n        ]\n\n    def categorized(self, sort_key=None):\n        L = []\n        for category_name in self.categories():\n            L.append(\n                (\n                    category_name,\n                    self.get_category(category_name, sort_key=sort_key),\n                )\n            )\n        return L\n\n    def categories(self):\n        return sorted(self._categories.keys())\n\n###The function: remove###\n    def _get_intrs_by_pairs(self, pairs):\n        introspectables = []\n        for pair in pairs:\n            category_name, discriminator = pair\n            intr = self._categories.get(category_name, {}).get(discriminator)\n            if intr is None:\n                raise KeyError((category_name, discriminator))\n            introspectables.append(intr)\n        return introspectables\n\n    def relate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if x is not y and y not in L:\n                L.append(y)\n\n    def unrelate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.get(x, [])\n            if y in L:\n                L.remove(y)\n\n    def related(self, intr):\n        category_name, discriminator = intr.category_name, intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n", "prompt": "Please write a python function called 'remove' base the context. Remove an introspection object from the Introspector instance. It first retrieves the introspection object based on the category name and discriminator. If the object is found, it removes all references to the object and deletes it from the category dictionary.:param self: Introspector. An instance of the Introspector class.\n:param category_name: str. The name of the category where the introspection object belongs.\n:param discriminator: The discriminator of the introspection object.\n:return: No return values..\n        The context you need to refer to is as follows: class Introspector:\n    def __init__(self):\n        self._refs = {}\n        self._categories = {}\n        self._counter = 0\n\n    def add(self, intr):\n        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1\n\n    def get(self, category_name, discriminator, default=None):\n        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator, default)\n        return intr\n\n    def get_category(self, category_name, default=None, sort_key=None):\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        values = category.values()\n        values = sorted(set(values), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self.related(intr)}\n            for intr in values\n        ]\n\n    def categorized(self, sort_key=None):\n        L = []\n        for category_name in self.categories():\n            L.append(\n                (\n                    category_name,\n                    self.get_category(category_name, sort_key=sort_key),\n                )\n            )\n        return L\n\n    def categories(self):\n        return sorted(self._categories.keys())\n\n###The function: remove###\n    def _get_intrs_by_pairs(self, pairs):\n        introspectables = []\n        for pair in pairs:\n            category_name, discriminator = pair\n            intr = self._categories.get(category_name, {}).get(discriminator)\n            if intr is None:\n                raise KeyError((category_name, discriminator))\n            introspectables.append(intr)\n        return introspectables\n\n    def relate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if x is not y and y not in L:\n                L.append(y)\n\n    def unrelate(self, *pairs):\n        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.get(x, [])\n            if y in L:\n                L.remove(y)\n\n    def related(self, intr):\n        category_name, discriminator = intr.category_name, intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n", "test_list": ["def test_remove(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    intr2 = DummyIntrospectable()\n    intr2.category_name = 'category2'\n    intr2.discriminator = 'discriminator2'\n    intr2.discriminator_hash = 'discriminator2_hash'\n    inst.add(intr)\n    inst.add(intr2)\n    inst.relate(('category', 'discriminator'), ('category2', 'discriminator2'))\n    inst.remove('category', 'discriminator')\n    self.assertEqual(inst._categories, {'category': {}, 'category2': {'discriminator2': intr2, 'discriminator2_hash': intr2}})\n    self.assertEqual(inst._refs.get(intr), None)\n    self.assertEqual(inst._refs[intr2], [])"], "requirements": {"Input-Output Conditions": {"requirement": "The 'remove' function should ensure that the inputs 'category_name' and 'discriminator' are of type 'str'. If not, it should raise a TypeError.", "unit_test": ["def test_remove_input_type_check(self):\n    inst = self._makeOne()\n    with self.assertRaises(TypeError):\n        inst.remove(123, 'discriminator')\n    with self.assertRaises(TypeError):\n        inst.remove('category', 456)"], "test": "tests/test_registry.py::TestIntrospector::test_remove_input_type_check"}, "Exception Handling": {"requirement": "The 'remove' function should raise a KeyError with a descriptive message if the introspection object is not found in the specified category.", "unit_test": ["def test_remove_key_error(self):\n    inst = self._makeOne()\n    with self.assertRaises(KeyError) as cm:\n        inst.remove('nonexistent_category', 'nonexistent_discriminator')\n    self.assertEqual(str(cm.exception), \"('nonexistent_category', 'nonexistent_discriminator')\")"], "test": "tests/test_registry.py::TestIntrospector::test_remove_key_error"}, "Edge Case Handling": {"requirement": "The 'remove' function should handle the case where the category exists but the discriminator does not, by raising a KeyError.", "unit_test": ["def test_remove_nonexistent_discriminator(self):\n    inst = self._makeOne()\n    inst.add(DummyIntrospectable())\n    with self.assertRaises(KeyError):\n        inst.remove('category', 'nonexistent_discriminator')"], "test": "tests/test_registry.py::TestIntrospector::test_remove_nonexistent_discriminator"}, "Functionality Extension": {"requirement": "Extend the 'remove' function to return a boolean indicating whether an introspection object was successfully removed.", "unit_test": ["def test_remove_return_value(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    inst.add(intr)\n    result = inst.remove('category', 'discriminator')\n    self.assertTrue(result)\n    result = inst.remove('category', 'discriminator')\n    self.assertFalse(result)"], "test": "tests/test_registry.py::TestIntrospector::test_remove_return_value"}, "Annotation Coverage": {"requirement": "Ensure that the 'remove' function has complete docstring coverage, including parameter types and a description of the function's behavior, including two parameters: 'category_name': str, 'discriminator': str, and a return type: bool.", "unit_test": ["def test_remove_docstring(self):\n    inst = self._makeOne()\n    self.assertTrue(inst.remove.__doc__ is not None)\n    self.assertIn(':param category_name: str', inst.remove.__doc__)\n    self.assertIn(':param discriminator: str', inst.remove.__doc__)\n    self.assertIn(':return: bool', inst.remove.__doc__)"], "test": "tests/test_registry.py::TestIntrospector::test_remove_docstring"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity less than 3, indicating a simple, linear function.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(inst.remove).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 3"], "test": "tests/test_registry.py::TestIntrospector::test_code_complexity"}, "Code Standard": {"requirement": "The 'remove' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(Introspector.remove)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_registry.py::TestIntrospector::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'remove' function should utilize the '_categories' and '_refs' attributes of the Introspector class.", "unit_test": ["def test_remove_context_usage(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    inst.add(intr)\n    inst.remove('category', 'discriminator')\n    self.assertNotIn('discriminator', inst._categories.get('category', {}))\n    self.assertNotIn(intr, inst._refs)"], "test": "tests/test_registry.py::TestIntrospector::test_remove_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'remove' function should correctly update the '_categories' and '_refs' attributes by removing the introspection object and its references.", "unit_test": ["def test_remove_context_correctness(self):\n    inst = self._makeOne()\n    intr = DummyIntrospectable()\n    inst.add(intr)\n    inst.relate(('category', 'discriminator'), ('category2', 'discriminator2'))\n    inst.remove('category', 'discriminator')\n    self.assertNotIn('discriminator', inst._categories.get('category', {}))\n    self.assertEqual(inst._refs.get(intr), None)"], "test": "tests/test_registry.py::TestIntrospector::test_remove_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "mrjob.job.MRJob.set_status", "type": "method", "project_path": "System/mrjob", "completion_path": "System/mrjob/mrjob/job.py", "signature_position": [587, 587], "body_position": [594, 599], "dependency": {"intra_class": ["mrjob.job.MRJob.stderr"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function sets the job status in Hadoop streaming by printing a message to the standard error stream of the input MRJob instance. It is also used as a keepalive mechanism to prevent the job from timing out. The format of the message is \"reporter:status:{message}\\n\".", "Arguments": ":param self: MRJob. An instance of the MRJob class.\n:param msg: String. The message to set as the job status.\n:return: No return values."}, "tests": ["tests/test_job.py::CountersAndStatusTestCase::test_counters_and_status"], "indent": 4, "domain": "System", "code": "    def set_status(self, msg):\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\n\n        This is also a good way of doing a keepalive for a job that goes a\n        long time between outputs; Hadoop streaming usually times out jobs\n        that give no output for longer than 10 minutes.\n        \"\"\"\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "context": "class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n###The function: set_status###\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n    def add_passthru_arg(self, *args, **kwargs):\n        \"\"\"Function to create options which both the job runner\n        and the job itself respect (we use this for protocols, for example).\n\n        Use it like you would use\n        :py:func:`argparse.ArgumentParser.add_argument`::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_passthru_arg(\n                    '--max-ngram-size', type=int, default=4, help='...')\n\n        If you want to pass files through to the mapper/reducer, use\n        :py:meth:`add_file_arg` instead.\n\n        If you want to pass through a built-in option (e.g. ``--runner``, use\n        :py:meth:`pass_arg_through` instead.\n        \"\"\"\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "prompt": "Please write a python function called 'set_status' base the context. This function sets the job status in Hadoop streaming by printing a message to the standard error stream of the input MRJob instance. It is also used as a keepalive mechanism to prevent the job from timing out. The format of the message is \"reporter:status:{message}\\n\".:param self: MRJob. An instance of the MRJob class.\n:param msg: String. The message to set as the job status.\n:return: No return values..\n        The context you need to refer to is as follows: class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n###The function: set_status###\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n    def add_passthru_arg(self, *args, **kwargs):\n        \"\"\"Function to create options which both the job runner\n        and the job itself respect (we use this for protocols, for example).\n\n        Use it like you would use\n        :py:func:`argparse.ArgumentParser.add_argument`::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_passthru_arg(\n                    '--max-ngram-size', type=int, default=4, help='...')\n\n        If you want to pass files through to the mapper/reducer, use\n        :py:meth:`add_file_arg` instead.\n\n        If you want to pass through a built-in option (e.g. ``--runner``, use\n        :py:meth:`pass_arg_through` instead.\n        \"\"\"\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "test_list": ["def test_counters_and_status(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.increment_counter('Foo', 'Bar')\n    mr_job.set_status('Initializing qux gradients...')\n    mr_job.increment_counter('Foo', 'Bar')\n    mr_job.increment_counter('Foo', 'Baz', 20)\n    mr_job.set_status('Sorting metasyntactic variables...')\n    parsed_stderr = parse_mr_job_stderr(mr_job.stderr.getvalue())\n    self.assertEqual(parsed_stderr, {'counters': {'Foo': {'Bar': 2, 'Baz': 20}}, 'statuses': ['Initializing qux gradients...', 'Sorting metasyntactic variables...'], 'other': []})"], "requirements": {"Input-Output Conditions": {"requirement": "The 'set_status' function should accept only string inputs for the 'msg' parameter and should print the status message in the format 'reporter:status:{message}\\n' to the standard error stream.", "unit_test": ["def test_set_status_input_output_conditions(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:Test status message\\n', stderr_output)"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_input_output_conditions"}, "Exception Handling": {"requirement": "The 'set_status' function should raise a TypeError if the 'msg' parameter is not a string.", "unit_test": ["def test_set_status_exception_handling(self):\n    mr_job = MRJob([]).sandbox()\n    with self.assertRaises(TypeError):\n        mr_job.set_status(123)"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_exception_handling"}, "Edge Case Handling": {"requirement": "The 'set_status' function should handle empty string inputs gracefully by printing 'reporter:status:\\n' to the standard error stream.", "unit_test": ["def test_set_status_edge_case_handling(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:\\n', stderr_output)"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'set_status' function to accept an optional 'timestamp' parameter that, when provided, appends the current timestamp to the status message.", "unit_test": ["def test_set_status_functionality_extension(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message', timestamp=True)\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertRegex(stderr_output, r'reporter:status:Test status message \\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\]\\n')"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'set_status' function is fully documented with parameter and return type annotations, including one parameters: 'msg': str, and a return type: None.", "unit_test": ["def test_set_status_annotation_coverage(self):\n    annotations = MRJob.set_status.__annotations__\n    self.assertEqual(annotations['msg'], str)\n    self.assertEqual(annotations['return'], None)"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_annotation_coverage"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity less than 2, indicating a simple, linear function.", "unit_test": ["def test_code_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(MRJob.set_status).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 2"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_code_complexity"}, "Code Standard": {"requirement": "The 'set_status' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(MRJob.set_status)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'set_status' function should utilize the 'stderr' property of the MRJob instance to print the status message.", "unit_test": ["def test_set_status_context_usage_verification(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:Test status message\\n', stderr_output)"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'set_status' function correctly uses the 'stderr' property to ensure messages are printed to the correct stream.", "unit_test": ["def test_set_status_context_usage_correctness_verification(self):\n    mr_job = MRJob([]).sandbox()\n    mr_job.set_status('Test status message')\n    stderr_output = mr_job.stderr.getvalue()\n    self.assertIn('reporter:status:Test status message\\n', stderr_output)"], "test": "tests/test_job.py::CountersAndStatusTestCase::test_set_status_context_usage_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/registry.py", "signature_position": [232, 232], "body_position": [233, 234], "dependency": {"intra_class": ["pyramid.registry.Introspectable._assert_resolved", "pyramid.registry.Introspectable.discriminator"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "Calculate the hash of the discriminator of the Introspectable instance.", "Arguments": ":param self: Introspectable. An instance of the Introspectable class.\n:return: int. The hash value of the discriminator in the instance."}, "tests": ["tests/test_registry.py::TestIntrospectable::test_discriminator_hash"], "indent": 4, "domain": "Internet", "code": "    def discriminator_hash(self):\n        self._assert_resolved()\n        return hash(self.discriminator)\n", "context": "class Introspectable(dict):\n\n    order = 0  # mutated by introspector.add\n    action_info = None  # mutated by self.register\n\n    def __init__(self, category_name, discriminator, title, type_name):\n        self.category_name = category_name\n        self.discriminator = discriminator\n        self.title = title\n        self.type_name = type_name\n        self._relations = []\n\n    def relate(self, category_name, discriminator):\n        self._relations.append((True, category_name, discriminator))\n\n    def unrelate(self, category_name, discriminator):\n        self._relations.append((False, category_name, discriminator))\n\n    def _assert_resolved(self):\n        assert undefer(self.discriminator) is self.discriminator\n\n    @property\n###The function: discriminator_hash###\n    def __hash__(self):\n        self._assert_resolved()\n        return hash((self.category_name,) + (self.discriminator,))\n\n    def __repr__(self):\n        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )\n\n    def __bool__(self):\n        return True\n\n    def register(self, introspector, action_info):\n        self.discriminator = undefer(self.discriminator)\n        self.action_info = action_info\n        introspector.add(self)\n        for relate, category_name, discriminator in self._relations:\n            discriminator = undefer(discriminator)\n            if relate:\n                method = introspector.relate\n            else:\n                method = introspector.unrelate\n            method(\n                (self.category_name, self.discriminator),\n                (category_name, discriminator),\n            )\n", "prompt": "Please write a python function called 'discriminator_hash' base the context. Calculate the hash of the discriminator of the Introspectable instance.:param self: Introspectable. An instance of the Introspectable class.\n:return: int. The hash value of the discriminator in the instance..\n        The context you need to refer to is as follows: class Introspectable(dict):\n\n    order = 0  # mutated by introspector.add\n    action_info = None  # mutated by self.register\n\n    def __init__(self, category_name, discriminator, title, type_name):\n        self.category_name = category_name\n        self.discriminator = discriminator\n        self.title = title\n        self.type_name = type_name\n        self._relations = []\n\n    def relate(self, category_name, discriminator):\n        self._relations.append((True, category_name, discriminator))\n\n    def unrelate(self, category_name, discriminator):\n        self._relations.append((False, category_name, discriminator))\n\n    def _assert_resolved(self):\n        assert undefer(self.discriminator) is self.discriminator\n\n    @property\n###The function: discriminator_hash###\n    def __hash__(self):\n        self._assert_resolved()\n        return hash((self.category_name,) + (self.discriminator,))\n\n    def __repr__(self):\n        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )\n\n    def __bool__(self):\n        return True\n\n    def register(self, introspector, action_info):\n        self.discriminator = undefer(self.discriminator)\n        self.action_info = action_info\n        introspector.add(self)\n        for relate, category_name, discriminator in self._relations:\n            discriminator = undefer(discriminator)\n            if relate:\n                method = introspector.relate\n            else:\n                method = introspector.unrelate\n            method(\n                (self.category_name, self.discriminator),\n                (category_name, discriminator),\n            )\n", "test_list": ["def test_discriminator_hash(self):\n    inst = self._makeOnePopulated()\n    self.assertEqual(inst.discriminator_hash, hash(inst.discriminator))"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'discriminator_hash' should return an integer hash value for the discriminator attribute of the Introspectable instance.", "unit_test": ["def test_discriminator_hash_output_type(self):\n    inst = self._makeOnePopulated()\n    self.assertIsInstance(inst.discriminator_hash, int)"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash"}, "Exception Handling": {"requirement": "The function 'discriminator_hash' should raise a TypeError if the discriminator is not hashable.", "unit_test": ["def test_discriminator_hash_non_hashable(self):\n    inst = self._makeOnePopulated()\n    inst.discriminator = ['non-hashable']\n    with self.assertRaises(TypeError):\n        _ = inst.discriminator_hash"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_non_hashable"}, "Edge Case Handling": {"requirement": "The function 'discriminator_hash' should correctly handle the case where the discriminator is an empty string.", "unit_test": ["def test_discriminator_hash_empty_discriminator(self):\n    inst = self._makeOnePopulated()\n    inst.discriminator = ''\n    self.assertEqual(inst.discriminator_hash, hash(''))"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_empty_discriminator"}, "Functionality Extension": {"requirement": "Extend the 'discriminator_hash' function to include the category_name in the hash calculation.", "unit_test": ["def test_discriminator_hash_with_category(self):\n    inst = self._makeOnePopulated()\n    expected_hash = hash((inst.category_name, inst.discriminator))\n    self.assertEqual(inst.discriminator_hash, expected_hash)"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_with_category"}, "Annotation Coverage": {"requirement": "Ensure that the 'discriminator_hash' function is properly documented with a docstring explaining its purpose : 'Calculate the hash'.", "unit_test": ["def test_discriminator_hash_docstring(self):\n    self.assertIsNotNone(discriminator_hash.__doc__)\n    self.assertIn('Calculate the hash', discriminator_hash.__doc__)"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_docstring"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    from pyramid.interfaces import Introspectable\n    source_code = inspect.getsource(Introspectable.discriminator_hash).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity == 1"], "test": "tests/test_registry.py::TestIntrospectable::test_code_complexity"}, "Code Standard": {"requirement": "The function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspectable\n    code_string = inspect.getsource(Introspectable.discriminator_hash)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_registry.py::TestIntrospectable::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'discriminator_hash' function should utilize the 'discriminator' attribute from the Introspectable class.", "unit_test": ["def test_discriminator_hash_uses_discriminator(self):\n    inst = self._makeOnePopulated()\n    self.assertIn('discriminator', inst.__dict__)\n    _ = inst.discriminator_hash"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_uses_discriminator"}, "Context Usage Correctness Verification": {"requirement": "The 'discriminator_hash' function should correctly use the 'discriminator' attribute to compute the hash value.", "unit_test": ["def test_discriminator_hash_correct_usage(self):\n    inst = self._makeOnePopulated()\n    expected_hash = hash(inst.discriminator)\n    self.assertEqual(inst.discriminator_hash, expected_hash)"], "test": "tests/test_registry.py::TestIntrospectable::test_discriminator_hash_correct_usage"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "type": "method", "project_path": "System/mrjob", "completion_path": "System/mrjob/mrjob/job.py", "signature_position": [1187, 1187], "body_position": [1205, 1207], "dependency": {"intra_class": ["mrjob.job.MRJob._passthru_arg_dests", "mrjob.job.MRJob.arg_parser"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function is used to add a command-line argument that both the job runner and the job itself will respect. It creates options that can be used by the job to configure its behavior. The options are added to the argument parser of the job.", "Arguments": ":param self: MRJob. An instance of the MRJob class.\n:param *args: Variable length argument list. The arguments to be passed to  the argument parser.\n:param **kwargs: Arbitrary keyword arguments. The keyword arguments to be passed to the argument parser.\n:return: No return values."}, "tests": ["tests/test_job.py::CommandLineArgsTestCase::test_bad_option_types"], "indent": 4, "domain": "System", "code": "    def add_passthru_arg(self, *args, **kwargs):\n        \"\"\"Function to create options which both the job runner\n        and the job itself respect (we use this for protocols, for example).\n\n        Use it like you would use\n        :py:func:`argparse.ArgumentParser.add_argument`::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_passthru_arg(\n                    '--max-ngram-size', type=int, default=4, help='...')\n\n        If you want to pass files through to the mapper/reducer, use\n        :py:meth:`add_file_arg` instead.\n\n        If you want to pass through a built-in option (e.g. ``--runner``, use\n        :py:meth:`pass_arg_through` instead.\n        \"\"\"\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "context": "class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    def set_status(self, msg):\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\n\n        This is also a good way of doing a keepalive for a job that goes a\n        long time between outputs; Hadoop streaming usually times out jobs\n        that give no output for longer than 10 minutes.\n        \"\"\"\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n###The function: add_passthru_arg###\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "prompt": "Please write a python function called 'add_passthru_arg' base the context. This function is used to add a command-line argument that both the job runner and the job itself will respect. It creates options that can be used by the job to configure its behavior. The options are added to the argument parser of the job.:param self: MRJob. An instance of the MRJob class.\n:param *args: Variable length argument list. The arguments to be passed to  the argument parser.\n:param **kwargs: Arbitrary keyword arguments. The keyword arguments to be passed to the argument parser.\n:return: No return values..\n        The context you need to refer to is as follows: class MRJob(object):\n    \"\"\"The base class for all MapReduce jobs. See :py:meth:`__init__`\n    for details.\"\"\"\n\n    def __init__(self, args=None):\n        \"\"\"Entry point for running your job from other Python code.\n\n        You can pass in command-line arguments, and the job will act the same\n        way it would if it were run from the command line. For example, to\n        run your job on EMR::\n\n            mr_job = MRYourJob(args=['-r', 'emr'])\n            with mr_job.make_runner() as runner:\n                ...\n\n        Passing in ``None`` is the same as passing in ``sys.argv[1:]``\n\n        For a full list of command-line arguments, run:\n        ``python -m mrjob.job --help``\n\n        :param args: Arguments to your script (switches and input files)\n\n        .. versionchanged:: 0.7.0\n\n           Previously, *args* set to ``None`` was equivalent to ``[]``.\n        \"\"\"\n        # make sure we respect the $TZ (time zone) environment variable\n        if hasattr(time, 'tzset'):\n            time.tzset()\n\n        # argument dests for args to pass through\n        self._passthru_arg_dests = set()\n        self._file_arg_dests = set()\n\n        self.arg_parser = ArgumentParser(usage=self._usage(),\n                                         add_help=False)\n        self.configure_args()\n\n        if args is None:\n            self._cl_args = sys.argv[1:]\n        else:\n            # don't pass sys.argv to self.arg_parser, and have it\n            # raise an exception on error rather than printing to stderr\n            # and exiting.\n            self._cl_args = args\n\n            def error(msg):\n                raise ValueError(msg)\n\n            self.arg_parser.error = error\n\n        self.load_args(self._cl_args)\n\n        # Make it possible to redirect stdin, stdout, and stderr, for testing\n        # See stdin, stdout, stderr properties and sandbox(), below.\n        self._stdin = None\n        self._stdout = None\n        self._stderr = None\n\n    # by default, self.stdin, self.stdout, and self.stderr are sys.std*.buffer\n    # if it exists, and otherwise sys.std* otherwise (they should always deal\n    # with bytes, not Unicode).\n    #\n    # *buffer* is pretty much a Python 3 thing, though some platforms\n    # (notably Jupyterhub) don't have it. See #1441\n\n    @property\n    def stdin(self):\n        return self._stdin or getattr(sys.stdin, 'buffer', sys.stdin)\n\n    @property\n    def stdout(self):\n        return self._stdout or getattr(sys.stdout, 'buffer', sys.stdout)\n\n    @property\n    def stderr(self):\n        return self._stderr or getattr(sys.stderr, 'buffer', sys.stderr)\n\n    def _usage(self):\n        return \"%(prog)s [options] [input files]\"\n\n    def _print_help(self, options):\n        \"\"\"Print help for this job. This will either print runner\n        or basic help. Override to allow other kinds of help.\"\"\"\n        if options.runner:\n            _print_help_for_runner(\n                self._runner_opt_names_for_help(), options.deprecated)\n        else:\n            _print_basic_help(self.arg_parser,\n                              self._usage(),\n                              options.deprecated,\n                              options.verbose)\n\n    def _runner_opt_names_for_help(self):\n        opts = set(self._runner_class().OPT_NAMES)\n\n        if self.options.runner == 'spark':\n            # specific to Spark runner, but command-line only, so it doesn't\n            # appear in SparkMRJobRunner.OPT_NAMES (see #2040)\n            opts.add('max_output_files')\n\n        return opts\n\n    def _non_option_kwargs(self):\n        \"\"\"Keyword arguments to runner constructor that can't be set\n        in mrjob.conf.\n\n        These should match the (named) arguments to\n        :py:meth:`~mrjob.runner.MRJobRunner.__init__`.\n        \"\"\"\n        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n\n    def _kwargs_from_switches(self, keys):\n        return dict(\n            (key, getattr(self.options, key))\n            for key in keys if hasattr(self.options, key)\n        )\n\n    def _job_kwargs(self):\n        \"\"\"Keyword arguments to the runner class that can be specified\n        by the job/launcher itself.\"\"\"\n        # use the most basic combiners; leave magic like resolving paths\n        # and blanking out jobconf values to the runner\n        return dict(\n            # command-line has the final say on jobconf and libjars\n            jobconf=combine_dicts(\n                self.jobconf(), self.options.jobconf),\n            libjars=combine_lists(\n                self.libjars(), self.options.libjars),\n            partitioner=self.partitioner(),\n            sort_values=self.sort_values(),\n            # TODO: should probably put self.options last below for consistency\n            upload_archives=combine_lists(\n                self.options.upload_archives, self.archives()),\n            upload_dirs=combine_lists(\n                self.options.upload_dirs, self.dirs()),\n            upload_files=combine_lists(\n                self.options.upload_files, self.files()),\n        )\n\n    ### Defining one-step streaming jobs ###\n\n    def mapper(self, key, value):\n        \"\"\"Re-define this to define the mapper for a one-step job.\n\n        Yields zero or more tuples of ``(out_key, out_value)``.\n\n        :param key: A value parsed from input.\n        :param value: A value parsed from input.\n\n        If you don't re-define this, your job will have a mapper that simply\n        yields ``(key, value)`` as-is.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``key`` will be ``None``\n         - ``value`` will be the raw input line, with newline stripped.\n         - ``out_key`` and ``out_value`` must be JSON-encodable: numeric,\n           unicode, boolean, ``None``, list, or dict whose keys are unicodes.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer(self, key, values):\n        \"\"\"Re-define this to define the reducer for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by the\n                      mapper which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner(self, key, values):\n        \"\"\"Re-define this to define the combiner for a one-step job.\n\n        Yields one or more tuples of ``(out_key, out_value)``\n\n        :param key: A key which was yielded by the mapper\n        :param value: A generator which yields all values yielded by one mapper\n                      task/node which correspond to ``key``.\n\n        By default (if you don't mess with :ref:`job-protocols`):\n         - ``out_key`` and ``out_value`` must be JSON-encodable.\n         - ``key`` and ``value`` will have been decoded from JSON (so tuples\n           will become lists).\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_init(self):\n        \"\"\"Re-define this to define an action to run before the mapper\n        processes any input.\n\n        One use for this function is to initialize mapper-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_final(self):\n        \"\"\"Re-define this to define an action to run after the mapper reaches\n        the end of input.\n\n        One way to use this is to store a total in an instance variable, and\n        output it after reading all input data. See :py:mod:`mrjob.examples`\n        for an example.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_cmd(self):\n        \"\"\"Re-define this to define the mapper for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        important specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def mapper_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the mapper's\n        input before it gets to your job's mapper in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def mapper_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def mapper_raw(self, input_path, input_uri):\n        \"\"\"Re-define this to make Hadoop pass one input file to each\n        mapper.\n\n        :param input_path: a local path that the input file has been copied to\n        :param input_uri: the URI of the input file on HDFS, S3, etc\n\n        .. versionadded:: 0.6.3\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_init(self):\n        \"\"\"Re-define this to define an action to run before the reducer\n        processes any input.\n\n        One use for this function is to initialize reducer-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_final(self):\n        \"\"\"Re-define this to define an action to run after the reducer reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_cmd(self):\n        \"\"\"Re-define this to define the reducer for a one-step job **as a shell\n        command.** If you define your mapper this way, the command will be\n        passed unchanged to Hadoop Streaming, with some minor exceptions. For\n        specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def reducer_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def reducer_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the reducer's\n        input before it gets to your job's reducer in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def reducer_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_init(self):\n        \"\"\"Re-define this to define an action to run before the combiner\n        processes any input.\n\n        One use for this function is to initialize combiner-specific helper\n        structures.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_final(self):\n        \"\"\"Re-define this to define an action to run after the combiner reaches\n        the end of input.\n\n        Yields one or more tuples of ``(out_key, out_value)``.\n\n        By default, ``out_key`` and ``out_value`` must be JSON-encodable;\n        re-define :py:attr:`INTERNAL_PROTOCOL` to change this.\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_cmd(self):\n        \"\"\"Re-define this to define the combiner for a one-step job **as a\n        shell command.** If you define your mapper this way, the command will\n        be passed unchanged to Hadoop Streaming, with some minor exceptions.\n        For specifics, see :ref:`cmd-steps`.\n\n        Basic example::\n\n            def combiner_cmd(self):\n                return 'cat'\n        \"\"\"\n        raise NotImplementedError\n\n    def combiner_pre_filter(self):\n        \"\"\"Re-define this to specify a shell command to filter the combiner's\n        input before it gets to your job's combiner in a one-step job. For\n        important specifics, see :ref:`cmd-filters`.\n\n        Basic example::\n\n            def combiner_pre_filter(self):\n                return 'grep \"ponies\"'\n        \"\"\"\n        raise NotImplementedError\n\n    ### Defining one-step Spark jobs ###\n\n    def spark(self, input_path, output_path):\n        \"\"\"Re-define this with Spark code to run. You can read input\n        with *input_path* and output with *output_path*.\n\n        .. warning::\n\n           Prior to v0.6.8, to pass job methods into Spark\n           (``rdd.flatMap(self.some_method)``), you first had to call\n           :py:meth:`self.sandbox() <mrjob.job.MRJob.sandbox>`; otherwise\n           Spark would error because *self* was not serializable.\n        \"\"\"\n        raise NotImplementedError\n\n    def spark_args(self):\n        \"\"\"Redefine this to pass custom arguments to Spark.\"\"\"\n        return []\n\n    ### Defining multi-step jobs ###\n\n    def steps(self):\n        \"\"\"Re-define this to make a multi-step job.\n\n        If you don't re-define this, we'll automatically create a one-step\n        job using any of :py:meth:`mapper`, :py:meth:`mapper_init`,\n        :py:meth:`mapper_final`, :py:meth:`reducer_init`,\n        :py:meth:`reducer_final`, and :py:meth:`reducer` that you've\n        re-defined. For example::\n\n            def steps(self):\n                return [MRStep(mapper=self.transform_input,\n                               reducer=self.consolidate_1),\n                        MRStep(reducer_init=self.log_mapper_init,\n                               reducer=self.consolidate_2)]\n\n        :return: a list of steps constructed with\n                 :py:class:`~mrjob.step.MRStep` or other classes in\n                 :py:mod:`mrjob.step`.\n        \"\"\"\n        # only include methods that have been redefined\n        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []\n\n    def increment_counter(self, group, counter, amount=1):\n        \"\"\"Increment a counter in Hadoop streaming by printing to stderr.\n\n        :type group: str\n        :param group: counter group\n        :type counter: str\n        :param counter: description of the counter\n        :type amount: int\n        :param amount: how much to increment the counter by\n\n        Commas in ``counter`` or ``group`` will be automatically replaced\n        with semicolons (commas confuse Hadoop streaming).\n        \"\"\"\n        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    def set_status(self, msg):\n        \"\"\"Set the job status in hadoop streaming by printing to stderr.\n\n        This is also a good way of doing a keepalive for a job that goes a\n        long time between outputs; Hadoop streaming usually times out jobs\n        that give no output for longer than 10 minutes.\n        \"\"\"\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n\n    ### Running the job ###\n\n    @classmethod\n    def run(cls):\n        \"\"\"Entry point for running job from the command-line.\n\n        This is also the entry point when a mapper or reducer is run\n        by Hadoop Streaming.\n\n        Does one of:\n\n        * Run a mapper (:option:`--mapper`). See :py:meth:`run_mapper`\n        * Run a combiner (:option:`--combiner`). See :py:meth:`run_combiner`\n        * Run a reducer (:option:`--reducer`). See :py:meth:`run_reducer`\n        * Run the entire job. See :py:meth:`run_job`\n        \"\"\"\n        # load options from the command line\n        cls().execute()\n\n    def run_job(self):\n        \"\"\"Run the all steps of the job, logging errors (and debugging output\n        if :option:`--verbose` is specified) to STDERR and streaming the\n        output to STDOUT.\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # self.stderr is strictly binary, need to wrap it so it's possible\n        # to log to it in Python 3\n        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()\n\n    @classmethod\n    def set_up_logging(cls, quiet=False, verbose=False, stream=None):\n        \"\"\"Set up logging when running from the command line. This is also\n        used by the various command-line utilities.\n\n        :param bool quiet: If true, don't log. Overrides *verbose*.\n        :param bool verbose: If true, set log level to ``DEBUG`` (default is\n                             ``INFO``)\n        :param bool stream: Stream to log to (default is ``sys.stderr``)\n        \"\"\"\n        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)\n\n    def _should_cat_output(self):\n        if self.options.cat_output is None:\n            return not self.options.output_dir\n        else:\n            return self.options.cat_output\n\n    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed\n        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()\n\n    def make_runner(self):\n        \"\"\"Make a runner based on command-line arguments, so we can\n        launch this job on EMR, on Hadoop, or locally.\n\n        :rtype: :py:class:`mrjob.runner.MRJobRunner`\n        \"\"\"\n        bad_words = (\n            '--mapper', '--reducer', '--combiner', '--step-num', '--spark')\n        for w in bad_words:\n            if w in sys.argv:\n                raise UsageError(\"make_runner() was called with %s. This\"\n                                 \" probably means you tried to use it from\"\n                                 \" __main__, which doesn't work.\" % w)\n\n        runner_class = self._runner_class()\n        kwargs = self._runner_kwargs()\n\n        # screen out most false-ish args so that it's readable\n        log.debug('making runner: %s(%s, ...)' % (\n            runner_class.__name__,\n            ', '.join('%s=%s' % (k, v)\n                      for k, v in sorted(kwargs.items())\n                      if v not in (None, [], {}))))\n\n        return self._runner_class()(**self._runner_kwargs())\n\n    def _runner_class(self):\n        \"\"\"Runner class as indicated by ``--runner``. Defaults to ``'inline'``.\n        \"\"\"\n        return _runner_class(self.options.runner or 'inline')\n\n    def _runner_kwargs(self):\n        \"\"\"If we're building an inline or Spark runner,\n        include mrjob_cls in kwargs.\"\"\"\n        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs\n\n    def _get_step(self, step_num, expected_type):\n        \"\"\"Helper for run_* methods\"\"\"\n        steps = self.steps()\n        if not 0 <= step_num < len(steps):\n            raise ValueError('Out-of-range step: %d' % step_num)\n        step = steps[step_num]\n        if not isinstance(step, expected_type):\n            raise TypeError('Step %d is not a %s', expected_type.__name__)\n        return step\n\n    def run_mapper(self, step_num=0):\n        \"\"\"Run the mapper and final mapper action for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_combiner(self, step_num=0):\n        \"\"\"Run the combiner for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        If we encounter a line that can't be decoded by our input protocol,\n        or a tuple that can't be encoded by our output protocol, we'll\n        increment a counter rather than raising an exception. If\n        --strict-protocols is set, then an exception is raised\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def run_reducer(self, step_num=0):\n        \"\"\"Run the reducer for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'reducer')\n\n        for k, v in self.reduce_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n\n    def map_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`mapper_init`,\n        :py:meth:`mapper`/:py:meth:`mapper_raw`, and :py:meth:`mapper_final`\n        for one map task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_mapper` essentially wraps this method with code to handle\n        reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        mapper = step['mapper']\n        mapper_raw = step['mapper_raw']\n        mapper_init = step['mapper_init']\n        mapper_final = step['mapper_final']\n\n        if mapper_init:\n            for k, v in mapper_init() or ():\n                yield k, v\n\n        if mapper_raw:\n            if len(self.options.args) != 2:\n                raise ValueError('Wrong number of args')\n            input_path, input_uri = self.options.args\n            for k, v in mapper_raw(input_path, input_uri) or ():\n                yield k, v\n        else:\n            for key, value in pairs:\n                for k, v in mapper(key, value) or ():\n                    yield k, v\n\n        if mapper_final:\n            for k, v in mapper_final() or ():\n                yield k, v\n\n    def combine_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`combiner_init`,\n        :py:meth:`combiner`, and :py:meth:`combiner_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_combiner` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'combiner', step_num):\n            yield k, v\n\n    def reduce_pairs(self, pairs, step_num=0):\n        \"\"\"Runs :py:meth:`reducer_init`,\n        :py:meth:`reducer`, and :py:meth:`reducer_final`\n        for one reduce task in one step.\n\n        Takes in a sequence of (key, value) pairs as input, and yields\n        (key, value) pairs as output.\n\n        :py:meth:`run_reducer` essentially wraps this method with code to\n        handle reading/decoding input and writing/encoding output.\n\n        .. versionadded:: 0.6.7\n        \"\"\"\n        for k, v in self._combine_or_reduce_pairs(pairs, 'reducer', step_num):\n            yield k, v\n\n    def _combine_or_reduce_pairs(self, pairs, mrc, step_num=0):\n        \"\"\"Helper for :py:meth:`combine_pairs` and :py:meth:`reduce_pairs`.\"\"\"\n        step = self._get_step(step_num, MRStep)\n\n        task = step[mrc]\n        task_init = step[mrc + '_init']\n        task_final = step[mrc + '_final']\n        if task is None:\n            raise ValueError('No %s in step %d' % (mrc, step_num))\n\n        if task_init:\n            for k, v in task_init() or ():\n                yield k, v\n\n        # group all values of the same key together, and pass to the reducer\n        #\n        # be careful to use generators for everything, to allow for\n        # very large groupings of values\n        for key, pairs_for_key in itertools.groupby(pairs, lambda k_v: k_v[0]):\n            values = (value for _, value in pairs_for_key)\n            for k, v in task(key, values) or ():\n                yield k, v\n\n        if task_final:\n            for k, v in task_final() or ():\n                yield k, v\n\n    def run_spark(self, step_num):\n        \"\"\"Run the Spark code for the given step.\n\n        :type step_num: int\n        :param step_num: which step to run (0-indexed)\n\n        Called from :py:meth:`run`. You'd probably only want to call this\n        directly from automated tests.\n        \"\"\"\n        step = self._get_step(step_num, SparkStep)\n\n        if len(self.options.args) != 2:\n            raise ValueError('Wrong number of args')\n        input_path, output_path = self.options.args\n\n        spark_method = step.spark\n        spark_method(input_path, output_path)\n\n    def _steps_desc(self):\n        step_descs = []\n        for step_num, step in enumerate(self.steps()):\n            step_descs.append(step.description(step_num))\n        return step_descs\n\n    @classmethod\n    def mr_job_script(cls):\n        \"\"\"Path of this script. This returns the file containing\n        this class, or ``None`` if there isn't any (e.g. it was\n        defined from the command line interface.)\"\"\"\n        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n\n    ### Other useful utilities ###\n\n    def _read_input(self):\n        \"\"\"Read from stdin, or one more files, or directories.\n        Yield one line at time.\n\n        - Resolve globs (``foo_*.gz``).\n        - Decompress ``.gz`` and ``.bz2`` files.\n        - If path is ``-``, read from STDIN.\n        - Recursively read all files in a directory\n        \"\"\"\n        paths = self.options.args or ['-']\n\n        for path in paths:\n            if path == '-':\n                for line in self.stdin:\n                    yield line\n            else:\n                with open(path, 'rb') as f:\n                    for line in to_lines(decompress(f, path)):\n                        yield line\n\n    def _wrap_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing\n        for the given step.\n\n        Returns a tuple of ``(read_lines, write_line)``\n\n        ``read_lines()`` is a function that reads lines from input, decodes\n            them, and yields key, value pairs.\n        ``write_line()`` is a function that takes key and value as args,\n            encodes them, and writes a line to output.\n\n        :param step_num: which step to run (e.g. 0)\n        :param step_type: ``'mapper'``, ``'reducer'``, or ``'combiner'`` from\n                          :py:mod:`mrjob.step`\n        \"\"\"\n        read, write = self.pick_protocols(step_num, step_type)\n\n        def read_lines():\n            for line in self._read_input():\n                key, value = read(line.rstrip(b'\\r\\n'))\n                yield key, value\n\n        def write_line(key, value):\n            self.stdout.write(write(key, value))\n            self.stdout.write(b'\\n')\n\n        return read_lines, write_line\n\n    def _step_key(self, step_num, step_type):\n        return '%d-%s' % (step_num, step_type)\n\n    def _script_step_mapping(self, steps_desc):\n        \"\"\"Return a mapping of ``self._step_key(step_num, step_type)`` ->\n        (place in sort order of all *script* steps), for the purposes of\n        choosing which protocols to use for input and output.\n\n        Non-script steps do not appear in the mapping.\n        \"\"\"\n        mapping = {}\n        script_step_num = 0\n        for i, step in enumerate(steps_desc):\n\n            if 'mapper' in step and step['mapper']['type'] == 'script':\n                k = self._step_key(i, 'mapper')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n            if 'reducer' in step and step['reducer']['type'] == 'script':\n                k = self._step_key(i, 'reducer')\n                mapping[k] = script_step_num\n                script_step_num += 1\n\n        return mapping\n\n    def _mapper_output_protocol(self, step_num, step_map):\n        map_key = self._step_key(step_num, 'mapper')\n        if map_key in step_map:\n            if step_map[map_key] >= (len(step_map) - 1):\n                return self.output_protocol()\n            else:\n                return self.internal_protocol()\n        else:\n            # mapper is not a script substep, so protocols don't apply at all\n            return RawValueProtocol()\n\n    def _pick_protocol_instances(self, step_num, step_type):\n        steps_desc = self._steps_desc()\n\n        step_map = self._script_step_mapping(steps_desc)\n\n        # pick input protocol\n\n        if step_type == 'combiner':\n            # Combiners read and write the mapper's output protocol because\n            # they have to be able to run 0-inf times without changing the\n            # format of the data.\n            # Combiners for non-script substeps can't use protocols, so this\n            # function will just give us RawValueProtocol() in that case.\n            previous_mapper_output = self._mapper_output_protocol(\n                step_num, step_map)\n            return previous_mapper_output, previous_mapper_output\n        else:\n            step_key = self._step_key(step_num, step_type)\n\n            if step_key not in step_map:\n                raise ValueError(\n                    \"Can't pick a protocol for a non-script step\")\n\n            real_num = step_map[step_key]\n            if real_num == (len(step_map) - 1):\n                write = self.output_protocol()\n            else:\n                write = self.internal_protocol()\n\n            if real_num == 0:\n                read = self.input_protocol()\n            else:\n                read = self.internal_protocol()\n            return read, write\n\n    def pick_protocols(self, step_num, step_type):\n        \"\"\"Pick the protocol classes to use for reading and writing for the\n        given step.\n\n        :type step_num: int\n        :param step_num: which step to run (e.g. ``0`` for the first step)\n        :type step_type: str\n        :param step_type: one of `'mapper'`, `'combiner'`, or `'reducer'`\n        :return: (read_function, write_function)\n\n        By default, we use one protocol for reading input, one\n        internal protocol for communication between steps, and one\n        protocol for final output (which is usually the same as the\n        internal protocol). Protocols can be controlled by setting\n        :py:attr:`INPUT_PROTOCOL`, :py:attr:`INTERNAL_PROTOCOL`, and\n        :py:attr:`OUTPUT_PROTOCOL`.\n\n        Re-define this if you need fine control over which protocols\n        are used by which steps.\n        \"\"\"\n\n        # wrapping functionality like this makes testing much simpler\n        p_read, p_write = self._pick_protocol_instances(step_num, step_type)\n\n        return p_read.read, p_write.write\n\n    ### Command-line arguments ###\n\n    def configure_args(self):\n        \"\"\"Define arguments for this script. Called from :py:meth:`__init__()`.\n\n        Re-define to define custom command-line arguments or pass\n        through existing ones::\n\n            def configure_args(self):\n                super(MRYourJob, self).configure_args()\n\n                self.add_passthru_arg(...)\n                self.add_file_arg(...)\n                self.pass_arg_through(...)\n                ...\n        \"\"\"\n        self.arg_parser.add_argument(\n            dest='args', nargs='*',\n            help=('input paths to read (or stdin if not set). If --spark'\n                  ' is set, the input and output path for the spark job.'))\n\n        _add_basic_args(self.arg_parser)\n        _add_job_args(self.arg_parser)\n        _add_runner_args(self.arg_parser)\n        _add_step_args(self.arg_parser, include_deprecated=True)\n\n    def load_args(self, args):\n        \"\"\"Load command-line options into ``self.options``.\n\n        Called from :py:meth:`__init__()` after :py:meth:`configure_args`.\n\n        :type args: list of str\n        :param args: a list of command line arguments. ``None`` will be\n                     treated the same as ``[]``.\n\n        Re-define if you want to post-process command-line arguments::\n\n            def load_args(self, args):\n                super(MRYourJob, self).load_args(args)\n\n                self.stop_words = self.options.stop_words.split(',')\n                ...\n        \"\"\"\n        if hasattr(self.arg_parser, 'parse_intermixed_args'):\n            # restore old optparse behavior on Python 3.7+. See #1701\n            self.options = self.arg_parser.parse_intermixed_args(args)\n        else:\n            self.options = self.arg_parser.parse_args(args)\n\n        if self.options.help:\n            self._print_help(self.options)\n            sys.exit(0)\n\n    def add_file_arg(self, *args, **kwargs):\n        \"\"\"Add a command-line option that sends an external file\n        (e.g. a SQLite DB) to Hadoop::\n\n             def configure_args(self):\n                super(MRYourJob, self).configure_args()\n                self.add_file_arg('--scoring-db', help=...)\n\n        This does the right thing: the file will be uploaded to the working\n        dir of the script on Hadoop, and the script will be passed the same\n        option, but with the local name of the file in the script's working\n        directory.\n\n        .. note::\n\n           If you pass a file to a job, best practice is to lazy-load its\n           contents (e.g. make a method that opens the file the first time\n           you call it) rather than loading it in your job's constructor or\n           :py:meth:`load_args`. Not only is this more efficient, it's\n           necessary if you want to run your job in a Spark executor\n           (because the file may not be in the same place in a Spark driver).\n\n        .. note::\n\n           We suggest against sending Berkeley DBs to your job, as\n           Berkeley DB is not forwards-compatible (so a Berkeley DB that you\n           construct on your computer may not be readable from within\n           Hadoop). Use SQLite databases instead. If all you need is an on-disk\n           hash table, try out the :py:mod:`sqlite3dbm` module.\n\n        .. versionchanged:: 0.6.6\n\n           now accepts explicit ``type=str``\n\n        .. versionchanged:: 0.6.8\n\n           fully supported on Spark, including ``local[*]`` master\n        \"\"\"\n        if kwargs.get('type') not in (None, str):\n            raise ArgumentTypeError(\n                'file options must take strings')\n\n        if kwargs.get('action') not in (None, 'append', 'store'):\n            raise ArgumentTypeError(\n                \"file options must use the actions 'store' or 'append'\")\n\n        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._file_arg_dests.add(pass_opt.dest)\n\n###The function: add_passthru_arg###\n    def pass_arg_through(self, opt_str):\n        \"\"\"Pass the given argument through to the job.\"\"\"\n\n        # _actions is hidden but the interface appears to be stable,\n        # and there's no non-hidden interface we can use\n        for action in self.arg_parser._actions:\n            if opt_str in action.option_strings or opt_str == action.dest:\n                self._passthru_arg_dests.add(action.dest)\n                break\n        else:\n            raise ValueError('unknown arg: %s', opt_str)\n\n    def is_task(self):\n        \"\"\"True if this is a mapper, combiner, reducer, or Spark script.\n\n        This is mostly useful inside :py:meth:`load_args`, to disable\n        loading args when we aren't running inside Hadoop.\n        \"\"\"\n        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)\n\n    ### protocols ###\n\n    def input_protocol(self):\n        \"\"\"Instance of the protocol to use to convert input lines to Python\n        objects. Default behavior is to return an instance of\n        :py:attr:`INPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INPUT_PROTOCOL, type):\n            log.warning('INPUT_PROTOCOL should be a class, not %s' %\n                        self.INPUT_PROTOCOL)\n        return self.INPUT_PROTOCOL()\n\n    def internal_protocol(self):\n        \"\"\"Instance of the protocol to use to communicate between steps.\n        Default behavior is to return an instance of\n        :py:attr:`INTERNAL_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.INTERNAL_PROTOCOL, type):\n            log.warning('INTERNAL_PROTOCOL should be a class, not %s' %\n                        self.INTERNAL_PROTOCOL)\n        return self.INTERNAL_PROTOCOL()\n\n    def output_protocol(self):\n        \"\"\"Instance of the protocol to use to convert Python objects to output\n        lines. Default behavior is to return an instance of\n        :py:attr:`OUTPUT_PROTOCOL`.\n        \"\"\"\n        if not isinstance(self.OUTPUT_PROTOCOL, type):\n            log.warning('OUTPUT_PROTOCOL should be a class, not %s' %\n                        self.OUTPUT_PROTOCOL)\n        return self.OUTPUT_PROTOCOL()\n\n    #: Protocol for reading input to the first mapper in your job.\n    #: Default: :py:class:`RawValueProtocol`.\n    #:\n    #: For example you know your input data were in JSON format, you could\n    #: set::\n    #:\n    #:     INPUT_PROTOCOL = JSONValueProtocol\n    #:\n    #: in your class, and your initial mapper would receive decoded JSONs\n    #: rather than strings.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INPUT_PROTOCOL = RawValueProtocol\n\n    #: Protocol for communication between steps and final output.\n    #: Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example if your step output weren't JSON-encodable, you could set::\n    #:\n    #:     INTERNAL_PROTOCOL = PickleProtocol\n    #:\n    #: and step output would be encoded as string-escaped pickles.\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    INTERNAL_PROTOCOL = JSONProtocol\n\n    #: Protocol to use for writing output. Default: :py:class:`JSONProtocol`.\n    #:\n    #: For example, if you wanted the final output in repr, you could set::\n    #:\n    #:     OUTPUT_PROTOCOL = ReprProtocol\n    #:\n    #: See :py:data:`mrjob.protocol` for the full list of protocols.\n    OUTPUT_PROTOCOL = JSONProtocol\n\n    def parse_output(self, chunks):\n        \"\"\"Parse the final output of this MRJob (as a stream of byte chunks)\n        into a stream of ``(key, value)``.\n        \"\"\"\n        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)\n\n    ### Hadoop Input/Output Formats ###\n\n    #: Optional name of an optional Hadoop ``InputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.lib.NLineInputFormat'``.\n    #:\n    #: Passed to Hadoop with the *first* step of this job with the\n    #: ``-inputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_input_format` or the *hadoop_input_format* argument to\n    #: :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_INPUT_FORMAT = None\n\n    def hadoop_input_format(self):\n        \"\"\"Optional Hadoop ``InputFormat`` class to parse input for\n        the first step of the job.\n\n        Normally, setting :py:attr:`HADOOP_INPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_INPUT_FORMAT\n\n    #: Optional name of an optional Hadoop ``OutputFormat`` class, e.g.\n    #: ``'org.apache.hadoop.mapred.FileOutputFormat'``.\n    #:\n    #: Passed to Hadoop with the *last* step of this job with the\n    #: ``-outputformat`` option.\n    #:\n    #: If you require more sophisticated behavior, try\n    #: :py:meth:`hadoop_output_format` or the *hadoop_output_format* argument\n    #: to :py:meth:`mrjob.runner.MRJobRunner.__init__`.\n    HADOOP_OUTPUT_FORMAT = None\n\n    def hadoop_output_format(self):\n        \"\"\"Optional Hadoop ``OutputFormat`` class to write output for\n        the last step of the job.\n\n        Normally, setting :py:attr:`HADOOP_OUTPUT_FORMAT` is sufficient;\n        redefining this method is only for when you want to get fancy.\n        \"\"\"\n        return self.HADOOP_OUTPUT_FORMAT\n\n    ### Libjars ###\n\n    #: Optional list of paths of jar files to run our job with using Hadoop's\n    #: ``-libjars`` option.\n    #:\n    #: ``~`` and environment variables\n    #: in paths be expanded, and relative paths will be interpreted as\n    #: relative to the directory containing the script (not the current\n    #: working directory).\n    #:\n    #: If you require more sophisticated behavior, try overriding\n    #: :py:meth:`libjars`.\n    LIBJARS = []\n\n    def libjars(self):\n        \"\"\"Optional list of paths of jar files to run our job with using\n        Hadoop's ``-libjars`` option. Normally setting :py:attr:`LIBJARS`\n        is sufficient. Paths from :py:attr:`LIBJARS` are interpreted as\n        relative to the the directory containing the script (paths from the\n        command-line are relative to the current working directory).\n\n        Note that ``~`` and environment variables in paths will always be\n        expanded by the job runner (see :mrjob-opt:`libjars`).\n\n        .. versionchanged:: 0.6.6\n\n           re-defining this no longer clobbers the command-line\n           ``--libjars`` option\n        \"\"\"\n        script_dir = os.path.dirname(self.mr_job_script())\n\n        paths = []\n\n        # libjar paths will eventually be combined with combine_path_lists,\n        # which will expand environment variables. We don't want to assume\n        # a path like $MY_DIR/some.jar is always relative ($MY_DIR could start\n        # with /), but we also don't want to expand environment variables\n        # prematurely.\n        for path in self.LIBJARS or []:\n            if os.path.isabs(expand_path(path)):\n                paths.append(path)\n            else:\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Partitioning ###\n\n    #: Optional Hadoop partitioner class to use to determine how mapper\n    #: output should be sorted and distributed to reducers. For example:\n    #: ``'org.apache.hadoop.mapred.lib.HashPartitioner'``.\n    #:\n    #: If you require more sophisticated behavior, try :py:meth:`partitioner`.\n    PARTITIONER = None\n\n    def partitioner(self):\n        \"\"\"Optional Hadoop partitioner class to use to determine how mapper\n        output should be sorted and distributed to reducers.\n\n        By default, returns :py:attr:`PARTITIONER`.\n\n        You probably don't need to re-define this; it's just here for\n        completeness.\n        \"\"\"\n        return self.PARTITIONER\n\n    ### Uploading support files ###\n\n    #: Optional list of archives to upload and unpack in the job's working\n    #: directory. These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: By default, the directory will have the same name as the archive\n    #: (e.g. ``foo.tar.gz/``). To change the directory's name, append\n    #: ``#<name>``::\n    #:\n    #:     ARCHIVES = ['data/foo.tar.gz#foo']\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`archives` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    ARCHIVES = []\n\n    #: Optional list of directories to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a directory to be copied with a name other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo#bar``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`dirs` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    DIRS = []\n\n    #: Optional list of files to upload to the job's working directory.\n    #: These can be URIs or paths on the local filesystem.\n    #:\n    #: Relative paths will be interpreted as relative to the directory\n    #: containing the script (not the current working directory).\n    #\n    #: Environment variables and ``~`` in paths will be expanded.\n    #:\n    #: If you want a file to be uploaded to a filename other than it's own,\n    #: append ``#<name>`` (e.g. ``data/foo.json#bar.json``).\n    #:\n    #: If you need to dynamically generate a list of files, override\n    #: :py:meth:`files` instead.\n    #:\n    #: .. versionadded:: 0.6.4\n    FILES = []\n\n    def archives(self):\n        \"\"\"Like :py:attr:`ARCHIVES`, except that it can return a dynamically\n        generated list of archives to upload and unpack. Overriding\n        this method disables :py:attr:`ARCHIVES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--archives``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('ARCHIVES')\n\n    def dirs(self):\n        \"\"\"Like :py:attr:`DIRS`, except that it can return a dynamically\n        generated list of directories to upload. Overriding\n        this method disables :py:attr:`DIRS`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--dirs``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('DIRS')\n\n    def files(self):\n        \"\"\"Like :py:attr:`FILES`, except that it can return a dynamically\n        generated list of files to upload. Overriding\n        this method disables :py:attr:`FILES`.\n\n        Paths returned by this method are relative to the working directory\n        (not the script). Note that the job runner will *always* expand\n        environment variables and ``~`` in paths returned by this method.\n\n        You do not have to worry about inadvertently disabling ``--files``;\n        this switch is handled separately.\n\n        .. versionadded:: 0.6.4\n        \"\"\"\n        return self._upload_attr('FILES')\n\n    def _upload_attr(self, attr_name):\n        \"\"\"Helper for :py:meth:`archives`, :py:meth:`dirs`, and\n        :py:meth:`files`\"\"\"\n        attr_value = getattr(self, attr_name)\n\n        # catch path instead of a list of paths\n        if isinstance(attr_value, string_types):\n            raise TypeError('%s must be a list or other sequence.' % attr_name)\n\n        script_dir = os.path.dirname(self.mr_job_script())\n        paths = []\n\n        for path in attr_value:\n            expanded_path = expand_path(path)\n\n            if os.path.isabs(expanded_path):\n                paths.append(path)\n            else:\n                # relative subdirs are confusing; people will expect them\n                # to appear in a subdir, not the same directory as the script,\n                # but Hadoop doesn't work that way\n                if os.sep in path.rstrip(os.sep) and '#' not in path:\n                    log.warning(\n                        '%s: %s will appear in same directory as job script,'\n                        ' not a subdirectory' % (attr_name, path))\n\n                paths.append(os.path.join(script_dir, path))\n\n        return paths\n\n    ### Jobconf ###\n\n    #: Optional jobconf arguments we should always pass to Hadoop. This\n    #: is a map from property name to value. e.g.:\n    #:\n    #: ``{'stream.num.map.output.key.fields': '4'}``\n    #:\n    #: It's recommended that you only use this to hard-code things that\n    #: affect the semantics of your job, and leave performance tweaks to\n    #: the command line or whatever you use to launch your job.\n    JOBCONF = {}\n\n    def jobconf(self):\n        \"\"\"``-D`` args to pass to hadoop streaming. This should be a map\n        from property name to value. By default, returns :py:attr:`JOBCONF`.\n\n        .. versionchanged:: 0.6.6\n\n           re-defining longer clobbers command-line\n           ``--jobconf`` options.\n        \"\"\"\n        return dict(self.JOBCONF)\n\n    ### Secondary Sort ###\n\n    #: Set this to ``True`` if you would like reducers to receive the values\n    #: associated with any key in sorted order (sorted by their *encoded*\n    #: value). Also known as secondary sort.\n    #:\n    #: This can be useful if you expect more values than you can fit in memory\n    #: to be associated with one key, but you want to apply information in\n    #: a small subset of these values to information in the other values.\n    #: For example, you may want to convert counts to percentages, and to do\n    #: this you first need to know the total count.\n    #:\n    #: Even though values are sorted by their encoded value, most encodings\n    #: will sort strings in order. For example, you could have values like:\n    #: ``['A', <total>]``, ``['B', <count_name>, <count>]``, and the value\n    #: containing the total should come first regardless of what protocol\n    #: you're using.\n    #:\n    #: See :py:meth:`jobconf()` and :py:meth:`partitioner()` for more about\n    SORT_VALUES = None\n\n    def sort_values(self):\n        \"\"\"A method that by default, just returns the value of\n        :py:attr:`SORT_VALUES`. Mostly exists for the sake\n        of consistency, but you could override it if you wanted to make\n        secondary sort configurable.\"\"\"\n        return self.SORT_VALUES\n\n    ### Testing ###\n\n    def sandbox(self, stdin=None, stdout=None, stderr=None):\n        \"\"\"Redirect stdin, stdout, and stderr for automated testing.\n\n        You can set stdin, stdout, and stderr to file objects. By\n        default, they'll be set to empty ``BytesIO`` objects.\n        You can then access the job's file handles through ``self.stdin``,\n        ``self.stdout``, and ``self.stderr``. See :ref:`testing` for more\n        information about testing.\n\n        You may call sandbox multiple times (this will essentially clear\n        the file handles).\n\n        ``stdin`` is empty by default. You can set it to anything that yields\n        lines::\n\n            mr_job.sandbox(stdin=BytesIO(b'some_data\\\\n'))\n\n        or, equivalently::\n\n            mr_job.sandbox(stdin=[b'some_data\\\\n'])\n\n        For convenience, this sandbox() returns self, so you can do::\n\n            mr_job = MRJobClassToTest().sandbox()\n\n        Simple testing example::\n\n            mr_job = MRYourJob.sandbox()\n            self.assertEqual(list(mr_job.reducer('foo', ['a', 'b'])), [...])\n\n        More complex testing example::\n\n            from BytesIO import BytesIO\n\n            from mrjob.parse import parse_mr_job_stderr\n            from mrjob.protocol import JSONProtocol\n\n            mr_job = MRYourJob(args=[...])\n\n            fake_input = '\"foo\"\\\\t\"bar\"\\\\n\"foo\"\\\\t\"baz\"\\\\n'\n            mr_job.sandbox(stdin=BytesIO(fake_input))\n\n            mr_job.run_reducer(link_num=0)\n\n            self.assertEqual(mrjob.stdout.getvalue(), ...)\n            self.assertEqual(parse_mr_job_stderr(mr_job.stderr), ...)\n\n        .. note::\n\n           If you are using Spark, it's recommended you only pass in\n           :py:class:`io.BytesIO` or other serializable alternatives to file\n           objects. *stdin*, *stdout*, and *stderr* get stored as job\n           attributes, which means if they aren't serializable, neither\n           is the job instance or its methods.\n        \"\"\"\n        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self\n", "test_list": ["def test_bad_option_types(self):\n    mr_job = MRJob(args=[])\n    self.assertRaises(ValueError, mr_job.add_passthru_arg, '--stop-words', dest='stop_words', type='set', default=None)\n    self.assertRaises(ValueError, mr_job.add_passthru_arg, '--leave-a-msg', dest='leave_a_msg', action='callback', default=None)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'add_passthru_arg' function should only accept arguments that are valid for the argument parser, ensuring that the input types for options are either 'str', 'int', 'float', or 'bool'.", "unit_test": ["def test_valid_option_types(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--valid-str', dest='valid_str', type=str, default='default')\n    mr_job.add_passthru_arg('--valid-int', dest='valid_int', type=int, default=0)\n    mr_job.add_passthru_arg('--valid-float', dest='valid_float', type=float, default=0.0)\n    mr_job.add_passthru_arg('--valid-bool', dest='valid_bool', type=bool, default=False)\n    self.assertIn('valid_str', mr_job._passthru_arg_dests)\n    self.assertIn('valid_int', mr_job._passthru_arg_dests)\n    self.assertIn('valid_float', mr_job._passthru_arg_dests)\n    self.assertIn('valid_bool', mr_job._passthru_arg_dests)"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_valid_option_types"}, "Exception Handling": {"requirement": "The 'add_passthru_arg' function should raise a ValueError with a descriptive message: 'Unsupported argument type: dict' if an unsupported type is provided for the argument.", "unit_test": ["def test_unsupported_option_type(self):\n    mr_job = MRJob(args=[])\n    with self.assertRaises(ValueError) as cm:\n        mr_job.add_passthru_arg('--unsupported-type', dest='unsupported', type=dict, default=None)\n    self.assertEqual(str(cm.exception), 'Unsupported argument type: dict')"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_unsupported_option_type"}, "Edge Case Handling": {"requirement": "The 'add_passthru_arg' function should handle edge cases where no type is specified by defaulting to 'str'.", "unit_test": ["def test_default_type(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--default-type', dest='default_type')\n    self.assertIn('default_type', mr_job._passthru_arg_dests)\n    self.assertEqual(mr_job.arg_parser.get_default('default_type'), None)"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_default_type"}, "Functionality Extension": {"requirement": "Extend the 'add_passthru_arg' function to support a 'choices' parameter, allowing only specific values for an argument.", "unit_test": ["def test_choices_parameter(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--color', dest='color', choices=['red', 'green', 'blue'], default='red')\n    self.assertIn('color', mr_job._passthru_arg_dests)\n    self.assertEqual(mr_job.arg_parser.get_default('color'), 'red')"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_default_type"}, "Annotation Coverage": {"requirement": "Ensure that the 'add_passthru_arg' function is fully annotated with parameter and return types, including two parameters: 'args': tuple, 'kwargs': dict, and a return type: None.", "unit_test": ["def test_function_annotations(self):\n    annotations = MRJob.add_passthru_arg.__annotations__\n    self.assertEqual(annotations['args'], tuple)\n    self.assertEqual(annotations['kwargs'], dict)\n    self.assertEqual(annotations['return'], None)"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_function_annotations"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": ["def test_code_complexity(self1):\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(MRJob.add_passthru_arg).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity == 1"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_code_complexity"}, "Code Standard": {"requirement": "Ensure that the 'add_passthru_arg' function follows PEP 8 style guidelines, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    code_string = inspect.getsource(MRJob.add_passthru_arg)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'add_passthru_arg' function utilizes the 'arg_parser' attribute of the MRJob class to add arguments.", "unit_test": ["def test_arg_parser_usage(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--test-arg', dest='test_arg', type=str)\n    self.assertTrue(any('--test-arg' in action.option_strings for action in mr_job.arg_parser._actions))"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_arg_parser_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'add_passthru_arg' function correctly adds the destination of the argument to the '_passthru_arg_dests' set.", "unit_test": ["def test_passthru_arg_dests_update(self):\n    mr_job = MRJob(args=[])\n    mr_job.add_passthru_arg('--test-arg', dest='test_arg', type=str)\n    self.assertIn('test_arg', mr_job._passthru_arg_dests)"], "test": "tests/test_job.py::CommandLineArgsTestCase::test_passthru_arg_dests_update"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "mingus.containers.note.Note.to_hertz", "type": "method", "project_path": "Multimedia/mingus", "completion_path": "Multimedia/mingus/mingus/containers/note.py", "signature_position": [226, 226], "body_position": [233, 234], "dependency": {"intra_class": ["mingus.containers.note.Note.__int__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function converts a given Note instance to Hertz (frequency in cycles per second).\n", "Arguments": ":param self: Note. An instance of the Note class.\n:param standard_pitch: float. The pitch of A-4, from which the rest of the notes are calculated. It defaults to 440 if not specified.\n:return: float. The frequency of the Note in Hertz.\n"}, "tests": ["tests/unit/containers/test_note.py::test_Note::test_to_hertz"], "indent": 4, "domain": "Multimedia", "code": "    def to_hertz(self, standard_pitch=440):\n        \"\"\"Return the Note in Hz.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n        # int(Note(\"A\")) == 57\n        diff = self.__int__() - 57\n        return 2 ** (diff / 12.0) * standard_pitch\n", "context": "class Note(object):\n\n    \"\"\"A note object.\n\n    In the mingus.core module, notes are generally represented by strings.\n    Most of the times, this is not enough. We want to set the octave and\n    maybe the amplitude, vibrato or other dynamics. Then we want to store\n    the notes in bars, the bars in tracks, the tracks in compositions, etc.\n\n    We could do this with a number of lists, but ultimately it is a lot\n    easier to use objects. The Note class provides an easy way to deal with\n    notes in an object oriented matter.\n\n    You can use the class NoteContainer to group Notes together in intervals\n    and chords.\n    \"\"\"\n\n    name = _DEFAULT_NAME\n    octave = _DEFAULT_OCTAVE\n    channel = _DEFAULT_CHANNEL\n    velocity = _DEFAULT_VELOCITY\n\n    def __init__(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            dynamics[\"velocity\"] = velocity\n        if channel is not None:\n            dynamics[\"channel\"] = channel\n\n        if isinstance(name, six.string_types):\n            self.set_note(name, octave, dynamics)\n        elif hasattr(name, \"name\"):\n            # Hardcopy Note object\n            self.set_note(name.name, name.octave, name.dynamics)\n        elif isinstance(name, int):\n            self.from_int(name)\n        else:\n            raise NoteFormatError(\"Don't know what to do with name object: %r\" % name)\n\n    @property\n    def dynamics(self):\n        \"\"\"\n        .. deprecated:: Provided only for compatibility with existing code.\n        \"\"\"\n        return {\n            \"channel\": self.channel,\n            \"velocity\": self.velocity,\n        }\n\n    def set_channel(self, channel):\n        if not 0 <= channel < 16:\n            raise ValueError(\"MIDI channel must be 0-15\")\n        self.channel = channel\n\n    def set_velocity(self, velocity):\n        if not 0 <= velocity < 128:\n            raise ValueError(\"MIDI velocity must be 0-127\")\n        self.velocity = velocity\n\n    def set_note(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"Set the note to name in octave with dynamics.\n\n        Return the objects if it succeeded, raise an NoteFormatError\n        otherwise.\n\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        :return:\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            self.set_velocity(velocity)\n        elif \"velocity\" in dynamics:\n            self.set_velocity(dynamics[\"velocity\"])\n\n        if channel is not None:\n            self.set_channel(channel)\n        if \"channel\" in dynamics:\n            self.set_channel(dynamics[\"channel\"])\n\n        dash_index = name.split(\"-\")\n        if len(dash_index) == 1:\n            if notes.is_valid_note(name):\n                self.name = name\n                self.octave = octave\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        elif len(dash_index) == 2:\n            note, octave = dash_index\n            if notes.is_valid_note(note):\n                self.name = note\n                self.octave = int(octave)\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        else:\n            raise NoteFormatError(\"Invalid note representation: %r\" % name)\n\n    def empty(self):\n        \"\"\"Remove the data in the instance.\"\"\"\n        # TODO: Review these two. This seems to leave the object in an invalid state\n        self.name = \"\"\n        self.octave = 0\n\n        self.channel = _DEFAULT_CHANNEL\n        self.velocity = _DEFAULT_VELOCITY\n\n    def augment(self):\n        \"\"\"Call notes.augment with this note as argument.\"\"\"\n        self.name = notes.augment(self.name)\n\n    def diminish(self):\n        \"\"\"Call notes.diminish with this note as argument.\"\"\"\n        self.name = notes.diminish(self.name)\n\n    def change_octave(self, diff):\n        \"\"\"Change the octave of the note to the current octave + diff.\"\"\"\n        self.octave += diff\n        if self.octave < 0:\n            self.octave = 0\n\n    def octave_up(self):\n        \"\"\"Increment the current octave with 1.\"\"\"\n        self.change_octave(1)\n\n    def octave_down(self):\n        \"\"\"Decrement the current octave with 1.\"\"\"\n        self.change_octave(-1)\n\n    def remove_redundant_accidentals(self):\n        \"\"\"Call notes.remove_redundant_accidentals on this note's name.\"\"\"\n        self.name = notes.remove_redundant_accidentals(self.name)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose the note up or down the interval.\n\n        Examples:\n        >>> a = Note('A')\n        >>> a.transpose('3')\n        >>> a\n        'C#-5'\n        >>> a.transpose('3', False)\n        >>> a\n        'A-4'\n        \"\"\"\n        from mingus.core import intervals\n        (old, o_octave) = (self.name, self.octave)\n        self.name = intervals.from_shorthand(self.name, interval, up)\n        if up:\n            if self < Note(old, o_octave):\n                self.octave += 1\n        else:\n            if self > Note(old, o_octave):\n                self.octave -= 1\n\n    def from_int(self, integer):\n        \"\"\"Set the Note corresponding to the integer.\n\n        0 is a C on octave 0, 12 is a C on octave 1, etc.\n\n        Example:\n        >>> Note().from_int(12)\n        'C-1'\n        \"\"\"\n        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self\n\n    def measure(self, other):\n        \"\"\"Return the number of semitones between this Note and the other.\n\n        Examples:\n        >>> Note('C').measure(Note('D'))\n        2\n        >>> Note('D').measure(Note('C'))\n        -2\n        \"\"\"\n        return int(other) - int(self)\n\n###The function: to_hertz###\n    def from_hertz(self, hertz, standard_pitch=440):\n        \"\"\"Set the Note name and pitch, calculated from the hertz value.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n        value = (\n            log((float(hertz) * 1024) / standard_pitch, 2) + 1.0 / 24\n        ) * 12 + 9  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self\n\n    def to_shorthand(self):\n        \"\"\"Give the traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note('C-4').to_shorthand()\n        \"c'\"\n        >>> Note('C-3').to_shorthand()\n        'c'\n        >>> Note('C-2').to_shorthand()\n        'C'\n        >>> Note('C-1').to_shorthand()\n        'C,'\n        \"\"\"\n        if self.octave < 3:\n            res = self.name\n        else:\n            res = str.lower(self.name)\n        o = self.octave - 3\n        while o < -1:\n            res += \",\"\n            o += 1\n        while o > 0:\n            res += \"'\"\n            o -= 1\n        return res\n\n    def from_shorthand(self, shorthand):\n        \"\"\"Convert from traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note().from_shorthand(\"C,,\")\n        'C-0'\n        >>> Note().from_shorthand(\"C\")\n        'C-2'\n        >>> Note().from_shorthand(\"c'\")\n        'C-4'\n        \"\"\"\n        name = \"\"\n        octave = 0\n        for x in shorthand:\n            if x in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]:\n                name = str.upper(x)\n                octave = 3\n            elif x in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                name = x\n                octave = 2\n            elif x in [\"#\", \"b\"]:\n                name += x\n            elif x == \",\":\n                octave -= 1\n            elif x == \"'\":\n                octave += 1\n        return self.set_note(name, octave, {})\n\n    def __int__(self):\n        \"\"\"Return the current octave multiplied by twelve and add\n        notes.note_to_int to it.\n        \n        This means a C-0 returns 0, C-1 returns 12, etc. This method allows\n        you to use int() on Notes.\n        \"\"\"\n        res = self.octave * 12 + notes.note_to_int(self.name[0])\n        for n in self.name[1:]:\n            if n == \"#\":\n                res += 1\n            elif n == \"b\":\n                res -= 1\n        return res\n\n    def __lt__(self, other):\n        \"\"\"Enable the comparing operators on Notes (>, <, \\ ==, !=, >= and <=).\n\n        So we can sort() Intervals, etc.\n\n        Examples:\n        >>> Note('C', 4) < Note('B', 4)\n        True\n        >>> Note('C', 4) > Note('B', 4)\n        False\n        \"\"\"\n        if other is None:\n            return False\n        return int(self) < int(other)\n\n    def __eq__(self, other):\n        \"\"\"Compare Notes for equality by comparing their note values.\"\"\"\n        if other is None:\n            return False\n        return int(self) == int(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        return not (self < other or self == other)\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __ge__(self, other):\n        return not self < other\n\n    def __repr__(self):\n        \"\"\"Return a helpful representation for printing Note classes.\"\"\"\n        return \"'%s-%d'\" % (self.name, self.octave)\n", "prompt": "Please write a python function called 'to_hertz' base the context. This function converts a given Note instance to Hertz (frequency in cycles per second).\n:param self: Note. An instance of the Note class.\n:param standard_pitch: float. The pitch of A-4, from which the rest of the notes are calculated. It defaults to 440 if not specified.\n:return: float. The frequency of the Note in Hertz.\n.\n        The context you need to refer to is as follows: class Note(object):\n\n    \"\"\"A note object.\n\n    In the mingus.core module, notes are generally represented by strings.\n    Most of the times, this is not enough. We want to set the octave and\n    maybe the amplitude, vibrato or other dynamics. Then we want to store\n    the notes in bars, the bars in tracks, the tracks in compositions, etc.\n\n    We could do this with a number of lists, but ultimately it is a lot\n    easier to use objects. The Note class provides an easy way to deal with\n    notes in an object oriented matter.\n\n    You can use the class NoteContainer to group Notes together in intervals\n    and chords.\n    \"\"\"\n\n    name = _DEFAULT_NAME\n    octave = _DEFAULT_OCTAVE\n    channel = _DEFAULT_CHANNEL\n    velocity = _DEFAULT_VELOCITY\n\n    def __init__(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            dynamics[\"velocity\"] = velocity\n        if channel is not None:\n            dynamics[\"channel\"] = channel\n\n        if isinstance(name, six.string_types):\n            self.set_note(name, octave, dynamics)\n        elif hasattr(name, \"name\"):\n            # Hardcopy Note object\n            self.set_note(name.name, name.octave, name.dynamics)\n        elif isinstance(name, int):\n            self.from_int(name)\n        else:\n            raise NoteFormatError(\"Don't know what to do with name object: %r\" % name)\n\n    @property\n    def dynamics(self):\n        \"\"\"\n        .. deprecated:: Provided only for compatibility with existing code.\n        \"\"\"\n        return {\n            \"channel\": self.channel,\n            \"velocity\": self.velocity,\n        }\n\n    def set_channel(self, channel):\n        if not 0 <= channel < 16:\n            raise ValueError(\"MIDI channel must be 0-15\")\n        self.channel = channel\n\n    def set_velocity(self, velocity):\n        if not 0 <= velocity < 128:\n            raise ValueError(\"MIDI velocity must be 0-127\")\n        self.velocity = velocity\n\n    def set_note(self, name=\"C\", octave=4, dynamics=None, velocity=None, channel=None):\n        \"\"\"Set the note to name in octave with dynamics.\n\n        Return the objects if it succeeded, raise an NoteFormatError\n        otherwise.\n\n        :param name:\n        :param octave:\n        :param dynamics: Deprecated. Use `velocity` and `channel` directly.\n        :param int velocity: Integer (0-127)\n        :param int channel: Integer (0-15)\n        :return:\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n\n        if velocity is not None:\n            self.set_velocity(velocity)\n        elif \"velocity\" in dynamics:\n            self.set_velocity(dynamics[\"velocity\"])\n\n        if channel is not None:\n            self.set_channel(channel)\n        if \"channel\" in dynamics:\n            self.set_channel(dynamics[\"channel\"])\n\n        dash_index = name.split(\"-\")\n        if len(dash_index) == 1:\n            if notes.is_valid_note(name):\n                self.name = name\n                self.octave = octave\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        elif len(dash_index) == 2:\n            note, octave = dash_index\n            if notes.is_valid_note(note):\n                self.name = note\n                self.octave = int(octave)\n                return self\n            else:\n                raise NoteFormatError(\"Invalid note representation: %r\" % name)\n        else:\n            raise NoteFormatError(\"Invalid note representation: %r\" % name)\n\n    def empty(self):\n        \"\"\"Remove the data in the instance.\"\"\"\n        # TODO: Review these two. This seems to leave the object in an invalid state\n        self.name = \"\"\n        self.octave = 0\n\n        self.channel = _DEFAULT_CHANNEL\n        self.velocity = _DEFAULT_VELOCITY\n\n    def augment(self):\n        \"\"\"Call notes.augment with this note as argument.\"\"\"\n        self.name = notes.augment(self.name)\n\n    def diminish(self):\n        \"\"\"Call notes.diminish with this note as argument.\"\"\"\n        self.name = notes.diminish(self.name)\n\n    def change_octave(self, diff):\n        \"\"\"Change the octave of the note to the current octave + diff.\"\"\"\n        self.octave += diff\n        if self.octave < 0:\n            self.octave = 0\n\n    def octave_up(self):\n        \"\"\"Increment the current octave with 1.\"\"\"\n        self.change_octave(1)\n\n    def octave_down(self):\n        \"\"\"Decrement the current octave with 1.\"\"\"\n        self.change_octave(-1)\n\n    def remove_redundant_accidentals(self):\n        \"\"\"Call notes.remove_redundant_accidentals on this note's name.\"\"\"\n        self.name = notes.remove_redundant_accidentals(self.name)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose the note up or down the interval.\n\n        Examples:\n        >>> a = Note('A')\n        >>> a.transpose('3')\n        >>> a\n        'C#-5'\n        >>> a.transpose('3', False)\n        >>> a\n        'A-4'\n        \"\"\"\n        from mingus.core import intervals\n        (old, o_octave) = (self.name, self.octave)\n        self.name = intervals.from_shorthand(self.name, interval, up)\n        if up:\n            if self < Note(old, o_octave):\n                self.octave += 1\n        else:\n            if self > Note(old, o_octave):\n                self.octave -= 1\n\n    def from_int(self, integer):\n        \"\"\"Set the Note corresponding to the integer.\n\n        0 is a C on octave 0, 12 is a C on octave 1, etc.\n\n        Example:\n        >>> Note().from_int(12)\n        'C-1'\n        \"\"\"\n        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self\n\n    def measure(self, other):\n        \"\"\"Return the number of semitones between this Note and the other.\n\n        Examples:\n        >>> Note('C').measure(Note('D'))\n        2\n        >>> Note('D').measure(Note('C'))\n        -2\n        \"\"\"\n        return int(other) - int(self)\n\n###The function: to_hertz###\n    def from_hertz(self, hertz, standard_pitch=440):\n        \"\"\"Set the Note name and pitch, calculated from the hertz value.\n\n        The standard_pitch argument can be used to set the pitch of A-4,\n        from which the rest is calculated.\n        \"\"\"\n        value = (\n            log((float(hertz) * 1024) / standard_pitch, 2) + 1.0 / 24\n        ) * 12 + 9  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self\n\n    def to_shorthand(self):\n        \"\"\"Give the traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note('C-4').to_shorthand()\n        \"c'\"\n        >>> Note('C-3').to_shorthand()\n        'c'\n        >>> Note('C-2').to_shorthand()\n        'C'\n        >>> Note('C-1').to_shorthand()\n        'C,'\n        \"\"\"\n        if self.octave < 3:\n            res = self.name\n        else:\n            res = str.lower(self.name)\n        o = self.octave - 3\n        while o < -1:\n            res += \",\"\n            o += 1\n        while o > 0:\n            res += \"'\"\n            o -= 1\n        return res\n\n    def from_shorthand(self, shorthand):\n        \"\"\"Convert from traditional Helmhotz pitch notation.\n\n        Examples:\n        >>> Note().from_shorthand(\"C,,\")\n        'C-0'\n        >>> Note().from_shorthand(\"C\")\n        'C-2'\n        >>> Note().from_shorthand(\"c'\")\n        'C-4'\n        \"\"\"\n        name = \"\"\n        octave = 0\n        for x in shorthand:\n            if x in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]:\n                name = str.upper(x)\n                octave = 3\n            elif x in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n                name = x\n                octave = 2\n            elif x in [\"#\", \"b\"]:\n                name += x\n            elif x == \",\":\n                octave -= 1\n            elif x == \"'\":\n                octave += 1\n        return self.set_note(name, octave, {})\n\n    def __int__(self):\n        \"\"\"Return the current octave multiplied by twelve and add\n        notes.note_to_int to it.\n        \n        This means a C-0 returns 0, C-1 returns 12, etc. This method allows\n        you to use int() on Notes.\n        \"\"\"\n        res = self.octave * 12 + notes.note_to_int(self.name[0])\n        for n in self.name[1:]:\n            if n == \"#\":\n                res += 1\n            elif n == \"b\":\n                res -= 1\n        return res\n\n    def __lt__(self, other):\n        \"\"\"Enable the comparing operators on Notes (>, <, \\ ==, !=, >= and <=).\n\n        So we can sort() Intervals, etc.\n\n        Examples:\n        >>> Note('C', 4) < Note('B', 4)\n        True\n        >>> Note('C', 4) > Note('B', 4)\n        False\n        \"\"\"\n        if other is None:\n            return False\n        return int(self) < int(other)\n\n    def __eq__(self, other):\n        \"\"\"Compare Notes for equality by comparing their note values.\"\"\"\n        if other is None:\n            return False\n        return int(self) == int(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __gt__(self, other):\n        return not (self < other or self == other)\n\n    def __le__(self, other):\n        return self < other or self == other\n\n    def __ge__(self, other):\n        return not self < other\n\n    def __repr__(self):\n        \"\"\"Return a helpful representation for printing Note classes.\"\"\"\n        return \"'%s-%d'\" % (self.name, self.octave)\n", "test_list": ["def test_to_hertz(self):\n    self.assertEqual(Note('A', 0).to_hertz(), 27.5)\n    self.assertEqual(Note('A', 1).to_hertz(), 55)\n    self.assertEqual(Note('A', 2).to_hertz(), 110)\n    self.assertEqual(Note('A', 3).to_hertz(), 220)\n    self.assertEqual(Note('A', 4).to_hertz(), 440)\n    self.assertEqual(Note('A', 5).to_hertz(), 880)\n    self.assertEqual(Note('A', 6).to_hertz(), 1760)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'to_hertz' function should return a float value representing the frequency in Hertz for a valid Note instance. The input Note instance should have valid 'name' and 'octave' attributes.", "unit_test": ["def test_to_hertz_input_output(self):\n    note = Note('C', 4)\n    result = note.to_hertz()\n    self.assertIsInstance(result, float)\n    self.assertGreater(result, 0)"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_input_output"}, "Exception Handling": {"requirement": "The 'to_hertz' function should raise a ValueError if the Note instance has an invalid note name.", "unit_test": ["def test_to_hertz_invalid_note_name(self):\n    with self.assertRaises(ValueError):\n        Note('H', 4).to_hertz()"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_invalid_note_name"}, "Edge Case Handling": {"requirement": "The 'to_hertz' function should correctly handle edge cases such as the lowest and highest possible octaves for a note.", "unit_test": ["def test_to_hertz_edge_cases(self):\n    self.assertEqual(Note('C', 0).to_hertz(), 16.35)\n    self.assertEqual(Note('B', 8).to_hertz(), 7902.13)"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'to_hertz' function to accept an optional 'standard_pitch' parameter to allow conversion based on different tuning standards.", "unit_test": ["def test_to_hertz_with_standard_pitch(self):\n    self.assertAlmostEqual(Note('A', 4).to_hertz(standard_pitch=432), 432.0, places=1)"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_with_standard_pitch"}, "Annotation Coverage": {"requirement": "Ensure that the 'to_hertz' function has complete docstring coverage, including parameter types and return type, including two parameters: self: Note, 'standard_pitch': float, and a return type: float.", "unit_test": ["def test_to_hertz_docstring(self):\n    self.assertIn(':param self: Note', Note.to_hertz.__doc__)\n    self.assertIn(':param standard_pitch: float', Note.to_hertz.__doc__)\n    self.assertIn(':return: float', Note.to_hertz.__doc__)"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_docstring"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(Note.to_hertz).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity == 1"], "test": "tests/unit/containers/test_note.py::test_Note::test_code_complexity"}, "Code Standard": {"requirement": "The 'to_hertz' function should adhere to PEP 8 standards, including proper naming conventions and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    code_string = inspect.getsource(Note.to_hertz)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/unit/containers/test_note.py::test_Note::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'to_hertz' function should utilize the '__int__' method of the Note class to convert the note to an integer representation for frequency calculation.", "unit_test": ["def test_to_hertz_context_usage(self):\n    note = Note()\n    with unittest.mock.patch.object(Note, '__int__', return_value=57) as mock_int:\n        note.to_hertz()\n        mock_int.assert_called_once()"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'to_hertz' function should correctly use the '__int__' method to determine the semitone distance from A4 for frequency calculation.", "unit_test": ["def test_to_hertz_context_correctness(self):\n    note = Note()\n    with unittest.mock.patch.object(Note, '__int__', return_value=57):\n        frequency = note.to_hertz()\n        self.assertAlmostEqual(frequency, 440 * 2**((57 - 69) / 12), places=2)"], "test": "tests/unit/containers/test_note.py::test_Note::test_to_hertz_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "datasette.utils.asgi.Request.fake", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/utils/asgi.py", "signature_position": [128, 128], "body_position": [130, 142], "dependency": {"intra_class": ["datasette.utils.asgi.Request.__init__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function is a class method that creates a fake Request object for testing purposes. It takes in parameters such as the path with query string, method, scheme, and url variables, and constructs a Request object with the given values.", "Arguments": ":param cls: Class. The class itself.\n:param path_with_query_string: String. The path with query string for the Request object.\n:param method: String. The HTTP method for the Request object. Defaults to \"GET\" if not specified.\n:param scheme: String. The scheme for the Request object. Defaults to \"http\" if not specified.\n:param url_vars: Dictionary. The URL variables for the Request object. Defaults to None if not specified.\n:return: Request. The created Request object."}, "tests": ["tests/test_filters.py::test_through_filters_from_request", "tests/test_facets.py::test_array_facet_results", "tests/test_facets.py::test_column_facet_suggest_skip_if_enabled_by_metadata", "tests/test_utils.py::test_path_with_removed_args", "tests/test_facets.py::test_date_facet_results"], "indent": 4, "domain": "Database", "code": "\n    @classmethod\n    def fake(cls, path_with_query_string, method=\"GET\", scheme=\"http\", url_vars=None):\n        \"\"\"Useful for constructing Request objects for tests\"\"\"\n        path, _, query_string = path_with_query_string.partition(\"?\")\n        scope = {\n            \"http_version\": \"1.1\",\n            \"method\": method,\n            \"path\": path,\n            \"raw_path\": path_with_query_string.encode(\"latin-1\"),\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"scheme\": scheme,\n            \"type\": \"http\",\n        }\n        if url_vars:\n", "context": "class Request:\n    def __init__(self, scope, receive):\n        self.scope = scope\n        self.receive = receive\n\n    def __repr__(self):\n        return '<asgi.Request method=\"{}\" url=\"{}\">'.format(self.method, self.url)\n\n    @property\n    def method(self):\n        return self.scope[\"method\"]\n\n    @property\n    def url(self):\n        return urlunparse(\n            (self.scheme, self.host, self.path, None, self.query_string, None)\n        )\n\n    @property\n    def url_vars(self):\n        return (self.scope.get(\"url_route\") or {}).get(\"kwargs\") or {}\n\n    @property\n    def scheme(self):\n        return self.scope.get(\"scheme\") or \"http\"\n\n    @property\n    def headers(self):\n        return {\n            k.decode(\"latin-1\").lower(): v.decode(\"latin-1\")\n            for k, v in self.scope.get(\"headers\") or []\n        }\n\n    @property\n    def host(self):\n        return self.headers.get(\"host\") or \"localhost\"\n\n    @property\n    def cookies(self):\n        cookies = SimpleCookie()\n        cookies.load(self.headers.get(\"cookie\", \"\"))\n        return {key: value.value for key, value in cookies.items()}\n\n    @property\n    def path(self):\n        if self.scope.get(\"raw_path\") is not None:\n            return self.scope[\"raw_path\"].decode(\"latin-1\").partition(\"?\")[0]\n        else:\n            path = self.scope[\"path\"]\n            if isinstance(path, str):\n                return path\n            else:\n                return path.decode(\"utf-8\")\n\n    @property\n    def query_string(self):\n        return (self.scope.get(\"query_string\") or b\"\").decode(\"latin-1\")\n\n    @property\n    def full_path(self):\n        qs = self.query_string\n        return \"{}{}\".format(self.path, (\"?\" + qs) if qs else \"\")\n\n    @property\n    def args(self):\n\n        # parsed_params = MultiParams()\n        # query_string = self.scope.get('query_string', b'').decode('utf-8')\n        # if query_string:\n        #     for key, value in parse_qs(query_string).items():\n        #         parsed_params.add(key, value)\n        # return parsed_params\n        return MultiParams(parse_qs(qs=self.query_string, keep_blank_values=True))\n\n\n    @property\n    def actor(self):\n        return self.scope.get(\"actor\", None)\n\n    async def post_body(self):\n        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            assert message[\"type\"] == \"http.request\", message\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body\n\n    async def post_vars(self):\n        body = await self.post_body()\n        return dict(parse_qsl(body.decode(\"utf-8\"), keep_blank_values=True))\n###The function: fake###            scope[\"url_route\"] = {\"kwargs\": url_vars}\n        return cls(scope, None)\n", "prompt": "Please write a python function called 'fake' base the context. This function is a class method that creates a fake Request object for testing purposes. It takes in parameters such as the path with query string, method, scheme, and url variables, and constructs a Request object with the given values.:param cls: Class. The class itself.\n:param path_with_query_string: String. The path with query string for the Request object.\n:param method: String. The HTTP method for the Request object. Defaults to \"GET\" if not specified.\n:param scheme: String. The scheme for the Request object. Defaults to \"http\" if not specified.\n:param url_vars: Dictionary. The URL variables for the Request object. Defaults to None if not specified.\n:return: Request. The created Request object..\n        The context you need to refer to is as follows: class Request:\n    def __init__(self, scope, receive):\n        self.scope = scope\n        self.receive = receive\n\n    def __repr__(self):\n        return '<asgi.Request method=\"{}\" url=\"{}\">'.format(self.method, self.url)\n\n    @property\n    def method(self):\n        return self.scope[\"method\"]\n\n    @property\n    def url(self):\n        return urlunparse(\n            (self.scheme, self.host, self.path, None, self.query_string, None)\n        )\n\n    @property\n    def url_vars(self):\n        return (self.scope.get(\"url_route\") or {}).get(\"kwargs\") or {}\n\n    @property\n    def scheme(self):\n        return self.scope.get(\"scheme\") or \"http\"\n\n    @property\n    def headers(self):\n        return {\n            k.decode(\"latin-1\").lower(): v.decode(\"latin-1\")\n            for k, v in self.scope.get(\"headers\") or []\n        }\n\n    @property\n    def host(self):\n        return self.headers.get(\"host\") or \"localhost\"\n\n    @property\n    def cookies(self):\n        cookies = SimpleCookie()\n        cookies.load(self.headers.get(\"cookie\", \"\"))\n        return {key: value.value for key, value in cookies.items()}\n\n    @property\n    def path(self):\n        if self.scope.get(\"raw_path\") is not None:\n            return self.scope[\"raw_path\"].decode(\"latin-1\").partition(\"?\")[0]\n        else:\n            path = self.scope[\"path\"]\n            if isinstance(path, str):\n                return path\n            else:\n                return path.decode(\"utf-8\")\n\n    @property\n    def query_string(self):\n        return (self.scope.get(\"query_string\") or b\"\").decode(\"latin-1\")\n\n    @property\n    def full_path(self):\n        qs = self.query_string\n        return \"{}{}\".format(self.path, (\"?\" + qs) if qs else \"\")\n\n    @property\n    def args(self):\n\n        # parsed_params = MultiParams()\n        # query_string = self.scope.get('query_string', b'').decode('utf-8')\n        # if query_string:\n        #     for key, value in parse_qs(query_string).items():\n        #         parsed_params.add(key, value)\n        # return parsed_params\n        return MultiParams(parse_qs(qs=self.query_string, keep_blank_values=True))\n\n\n    @property\n    def actor(self):\n        return self.scope.get(\"actor\", None)\n\n    async def post_body(self):\n        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            assert message[\"type\"] == \"http.request\", message\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body\n\n    async def post_vars(self):\n        body = await self.post_body()\n        return dict(parse_qsl(body.decode(\"utf-8\"), keep_blank_values=True))\n###The function: fake###            scope[\"url_route\"] = {\"kwargs\": url_vars}\n        return cls(scope, None)\n", "test_list": ["@pytest.mark.asyncio\nasync def test_through_filters_from_request(app_client):\n    request = Request.fake('/?_through={\"table\":\"roadside_attraction_characteristics\",\"column\":\"characteristic_id\",\"value\":\"1\"}')\n    filter_args = await through_filters(request=request, datasette=app_client.ds, table='roadside_attractions', database='fixtures')()\n    assert filter_args.where_clauses == ['pk in (select attraction_id from roadside_attraction_characteristics where characteristic_id = :p0)']\n    assert filter_args.params == {'p0': '1'}\n    assert filter_args.human_descriptions == ['roadside_attraction_characteristics.characteristic_id = \"1\"']\n    assert filter_args.extra_context == {}", "@pytest.mark.asyncio\n@pytest.mark.skipif(not detect_json1(), reason='Requires the SQLite json1 module')\nasync def test_array_facet_results(app_client):\n    facet = ArrayFacet(app_client.ds, Request.fake('/?_facet_array=tags'), database='fixtures', sql='select * from facetable', table='facetable')\n    buckets, timed_out = await facet.facet_results()\n    assert [] == timed_out\n    assert [{'name': 'tags', 'type': 'array', 'results': [{'value': 'tag1', 'label': 'tag1', 'count': 2, 'toggle_url': 'http://localhost/?_facet_array=tags&tags__arraycontains=tag1', 'selected': False}, {'value': 'tag2', 'label': 'tag2', 'count': 1, 'toggle_url': 'http://localhost/?_facet_array=tags&tags__arraycontains=tag2', 'selected': False}, {'value': 'tag3', 'label': 'tag3', 'count': 1, 'toggle_url': 'http://localhost/?_facet_array=tags&tags__arraycontains=tag3', 'selected': False}], 'hideable': True, 'toggle_url': '/', 'truncated': False}] == buckets", "@pytest.mark.asyncio\nasync def test_column_facet_suggest_skip_if_enabled_by_metadata(app_client):\n    facet = ColumnFacet(app_client.ds, Request.fake('/'), database='fixtures', sql='select * from facetable', table='facetable', metadata={'facets': ['_city_id']})\n    suggestions = [s['name'] for s in await facet.suggest()]\n    assert ['created', 'planet_int', 'on_earth', 'state', '_neighborhood', 'tags', 'complex_array'] == suggestions", "@pytest.mark.parametrize('path,args,expected', [('/foo?bar=1', {'bar'}, '/foo'), ('/foo?bar=1&baz=2', {'bar'}, '/foo?baz=2'), ('/foo?bar=1&bar=2&bar=3', {'bar': '2'}, '/foo?bar=1&bar=3')])\ndef test_path_with_removed_args(path, args, expected):\n    request = Request.fake(path)\n    actual = utils.path_with_removed_args(request, args)\n    assert expected == actual\n    request = Request.fake('/')\n    actual = utils.path_with_removed_args(request, args, path=path)\n    assert expected == actual", "@pytest.mark.asyncio\nasync def test_date_facet_results(app_client):\n    facet = DateFacet(app_client.ds, Request.fake('/?_facet_date=created'), database='fixtures', sql='select * from facetable', table='facetable')\n    buckets, timed_out = await facet.facet_results()\n    assert [] == timed_out\n    assert [{'name': 'created', 'type': 'date', 'results': [{'value': '2019-01-14', 'label': '2019-01-14', 'count': 4, 'toggle_url': 'http://localhost/?_facet_date=created&created__date=2019-01-14', 'selected': False}, {'value': '2019-01-15', 'label': '2019-01-15', 'count': 4, 'toggle_url': 'http://localhost/?_facet_date=created&created__date=2019-01-15', 'selected': False}, {'value': '2019-01-17', 'label': '2019-01-17', 'count': 4, 'toggle_url': 'http://localhost/?_facet_date=created&created__date=2019-01-17', 'selected': False}, {'value': '2019-01-16', 'label': '2019-01-16', 'count': 3, 'toggle_url': 'http://localhost/?_facet_date=created&created__date=2019-01-16', 'selected': False}], 'hideable': True, 'toggle_url': '/', 'truncated': False}] == buckets"], "requirements": {"Input-Output Conditions": {"requirement": "The 'fake' method should correctly construct a Request object with the specified path_with_query_string, method, scheme, and url_vars, ensuring that all properties of the Request object reflect these inputs accurately.", "unit_test": "@pytest.mark.asyncio\ndef test_fake_method_input_output_conditions():\n    request = Request.fake('/test?query=1', method='POST', scheme='https', url_vars={'var1': 'value1'})\n    assert request.path == '/test'\n    assert request.query_string == 'query=1'\n    assert request.method == 'POST'\n    assert request.scheme == 'https'\n    assert request.url_vars == {'var1': 'value1'}", "test": "tests/test_filters.py::test_fake_method_input_output_conditions"}, "Exception Handling": {"requirement": "The 'fake' method should raise a ValueError if the path_with_query_string is not a valid string or if the method is not a valid HTTP method.", "unit_test": "def test_fake_method_exception_handling():\n    with pytest.raises(ValueError):\n        Request.fake(123)\n    with pytest.raises(ValueError):\n        Request.fake('/test', method='INVALID')", "test": "tests/test_filters.py::test_fake_method_exception_handling"}, "Edge Case Handling": {"requirement": "The 'fake' method should handle edge cases such as an empty path_with_query_string or missing url_vars gracefully, defaulting to reasonable values.", "unit_test": "@pytest.mark.asyncio\ndef test_fake_method_edge_case_handling():\n    request = Request.fake('')\n    assert request.path == ''\n    assert request.query_string == ''\n    assert request.url_vars == {}\n    request = Request.fake('/test', url_vars=None)\n    assert request.url_vars == {}", "test": "tests/test_filters.py::test_fake_method_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'fake' method to accept headers as an optional parameter, allowing the creation of Request objects with custom headers.", "unit_test": "@pytest.mark.asyncio\ndef test_fake_method_functionality_extension():\n    headers = [(b'content-type', b'application/json')]\n    request = Request.fake('/test', headers=headers)\n    assert request.headers['content-type'] == 'application/json'", "test": "tests/test_filters.py::test_fake_method_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'fake' method are annotated with appropriate type hints.", "unit_test": "def test_fake_method_annotation_coverage():\n    from typing import get_type_hints\n    hints = get_type_hints(Request.fake)\n    assert hints['path_with_query_string'] == str\n    assert hints['method'] == str\n    assert hints['scheme'] == str\n    assert hints['url_vars'] == dict\n    assert hints['return'] == Request", "test": "tests/test_filters.py::test_fake_method_annotation_coverage"}, "Code Complexity": {"requirement": "The 'fake' method should maintain a cyclomatic complexity less than 2, indicating a simple, linear function.", "unit_test": ["def test_code_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(Request.fake).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 2"], "test": "tests/test_filters.py::test_code_complexity"}, "Code Standard": {"requirement": "The 'fake' method should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": "def test_fake_method_code_standard():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/request.py'])\n    assert result.total_errors == 0", "test": "tests/test_filters.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'fake' method should utilize the Request class context correctly, ensuring that the constructed Request object behaves as expected.", "unit_test": "@pytest.mark.asyncio\ndef test_fake_method_context_usage_verification():\n    request = Request.fake('/test')\n    assert isinstance(request, Request)\n    assert request.full_path == '/test'", "test": "tests/test_filters.py::test_fake_method_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'fake' method correctly uses the Request class's properties and methods to construct a valid Request object.", "unit_test": "@pytest.mark.asyncio\ndef test_fake_method_context_usage_correctness_verification():\n    request = Request.fake('/test?query=1')\n    assert request.url == 'http://localhost/test?query=1'\n    assert request.headers == {}", "test": "tests/test_filters.py::test_fake_method_context_usage_correctness_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "type": "method", "project_path": "Communications/Wikipedia-API", "completion_path": "Communications/Wikipedia-API/wikipediaapi/__init__.py", "signature_position": [1068, 1068], "body_position": [1069, 1071], "dependency": {"intra_class": ["wikipediaapi.WikipediaPage._called", "wikipediaapi.WikipediaPage.pageid", "wikipediaapi.WikipediaPage.title"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a string representation of a WikipediaPage object. It checks if any recorded methods have been called, and if so, it includes the title, pageid, and ns in the string: \"{title} (id: {page id}, ns: {ns})\". Otherwise, it includes only the title and ns attributes in the string: \"{title} (id: ??, ns: {ns})\"", "Arguments": ":param self: WikipediaPage. An instance of the WikipediaPage class.\n:return: String. The string representation of the WikipediaPage object."}, "tests": ["tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_after_fetching", "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_before_fetching"], "indent": 4, "domain": "Communications", "code": "    def __repr__(self):\n        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n", "context": "class WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns last section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n###The function: __repr__###", "prompt": "Please write a python function called '__repr__' base the context. This function returns a string representation of a WikipediaPage object. It checks if any recorded methods have been called, and if so, it includes the title, pageid, and ns in the string: \"{title} (id: {page id}, ns: {ns})\". Otherwise, it includes only the title and ns attributes in the string: \"{title} (id: ??, ns: {ns})\":param self: WikipediaPage. An instance of the WikipediaPage class.\n:return: String. The string representation of the WikipediaPage object..\n        The context you need to refer to is as follows: class WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns last section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n###The function: __repr__###", "test_list": ["def test_repr_after_fetching(self):\n    page = self.wiki.page('Test_1')\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0)')\n    self.assertEqual(page.pageid, 4)\n    self.assertEqual(repr(page), 'Test 1 (id: 4, ns: 0)')", "def test_repr_before_fetching(self):\n    page = self.wiki.page('Test_1')\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0)')"], "requirements": {"Input-Output Conditions": {"requirement": "The __repr__ method should return a string with the title, pageid, and ns attributes formatted correctly. If pageid is not available, it should be represented as '??'.", "unit_test": ["def test_repr_output_format(self):\n    page = self.wiki.page('Test_1')\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0)')\n    page.pageid = 4\n    self.assertEqual(repr(page), 'Test_1 (id: 4, ns: 0)')"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_output_format"}, "Exception Handling": {"requirement": "The __repr__ method should handle exceptions gracefully if any attribute is missing or invalid, and it should return a default string representation.", "unit_test": ["def test_repr_exception_handling(self):\n    page = self.wiki.page('Test_1')\n    del page._attributes['title']\n    try:\n        repr(page)\n    except Exception as e:\n        self.fail(f'__repr__ raised an exception {e}')"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_exception_handling"}, "Edge Case Handling": {"requirement": "The __repr__ method should handle edge cases where the title or ns is an empty string or None, and it should still return a valid string representation.", "unit_test": ["def test_repr_edge_cases(self):\n    page = self.wiki.page('')\n    page._attributes['ns'] = None\n    self.assertEqual(repr(page), ' (id: ??, ns: None)')"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_edge_cases"}, "Functionality Extension": {"requirement": "Extend the __repr__ method to include the language attribute in the string representation if it is different from the default 'en'.", "unit_test": ["def test_repr_with_language(self):\n    page = self.wiki.page('Test_1')\n    page._attributes['language'] = 'fr'\n    self.assertEqual(repr(page), 'Test_1 (id: ??, ns: 0, lang: fr)')"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_with_language"}, "Annotation Coverage": {"requirement": "Ensure that the __repr__ method has complete annotation coverage, including parameter types and return types.", "unit_test": ["def test_repr_annotation_coverage(self):\n    annotations = WikipediaPage.__repr__.__annotations__\n    self.assertIn('return', annotations)\n    self.assertEqual(annotations['return'], str)"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_annotation_coverage"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity less than 2, indicating a simple, linear function.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    source_code = inspect.getsource(self.__repr__).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 2"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_code_complexity"}, "Code Standard": {"requirement": "The __repr__ method should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    code_string = inspect.getsource(self.__repr__)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the __repr__ method uses the necessary context attributes such as title, pageid, and ns from the WikipediaPage class.", "unit_test": ["def test_repr_context_usage(self):\n    page = self.wiki.page('Test_1')\n    self.assertTrue(hasattr(page, '_attributes'))\n    self.assertIn('title', page._attributes)\n    self.assertIn('ns', page._attributes)"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the __repr__ method correctly uses the context attributes to construct the string representation, particularly checking the _called dictionary for method invocation.", "unit_test": ["def test_repr_context_correctness(self):\n    page = self.wiki.page('Test_1')\n    page._called['info'] = True\n    page.pageid = 4\n    self.assertEqual(repr(page), 'Test_1 (id: 4, ns: 0)')"], "test": "tests/wikipedia_page_test.py::TestWikipediaPage::test_repr_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "type": "method", "project_path": "Security/msticpy", "completion_path": "Security/msticpy/msticpy/config/query_editor.py", "signature_position": [299, 299], "body_position": [301, 305], "dependency": {"intra_class": ["msticpy.config.query_editor.QueryParameterEditWidget._blank_parameter", "msticpy.config.query_editor.QueryParameterEditWidget._changed_data", "msticpy.config.query_editor.QueryParameterEditWidget.param_container", "msticpy.config.query_editor.QueryParameterEditWidget.parameter_dropdown"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function deletes a parameter item from the QueryParameterEditWidget instance. It removes the parameter from the parameters dictionary and clears the input widgets. It also sets the changed data flag to True.", "Arguments": ":param self: QueryParameterEditWidget. An instance of the QueryParameterEditWidget class.\n:param button: The button that triggered the delete action. It is not used in the function.\n:return: No return values."}, "tests": ["tests/config/test_query_editor.py::test_remove_parameter"], "indent": 4, "domain": "Security", "code": "    def delete_parameter(self, button):\n        \"\"\"Delete parameter item.\"\"\"\n        del button\n        del self.param_container.parameters[self.parameter_dropdown.value]\n        # Clear the input widgets\n        self._blank_parameter()\n        self._changed_data = True\n", "context": "class QueryParameterEditWidget(IPyDisplayMixin):\n    \"\"\"Class to manage editing of query parameters.\n\n    This class provides a graphical user interface for editing query parameters.\n    It allows users to add, modify, and delete parameters, as well as specify\n    their names, descriptions, types, default values, and whether they are mandatory.\n\n    Attributes\n    ----------\n    _changed_data : bool\n        A flag indicating whether the data has been changed.\n    param_container : Union[Query, QueryDefaults]\n        The container for the query parameters.\n    parameter_dropdown : ipywidgets.Select\n        A widget for selecting a parameter to edit.\n    parameter_name_widget : ipywidgets.Text\n        A widget for editing the name of a parameter.\n    description_widget : ipywidgets.Text\n        A widget for editing the description of a parameter.\n    type_widget : ipywidgets.Dropdown\n        A widget for selecting the type of a parameter.\n    default_reqd_widget : ipywidgets.Checkbox\n        A widget for indicating whether a default value is required for a parameter.\n\n    \"\"\"\n\n    def __init__(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Initialize the class.\"\"\"\n        self._changed_data = False\n        self.param_container = container\n        self.parameter_dropdown = widgets.Select(\n            description=\"Parameters\",\n            size=5,\n            options=list(\n                self.param_container.parameters.keys()\n                if self.param_container.parameters\n                else []\n            ),\n            **sel_fmt(height=\"100px\"),\n        )\n        # Create widgets for the Parameter fields\n        self.parameter_name_widget = widgets.Text(description=\"Name\", **txt_fmt())\n        self.description_widget = widgets.Text(description=\"Description\", **txt_fmt())\n        self.type_widget = widgets.Dropdown(\n            description=\"Type\", options=_PARAM_OPTIONS, **sel_fmt(height=\"30px\")\n        )\n        self.default_reqd_widget = widgets.Checkbox(description=\"Use a default value\")\n        self.default_widget = widgets.Text(description=\"Default Value\", **txt_fmt())\n\n        # Create buttons\n        self.add_parameter_button = widgets.Button(description=\"New Parameter\")\n        self.save_parameter_button = widgets.Button(description=\"Save Parameter\")\n        self.delete_parameter_button = widgets.Button(description=\"Delete Parameter\")\n\n        # Attach the functions to buttons\n        self.add_parameter_button.on_click(self.add_parameter)\n        self.save_parameter_button.on_click(self.save_parameter)\n        self.delete_parameter_button.on_click(self.delete_parameter)\n        self.parameter_dropdown.observe(self.populate_widgets, names=\"value\")\n\n        # Create a widget for adding, editing, and deleting Parameters\n        self.layout = widgets.VBox(\n            [\n                widgets.HBox(\n                    [\n                        self.parameter_dropdown,\n                        widgets.VBox(\n                            [\n                                self.add_parameter_button,\n                                self.delete_parameter_button,\n                            ]\n                        ),\n                    ]\n                ),\n                widgets.VBox(\n                    children=[\n                        self.parameter_name_widget,\n                        self.description_widget,\n                        self.type_widget,\n                        widgets.HBox([self.default_reqd_widget, self.default_widget]),\n                        self.save_parameter_button,\n                    ],\n                    **box_layout(),\n                ),\n            ]\n        )\n        if self.param_container and self.param_container.parameters:\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n\n    @property\n    def changed_data(self):\n        \"\"\"Return True if data has changed.\"\"\"\n        return self._changed_data\n\n    def reset_changed_data(self):\n        \"\"\"Reset changed data flag.\"\"\"\n        self._changed_data = False\n\n    def set_param_container(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Set the parameter container.\"\"\"\n        self.param_container = container\n        if self.param_container and self.param_container.parameters:\n            self.parameter_dropdown.options = list(\n                self.param_container.parameters.keys()\n            )\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n        else:\n            self.parameter_dropdown.options = []\n            self._blank_parameter()\n\n    # Define a function to add a new Parameter to the selected Query\n    def add_parameter(self, button):\n        \"\"\"Add a new parameter.\"\"\"\n        del button\n        # Clear the input widgets\n        self._blank_parameter()\n        self.parameter_name_widget.value = \"new_parameter\"\n\n    def _blank_parameter(self):\n        \"\"\"Clear the parameter widgets.\"\"\"\n        self.parameter_name_widget.value = \"\"\n        self.description_widget.value = \"\"\n        self.type_widget.value = _PARAM_OPTIONS[0]\n        self.default_widget.value = \"\"\n        self.default_reqd_widget.value = False\n\n    # Define a function to populate the Parameter widgets with the values of the selected Parameter\n    def populate_widgets(self, change):\n        \"\"\"Populate parameter value in widgets.\"\"\"\n        parameter = self.param_container.parameters[change.new]\n        self.parameter_name_widget.value = change.new\n        self.description_widget.value = parameter.description\n        self.type_widget.value = parameter.datatype\n        self.default_reqd_widget.value = parameter.default is not None\n        self.default_widget.value = parameter.default or \"\"\n\n    # Define a function to edit the selected Parameter with the values from the widgets\n    def save_parameter(self, button):\n        \"\"\"Save currently edited parameter.\"\"\"\n        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        parameter = QueryParameter(\n            description=self.description_widget.value,\n            datatype=self.type_widget.value or _PARAM_OPTIONS[0],\n        )\n        parameter.default = (\n            self.default_widget.value if self.default_reqd_widget.value else None\n        )\n        self.param_container.parameters[param_name] = parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n\n    # Define a function to delete the selected Parameter from the selected Query\n###The function: delete_parameter###", "prompt": "Please write a python function called 'delete_parameter' base the context. This function deletes a parameter item from the QueryParameterEditWidget instance. It removes the parameter from the parameters dictionary and clears the input widgets. It also sets the changed data flag to True.:param self: QueryParameterEditWidget. An instance of the QueryParameterEditWidget class.\n:param button: The button that triggered the delete action. It is not used in the function.\n:return: No return values..\n        The context you need to refer to is as follows: class QueryParameterEditWidget(IPyDisplayMixin):\n    \"\"\"Class to manage editing of query parameters.\n\n    This class provides a graphical user interface for editing query parameters.\n    It allows users to add, modify, and delete parameters, as well as specify\n    their names, descriptions, types, default values, and whether they are mandatory.\n\n    Attributes\n    ----------\n    _changed_data : bool\n        A flag indicating whether the data has been changed.\n    param_container : Union[Query, QueryDefaults]\n        The container for the query parameters.\n    parameter_dropdown : ipywidgets.Select\n        A widget for selecting a parameter to edit.\n    parameter_name_widget : ipywidgets.Text\n        A widget for editing the name of a parameter.\n    description_widget : ipywidgets.Text\n        A widget for editing the description of a parameter.\n    type_widget : ipywidgets.Dropdown\n        A widget for selecting the type of a parameter.\n    default_reqd_widget : ipywidgets.Checkbox\n        A widget for indicating whether a default value is required for a parameter.\n\n    \"\"\"\n\n    def __init__(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Initialize the class.\"\"\"\n        self._changed_data = False\n        self.param_container = container\n        self.parameter_dropdown = widgets.Select(\n            description=\"Parameters\",\n            size=5,\n            options=list(\n                self.param_container.parameters.keys()\n                if self.param_container.parameters\n                else []\n            ),\n            **sel_fmt(height=\"100px\"),\n        )\n        # Create widgets for the Parameter fields\n        self.parameter_name_widget = widgets.Text(description=\"Name\", **txt_fmt())\n        self.description_widget = widgets.Text(description=\"Description\", **txt_fmt())\n        self.type_widget = widgets.Dropdown(\n            description=\"Type\", options=_PARAM_OPTIONS, **sel_fmt(height=\"30px\")\n        )\n        self.default_reqd_widget = widgets.Checkbox(description=\"Use a default value\")\n        self.default_widget = widgets.Text(description=\"Default Value\", **txt_fmt())\n\n        # Create buttons\n        self.add_parameter_button = widgets.Button(description=\"New Parameter\")\n        self.save_parameter_button = widgets.Button(description=\"Save Parameter\")\n        self.delete_parameter_button = widgets.Button(description=\"Delete Parameter\")\n\n        # Attach the functions to buttons\n        self.add_parameter_button.on_click(self.add_parameter)\n        self.save_parameter_button.on_click(self.save_parameter)\n        self.delete_parameter_button.on_click(self.delete_parameter)\n        self.parameter_dropdown.observe(self.populate_widgets, names=\"value\")\n\n        # Create a widget for adding, editing, and deleting Parameters\n        self.layout = widgets.VBox(\n            [\n                widgets.HBox(\n                    [\n                        self.parameter_dropdown,\n                        widgets.VBox(\n                            [\n                                self.add_parameter_button,\n                                self.delete_parameter_button,\n                            ]\n                        ),\n                    ]\n                ),\n                widgets.VBox(\n                    children=[\n                        self.parameter_name_widget,\n                        self.description_widget,\n                        self.type_widget,\n                        widgets.HBox([self.default_reqd_widget, self.default_widget]),\n                        self.save_parameter_button,\n                    ],\n                    **box_layout(),\n                ),\n            ]\n        )\n        if self.param_container and self.param_container.parameters:\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n\n    @property\n    def changed_data(self):\n        \"\"\"Return True if data has changed.\"\"\"\n        return self._changed_data\n\n    def reset_changed_data(self):\n        \"\"\"Reset changed data flag.\"\"\"\n        self._changed_data = False\n\n    def set_param_container(self, container: Union[Query, QueryDefaults]):\n        \"\"\"Set the parameter container.\"\"\"\n        self.param_container = container\n        if self.param_container and self.param_container.parameters:\n            self.parameter_dropdown.options = list(\n                self.param_container.parameters.keys()\n            )\n            init_change = CustomChange(new=next(iter(self.param_container.parameters)))\n            self.populate_widgets(init_change)\n        else:\n            self.parameter_dropdown.options = []\n            self._blank_parameter()\n\n    # Define a function to add a new Parameter to the selected Query\n    def add_parameter(self, button):\n        \"\"\"Add a new parameter.\"\"\"\n        del button\n        # Clear the input widgets\n        self._blank_parameter()\n        self.parameter_name_widget.value = \"new_parameter\"\n\n    def _blank_parameter(self):\n        \"\"\"Clear the parameter widgets.\"\"\"\n        self.parameter_name_widget.value = \"\"\n        self.description_widget.value = \"\"\n        self.type_widget.value = _PARAM_OPTIONS[0]\n        self.default_widget.value = \"\"\n        self.default_reqd_widget.value = False\n\n    # Define a function to populate the Parameter widgets with the values of the selected Parameter\n    def populate_widgets(self, change):\n        \"\"\"Populate parameter value in widgets.\"\"\"\n        parameter = self.param_container.parameters[change.new]\n        self.parameter_name_widget.value = change.new\n        self.description_widget.value = parameter.description\n        self.type_widget.value = parameter.datatype\n        self.default_reqd_widget.value = parameter.default is not None\n        self.default_widget.value = parameter.default or \"\"\n\n    # Define a function to edit the selected Parameter with the values from the widgets\n    def save_parameter(self, button):\n        \"\"\"Save currently edited parameter.\"\"\"\n        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        parameter = QueryParameter(\n            description=self.description_widget.value,\n            datatype=self.type_widget.value or _PARAM_OPTIONS[0],\n        )\n        parameter.default = (\n            self.default_widget.value if self.default_reqd_widget.value else None\n        )\n        self.param_container.parameters[param_name] = parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n\n    # Define a function to delete the selected Parameter from the selected Query\n###The function: delete_parameter###", "test_list": ["def test_remove_parameter(query):\n    \"\"\"Test removing a parameter.\"\"\"\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert len(editor.param_container.parameters) == 1\n    editor.delete_parameter(None)\n    assert len(editor.param_container.parameters) == 0"], "requirements": {"Input-Output Conditions": {"requirement": "The delete_parameter function should remove the valid parameters. If parameters are not valid, please raise TypeError.", "unit_test": ["def test_delete_parameter_updates_dropdown(query):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert 'new_parameter' in editor.parameter_dropdown.options\n    editor.delete_parameter(None)\n    assert 'new_parameter' not in editor.parameter_dropdown.options"], "test": "tests/config/test_query_editor.py::test_remove_parameter"}, "Exception Handling": {"requirement": "The delete_parameter function should handle cases where no parameter is selected gracefully, without raising an exception.", "unit_test": ["def test_delete_parameter_no_selection(query):\n    editor = QueryParameterEditWidget(query)\n    try:\n        editor.delete_parameter(None)\n        assert True  # No exception should be raised\n    except Exception:\n        assert False, 'Exception was raised when no parameter was selected'"], "test": "tests/config/test_query_editor.py::test_delete_parameter_no_selection"}, "Edge Case Handling": {"requirement": "The delete_parameter function should handle the case where the parameters dictionary is empty without errors.", "unit_test": ["def test_delete_parameter_empty_dict(query):\n    editor = QueryParameterEditWidget(query)\n    editor.param_container.parameters.clear()\n    try:\n        editor.delete_parameter(None)\n        assert True  # No exception should be raised\n    except Exception:\n        assert False, 'Exception was raised with empty parameters dictionary'"], "test": "tests/config/test_query_editor.py::test_delete_parameter_empty_dict"}, "Functionality Extension": {"requirement": "Extend the delete_parameter function to log a message:'Deleted parameter: new_parameter' in caplog.text' indicating which parameter was deleted.", "unit_test": ["def test_delete_parameter_logs_message(query, caplog):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    with caplog.at_level(logging.INFO):\n        editor.delete_parameter(None)\n    assert 'Deleted parameter: new_parameter' in caplog.text"], "test": "tests/config/test_query_editor.py::test_delete_parameter_logs_message"}, "Annotation Coverage": {"requirement": "Ensure that the delete_parameter function includes type annotations for its parameters, including parameters: 'button': Any.", "unit_test": ["def test_delete_parameter_annotations():\n    import inspect\n    sig = inspect.signature(QueryParameterEditWidget.delete_parameter)\n    assert sig.parameters['button'].annotation == Any"], "test": "tests/config/test_query_editor.py::test_delete_parameter_annotations"}, "Code Complexity": {"requirement": "The method should maintain a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    from pyramid.registry import Introspector\n    source_code = inspect.getsource(Introspector.remove).strip()\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity == 1"], "test": "tests/config/test_query_editor.py::test_code_complexity"}, "Code Standard": {"requirement": "The delete_parameter function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(Introspector.remove)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/config/test_query_editor.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The delete_parameter function should utilize the parameter_dropdown and param_container attributes of the QueryParameterEditWidget class.", "unit_test": ["def test_delete_parameter_context_usage(query):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert 'new_parameter' in editor.param_container.parameters\n    editor.delete_parameter(None)\n    assert 'new_parameter' not in editor.param_container.parameters"], "test": "tests/config/test_query_editor.py::test_delete_parameter_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The delete_parameter function should correctly update the _changed_data flag to True after a parameter is deleted.", "unit_test": ["def test_delete_parameter_changed_data_flag(query):\n    editor = QueryParameterEditWidget(query)\n    editor.add_parameter(None)\n    _add_test_param(editor)\n    editor.save_parameter(None)\n    assert not editor.changed_data\n    editor.delete_parameter(None)\n    assert editor.changed_data"], "test": "tests/config/test_query_editor.py::test_delete_parameter_changed_data_flag"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "dash.development.base_component.Component._traverse", "type": "method", "project_path": "Software-Development/dash", "completion_path": "Software-Development/dash/dash/development/base_component.py", "signature_position": [319, 319], "body_position": [321, 322], "dependency": {"intra_class": ["dash.development.base_component.Component._traverse_with_paths"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function traverses the tree structure of a Component instance and yields the second value in each item in the tree.", "Arguments": ":param self: Component. An instance of the Component class.\n:return: Yields each item in the tree."}, "tests": ["tests/unit/development/test_base_component.py::test_debc011_traverse_with_tuples", "tests/unit/development/test_base_component.py::test_debc010_traverse_full_tree"], "indent": 4, "domain": "Software-Development", "code": "    def _traverse(self):\n        \"\"\"Yield each item in the tree.\"\"\"\n        for t in self._traverse_with_paths():\n            yield t[1]\n", "context": "class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n    def to_plotly_json(self):\n        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n###The function: _traverse###\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "prompt": "Please write a python function called '_traverse' base the context. This function traverses the tree structure of a Component instance and yields the second value in each item in the tree.:param self: Component. An instance of the Component class.\n:return: Yields each item in the tree..\n        The context you need to refer to is as follows: class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n    def to_plotly_json(self):\n        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n###The function: _traverse###\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "test_list": ["def test_debc011_traverse_with_tuples():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = tuple(c2.children)\n    c.children = tuple(c.children)\n    elements = [i for i in c._traverse()]\n    assert elements == list(c.children) + [c3] + [c2] + list(c2.children)", "def test_debc010_traverse_full_tree():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    elements = [i for i in c._traverse()]\n    assert elements == c.children + [c3] + [c2] + c2.children"], "requirements": {"Input-Output Conditions": {"requirement": "The `_traverse` function should be defined as a generator method on the `Component` class. It must yield the second element from each `(path, component)` tuple produced by the `_traverse_with_paths` method. The return type must be a generator yielding Component instances or their children in traversal order.", "unit_test": ["def test_traverse_basic():\n    parent = Component(children=[Component(id=\"a\"), Component(id=\"b\")])\n    for comp in parent._traverse():\n        assert isinstance(comp, Component)"], "test": "tests/unit/development/test_base_component.py::test_traverse_basic"}, "Exception Handling": {"requirement": "The '_traverse' function should raise a TypeError if any item in the tree is not a tuple or does not have at least two elements.", "unit_test": ["def test_traverse_raises_type_error():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a'), (2,)]  # Second item is not a tuple with two elements\n    c.children = [(4, 'd'), 'not a tuple']\n    try:\n        elements = [i for i in c._traverse()]\n    except TypeError as e:\n        assert str(e) == 'Each item in the tree must be a tuple with at least two elements.'"], "test": "tests/unit/development/test_base_component.py::test_traverse_correct_order"}, "Edge Case Handling": {"requirement": "The '_traverse' function should handle an empty tree gracefully by yielding nothing.", "unit_test": ["def test_traverse_empty_tree():\n    c = Component()\n    c.children = []\n    elements = [i for i in c._traverse()]\n    assert elements == []"], "test": "tests/unit/development/test_base_component.py::test_traverse_empty_tree"}, "Functionality Extension": {"requirement": "Extend the '_traverse' function to accept an optional parameter that specifies which index of the tuple to yield.", "unit_test": ["def test_traverse_with_index_parameter():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a', 'x'), (2, 'b', 'y'), (3, 'c', 'z')]\n    c.children = [(4, 'd', 'w'), (5, 'e', 'v')]\n    elements = [i for i in c._traverse(index=2)]\n    assert elements == ['w', 'v', 'x', 'y', 'z']"], "test": "tests/unit/development/test_base_component.py::test_traverse_with_index_parameter"}, "Annotation Coverage": {"requirement": "Ensure that the '_traverse' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_traverse_annotations():\n    from typing import Generator\n    assert '_traverse' in Component.__dict__\n    assert Component._traverse.__annotations__ == {'return': Generator}"], "test": "tests/unit/development/test_base_component.py::test_traverse_annotations"}, "Code Complexity": {"requirement": "The '_traverse' function should have a cyclomatic complexity of no more than 2.", "unit_test": ["def test_traverse_cyclomatic_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    from textwrap import dedent\n    source =  dedent(inspect.getsource(Component._traverse))\n    complexity = cc_visit(source)\n    assert complexity[0].complexity <= 2"], "test": "tests/unit/development/test_base_component.py::test_traverse_cyclomatic_complexity"}, "Code Standard": {"requirement": "The '_traverse' function should adhere to PEP 8 standards, including proper indentation and line length.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(Component._traverse)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/unit/development/test_base_component.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The '_traverse' function should utilize the '_traverse_with_paths' method from the Component class context.", "unit_test": ["def test_traverse_uses_traverse_with_paths():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a'), (2, 'b')]\n    c.children = [(3, 'c'), (4, 'd')]\n    paths = [p for p, _ in c._traverse_with_paths()]\n    assert all(isinstance(p, str) for p in paths)"], "test": "tests/unit/development/test_base_component.py::test_traverse_uses_traverse_with_paths"}, "Context Usage Correctness Verification": {"requirement": "The '_traverse' function should correctly use the '_traverse_with_paths' method to ensure the correct traversal order.", "unit_test": ["def test_traverse_correct_order():\n    c, c1, c2, c3, c4, c5 = nested_tree()\n    c2.children = [(1, 'a'), (2, 'b')]\n    c.children = [(3, 'c'), (4, 'd')]\n    elements = [i for i in c._traverse()]\n    assert elements == ['c', 'd', 'a', 'b']"], "test": "tests/unit/development/test_base_component.py::test_traverse_correct_order"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "type": "method", "project_path": "Security/pycoin", "completion_path": "Security/pycoin/pycoin/blockchain/BlockChain.py", "signature_position": [61, 61], "body_position": [62, 73], "dependency": {"intra_class": ["pycoin.blockchain.BlockChain.BlockChain._locked_chain", "pycoin.blockchain.BlockChain.BlockChain._longest_chain_cache", "pycoin.blockchain.BlockChain.BlockChain._longest_local_block_chain", "pycoin.blockchain.BlockChain.BlockChain.length", "pycoin.blockchain.BlockChain.BlockChain.parent_hash", "pycoin.blockchain.BlockChain.BlockChain.weight_lookup"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a tuple containing information about a block in the blockchain at the given index. It first checks if the index is negative, and if so, it adjusts it to be a positive index relative to the end of the blockchain. Then, it checks if the index is within the range of the locked chain. If it is, it returns the corresponding block from the locked chain. If the index is outside the range of the locked chain, it retrieves the block from the longest local block chain or the longest chain cache, depending on the index value. Finally, it looks up the weight of the block using the weight lookup dictionary and returns a tuple containing the block's hash, parent hash, and weight.", "Arguments": ":param self: BlockChain. An instance of the BlockChain class.\n:param index: Integer. The index of the block to retrieve.\n:return: Tuple. A tuple containing the block's hash, parent hash, and weight."}, "tests": ["tests/blockchain_test.py::BlockchainTestCase::test_large", "tests/blockchain_test.py::BlockchainTestCase::test_chain_locking", "tests/blockchain_test.py::BlockchainTestCase::test_basic"], "indent": 4, "domain": "Security", "code": "    def tuple_for_index(self, index):\n        if index < 0:\n            index = self.length() + index\n        size = len(self._locked_chain)\n        if index < size:\n            return self._locked_chain[index]\n        index -= size\n\n        longest_chain = self._longest_local_block_chain()\n        the_hash = longest_chain[-index-1]\n        parent_hash = self.parent_hash if index <= 0 else self._longest_chain_cache[-index]\n        weight = self.weight_lookup.get(the_hash)\n        return (the_hash, parent_hash, weight)\n", "context": "class BlockChain(object):\n    def __init__(self, parent_hash=ZERO_HASH, unlocked_block_storage={}, did_lock_to_index_f=None):\n        self.parent_hash = parent_hash\n        self.hash_to_index_lookup = {}\n        self.weight_lookup = {}\n        self.chain_finder = ChainFinder()\n        self.change_callbacks = weakref.WeakSet()\n        self._longest_chain_cache = None\n        self.did_lock_to_index_f = did_lock_to_index_f\n        self.unlocked_block_storage = unlocked_block_storage\n\n        self._locked_chain = []\n\n    def preload_locked_blocks(self, headers_iter):\n        self._locked_chain = []\n        the_hash = self.parent_hash\n        for idx, h in enumerate(headers_iter):\n            the_hash = h.hash()\n            self._locked_chain.append((the_hash, h.previous_block_hash, h.difficulty))\n            self.hash_to_index_lookup[the_hash] = idx\n        self.parent_hash = the_hash\n\n    def is_hash_known(self, the_hash):\n        return the_hash in self.hash_to_index_lookup\n\n    def length(self):\n        return len(self._longest_local_block_chain()) + len(self._locked_chain)\n\n    def locked_length(self):\n        return len(self._locked_chain)\n\n    def unlocked_length(self):\n        return len(self._longest_local_block_chain())\n\n###The function: tuple_for_index###\n    def last_block_hash(self):\n        if self.length() == 0:\n            return self.parent_hash\n        return self.hash_for_index(-1)\n\n    def hash_for_index(self, index):\n        return self.tuple_for_index(index)[0]\n\n    def index_for_hash(self, the_hash):\n        return self.hash_to_index_lookup.get(the_hash)\n\n    def add_change_callback(self, callback):\n        self.change_callbacks.add(callback)\n\n    def lock_to_index(self, index):\n        old_length = len(self._locked_chain)\n        index -= old_length\n        longest_chain = self._longest_local_block_chain()\n        if index < 1:\n            return\n        excluded = set()\n        for idx in range(index):\n            the_hash = longest_chain[-idx-1]\n            parent_hash = self.parent_hash if idx <= 0 else self._longest_chain_cache[-idx]\n            weight = self.weight_lookup.get(the_hash)\n            item = (the_hash, parent_hash, weight)\n            self._locked_chain.append(item)\n            excluded.add(the_hash)\n        if self.did_lock_to_index_f:\n            self.did_lock_to_index_f(self._locked_chain[old_length:old_length+index], old_length)\n        old_chain_finder = self.chain_finder\n        self.chain_finder = ChainFinder()\n        self._longest_chain_cache = None\n\n        def iterate():\n            for tree in old_chain_finder.trees_from_bottom.values():\n                for c in tree:\n                    if c in excluded:\n                        break\n                    excluded.add(c)\n                    if c in old_chain_finder.parent_lookup:\n                        yield (c, old_chain_finder.parent_lookup[c])\n        self.chain_finder.load_nodes(iterate())\n        self.parent_hash = the_hash\n\n    def _longest_local_block_chain(self):\n        if self._longest_chain_cache is None:\n            max_weight = 0\n            longest = []\n            for chain in self.chain_finder.all_chains_ending_at(self.parent_hash):\n                weight = sum(self.weight_lookup.get(h, 0) for h in chain)\n                if weight > max_weight:\n                    longest = chain\n                    max_weight = weight\n            self._longest_chain_cache = longest[:-1]\n        return self._longest_chain_cache\n\n    def block_for_hash(self, h):\n        return self.unlocked_block_storage.get(h)\n\n    def add_headers(self, header_iter):\n        def iterate():\n            for header in header_iter:\n                h = header.hash()\n                self.weight_lookup[h] = header.difficulty\n                self.unlocked_block_storage[h] = header\n                yield h, header.previous_block_hash\n\n        old_longest_chain = self._longest_local_block_chain()\n\n        self.chain_finder.load_nodes(iterate())\n\n        self._longest_chain_cache = None\n        new_longest_chain = self._longest_local_block_chain()\n\n        if old_longest_chain and new_longest_chain:\n            old_path, new_path = self.chain_finder.find_ancestral_path(\n                old_longest_chain[0],\n                new_longest_chain[0]\n            )\n            old_path = old_path[:-1]\n            new_path = new_path[:-1]\n        else:\n            old_path = old_longest_chain\n            new_path = new_longest_chain\n        if old_path:\n            logger.debug(\"old_path is %r-%r\", old_path[0], old_path[-1])\n        if new_path:\n            logger.debug(\"new_path is %r-%r\", new_path[0], new_path[-1])\n            logger.debug(\"block chain now has %d elements\", self.length())\n\n        # return a list of operations:\n        # (\"add\"/\"remove\", the_hash, the_index)\n        ops = []\n        size = len(old_longest_chain) + len(self._locked_chain)\n        for idx, h in enumerate(old_path):\n            op = (\"remove\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            del self.hash_to_index_lookup[h]\n        size = len(new_longest_chain) + len(self._locked_chain)\n        for idx, h in reversed(list(enumerate(new_path))):\n            op = (\"add\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            self.hash_to_index_lookup[h] = size-idx-1\n        for callback in self.change_callbacks:\n            callback(self, ops)\n\n        return ops\n\n    def __repr__(self):\n        local_block_chain = self._longest_local_block_chain()\n        if local_block_chain:\n            finish = b2h_rev(local_block_chain[0])\n            start = b2h_rev(local_block_chain[-1])\n            longest_chain = \"longest chain %s to %s of size %d\" % (start, finish, self.unlocked_length())\n        else:\n            longest_chain = \"no unlocked elements\"\n        return \"<BlockChain with %d locked elements and %s>\" % (self.locked_length(), longest_chain)\n", "prompt": "Please write a python function called 'tuple_for_index' base the context. This function returns a tuple containing information about a block in the blockchain at the given index. It first checks if the index is negative, and if so, it adjusts it to be a positive index relative to the end of the blockchain. Then, it checks if the index is within the range of the locked chain. If it is, it returns the corresponding block from the locked chain. If the index is outside the range of the locked chain, it retrieves the block from the longest local block chain or the longest chain cache, depending on the index value. Finally, it looks up the weight of the block using the weight lookup dictionary and returns a tuple containing the block's hash, parent hash, and weight.:param self: BlockChain. An instance of the BlockChain class.\n:param index: Integer. The index of the block to retrieve.\n:return: Tuple. A tuple containing the block's hash, parent hash, and weight..\n        The context you need to refer to is as follows: class BlockChain(object):\n    def __init__(self, parent_hash=ZERO_HASH, unlocked_block_storage={}, did_lock_to_index_f=None):\n        self.parent_hash = parent_hash\n        self.hash_to_index_lookup = {}\n        self.weight_lookup = {}\n        self.chain_finder = ChainFinder()\n        self.change_callbacks = weakref.WeakSet()\n        self._longest_chain_cache = None\n        self.did_lock_to_index_f = did_lock_to_index_f\n        self.unlocked_block_storage = unlocked_block_storage\n\n        self._locked_chain = []\n\n    def preload_locked_blocks(self, headers_iter):\n        self._locked_chain = []\n        the_hash = self.parent_hash\n        for idx, h in enumerate(headers_iter):\n            the_hash = h.hash()\n            self._locked_chain.append((the_hash, h.previous_block_hash, h.difficulty))\n            self.hash_to_index_lookup[the_hash] = idx\n        self.parent_hash = the_hash\n\n    def is_hash_known(self, the_hash):\n        return the_hash in self.hash_to_index_lookup\n\n    def length(self):\n        return len(self._longest_local_block_chain()) + len(self._locked_chain)\n\n    def locked_length(self):\n        return len(self._locked_chain)\n\n    def unlocked_length(self):\n        return len(self._longest_local_block_chain())\n\n###The function: tuple_for_index###\n    def last_block_hash(self):\n        if self.length() == 0:\n            return self.parent_hash\n        return self.hash_for_index(-1)\n\n    def hash_for_index(self, index):\n        return self.tuple_for_index(index)[0]\n\n    def index_for_hash(self, the_hash):\n        return self.hash_to_index_lookup.get(the_hash)\n\n    def add_change_callback(self, callback):\n        self.change_callbacks.add(callback)\n\n    def lock_to_index(self, index):\n        old_length = len(self._locked_chain)\n        index -= old_length\n        longest_chain = self._longest_local_block_chain()\n        if index < 1:\n            return\n        excluded = set()\n        for idx in range(index):\n            the_hash = longest_chain[-idx-1]\n            parent_hash = self.parent_hash if idx <= 0 else self._longest_chain_cache[-idx]\n            weight = self.weight_lookup.get(the_hash)\n            item = (the_hash, parent_hash, weight)\n            self._locked_chain.append(item)\n            excluded.add(the_hash)\n        if self.did_lock_to_index_f:\n            self.did_lock_to_index_f(self._locked_chain[old_length:old_length+index], old_length)\n        old_chain_finder = self.chain_finder\n        self.chain_finder = ChainFinder()\n        self._longest_chain_cache = None\n\n        def iterate():\n            for tree in old_chain_finder.trees_from_bottom.values():\n                for c in tree:\n                    if c in excluded:\n                        break\n                    excluded.add(c)\n                    if c in old_chain_finder.parent_lookup:\n                        yield (c, old_chain_finder.parent_lookup[c])\n        self.chain_finder.load_nodes(iterate())\n        self.parent_hash = the_hash\n\n    def _longest_local_block_chain(self):\n        if self._longest_chain_cache is None:\n            max_weight = 0\n            longest = []\n            for chain in self.chain_finder.all_chains_ending_at(self.parent_hash):\n                weight = sum(self.weight_lookup.get(h, 0) for h in chain)\n                if weight > max_weight:\n                    longest = chain\n                    max_weight = weight\n            self._longest_chain_cache = longest[:-1]\n        return self._longest_chain_cache\n\n    def block_for_hash(self, h):\n        return self.unlocked_block_storage.get(h)\n\n    def add_headers(self, header_iter):\n        def iterate():\n            for header in header_iter:\n                h = header.hash()\n                self.weight_lookup[h] = header.difficulty\n                self.unlocked_block_storage[h] = header\n                yield h, header.previous_block_hash\n\n        old_longest_chain = self._longest_local_block_chain()\n\n        self.chain_finder.load_nodes(iterate())\n\n        self._longest_chain_cache = None\n        new_longest_chain = self._longest_local_block_chain()\n\n        if old_longest_chain and new_longest_chain:\n            old_path, new_path = self.chain_finder.find_ancestral_path(\n                old_longest_chain[0],\n                new_longest_chain[0]\n            )\n            old_path = old_path[:-1]\n            new_path = new_path[:-1]\n        else:\n            old_path = old_longest_chain\n            new_path = new_longest_chain\n        if old_path:\n            logger.debug(\"old_path is %r-%r\", old_path[0], old_path[-1])\n        if new_path:\n            logger.debug(\"new_path is %r-%r\", new_path[0], new_path[-1])\n            logger.debug(\"block chain now has %d elements\", self.length())\n\n        # return a list of operations:\n        # (\"add\"/\"remove\", the_hash, the_index)\n        ops = []\n        size = len(old_longest_chain) + len(self._locked_chain)\n        for idx, h in enumerate(old_path):\n            op = (\"remove\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            del self.hash_to_index_lookup[h]\n        size = len(new_longest_chain) + len(self._locked_chain)\n        for idx, h in reversed(list(enumerate(new_path))):\n            op = (\"add\", self.block_for_hash(h), size-idx-1)\n            ops.append(op)\n            self.hash_to_index_lookup[h] = size-idx-1\n        for callback in self.change_callbacks:\n            callback(self, ops)\n\n        return ops\n\n    def __repr__(self):\n        local_block_chain = self._longest_local_block_chain()\n        if local_block_chain:\n            finish = b2h_rev(local_block_chain[0])\n            start = b2h_rev(local_block_chain[-1])\n            longest_chain = \"longest chain %s to %s of size %d\" % (start, finish, self.unlocked_length())\n        else:\n            longest_chain = \"no unlocked elements\"\n        return \"<BlockChain with %d locked elements and %s>\" % (self.locked_length(), longest_chain)\n", "test_list": ["def test_large(self):\n    SIZE = 3000\n    ITEMS = [FakeBlock(i) for i in range(SIZE)]\n    ITEMS[0] = FakeBlock(0, parent_for_0)\n    BC = BlockChain(parent_for_0)\n    assert longest_block_chain(BC) == []\n    assert BC.locked_length() == 0\n    assert BC.length() == 0\n    assert set(BC.chain_finder.missing_parents()) == set()\n    ops = BC.add_headers(ITEMS)\n    assert ops == [('add', ITEMS[i], i) for i in range(SIZE)]\n    assert longest_block_chain(BC) == list(range(SIZE))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == SIZE\n    for i in range(SIZE):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None", "def test_chain_locking(self):\n    SIZE = 2000\n    COUNT = 200\n    ITEMS = [FakeBlock(i, i - 1) for i in range(SIZE * COUNT)]\n    ITEMS[0] = FakeBlock(0, parent_for_0)\n    BC = BlockChain(parent_for_0)\n    assert longest_block_chain(BC) == []\n    assert BC.locked_length() == 0\n    assert BC.length() == 0\n    assert set(BC.chain_finder.missing_parents()) == set()\n    for i in range(COUNT):\n        start, end = (i * SIZE, (i + 1) * SIZE)\n        lock_start = max(0, start - 10)\n        expected_parent = lock_start - 1 if lock_start else parent_for_0\n        assert BC.length() == start\n        assert BC.locked_length() == lock_start\n        ops = BC.add_headers(ITEMS[start:end])\n        assert ops == [('add', ITEMS[i], i) for i in range(start, end)]\n        assert longest_locked_block_chain(BC) == list(range(lock_start, end))\n        assert set(BC.chain_finder.missing_parents()) == {expected_parent}\n        assert BC.parent_hash == expected_parent\n        assert BC.locked_length() == lock_start\n        assert BC.length() == end\n        for i in range(start, end):\n            v = BC.tuple_for_index(i)\n            assert v[0] == i\n            assert v[1] == parent_for_0 if i == 0 else i\n        assert BC.index_for_hash(-1) is None\n        assert BC.locked_length() == max(0, lock_start)\n        BC.lock_to_index(end - 10)\n        assert BC.locked_length() == end - 10", "def test_basic(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(100)]\n    ITEMS[0] = FakeBlock(0, parent_for_0)\n    assert longest_block_chain(BC) == []\n    assert BC.length() == 0\n    assert BC.locked_length() == 0\n    assert set(BC.chain_finder.missing_parents()) == set()\n    assert BC.parent_hash == parent_for_0\n    assert BC.index_for_hash(0) is None\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[:5])\n    assert ops == [('add', ITEMS[i], i) for i in range(5)]\n    assert BC.parent_hash == parent_for_0\n    assert longest_block_chain(BC) == list(range(5))\n    assert BC.length() == 5\n    assert BC.locked_length() == 0\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    for i in range(5):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[:7])\n    assert ops == [('add', ITEMS[i], i) for i in range(5, 7)]\n    assert BC.parent_hash == parent_for_0\n    assert longest_block_chain(BC) == list(range(7))\n    assert BC.length() == 7\n    assert BC.locked_length() == 0\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    for i in range(7):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[10:14])\n    assert ops == []\n    assert BC.parent_hash == parent_for_0\n    assert longest_block_chain(BC) == [0, 1, 2, 3, 4, 5, 6]\n    assert BC.locked_length() == 0\n    assert BC.locked_length() == 0\n    assert BC.length() == 7\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0, 9}\n    for i in range(7):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[7:10])\n    assert ops == [('add', ITEMS[i], i) for i in range(7, 14)]\n    assert longest_block_chain(BC) == list(range(14))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == 14\n    for i in range(14):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[90:])\n    assert ops == []\n    assert longest_block_chain(BC) == list(range(14))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0, 89}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == 14\n    for i in range(14):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None\n    ops = BC.add_headers(ITEMS[14:90])\n    assert ops == [('add', ITEMS[i], i) for i in range(14, 100)]\n    assert longest_block_chain(BC) == list(range(100))\n    assert set(BC.chain_finder.missing_parents()) == {parent_for_0}\n    assert BC.parent_hash == parent_for_0\n    assert BC.locked_length() == 0\n    assert BC.length() == 100\n    for i in range(100):\n        v = BC.tuple_for_index(i)\n        assert v[0] == i\n        assert v[1] == parent_for_0 if i == 0 else i\n    assert BC.index_for_hash(-1) is None"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'tuple_for_index' should return a tuple of three elements: block's hash, parent hash, and weight. The input 'index' should be an integer, and the function should handle both positive and negative indices correctly.", "unit_test": ["def test_tuple_for_index_output_format(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(10)]\n    BC.add_headers(ITEMS)\n    for i in range(10):\n        result = BC.tuple_for_index(i)\n        assert isinstance(result, tuple)\n        assert len(result) == 3\n        assert isinstance(result[0], int)\n        assert isinstance(result[1], int)\n        assert isinstance(result[2], int)"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_output_format"}, "Exception Handling": {"requirement": "The function 'tuple_for_index' should raise an IndexError with a descriptive message: 'Index out of range for blockchain.'' if the index is out of range of the blockchain.", "unit_test": ["def test_tuple_for_index_out_of_range(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(5)]\n    BC.add_headers(ITEMS)\n    try:\n        BC.tuple_for_index(10)\n    except IndexError as e:\n        assert str(e) == 'Index out of range for blockchain.'"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_out_of_range"}, "Edge Case Handling": {"requirement": "The function 'tuple_for_index' should correctly handle the edge case where the blockchain is empty and return a tuple with None values.", "unit_test": ["def test_tuple_for_index_empty_blockchain(self):\n    BC = BlockChain(parent_for_0)\n    result = BC.tuple_for_index(0)\n    assert result == (None, None, None)"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_empty_blockchain"}, "Functionality Extension": {"requirement": "Extend the 'tuple_for_index' function to accept an optional parameter 'include_timestamp' which, if set to True, includes the block's timestamp in the returned tuple.", "unit_test": ["def test_tuple_for_index_with_timestamp(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i, timestamp=i*1000) for i in range(5)]\n    BC.add_headers(ITEMS)\n    result = BC.tuple_for_index(2, include_timestamp=True)\n    assert len(result) == 4\n    assert result[3] == 2000"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_with_timestamp"}, "Annotation Coverage": {"requirement": "Ensure that the 'tuple_for_index' function has complete type annotations for its parameters and return type, including one parameters: 'index': int, and a return type: tuple.", "unit_test": ["def test_tuple_for_index_annotations(self):\n    annotations = BlockChain.tuple_for_index.__annotations__\n    assert annotations['index'] == int\n    assert annotations['return'] == tuple"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'tuple_for_index' function should not exceed 5.", "unit_test": ["def test_tuple_for_index_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(BlockChain.tuple_for_index)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 5"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_complexity"}, "Code Standard": {"requirement": "The 'tuple_for_index' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(BlockChain.tuple_for_index)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'tuple_for_index' function should utilize the 'length', '_longest_local_block_chain' method of the BlockChain class.", "unit_test": ["def test_tuple_for_index_context_usage(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(5)]\n    BC.add_headers(ITEMS)\n    BC.tuple_for_index(2)\n    assert '_locked_chain' in BC.__dict__\n    assert '_longest_chain_cache' in BC.__dict__\n    assert 'weight_lookup' in BC.__dict__"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'tuple_for_index' function should correctly use the '_locked_chain' for indices within its range and '_longest_chain_cache' for indices outside of it.", "unit_test": ["def test_tuple_for_index_correct_context_usage(self):\n    BC = BlockChain(parent_for_0)\n    ITEMS = [FakeBlock(i) for i in range(5)]\n    BC.add_headers(ITEMS)\n    BC.lock_to_index(2)\n    result_locked = BC.tuple_for_index(1)\n    result_unlocked = BC.tuple_for_index(3)\n    assert result_locked[0] in [block[0] for block in BC._locked_chain]\n    assert result_unlocked[0] in BC._longest_local_block_chain()"], "test": "tests/blockchain_test.py::BlockchainTestCase::test_tuple_for_index_correct_context_usage"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "datasette.utils.asgi.Response.redirect", "type": "method", "project_path": "Database/datasette", "completion_path": "Database/datasette/datasette/utils/asgi.py", "signature_position": [411, 411], "body_position": [412, 414], "dependency": {"intra_class": ["datasette.utils.asgi.Response.__init__"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function creates a redirect response. It sets the \"Location\" header to the specified path and returns a Response instance with the given status code and headers.", "Arguments": ":param cls: Class. The class of the Response instance.\n:param path: String. The path to redirect to.\n:param status: Integer. The status code for the response. It defaults to 302 if not specified.\n:param headers: Dictionary. Additional headers to include in the response. It defaults to an empty dictionary if not specified.\n:return: Response. The created redirect response instance."}, "tests": ["tests/test_internals_response.py::test_response_redirect", "tests/test_internals_response.py::test_response_set_cookie"], "indent": 4, "domain": "Database", "code": "\n    @classmethod\n    def redirect(cls, path, status=302, headers=None):\n        headers = headers or {}\n", "context": "class Response:\n    def __init__(self, body=None, status=200, headers=None, content_type=\"text/plain\"):\n        self.body = body\n        self.status = status\n        self.headers = headers or {}\n        self._set_cookie_headers = []\n        self.content_type = content_type\n\n    async def asgi_send(self, send):\n        headers = {}\n        headers.update(self.headers)\n        headers[\"content-type\"] = self.content_type\n        raw_headers = [\n            [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n            for key, value in headers.items()\n        ]\n        for set_cookie in self._set_cookie_headers:\n            raw_headers.append([b\"set-cookie\", set_cookie.encode(\"utf-8\")])\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": raw_headers,\n            }\n        )\n        body = self.body\n        if not isinstance(body, bytes):\n            body = body.encode(\"utf-8\")\n        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,\n        samesite=\"lax\",\n    ):\n        assert samesite in SAMESITE_VALUES, \"samesite should be one of {}\".format(\n            SAMESITE_VALUES\n        )\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in (\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ):\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n        for prop_name, prop_value in ((\"secure\", secure), (\"httponly\", httponly)):\n            if prop_value:\n                cookie[key][prop_name] = True\n        self._set_cookie_headers.append(cookie.output(header=\"\").strip())\n\n    @classmethod\n    def html(cls, body, status=200, headers=None):\n        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n\n    @classmethod\n    def text(cls, body, status=200, headers=None):\n        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n\n    @classmethod\n    def json(cls, body, status=200, headers=None, default=None):\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n###The function: redirect###        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n", "prompt": "Please write a python function called 'redirect' base the context. This function creates a redirect response. It sets the \"Location\" header to the specified path and returns a Response instance with the given status code and headers.:param cls: Class. The class of the Response instance.\n:param path: String. The path to redirect to.\n:param status: Integer. The status code for the response. It defaults to 302 if not specified.\n:param headers: Dictionary. Additional headers to include in the response. It defaults to an empty dictionary if not specified.\n:return: Response. The created redirect response instance..\n        The context you need to refer to is as follows: class Response:\n    def __init__(self, body=None, status=200, headers=None, content_type=\"text/plain\"):\n        self.body = body\n        self.status = status\n        self.headers = headers or {}\n        self._set_cookie_headers = []\n        self.content_type = content_type\n\n    async def asgi_send(self, send):\n        headers = {}\n        headers.update(self.headers)\n        headers[\"content-type\"] = self.content_type\n        raw_headers = [\n            [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n            for key, value in headers.items()\n        ]\n        for set_cookie in self._set_cookie_headers:\n            raw_headers.append([b\"set-cookie\", set_cookie.encode(\"utf-8\")])\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": raw_headers,\n            }\n        )\n        body = self.body\n        if not isinstance(body, bytes):\n            body = body.encode(\"utf-8\")\n        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,\n        samesite=\"lax\",\n    ):\n        assert samesite in SAMESITE_VALUES, \"samesite should be one of {}\".format(\n            SAMESITE_VALUES\n        )\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in (\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ):\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n        for prop_name, prop_value in ((\"secure\", secure), (\"httponly\", httponly)):\n            if prop_value:\n                cookie[key][prop_name] = True\n        self._set_cookie_headers.append(cookie.output(header=\"\").strip())\n\n    @classmethod\n    def html(cls, body, status=200, headers=None):\n        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n\n    @classmethod\n    def text(cls, body, status=200, headers=None):\n        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n\n    @classmethod\n    def json(cls, body, status=200, headers=None, default=None):\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n###The function: redirect###        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n", "test_list": ["def test_response_redirect():\n    response = Response.redirect('/foo')\n    assert 302 == response.status\n    assert '/foo' == response.headers['Location']", "@pytest.mark.asyncio\nasync def test_response_set_cookie():\n    events = []\n\n    async def send(event):\n        events.append(event)\n    response = Response.redirect('/foo')\n    response.set_cookie('foo', 'bar', max_age=10, httponly=True)\n    await response.asgi_send(send)\n    assert [{'type': 'http.response.start', 'status': 302, 'headers': [[b'Location', b'/foo'], [b'content-type', b'text/plain'], [b'set-cookie', b'foo=bar; HttpOnly; Max-Age=10; Path=/; SameSite=lax']]}, {'type': 'http.response.body', 'body': b''}] == events"], "requirements": {"Input-Output Conditions": {"requirement": "The 'redirect' function should accept a string for 'path', an integer for 'status', and a dictionary for 'headers'. It should return a Response instance with the 'Location' header set to the specified path and the status code set to the specified status.", "unit_test": ["def test_redirect_input_output_conditions():\n    response = Response.redirect('/test', status=301, headers={'X-Test': 'value'})\n    assert isinstance(response, Response)\n    assert response.headers['Location'] == '/test'\n    assert response.status == 301\n    assert response.headers['X-Test'] == 'value'"], "test": "tests/test_internals_response.py::test_redirect_input_output_conditions"}, "Exception Handling": {"requirement": "The 'redirect' function should raise a TypeError if 'path' is not a string, 'status' is not an integer, or 'headers' is not a dictionary.", "unit_test": ["def test_redirect_exception_handling():\n    with pytest.raises(TypeError):\n        Response.redirect(123)\n    with pytest.raises(TypeError):\n        Response.redirect('/test', status='301')\n    with pytest.raises(TypeError):\n        Response.redirect('/test', headers='not-a-dict')"], "test": "tests/test_internals_response.py::test_redirect_exception_handling"}, "Edge Case Handling": {"requirement": "The 'redirect' function should handle edge cases where 'path' is an empty string or 'headers' is None.", "unit_test": ["def test_redirect_edge_case_handling():\n    response = Response.redirect('')\n    assert response.headers['Location'] == ''\n    response = Response.redirect('/test', headers=None)\n    assert response.headers['Location'] == '/test'"], "test": "tests/test_internals_response.py::test_redirect_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'redirect' function to allow setting a default content type of 'text/html' for the redirect response.", "unit_test": ["def test_redirect_functionality_extension():\n    response = Response.redirect('/test', status=301)\n    assert response.content_type == 'text/html; charset=utf-8'"], "test": "tests/test_internals_response.py::test_redirect_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'redirect' function has complete type annotations for all parameters and the return type, including three parameters: 'path': str, 'status': int, 'headers': dict, and a return type: Response.", "unit_test": ["def test_redirect_annotation_coverage():\n    from typing import get_type_hints\n    hints = get_type_hints(Response.redirect)\n    assert hints['path'] == str\n    assert hints['status'] == int\n    assert hints['headers'] == dict\n    assert hints['return'] == Response"], "test": "tests/test_internals_response.py::test_redirect_annotation_coverage"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'redirect' function should not exceed 1.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(Response.redirect)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 1"], "test": "tests/test_internals_response.py::test_code_complexity"}, "Code Standard": {"requirement": "The 'redirect' function should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(BlockChain.tuple_for_index)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_internals_response.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'redirect' function utilizes the 'Response' class from the provided context.", "unit_test": ["def test_redirect_context_usage_verification():\n    response = Response.redirect('/test')\n    assert isinstance(response, Response)"], "test": "tests/test_internals_response.py::test_redirect_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'redirect' function correctly uses the 'Response' class to set the 'Location' header and status code.", "unit_test": ["def test_redirect_context_usage_correctness_verification():\n    response = Response.redirect('/test', status=301)\n    assert response.headers['Location'] == '/test'\n    assert response.status == 301"], "test": "tests/test_internals_response.py::test_redirect_context_usage_correctness_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "type": "method", "project_path": "Communications/PySimpleSOAP", "completion_path": "Communications/PySimpleSOAP/pysimplesoap/simplexml.py", "signature_position": [116, 116], "body_position": [118, 121], "dependency": {"intra_class": ["pysimplesoap.simplexml.SimpleXMLElement.__document"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns the XML representation of the document. If the \"pretty\" parameter is set to False, it returns the XML representation without any formatting. If \"pretty\" is set to True, it returns the XML representation with indentation and line breaks for better readability.", "Arguments": ":param self: SimpleXMLElement. An instance of the SimpleXMLElement class.\n:param filename: String [optional]. The name of the file to save the XML representation. Defaults to None.\n:param pretty: Bool. Whether to format the XML representation with indentation and line breaks. Defaults to False.\n:return: String. The XML representation of the document."}, "tests": ["tests/simplexmlelement_test.py::TestSimpleXMLElement::test_marshall_cdata", "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_to_xml"], "indent": 4, "domain": "Communications", "code": "    def as_xml(self, filename=None, pretty=False):\n        \"\"\"Return the XML representation of the document\"\"\"\n        if not pretty:\n            return self.__document.toxml('UTF-8')\n        else:\n            return self.__document.toprettyxml(encoding='UTF-8')\n", "context": "class SimpleXMLElement(object):\n    \"\"\"Simple XML manipulation (simil PHP)\"\"\"\n\n    def __init__(self, text=None, elements=None, document=None,\n                 namespace=None, prefix=None, namespaces_map={}, jetty=False):\n        \"\"\"\n        :param namespaces_map: How to map our namespace prefix to that given by the client;\n          {prefix: received_prefix}\n        \"\"\"\n        self.__namespaces_map = namespaces_map\n        _rx = \"|\".join(namespaces_map.keys())  # {'external': 'ext', 'model': 'mod'} -> 'external|model'\n        self.__ns_rx = re.compile(r\"^(%s):.*$\" % _rx)  # And now we build an expression ^(external|model):.*$\n                                                       # to find prefixes in all xml nodes i.e.: <model:code>1</model:code>\n                                                       # and later change that to <mod:code>1</mod:code>\n        self.__ns = namespace\n        self.__prefix = prefix\n        self.__jetty = jetty                           # special list support\n\n        if text is not None:\n            try:\n                self.__document = xml.dom.minidom.parseString(text)\n            except:\n                log.error(text)\n                raise\n            self.__elements = [self.__document.documentElement]\n        else:\n            self.__elements = elements\n            self.__document = document\n\n    def add_child(self, name, text=None, ns=True):\n        \"\"\"Adding a child tag to a node\"\"\"\n        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __setattr__(self, tag, text):\n        \"\"\"Add text child tag node (short form)\"\"\"\n        if tag.startswith(\"_\"):\n            object.__setattr__(self, tag, text)\n        else:\n            ##log.debug('__setattr__(%s, %s)', tag, text)\n            self.add_child(tag, text)\n\n    def __delattr__(self, tag):\n        \"\"\"Remove a child tag (non recursive!)\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        for element in elements:\n            self._element.removeChild(element)\n\n    def add_comment(self, data):\n        \"\"\"Add an xml comment to this child\"\"\"\n        comment = self.__document.createComment(data)\n        self._element.appendChild(comment)\n\n###The function: as_xml###\n    if sys.version > '3':\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            return self._element.toxml()\n    else:\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            # NOTE: do not use self.as_xml('UTF-8') as it returns the whole xml doc\n            return self._element.toxml('UTF-8')\n\n    def get_name(self):\n        \"\"\"Return the tag name of this node\"\"\"\n        return self._element.tagName\n\n    def get_local_name(self):\n        \"\"\"Return the tag local name (prefix:name) of this node\"\"\"\n        return self._element.localName\n\n    def get_prefix(self):\n        \"\"\"Return the namespace prefix of this node\"\"\"\n        return self._element.prefix\n\n    def get_namespace_uri(self, ns):\n        \"\"\"Return the namespace uri for a prefix\"\"\"\n        element = self._element\n        while element is not None and element.attributes is not None:\n            try:\n                return element.attributes['xmlns:%s' % ns].value\n            except KeyError:\n                element = element.parentNode\n\n    def attributes(self):\n        \"\"\"Return a dict of attributes for this tag\"\"\"\n        #TODO: use slice syntax [:]?\n        return self._element.attributes\n\n    def __getitem__(self, item):\n        \"\"\"Return xml tag attribute value or a slice of attributes (iter)\"\"\"\n        ##log.debug('__getitem__(%s)', item)\n        if isinstance(item, basestring):\n            if self._element.hasAttribute(item):\n                return self._element.attributes[item].value\n        elif isinstance(item, slice):\n            # return a list with name:values\n            return list(self._element.attributes.items())[item]\n        else:\n            # return element by index (position)\n            element = self.__elements[item]\n            return SimpleXMLElement(\n                elements=[element],\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map\n            )\n\n    def add_attribute(self, name, value):\n        \"\"\"Set an attribute value from a string\"\"\"\n        self._element.setAttribute(name, value)\n\n    def __setitem__(self, item, value):\n        \"\"\"Set an attribute value\"\"\"\n        if isinstance(item, basestring):\n            self.add_attribute(item, value)\n        elif isinstance(item, slice):\n            # set multiple attributes at once\n            for k, v in value.items():\n                self.add_attribute(k, v)\n\n    def __delitem__(self, item):\n        \"Remove an attribute\"\n        self._element.removeAttribute(item)\n\n    def __call__(self, tag=None, ns=None, children=False, root=False,\n                 error=True, ):\n        \"\"\"Search (even in child nodes) and return a child tag by name\"\"\"\n        try:\n            if root:\n                # return entire document\n                return SimpleXMLElement(\n                    elements=[self.__document.documentElement],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map\n                )\n            if tag is None:\n                # if no name given, iterate over siblings (same level)\n                return self.__iter__()\n            if children:\n                # future: filter children? by ns?\n                return self.children()\n            elements = None\n            if isinstance(tag, int):\n                # return tag by index\n                elements = [self.__elements[tag]]\n            if ns and not elements:\n                for ns_uri in isinstance(ns, (tuple, list)) and ns or (ns, ):\n                    ##log.debug('searching %s by ns=%s', tag, ns_uri)\n                    elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                    if elements:\n                        break\n            if self.__ns and not elements:\n                ##log.debug('searching %s by ns=%s', tag, self.__ns)\n                elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n            if not elements:\n                ##log.debug('searching %s', tag)\n                elements = self._element.getElementsByTagName(tag)\n            if not elements:\n                ##log.debug(self._element.toxml())\n                if error:\n                    raise AttributeError(\"No elements found\")\n                else:\n                    return\n            return SimpleXMLElement(\n                elements=elements,\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map)\n        except AttributeError as e:\n            raise AttributeError(\"Tag not found: %s (%s)\" % (tag, e))\n\n    def __getattr__(self, tag):\n        \"\"\"Shortcut for __call__\"\"\"\n        return self.__call__(tag)\n\n    def __iter__(self):\n        \"\"\"Iterate over xml tags at this level\"\"\"\n        try:\n            for __element in self.__elements:\n                yield SimpleXMLElement(\n                    elements=[__element],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map)\n        except:\n            raise\n\n    def __dir__(self):\n        \"\"\"List xml children tags names\"\"\"\n        return [node.tagName for node\n                in self._element.childNodes\n                if node.nodeType != node.TEXT_NODE]\n\n    def children(self):\n        \"\"\"Return xml children tags element\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        if not elements:\n            return None\n            #raise IndexError(\"Tag %s has no children\" % self._element.tagName)\n        return SimpleXMLElement(\n            elements=elements,\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __len__(self):\n        \"\"\"Return element count\"\"\"\n        return len(self.__elements)\n\n    def __contains__(self, item):\n        \"\"\"Search for a tag name in this element or child nodes\"\"\"\n        return self._element.getElementsByTagName(item)\n\n    def __unicode__(self):\n        \"\"\"Returns the unicode text nodes of the current element\"\"\"\n        rc = ''\n        for node in self._element.childNodes:\n            if node.nodeType == node.TEXT_NODE or node.nodeType == node.CDATA_SECTION_NODE:\n                rc = rc + node.data\n        return rc\n\n    if sys.version > '3':\n        __str__ = __unicode__\n    else:\n        def __str__(self):\n            return self.__unicode__().encode('utf-8')\n\n    def __int__(self):\n        \"\"\"Returns the integer value of the current element\"\"\"\n        return int(self.__str__())\n\n    def __float__(self):\n        \"\"\"Returns the float value of the current element\"\"\"\n        try:\n            return float(self.__str__())\n        except:\n            raise IndexError(self._element.toxml())\n\n    _element = property(lambda self: self.__elements[0])\n\n    def unmarshall(self, types, strict=True):\n        #import pdb; pdb.set_trace()\n\n        \"\"\"Convert to python values the current serialized xml element\"\"\"\n        # types is a dict of {tag name: convertion function}\n        # strict=False to use default type conversion if not specified\n        # example: types={'p': {'a': int,'b': int}, 'c': [{'d':str}]}\n        #   expected xml: <p><a>1</a><b>2</b></p><c><d>hola</d><d>chau</d>\n        #   returnde value: {'p': {'a':1,'b':2}, `'c':[{'d':'hola'},{'d':'chau'}]}\n        d = {}\n        for node in self():\n            name = str(node.get_local_name())\n            ref_name_type = None\n            # handle multirefs: href=\"#id0\"\n            if 'href' in node.attributes().keys():\n                href = node['href'][1:]\n                for ref_node in self(root=True)(\"multiRef\"):\n                    if ref_node['id'] == href:\n                        node = ref_node\n                        ref_name_type = ref_node['xsi:type'].split(\":\")[1]\n                        break\n\n            try:\n                if isinstance(types, dict):\n                    fn = types[name]\n                    # custom array only in the response (not defined in the WSDL):\n                    # <results soapenc:arrayType=\"xsd:string[199]>\n                    if any([k for k,v in node[:] if 'arrayType' in k]) and not isinstance(fn, list):\n                        fn = [fn]\n                else:\n                    fn = types\n            except (KeyError, ) as e:\n                xmlns = node['xmlns'] or node.get_namespace_uri(node.get_prefix())\n                if 'xsi:type' in node.attributes().keys():\n                    xsd_type = node['xsi:type'].split(\":\")[1]\n                    try:\n                        # get fn type from SOAP-ENC:arrayType=\"xsd:string[28]\"\n                        if xsd_type == 'Array':\n                            array_type = [k for k,v in node[:] if 'arrayType' in k][0]\n                            xsd_type = node[array_type].split(\":\")[1]\n                            if \"[\" in xsd_type:\n                                xsd_type = xsd_type[:xsd_type.index(\"[\")]\n                            fn = [REVERSE_TYPE_MAP[xsd_type]]\n                        else:\n                            fn = REVERSE_TYPE_MAP[xsd_type]\n                    except:\n                        fn = None  # ignore multirefs!\n                elif xmlns == \"http://www.w3.org/2001/XMLSchema\":\n                    # self-defined schema, return the SimpleXMLElement\n                    # TODO: parse to python types if <s:element ref=\"s:schema\"/>\n                    fn = None\n                elif None in types:\n                    # <s:any/>, return the SimpleXMLElement\n                    # TODO: check position of None if inside <s:sequence>\n                    fn = None\n                elif strict:\n                    raise TypeError(\"Tag: %s invalid (type not found)\" % (name,))\n                else:\n                    # if not strict, use default type conversion\n                    fn = str\n\n            if isinstance(fn, list):\n                # append to existing list (if any) - unnested dict arrays -\n                value = d.setdefault(name, [])\n                children = node.children()\n                # TODO: check if this was really needed (get first child only)\n                ##if len(fn[0]) == 1 and children:\n                ##    children = children()\n                if fn and not isinstance(fn[0], dict):\n                    # simple arrays []\n                    for child in (children or []):\n                        tmp_dict = child.unmarshall(fn[0], strict)\n                        value.extend(tmp_dict.values())\n                elif (self.__jetty and len(fn[0]) > 1):\n                    # Jetty array style support [{k, v}]\n                    for parent in node:\n                        tmp_dict = {}    # unmarshall each value & mix\n                        for child in (node.children() or []):\n                            tmp_dict.update(child.unmarshall(fn[0], strict))\n                        value.append(tmp_dict)\n                else:  # .Net / Java\n                    for child in (children or []):\n                        value.append(child.unmarshall(fn[0], strict))\n\n            elif isinstance(fn, tuple):\n                value = []\n                _d = {}\n                children = node.children()\n                as_dict = len(fn) == 1 and isinstance(fn[0], dict)\n\n                for child in (children and children() or []):  # Readability counts\n                    if as_dict:\n                        _d.update(child.unmarshall(fn[0], strict))  # Merging pairs\n                    else:\n                        value.append(child.unmarshall(fn[0], strict))\n                if as_dict:\n                    value.append(_d)\n\n                if name in d:\n                    _tmp = list(d[name])\n                    _tmp.extend(value)\n                    value = tuple(_tmp)\n                else:\n                    value = tuple(value)\n\n            elif isinstance(fn, dict):\n                ##if ref_name_type is not None:\n                ##    fn = fn[ref_name_type]\n                children = node.children()\n                value = children and children.unmarshall(fn, strict)\n            else:\n                if fn is None:  # xsd:anyType not unmarshalled\n                    value = node\n                elif unicode(node) or (fn == str and unicode(node) != ''):\n                    try:\n                        # get special deserialization function (if any)\n                        fn = TYPE_UNMARSHAL_FN.get(fn, fn)\n                        if fn == str:\n                            # always return an unicode object:\n                            # (avoid encoding errors in py<3!)\n                            value = unicode(node)\n                        else:\n                            value = fn(unicode(node))\n                    except (ValueError, TypeError) as e:\n                        raise ValueError(\"Tag: %s: %s\" % (name, e))\n                else:\n                    value = None\n            d[name] = value\n        return d\n\n    def _update_ns(self, name):\n        \"\"\"Replace the defined namespace alias with tohse used by the client.\"\"\"\n        pref = self.__ns_rx.search(name)\n        if pref:\n            pref = pref.groups()[0]\n            try:\n                name = name.replace(pref, self.__namespaces_map[pref])\n            except KeyError:\n                log.warning('Unknown namespace alias %s' % name)\n        return name\n\n    def marshall(self, name, value, add_child=True, add_comments=False,\n                 ns=False, add_children_ns=True):\n        \"\"\"Analyze python value and add the serialized XML element using tag name\"\"\"\n        # Change node name to that used by a client\n        name = self._update_ns(name)\n\n        if isinstance(value, dict):  # serialize dict (<key>value</key>)\n            # for the first parent node, use the document target namespace\n            # (ns==True) or use the namespace string uri if passed (elements)\n            child = add_child and self.add_child(name, ns=ns) or self\n            for k, v in value.items():\n                if not add_children_ns:\n                    ns = False\n                elif hasattr(value, 'namespaces'):\n                    # for children, use the wsdl element target namespace:\n                    ns = value.namespaces.get(k)\n                else:\n                    # simple type\n                    ns = None\n                child.marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, tuple):  # serialize tuple (<key>value</key>)\n            child = add_child and self.add_child(name, ns=ns) or self\n            if not add_children_ns:\n                ns = False\n            for k, v in value:\n                getattr(self, name).marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, list):  # serialize lists\n            child = self.add_child(name, ns=ns)\n            if not add_children_ns:\n                ns = False\n            if add_comments:\n                child.add_comment(\"Repetitive array of:\")\n            for t in value:\n                child.marshall(name, t, False, add_comments=add_comments, ns=ns)\n        elif isinstance(value, (xml.dom.minidom.CDATASection, basestring)):  # do not convert strings or unicodes\n            self.add_child(name, value, ns=ns)\n        elif value is None:  # sent a empty tag?\n            self.add_child(name, ns=ns)\n        elif value in TYPE_MAP.keys():\n            # add commented placeholders for simple tipes (for examples/help only)\n            child = self.add_child(name, ns=ns)\n            child.add_comment(TYPE_MAP[value])\n        else:  # the rest of object types are converted to string\n            # get special serialization function (if any)\n            fn = TYPE_MARSHAL_FN.get(type(value), str)\n            self.add_child(name, fn(value), ns=ns)\n\n    def import_node(self, other):\n        x = self.__document.importNode(other._element, True)  # deep copy\n        self._element.appendChild(x)\n\n    def write_c14n(self, output=None, exclusive=True):\n        \"Generate the canonical version of the XML node\"\n        from . import c14n\n        xml = c14n.Canonicalize(self._element, output,\n                                unsuppressedPrefixes=[] if exclusive else None)\n        return xml\n", "prompt": "Please write a python function called 'as_xml' base the context. This function returns the XML representation of the document. If the \"pretty\" parameter is set to False, it returns the XML representation without any formatting. If \"pretty\" is set to True, it returns the XML representation with indentation and line breaks for better readability.:param self: SimpleXMLElement. An instance of the SimpleXMLElement class.\n:param filename: String [optional]. The name of the file to save the XML representation. Defaults to None.\n:param pretty: Bool. Whether to format the XML representation with indentation and line breaks. Defaults to False.\n:return: String. The XML representation of the document..\n        The context you need to refer to is as follows: class SimpleXMLElement(object):\n    \"\"\"Simple XML manipulation (simil PHP)\"\"\"\n\n    def __init__(self, text=None, elements=None, document=None,\n                 namespace=None, prefix=None, namespaces_map={}, jetty=False):\n        \"\"\"\n        :param namespaces_map: How to map our namespace prefix to that given by the client;\n          {prefix: received_prefix}\n        \"\"\"\n        self.__namespaces_map = namespaces_map\n        _rx = \"|\".join(namespaces_map.keys())  # {'external': 'ext', 'model': 'mod'} -> 'external|model'\n        self.__ns_rx = re.compile(r\"^(%s):.*$\" % _rx)  # And now we build an expression ^(external|model):.*$\n                                                       # to find prefixes in all xml nodes i.e.: <model:code>1</model:code>\n                                                       # and later change that to <mod:code>1</mod:code>\n        self.__ns = namespace\n        self.__prefix = prefix\n        self.__jetty = jetty                           # special list support\n\n        if text is not None:\n            try:\n                self.__document = xml.dom.minidom.parseString(text)\n            except:\n                log.error(text)\n                raise\n            self.__elements = [self.__document.documentElement]\n        else:\n            self.__elements = elements\n            self.__document = document\n\n    def add_child(self, name, text=None, ns=True):\n        \"\"\"Adding a child tag to a node\"\"\"\n        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __setattr__(self, tag, text):\n        \"\"\"Add text child tag node (short form)\"\"\"\n        if tag.startswith(\"_\"):\n            object.__setattr__(self, tag, text)\n        else:\n            ##log.debug('__setattr__(%s, %s)', tag, text)\n            self.add_child(tag, text)\n\n    def __delattr__(self, tag):\n        \"\"\"Remove a child tag (non recursive!)\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        for element in elements:\n            self._element.removeChild(element)\n\n    def add_comment(self, data):\n        \"\"\"Add an xml comment to this child\"\"\"\n        comment = self.__document.createComment(data)\n        self._element.appendChild(comment)\n\n###The function: as_xml###\n    if sys.version > '3':\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            return self._element.toxml()\n    else:\n        def __repr__(self):\n            \"\"\"Return the XML representation of this tag\"\"\"\n            # NOTE: do not use self.as_xml('UTF-8') as it returns the whole xml doc\n            return self._element.toxml('UTF-8')\n\n    def get_name(self):\n        \"\"\"Return the tag name of this node\"\"\"\n        return self._element.tagName\n\n    def get_local_name(self):\n        \"\"\"Return the tag local name (prefix:name) of this node\"\"\"\n        return self._element.localName\n\n    def get_prefix(self):\n        \"\"\"Return the namespace prefix of this node\"\"\"\n        return self._element.prefix\n\n    def get_namespace_uri(self, ns):\n        \"\"\"Return the namespace uri for a prefix\"\"\"\n        element = self._element\n        while element is not None and element.attributes is not None:\n            try:\n                return element.attributes['xmlns:%s' % ns].value\n            except KeyError:\n                element = element.parentNode\n\n    def attributes(self):\n        \"\"\"Return a dict of attributes for this tag\"\"\"\n        #TODO: use slice syntax [:]?\n        return self._element.attributes\n\n    def __getitem__(self, item):\n        \"\"\"Return xml tag attribute value or a slice of attributes (iter)\"\"\"\n        ##log.debug('__getitem__(%s)', item)\n        if isinstance(item, basestring):\n            if self._element.hasAttribute(item):\n                return self._element.attributes[item].value\n        elif isinstance(item, slice):\n            # return a list with name:values\n            return list(self._element.attributes.items())[item]\n        else:\n            # return element by index (position)\n            element = self.__elements[item]\n            return SimpleXMLElement(\n                elements=[element],\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map\n            )\n\n    def add_attribute(self, name, value):\n        \"\"\"Set an attribute value from a string\"\"\"\n        self._element.setAttribute(name, value)\n\n    def __setitem__(self, item, value):\n        \"\"\"Set an attribute value\"\"\"\n        if isinstance(item, basestring):\n            self.add_attribute(item, value)\n        elif isinstance(item, slice):\n            # set multiple attributes at once\n            for k, v in value.items():\n                self.add_attribute(k, v)\n\n    def __delitem__(self, item):\n        \"Remove an attribute\"\n        self._element.removeAttribute(item)\n\n    def __call__(self, tag=None, ns=None, children=False, root=False,\n                 error=True, ):\n        \"\"\"Search (even in child nodes) and return a child tag by name\"\"\"\n        try:\n            if root:\n                # return entire document\n                return SimpleXMLElement(\n                    elements=[self.__document.documentElement],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map\n                )\n            if tag is None:\n                # if no name given, iterate over siblings (same level)\n                return self.__iter__()\n            if children:\n                # future: filter children? by ns?\n                return self.children()\n            elements = None\n            if isinstance(tag, int):\n                # return tag by index\n                elements = [self.__elements[tag]]\n            if ns and not elements:\n                for ns_uri in isinstance(ns, (tuple, list)) and ns or (ns, ):\n                    ##log.debug('searching %s by ns=%s', tag, ns_uri)\n                    elements = self._element.getElementsByTagNameNS(ns_uri, tag)\n                    if elements:\n                        break\n            if self.__ns and not elements:\n                ##log.debug('searching %s by ns=%s', tag, self.__ns)\n                elements = self._element.getElementsByTagNameNS(self.__ns, tag)\n            if not elements:\n                ##log.debug('searching %s', tag)\n                elements = self._element.getElementsByTagName(tag)\n            if not elements:\n                ##log.debug(self._element.toxml())\n                if error:\n                    raise AttributeError(\"No elements found\")\n                else:\n                    return\n            return SimpleXMLElement(\n                elements=elements,\n                document=self.__document,\n                namespace=self.__ns,\n                prefix=self.__prefix,\n                jetty=self.__jetty,\n                namespaces_map=self.__namespaces_map)\n        except AttributeError as e:\n            raise AttributeError(\"Tag not found: %s (%s)\" % (tag, e))\n\n    def __getattr__(self, tag):\n        \"\"\"Shortcut for __call__\"\"\"\n        return self.__call__(tag)\n\n    def __iter__(self):\n        \"\"\"Iterate over xml tags at this level\"\"\"\n        try:\n            for __element in self.__elements:\n                yield SimpleXMLElement(\n                    elements=[__element],\n                    document=self.__document,\n                    namespace=self.__ns,\n                    prefix=self.__prefix,\n                    jetty=self.__jetty,\n                    namespaces_map=self.__namespaces_map)\n        except:\n            raise\n\n    def __dir__(self):\n        \"\"\"List xml children tags names\"\"\"\n        return [node.tagName for node\n                in self._element.childNodes\n                if node.nodeType != node.TEXT_NODE]\n\n    def children(self):\n        \"\"\"Return xml children tags element\"\"\"\n        elements = [__element for __element in self._element.childNodes\n                    if __element.nodeType == __element.ELEMENT_NODE]\n        if not elements:\n            return None\n            #raise IndexError(\"Tag %s has no children\" % self._element.tagName)\n        return SimpleXMLElement(\n            elements=elements,\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n\n    def __len__(self):\n        \"\"\"Return element count\"\"\"\n        return len(self.__elements)\n\n    def __contains__(self, item):\n        \"\"\"Search for a tag name in this element or child nodes\"\"\"\n        return self._element.getElementsByTagName(item)\n\n    def __unicode__(self):\n        \"\"\"Returns the unicode text nodes of the current element\"\"\"\n        rc = ''\n        for node in self._element.childNodes:\n            if node.nodeType == node.TEXT_NODE or node.nodeType == node.CDATA_SECTION_NODE:\n                rc = rc + node.data\n        return rc\n\n    if sys.version > '3':\n        __str__ = __unicode__\n    else:\n        def __str__(self):\n            return self.__unicode__().encode('utf-8')\n\n    def __int__(self):\n        \"\"\"Returns the integer value of the current element\"\"\"\n        return int(self.__str__())\n\n    def __float__(self):\n        \"\"\"Returns the float value of the current element\"\"\"\n        try:\n            return float(self.__str__())\n        except:\n            raise IndexError(self._element.toxml())\n\n    _element = property(lambda self: self.__elements[0])\n\n    def unmarshall(self, types, strict=True):\n        #import pdb; pdb.set_trace()\n\n        \"\"\"Convert to python values the current serialized xml element\"\"\"\n        # types is a dict of {tag name: convertion function}\n        # strict=False to use default type conversion if not specified\n        # example: types={'p': {'a': int,'b': int}, 'c': [{'d':str}]}\n        #   expected xml: <p><a>1</a><b>2</b></p><c><d>hola</d><d>chau</d>\n        #   returnde value: {'p': {'a':1,'b':2}, `'c':[{'d':'hola'},{'d':'chau'}]}\n        d = {}\n        for node in self():\n            name = str(node.get_local_name())\n            ref_name_type = None\n            # handle multirefs: href=\"#id0\"\n            if 'href' in node.attributes().keys():\n                href = node['href'][1:]\n                for ref_node in self(root=True)(\"multiRef\"):\n                    if ref_node['id'] == href:\n                        node = ref_node\n                        ref_name_type = ref_node['xsi:type'].split(\":\")[1]\n                        break\n\n            try:\n                if isinstance(types, dict):\n                    fn = types[name]\n                    # custom array only in the response (not defined in the WSDL):\n                    # <results soapenc:arrayType=\"xsd:string[199]>\n                    if any([k for k,v in node[:] if 'arrayType' in k]) and not isinstance(fn, list):\n                        fn = [fn]\n                else:\n                    fn = types\n            except (KeyError, ) as e:\n                xmlns = node['xmlns'] or node.get_namespace_uri(node.get_prefix())\n                if 'xsi:type' in node.attributes().keys():\n                    xsd_type = node['xsi:type'].split(\":\")[1]\n                    try:\n                        # get fn type from SOAP-ENC:arrayType=\"xsd:string[28]\"\n                        if xsd_type == 'Array':\n                            array_type = [k for k,v in node[:] if 'arrayType' in k][0]\n                            xsd_type = node[array_type].split(\":\")[1]\n                            if \"[\" in xsd_type:\n                                xsd_type = xsd_type[:xsd_type.index(\"[\")]\n                            fn = [REVERSE_TYPE_MAP[xsd_type]]\n                        else:\n                            fn = REVERSE_TYPE_MAP[xsd_type]\n                    except:\n                        fn = None  # ignore multirefs!\n                elif xmlns == \"http://www.w3.org/2001/XMLSchema\":\n                    # self-defined schema, return the SimpleXMLElement\n                    # TODO: parse to python types if <s:element ref=\"s:schema\"/>\n                    fn = None\n                elif None in types:\n                    # <s:any/>, return the SimpleXMLElement\n                    # TODO: check position of None if inside <s:sequence>\n                    fn = None\n                elif strict:\n                    raise TypeError(\"Tag: %s invalid (type not found)\" % (name,))\n                else:\n                    # if not strict, use default type conversion\n                    fn = str\n\n            if isinstance(fn, list):\n                # append to existing list (if any) - unnested dict arrays -\n                value = d.setdefault(name, [])\n                children = node.children()\n                # TODO: check if this was really needed (get first child only)\n                ##if len(fn[0]) == 1 and children:\n                ##    children = children()\n                if fn and not isinstance(fn[0], dict):\n                    # simple arrays []\n                    for child in (children or []):\n                        tmp_dict = child.unmarshall(fn[0], strict)\n                        value.extend(tmp_dict.values())\n                elif (self.__jetty and len(fn[0]) > 1):\n                    # Jetty array style support [{k, v}]\n                    for parent in node:\n                        tmp_dict = {}    # unmarshall each value & mix\n                        for child in (node.children() or []):\n                            tmp_dict.update(child.unmarshall(fn[0], strict))\n                        value.append(tmp_dict)\n                else:  # .Net / Java\n                    for child in (children or []):\n                        value.append(child.unmarshall(fn[0], strict))\n\n            elif isinstance(fn, tuple):\n                value = []\n                _d = {}\n                children = node.children()\n                as_dict = len(fn) == 1 and isinstance(fn[0], dict)\n\n                for child in (children and children() or []):  # Readability counts\n                    if as_dict:\n                        _d.update(child.unmarshall(fn[0], strict))  # Merging pairs\n                    else:\n                        value.append(child.unmarshall(fn[0], strict))\n                if as_dict:\n                    value.append(_d)\n\n                if name in d:\n                    _tmp = list(d[name])\n                    _tmp.extend(value)\n                    value = tuple(_tmp)\n                else:\n                    value = tuple(value)\n\n            elif isinstance(fn, dict):\n                ##if ref_name_type is not None:\n                ##    fn = fn[ref_name_type]\n                children = node.children()\n                value = children and children.unmarshall(fn, strict)\n            else:\n                if fn is None:  # xsd:anyType not unmarshalled\n                    value = node\n                elif unicode(node) or (fn == str and unicode(node) != ''):\n                    try:\n                        # get special deserialization function (if any)\n                        fn = TYPE_UNMARSHAL_FN.get(fn, fn)\n                        if fn == str:\n                            # always return an unicode object:\n                            # (avoid encoding errors in py<3!)\n                            value = unicode(node)\n                        else:\n                            value = fn(unicode(node))\n                    except (ValueError, TypeError) as e:\n                        raise ValueError(\"Tag: %s: %s\" % (name, e))\n                else:\n                    value = None\n            d[name] = value\n        return d\n\n    def _update_ns(self, name):\n        \"\"\"Replace the defined namespace alias with tohse used by the client.\"\"\"\n        pref = self.__ns_rx.search(name)\n        if pref:\n            pref = pref.groups()[0]\n            try:\n                name = name.replace(pref, self.__namespaces_map[pref])\n            except KeyError:\n                log.warning('Unknown namespace alias %s' % name)\n        return name\n\n    def marshall(self, name, value, add_child=True, add_comments=False,\n                 ns=False, add_children_ns=True):\n        \"\"\"Analyze python value and add the serialized XML element using tag name\"\"\"\n        # Change node name to that used by a client\n        name = self._update_ns(name)\n\n        if isinstance(value, dict):  # serialize dict (<key>value</key>)\n            # for the first parent node, use the document target namespace\n            # (ns==True) or use the namespace string uri if passed (elements)\n            child = add_child and self.add_child(name, ns=ns) or self\n            for k, v in value.items():\n                if not add_children_ns:\n                    ns = False\n                elif hasattr(value, 'namespaces'):\n                    # for children, use the wsdl element target namespace:\n                    ns = value.namespaces.get(k)\n                else:\n                    # simple type\n                    ns = None\n                child.marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, tuple):  # serialize tuple (<key>value</key>)\n            child = add_child and self.add_child(name, ns=ns) or self\n            if not add_children_ns:\n                ns = False\n            for k, v in value:\n                getattr(self, name).marshall(k, v, add_comments=add_comments, ns=ns)\n        elif isinstance(value, list):  # serialize lists\n            child = self.add_child(name, ns=ns)\n            if not add_children_ns:\n                ns = False\n            if add_comments:\n                child.add_comment(\"Repetitive array of:\")\n            for t in value:\n                child.marshall(name, t, False, add_comments=add_comments, ns=ns)\n        elif isinstance(value, (xml.dom.minidom.CDATASection, basestring)):  # do not convert strings or unicodes\n            self.add_child(name, value, ns=ns)\n        elif value is None:  # sent a empty tag?\n            self.add_child(name, ns=ns)\n        elif value in TYPE_MAP.keys():\n            # add commented placeholders for simple tipes (for examples/help only)\n            child = self.add_child(name, ns=ns)\n            child.add_comment(TYPE_MAP[value])\n        else:  # the rest of object types are converted to string\n            # get special serialization function (if any)\n            fn = TYPE_MARSHAL_FN.get(type(value), str)\n            self.add_child(name, fn(value), ns=ns)\n\n    def import_node(self, other):\n        x = self.__document.importNode(other._element, True)  # deep copy\n        self._element.appendChild(x)\n\n    def write_c14n(self, output=None, exclusive=True):\n        \"Generate the canonical version of the XML node\"\n        from . import c14n\n        xml = c14n.Canonicalize(self._element, output,\n                                unsuppressedPrefixes=[] if exclusive else None)\n        return xml\n", "test_list": ["def test_marshall_cdata(self):\n    span = SimpleXMLElement('<span/>')\n    cdata = CDATASection()\n    cdata.data = 'python'\n    span.add_child('a', cdata)\n    xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><span><a><![CDATA[python]]></a></span>'\n    self.eq(span.as_xml(), xml if PY2 else xml.encode('utf-8'))", "def test_to_xml(self):\n    xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><span><a href=\"python.org.ar\">pyar</a><prueba><i>1</i><float>1.5</float></prueba></span>'\n    self.eq(SimpleXMLElement(xml).as_xml(), xml if PY2 else xml.encode('utf-8'))\n    xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><span><a href=\"google.com\">google</a><a>yahoo</a><a>hotmail</a></span>'\n    self.eq(SimpleXMLElement(xml).as_xml(), xml if PY2 else xml.encode('utf-8'))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'as_xml' function should correctly return the XML representation of the document as a string. If the 'pretty' parameter is set to True, the XML should be formatted with indentation and line breaks.", "unit_test": ["def test_as_xml_pretty_formatting():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    pretty_xml = '<?xml version=\"1.0\" ?>\\n<root>\\n  <child>value</child>\\n</root>\\n'\n    assert element.as_xml(pretty=True) == pretty_xml"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_pretty_formatting"}, "Exception Handling": {"requirement": "The 'as_xml' function should raise a ValueError if the XML document is malformed or cannot be parsed.", "unit_test": ["def test_as_xml_malformed_xml():\n    malformed_xml = '<root><child>value</child>'  # Missing closing tag\n    try:\n        element = SimpleXMLElement(malformed_xml)\n        element.as_xml()\n    except ValueError as e:\n        assert str(e) == 'Malformed XML document'"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_malformed_xml"}, "Edge Case Handling": {"requirement": "The 'as_xml' function should handle empty XML documents gracefully and return an empty string.", "unit_test": ["def test_as_xml_empty_document():\n    empty_xml = ''\n    element = SimpleXMLElement(empty_xml)\n    assert element.as_xml() == ''"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_empty_document"}, "Functionality Extension": {"requirement": "Extend the 'as_xml' function to accept an optional 'encoding' parameter that specifies the character encoding of the XML output.", "unit_test": ["def test_as_xml_with_encoding():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    encoded_xml = '<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?><root><child>value</child></root>'\n    assert element.as_xml(encoding='ISO-8859-1') == encoded_xml"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_with_encoding"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'as_xml' function are annotated with type hints, including two parameters: 'pretty': bool, 'filename': str, and a return type: str.", "unit_test": ["def test_as_xml_annotations():\n    from typing import get_type_hints\n    hints = get_type_hints(SimpleXMLElement.as_xml)\n    assert hints == {'pretty': bool, 'filename': str, 'return': str}"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'as_xml' function should not exceed 2.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(SimpleXMLElement.as_xml)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 2"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_code_complexity"}, "Code Standard": {"requirement": "The 'as_xml' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    import textwrap\n    code_string = inspect.getsource(SimpleXMLElement.as_xml)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'as_xml' function should utilize the '__document' attribute of the SimpleXMLElement class to generate the XML representation.", "unit_test": ["def test_as_xml_uses_document():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    assert hasattr(element, '_SimpleXMLElement__document')\n    assert element.as_xml() == element._SimpleXMLElement__document.toxml()"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_uses_document"}, "Context Usage Correctness Verification": {"requirement": "The 'as_xml' function should correctly use the '__document' attribute to ensure the XML representation is accurate and complete.", "unit_test": ["def test_as_xml_correct_document_usage():\n    xml = '<root><child>value</child></root>'\n    element = SimpleXMLElement(xml)\n    assert element.as_xml() == element._SimpleXMLElement__document.toxml()"], "test": "tests/simplexmlelement_test.py::TestSimpleXMLElement::test_as_xml_correct_document_usage"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "type": "method", "project_path": "Multimedia/mingus", "completion_path": "Multimedia/mingus/mingus/containers/note_container.py", "signature_position": [293, 293], "body_position": [298, 302], "dependency": {"intra_class": ["mingus.containers.note_container.NoteContainer.notes"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns a list of unique note names from the current note container.\n", "Arguments": ":param self: NoteContainer, an instance of the NoteContainer class.\n:return: List. A list containing the unique note names from the current note container.\n"}, "tests": ["tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names"], "indent": 4, "domain": "Multimedia", "code": "    def get_note_names(self):\n        \"\"\"Return a list with all the note names in the current container.\n\n        Every name will only be mentioned once.\n        \"\"\"\n        res = []\n        for n in self.notes:\n            if n.name not in res:\n                res.append(n.name)\n        return res\n", "context": "class NoteContainer(object):\n\n    \"\"\"A container for notes.\n\n    The NoteContainer provides a container for the mingus.containers.Note\n    objects.\n\n    It can be used to store single and multiple notes and is required for\n    working with Bars.\n    \"\"\"\n\n    notes = []\n\n    def __init__(self, notes=None):\n        if notes is None:\n            notes = []\n        self.empty()\n        self.add_notes(notes)\n\n    def empty(self):\n        \"\"\"Empty the container.\"\"\"\n        self.notes = []\n\n    def add_note(self, note, octave=None, dynamics=None):\n        \"\"\"Add a note to the container and sorts the notes from low to high.\n\n        The note can either be a string, in which case you could also use\n        the octave and dynamics arguments, or a Note object.\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n        if isinstance(note, six.string_types):\n            if octave is not None:\n                note = Note(note, octave, dynamics)\n            elif len(self.notes) == 0:\n                note = Note(note, 4, dynamics)\n            else:\n                if Note(note, self.notes[-1].octave) < self.notes[-1]:\n                    note = Note(note, self.notes[-1].octave + 1, dynamics)\n                else:\n                    note = Note(note, self.notes[-1].octave, dynamics)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Object '%s' was not expected. \"\n                \"Expecting a mingus.containers.Note object.\" % note\n            )\n        if note not in self.notes:\n            self.notes.append(note)\n            self.notes.sort()\n        return self.notes\n\n    def add_notes(self, notes):\n        \"\"\"Feed notes to self.add_note.\n\n        The notes can either be an other NoteContainer, a list of Note\n        objects or strings or a list of lists formatted like this:\n        >>> notes = [['C', 5], ['E', 5], ['G', 6]]\n\n        or even:\n        >>> notes = [['C', 5, {'velocity': 20}], ['E', 6, {'velocity': 20}]]\n        \"\"\"\n        if hasattr(notes, \"notes\"):\n            for x in notes.notes:\n                self.add_note(x)\n            return self.notes\n        elif hasattr(notes, \"name\"):\n            self.add_note(notes)\n            return self.notes\n        elif isinstance(notes, six.string_types):\n            self.add_note(notes)\n            return self.notes\n        for x in notes:\n            if isinstance(x, list) and len(x) != 1:\n                if len(x) == 2:\n                    self.add_note(x[0], x[1])\n                else:\n                    self.add_note(x[0], x[1], x[2])\n            else:\n                self.add_note(x)\n        return self.notes\n\n    def from_chord(self, shorthand):\n        \"\"\"Shortcut to from_chord_shorthand.\"\"\"\n        return self.from_chord_shorthand(shorthand)\n\n    def from_chord_shorthand(self, shorthand):\n        \"\"\"Empty the container and add the notes in the shorthand.\n\n        See mingus.core.chords.from_shorthand for an up to date list of\n        recognized format.\n\n        Example:\n        >>> NoteContainer().from_chord_shorthand('Am')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n\n    def from_interval(self, startnote, shorthand, up=True):\n        \"\"\"Shortcut to from_interval_shorthand.\"\"\"\n        return self.from_interval_shorthand(startnote, shorthand, up)\n\n    def from_interval_shorthand(self, startnote, shorthand, up=True):\n        \"\"\"Empty the container and add the note described in the startnote and\n        shorthand.\n\n        See core.intervals for the recognized format.\n\n        Examples:\n        >>> nc = NoteContainer()\n        >>> nc.from_interval_shorthand('C', '5')\n        ['C-4', 'G-4']\n        >>> nc.from_interval_shorthand('C', '5', False)\n        ['F-3', 'C-4']\n        \"\"\"\n        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        n = Note(startnote.name, startnote.octave, startnote.dynamics)\n        n.transpose(shorthand, up)\n        self.add_notes([startnote, n])\n        return self\n\n    def from_progression(self, shorthand, key=\"C\"):\n        \"\"\"Shortcut to from_progression_shorthand.\"\"\"\n        return self.from_progression_shorthand(shorthand, key)\n\n    def from_progression_shorthand(self, shorthand, key=\"C\"):\n        \"\"\"Empty the container and add the notes described in the progressions\n        shorthand (eg. 'IIm6', 'V7', etc).\n\n        See mingus.core.progressions for all the recognized format.\n\n        Example:\n        >>> NoteContainer().from_progression_shorthand('VI')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        from mingus.core import progressions\n        self.empty()\n        chords = progressions.to_chords(shorthand, key)\n        # warning Throw error, not a valid shorthand\n\n        if chords == []:\n            return False\n        notes = chords[0]\n        self.add_notes(notes)\n        return self\n\n    def _consonance_test(self, testfunc, param=None):\n        \"\"\"Private function used for testing consonance/dissonance.\"\"\"\n        n = list(self.notes)\n        while len(n) > 1:\n            first = n[0]\n            for second in n[1:]:\n                if param is None:\n                    if not testfunc(first.name, second.name):\n                        return False\n                else:\n                    if not testfunc(first.name, second.name, param):\n                        return False\n            n = n[1:]\n        return True\n\n    def is_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_consonant, include_fourths)\n\n    def is_perfect_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are perfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_perfect_consonant, include_fourths)\n\n    def is_imperfect_consonant(self):\n        \"\"\"Test whether the notes are imperfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_imperfect_consonant)\n\n    def is_dissonant(self, include_fourths=False):\n        \"\"\"Test whether the notes are dissonants.\n\n        See the core.intervals module for a longer description.\n        \"\"\"\n        return not self.is_consonant(not include_fourths)\n\n    def remove_note(self, note, octave=-1):\n        \"\"\"Remove note from container.\n\n        The note can either be a Note object or a string representing the\n        note's name. If no specific octave is given, the note gets removed\n        in every octave.\n        \"\"\"\n        res = []\n        for x in self.notes:\n            if isinstance(note, six.string_types):\n                if x.name != note:\n                    res.append(x)\n                else:\n                    if x.octave != octave and octave != -1:\n                        res.append(x)\n            else:\n                if x != note:\n                    res.append(x)\n        self.notes = res\n        return res\n\n    def remove_notes(self, notes):\n        \"\"\"Remove notes from the containers.\n\n        This function accepts a list of Note objects or notes as strings and\n        also single strings or Note objects.\n        \"\"\"\n        if isinstance(notes, six.string_types):\n            return self.remove_note(notes)\n        elif hasattr(notes, \"name\"):\n            return self.remove_note(notes)\n        else:\n            for x in notes:\n                self.remove_note(x)\n            return self.notes\n\n    def remove_duplicate_notes(self):\n        \"\"\"Remove duplicate and enharmonic notes from the container.\"\"\"\n        res = []\n        for x in self.notes:\n            if x not in res:\n                res.append(x)\n        self.notes = res\n        return res\n\n    def sort(self):\n        \"\"\"Sort the notes in the container from low to high.\"\"\"\n        self.notes.sort()\n\n    def augment(self):\n        \"\"\"Augment all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.augment()\n\n    def diminish(self):\n        \"\"\"Diminish all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.diminish()\n\n    def determine(self, shorthand=False):\n        \"\"\"Determine the type of chord or interval currently in the\n        container.\"\"\"\n        return chords.determine(self.get_note_names(), shorthand)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose all the notes in the container up or down the given\n        interval.\"\"\"\n        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n\n###The function: get_note_names###\n    def __repr__(self):\n        \"\"\"Return a nice and clean string representing the note container.\"\"\"\n        return str(self.notes)\n\n    def __getitem__(self, item):\n        \"\"\"Enable the use of the container as a simple array.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0]\n        'C-4'\n        \"\"\"\n        return self.notes[item]\n\n    def __setitem__(self, item, value):\n        \"\"\"Enable the use of the [] notation on NoteContainers.\n\n        This function accepts Notes and notes as string.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0] = 'B'\n        >>> n\n        ['B-4', 'E-4', 'G-4']\n        \"\"\"\n        if isinstance(value, six.string_types):\n            n = Note(value)\n            self.notes[item] = n\n        else:\n            self.notes[item] = value\n        return self.notes\n\n    def __add__(self, notes):\n        \"\"\"Enable the use of the '+' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n + 'B'\n        ['C-4', 'E-4', 'G-4', 'B-4']\n        \"\"\"\n        self.add_notes(notes)\n        return self\n\n    def __sub__(self, notes):\n        \"\"\"Enable the use of the '-' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n - 'E'\n        ['C-4', 'G-4']\n        \"\"\"\n        self.remove_notes(notes)\n        return self\n\n    def __len__(self):\n        \"\"\"Return the number of notes in the container.\"\"\"\n        return len(self.notes)\n\n    def __eq__(self, other):\n        \"\"\"Enable the '==' operator for NoteContainer instances.\"\"\"\n        for x in self:\n            if x not in other:\n                return False\n        return True\n", "prompt": "Please write a python function called 'get_note_names' base the context. This function returns a list of unique note names from the current note container.\n:param self: NoteContainer, an instance of the NoteContainer class.\n:return: List. A list containing the unique note names from the current note container.\n.\n        The context you need to refer to is as follows: class NoteContainer(object):\n\n    \"\"\"A container for notes.\n\n    The NoteContainer provides a container for the mingus.containers.Note\n    objects.\n\n    It can be used to store single and multiple notes and is required for\n    working with Bars.\n    \"\"\"\n\n    notes = []\n\n    def __init__(self, notes=None):\n        if notes is None:\n            notes = []\n        self.empty()\n        self.add_notes(notes)\n\n    def empty(self):\n        \"\"\"Empty the container.\"\"\"\n        self.notes = []\n\n    def add_note(self, note, octave=None, dynamics=None):\n        \"\"\"Add a note to the container and sorts the notes from low to high.\n\n        The note can either be a string, in which case you could also use\n        the octave and dynamics arguments, or a Note object.\n        \"\"\"\n        if dynamics is None:\n            dynamics = {}\n        if isinstance(note, six.string_types):\n            if octave is not None:\n                note = Note(note, octave, dynamics)\n            elif len(self.notes) == 0:\n                note = Note(note, 4, dynamics)\n            else:\n                if Note(note, self.notes[-1].octave) < self.notes[-1]:\n                    note = Note(note, self.notes[-1].octave + 1, dynamics)\n                else:\n                    note = Note(note, self.notes[-1].octave, dynamics)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Object '%s' was not expected. \"\n                \"Expecting a mingus.containers.Note object.\" % note\n            )\n        if note not in self.notes:\n            self.notes.append(note)\n            self.notes.sort()\n        return self.notes\n\n    def add_notes(self, notes):\n        \"\"\"Feed notes to self.add_note.\n\n        The notes can either be an other NoteContainer, a list of Note\n        objects or strings or a list of lists formatted like this:\n        >>> notes = [['C', 5], ['E', 5], ['G', 6]]\n\n        or even:\n        >>> notes = [['C', 5, {'velocity': 20}], ['E', 6, {'velocity': 20}]]\n        \"\"\"\n        if hasattr(notes, \"notes\"):\n            for x in notes.notes:\n                self.add_note(x)\n            return self.notes\n        elif hasattr(notes, \"name\"):\n            self.add_note(notes)\n            return self.notes\n        elif isinstance(notes, six.string_types):\n            self.add_note(notes)\n            return self.notes\n        for x in notes:\n            if isinstance(x, list) and len(x) != 1:\n                if len(x) == 2:\n                    self.add_note(x[0], x[1])\n                else:\n                    self.add_note(x[0], x[1], x[2])\n            else:\n                self.add_note(x)\n        return self.notes\n\n    def from_chord(self, shorthand):\n        \"\"\"Shortcut to from_chord_shorthand.\"\"\"\n        return self.from_chord_shorthand(shorthand)\n\n    def from_chord_shorthand(self, shorthand):\n        \"\"\"Empty the container and add the notes in the shorthand.\n\n        See mingus.core.chords.from_shorthand for an up to date list of\n        recognized format.\n\n        Example:\n        >>> NoteContainer().from_chord_shorthand('Am')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n\n    def from_interval(self, startnote, shorthand, up=True):\n        \"\"\"Shortcut to from_interval_shorthand.\"\"\"\n        return self.from_interval_shorthand(startnote, shorthand, up)\n\n    def from_interval_shorthand(self, startnote, shorthand, up=True):\n        \"\"\"Empty the container and add the note described in the startnote and\n        shorthand.\n\n        See core.intervals for the recognized format.\n\n        Examples:\n        >>> nc = NoteContainer()\n        >>> nc.from_interval_shorthand('C', '5')\n        ['C-4', 'G-4']\n        >>> nc.from_interval_shorthand('C', '5', False)\n        ['F-3', 'C-4']\n        \"\"\"\n        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        n = Note(startnote.name, startnote.octave, startnote.dynamics)\n        n.transpose(shorthand, up)\n        self.add_notes([startnote, n])\n        return self\n\n    def from_progression(self, shorthand, key=\"C\"):\n        \"\"\"Shortcut to from_progression_shorthand.\"\"\"\n        return self.from_progression_shorthand(shorthand, key)\n\n    def from_progression_shorthand(self, shorthand, key=\"C\"):\n        \"\"\"Empty the container and add the notes described in the progressions\n        shorthand (eg. 'IIm6', 'V7', etc).\n\n        See mingus.core.progressions for all the recognized format.\n\n        Example:\n        >>> NoteContainer().from_progression_shorthand('VI')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n        from mingus.core import progressions\n        self.empty()\n        chords = progressions.to_chords(shorthand, key)\n        # warning Throw error, not a valid shorthand\n\n        if chords == []:\n            return False\n        notes = chords[0]\n        self.add_notes(notes)\n        return self\n\n    def _consonance_test(self, testfunc, param=None):\n        \"\"\"Private function used for testing consonance/dissonance.\"\"\"\n        n = list(self.notes)\n        while len(n) > 1:\n            first = n[0]\n            for second in n[1:]:\n                if param is None:\n                    if not testfunc(first.name, second.name):\n                        return False\n                else:\n                    if not testfunc(first.name, second.name, param):\n                        return False\n            n = n[1:]\n        return True\n\n    def is_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_consonant, include_fourths)\n\n    def is_perfect_consonant(self, include_fourths=True):\n        \"\"\"Test whether the notes are perfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_perfect_consonant, include_fourths)\n\n    def is_imperfect_consonant(self):\n        \"\"\"Test whether the notes are imperfect consonants.\n\n        See the core.intervals module for a longer description on\n        consonance.\n        \"\"\"\n        return self._consonance_test(intervals.is_imperfect_consonant)\n\n    def is_dissonant(self, include_fourths=False):\n        \"\"\"Test whether the notes are dissonants.\n\n        See the core.intervals module for a longer description.\n        \"\"\"\n        return not self.is_consonant(not include_fourths)\n\n    def remove_note(self, note, octave=-1):\n        \"\"\"Remove note from container.\n\n        The note can either be a Note object or a string representing the\n        note's name. If no specific octave is given, the note gets removed\n        in every octave.\n        \"\"\"\n        res = []\n        for x in self.notes:\n            if isinstance(note, six.string_types):\n                if x.name != note:\n                    res.append(x)\n                else:\n                    if x.octave != octave and octave != -1:\n                        res.append(x)\n            else:\n                if x != note:\n                    res.append(x)\n        self.notes = res\n        return res\n\n    def remove_notes(self, notes):\n        \"\"\"Remove notes from the containers.\n\n        This function accepts a list of Note objects or notes as strings and\n        also single strings or Note objects.\n        \"\"\"\n        if isinstance(notes, six.string_types):\n            return self.remove_note(notes)\n        elif hasattr(notes, \"name\"):\n            return self.remove_note(notes)\n        else:\n            for x in notes:\n                self.remove_note(x)\n            return self.notes\n\n    def remove_duplicate_notes(self):\n        \"\"\"Remove duplicate and enharmonic notes from the container.\"\"\"\n        res = []\n        for x in self.notes:\n            if x not in res:\n                res.append(x)\n        self.notes = res\n        return res\n\n    def sort(self):\n        \"\"\"Sort the notes in the container from low to high.\"\"\"\n        self.notes.sort()\n\n    def augment(self):\n        \"\"\"Augment all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.augment()\n\n    def diminish(self):\n        \"\"\"Diminish all the notes in the NoteContainer.\"\"\"\n        for n in self.notes:\n            n.diminish()\n\n    def determine(self, shorthand=False):\n        \"\"\"Determine the type of chord or interval currently in the\n        container.\"\"\"\n        return chords.determine(self.get_note_names(), shorthand)\n\n    def transpose(self, interval, up=True):\n        \"\"\"Transpose all the notes in the container up or down the given\n        interval.\"\"\"\n        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n\n###The function: get_note_names###\n    def __repr__(self):\n        \"\"\"Return a nice and clean string representing the note container.\"\"\"\n        return str(self.notes)\n\n    def __getitem__(self, item):\n        \"\"\"Enable the use of the container as a simple array.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0]\n        'C-4'\n        \"\"\"\n        return self.notes[item]\n\n    def __setitem__(self, item, value):\n        \"\"\"Enable the use of the [] notation on NoteContainers.\n\n        This function accepts Notes and notes as string.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n[0] = 'B'\n        >>> n\n        ['B-4', 'E-4', 'G-4']\n        \"\"\"\n        if isinstance(value, six.string_types):\n            n = Note(value)\n            self.notes[item] = n\n        else:\n            self.notes[item] = value\n        return self.notes\n\n    def __add__(self, notes):\n        \"\"\"Enable the use of the '+' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n + 'B'\n        ['C-4', 'E-4', 'G-4', 'B-4']\n        \"\"\"\n        self.add_notes(notes)\n        return self\n\n    def __sub__(self, notes):\n        \"\"\"Enable the use of the '-' operator on NoteContainers.\n\n        Example:\n        >>> n = NoteContainer(['C', 'E', 'G'])\n        >>> n - 'E'\n        ['C-4', 'G-4']\n        \"\"\"\n        self.remove_notes(notes)\n        return self\n\n    def __len__(self):\n        \"\"\"Return the number of notes in the container.\"\"\"\n        return len(self.notes)\n\n    def __eq__(self, other):\n        \"\"\"Enable the '==' operator for NoteContainer instances.\"\"\"\n        for x in self:\n            if x not in other:\n                return False\n        return True\n", "test_list": ["def test_get_note_names(self):\n    self.assertEqual(['A', 'C', 'E'], self.n3.get_note_names())\n    self.assertEqual(['A', 'C', 'E', 'F', 'G'], self.n4.get_note_names())\n    self.assertEqual(['A', 'C', 'E', 'F', 'G'], self.n5.get_note_names())"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'get_note_names' should return a list of unique note names as strings, ensuring no duplicates and maintaining the order of first appearance.", "unit_test": ["def test_get_note_names_unique(self):\n    self.n6 = NoteContainer(['A', 'C', 'A', 'E'])\n    self.assertEqual(['A', 'C', 'E'], self.n6.get_note_names())"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_unique"}, "Exception Handling": {"requirement": "The function 'get_note_names' should handle cases where the notes attribute is not a list and raise a TypeError with a descriptive message.", "unit_test": ["def test_get_note_names_type_error(self):\n    self.n7 = NoteContainer()\n    self.n7.notes = 'Not a list'\n    with self.assertRaises(TypeError):\n        self.n7.get_note_names()"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_type_error"}, "Edge Case Handling": {"requirement": "The function 'get_note_names' should return an empty list when the NoteContainer is empty.", "unit_test": ["def test_get_note_names_empty_container(self):\n    self.n8 = NoteContainer()\n    self.assertEqual([], self.n8.get_note_names())"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_empty_container"}, "Functionality Extension": {"requirement": "Extend the 'get_note_names' function to accept an optional parameter 'sort' that, when set to True, returns the note names sorted alphabetically.", "unit_test": ["def test_get_note_names_sorted(self):\n    self.n9 = NoteContainer(['E', 'C', 'A'])\n    self.assertEqual(['A', 'C', 'E'], self.n9.get_note_names(sort=True))"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_sorted"}, "Annotation Coverage": {"requirement": "Ensure that the 'get_note_names' function has complete type annotations for parameters and return types.", "unit_test": ["def test_get_note_names_annotations(self):\n    annotations = self.n10.get_note_names.__annotations__\n    self.assertEqual(annotations['return'], list)\n    self.assertEqual(annotations.get('sort', None), bool)"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_annotations"}, "Code Complexity": {"requirement": "The 'get_note_names' function should have a cyclomatic complexity of 1, indicating a simple, linear function.", "unit_test": ["def test_tuple_for_index_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(NoteContainer.get_note_names)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 5"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_tuple_for_index_complexity"}, "Code Standard": {"requirement": "The 'get_note_names' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(NoteContainer.get_note_names)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'get_note_names' function should utilize the 'notes' attribute from the NoteContainer class to retrieve note names.", "unit_test": ["def test_get_note_names_context_usage(self):\n    self.n12 = NoteContainer(['A', 'B', 'C'])\n    self.assertIn('notes', dir(self.n12))\n    self.assertEqual(['A', 'B', 'C'], self.n12.get_note_names())"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'get_note_names' function should correctly extract the 'name' attribute from each Note object in the 'notes' list.", "unit_test": ["def test_get_note_names_correct_extraction(self):\n    self.n13 = NoteContainer([Note('A'), Note('B'), Note('C')])\n    self.assertEqual(['A', 'B', 'C'], self.n13.get_note_names())"], "test": "tests/unit/containers/test_note_containers.py::test_NoteContainers::test_get_note_names_correct_extraction"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "type": "method", "project_path": "Text-Processing/pycorrector", "completion_path": "Text-Processing/pycorrector/pycorrector/en_spell.py", "signature_position": [99, 99], "body_position": [106, 109], "dependency": {"intra_class": ["pycorrector.en_spell.EnSpell.candidates", "pycorrector.en_spell.EnSpell.check_init"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function corrects the spelling of a given word by finding the most probable spelling correction. It first checks if the EnSpell instance has been initialized. Then, it calculates the probability of each candidate spelling correction for the word and sorts them in ascending order. Finally, it returns the correction with the highest probability.", "Arguments": ":param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word to be corrected.\n:return: String. The most probable spelling correction for the word."}, "tests": ["tests/en_spell_bug_fix_test.py::EnBugTestCase::test_en_bug_correct2"], "indent": 4, "domain": "Text-Processing", "code": "    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :param mini_prob:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n", "context": "class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n\n###The function: correct_word###\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "prompt": "Please write a python function called 'correct_word' base the context. This function corrects the spelling of a given word by finding the most probable spelling correction. It first checks if the EnSpell instance has been initialized. Then, it calculates the probability of each candidate spelling correction for the word and sorts them in ascending order. Finally, it returns the correction with the highest probability.:param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word to be corrected.\n:return: String. The most probable spelling correction for the word..\n        The context you need to refer to is as follows: class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n\n###The function: correct_word###\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "test_list": ["def test_en_bug_correct2(self):\n    \"\"\"\u6d4b\u8bd5\u82f1\u6587\u7ea0\u9519bug\"\"\"\n    print(spell.word_freq_dict.get('whould'))\n    print(spell.candidates('whould'))\n    a = spell.correct_word('whould')\n    print(a)\n    r = spell.correct('contend proble poety adress whould niether  quaties')\n    print(r)\n    assert spell.correct('whould')[0] == 'would'"], "requirements": {"Input-Output Conditions": {"requirement": "The 'correct_word' function should accept a string as input and return a string as output.", "unit_test": ["def test_correct_word_input_output():\n    assert isinstance(spell.correct_word('example'), str)\n    try:\n        spell.correct_word(123)\n    except TypeError:\n        assert True\n    else:\n        assert False"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_input_output"}, "Exception Handling": {"requirement": "The 'correct_word' function should raise a ValueError with a descriptive message: 'Input word cannot be an empty string.' if the input word is an empty string.", "unit_test": ["def test_correct_word_exception_handling():\n    try:\n        spell.correct_word('')\n    except ValueError as e:\n        assert str(e) == 'Input word cannot be an empty string.'\n    else:\n        assert False"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_exception_handling"}, "Edge Case Handling": {"requirement": "The 'correct_word' function should handle edge cases such as very short words (e.g., single-letter words) and return them unchanged if no correction is found.", "unit_test": ["def test_correct_word_edge_cases():\n    assert spell.correct_word('a') == 'a'\n    assert spell.correct_word('I') == 'I'"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'correct_word' function to accept an optional parameter 'max_candidates' to limit the number of candidate corrections considered.", "unit_test": ["def test_correct_word_functionality_extension():\n    assert len(spell.candidates('whould')) > 1\n    assert len(spell.candidates('whould', max_candidates=1)) == 1"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'correct_word' function has complete parameter and return type annotations.", "unit_test": ["def test_correct_word_annotation_coverage():\n    assert 'word: str' in spell.correct_word.__annotations__\n    assert 'return: str' in spell.correct_word.__annotations__"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_annotation_coverage"}, "Code Complexity": {"requirement": "The 'correct_word' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": ["def test_correct_word_code_complexity():\n    from radon.complexity import cc_visit\n    complexity = cc_visit(spell.correct_word)\n    assert complexity[0].complexity <= 5"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_code_complexity"}, "Code Standard": {"requirement": "The 'correct_word' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(spell.correct_word)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'correct_word' function utilizes the 'candidates' method from the EnSpell class to generate possible corrections.", "unit_test": ["def test_correct_word_context_usage():\n    original_candidates = spell.candidates\n    spell.candidates = lambda x: {'mock_candidate'}\n    assert spell.correct_word('whould') == 'mock_candidate'\n    spell.candidates = original_candidates"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'correct_word' function correctly uses the 'check_init' method to initialize the word frequency dictionary if needed.", "unit_test": ["def test_correct_word_context_usage_correctness():\n    spell.word_freq_dict = {}\n    spell.correct_word('whould')\n    assert spell.word_freq_dict != {}"], "test": "tests/en_spell_bug_fix_test.py::EnBugTestCase::test_correct_word_context_usage_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "type": "method", "project_path": "Software-Development/dash", "completion_path": "Software-Development/dash/dash/development/base_component.py", "signature_position": [203, 204], "body_position": [205, 228], "dependency": {"intra_class": ["dash.development.base_component.Component._namespace", "dash.development.base_component.Component._prop_names", "dash.development.base_component.Component._type"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function converts a Component instance into a JSON object that can be used by Plotly. It extracts the normal properties of the Component instance and adds them to the JSON object. It also adds any wildcard properties (properties starting with \"data-\" or \"aria-\") to the JSON object. Finally, it includes the properties, type and namespace of the Component instance in the JSON object.", "Arguments": ":param self: Component. An instance of the Component class.\n:return: JSON. The JSON representation of the Component instance."}, "tests": ["tests/unit/development/test_base_component.py::test_debc022_to_plotly_json_with_children", "tests/unit/development/test_base_component.py::test_debc012_to_plotly_json_full_tree", "tests/unit/development/test_base_component.py::test_debc020_to_plotly_json_without_children", "tests/unit/development/test_base_component.py::test_debc023_to_plotly_json_with_wildcards", "tests/unit/development/test_base_component.py::test_debc021_to_plotly_json_with_null_arguments"], "indent": 4, "domain": "Software-Development", "code": "    def to_plotly_json(self):\n        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json\n", "context": "class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n###The function: to_plotly_json###\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n    def _traverse(self):\n        \"\"\"Yield each item in the tree.\"\"\"\n        for t in self._traverse_with_paths():\n            yield t[1]\n\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "prompt": "Please write a python function called 'to_plotly_json' base the context. This function converts a Component instance into a JSON object that can be used by Plotly. It extracts the normal properties of the Component instance and adds them to the JSON object. It also adds any wildcard properties (properties starting with \"data-\" or \"aria-\") to the JSON object. Finally, it includes the properties, type and namespace of the Component instance in the JSON object.:param self: Component. An instance of the Component class.\n:return: JSON. The JSON representation of the Component instance..\n        The context you need to refer to is as follows: class Component(metaclass=ComponentMeta):\n    _children_props = []\n    _base_nodes = [\"children\"]\n\n    class _UNDEFINED:\n        def __repr__(self):\n            return \"undefined\"\n\n        def __str__(self):\n            return \"undefined\"\n\n    UNDEFINED = _UNDEFINED()\n\n    class _REQUIRED:\n        def __repr__(self):\n            return \"required\"\n\n        def __str__(self):\n            return \"required\"\n\n    REQUIRED = _REQUIRED()\n\n    def __init__(self, **kwargs):\n        import dash  # pylint: disable=import-outside-toplevel, cyclic-import\n\n        # pylint: disable=super-init-not-called\n        for k, v in list(kwargs.items()):\n            # pylint: disable=no-member\n            k_in_propnames = k in self._prop_names\n            k_in_wildcards = any(\n                k.startswith(w) for w in self._valid_wildcard_attributes\n            )\n            # e.g. \"The dash_core_components.Dropdown component (version 1.6.0)\n            # with the ID \"my-dropdown\"\n            id_suffix = f' with the ID \"{kwargs[\"id\"]}\"' if \"id\" in kwargs else \"\"\n            try:\n                # Get fancy error strings that have the version numbers\n                error_string_prefix = \"The `{}.{}` component (version {}){}\"\n                # These components are part of dash now, so extract the dash version:\n                dash_packages = {\n                    \"dash_html_components\": \"html\",\n                    \"dash_core_components\": \"dcc\",\n                    \"dash_table\": \"dash_table\",\n                }\n                if self._namespace in dash_packages:\n                    error_string_prefix = error_string_prefix.format(\n                        dash_packages[self._namespace],\n                        self._type,\n                        dash.__version__,\n                        id_suffix,\n                    )\n                else:\n                    # Otherwise import the package and extract the version number\n                    error_string_prefix = error_string_prefix.format(\n                        self._namespace,\n                        self._type,\n                        getattr(__import__(self._namespace), \"__version__\", \"unknown\"),\n                        id_suffix,\n                    )\n            except ImportError:\n                # Our tests create mock components with libraries that\n                # aren't importable\n                error_string_prefix = f\"The `{self._type}` component{id_suffix}\"\n\n            if not k_in_propnames and not k_in_wildcards:\n                allowed_args = \", \".join(\n                    sorted(self._prop_names)\n                )  # pylint: disable=no-member\n                raise TypeError(\n                    f\"{error_string_prefix} received an unexpected keyword argument: `{k}`\"\n                    f\"\\nAllowed arguments: {allowed_args}\"\n                )\n\n            if k not in self._base_nodes and isinstance(v, Component):\n                raise TypeError(\n                    error_string_prefix\n                    + \" detected a Component for a prop other than `children`\\n\"\n                    + f\"Prop {k} has value {v!r}\\n\\n\"\n                    + \"Did you forget to wrap multiple `children` in an array?\\n\"\n                    + 'For example, it must be html.Div([\"a\", \"b\", \"c\"]) not html.Div(\"a\", \"b\", \"c\")\\n'\n                )\n\n            if k == \"id\":\n                if isinstance(v, dict):\n                    for id_key, id_val in v.items():\n                        if not isinstance(id_key, str):\n                            raise TypeError(\n                                \"dict id keys must be strings,\\n\"\n                                + f\"found {id_key!r} in id {v!r}\"\n                            )\n                        if not isinstance(id_val, (str, int, float, bool)):\n                            raise TypeError(\n                                \"dict id values must be strings, numbers or bools,\\n\"\n                                + f\"found {id_val!r} in id {v!r}\"\n                            )\n                elif not isinstance(v, str):\n                    raise TypeError(f\"`id` prop must be a string or dict, not {v!r}\")\n\n            setattr(self, k, v)\n\n    def _set_random_id(self):\n\n        if hasattr(self, \"id\"):\n            return getattr(self, \"id\")\n\n        kind = f\"`{self._namespace}.{self._type}`\"  # pylint: disable=no-member\n\n        if getattr(self, \"persistence\", False):\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID with the `persistence` prop.\n                This is prohibited because persistence is tied to component IDs and\n                auto-generated IDs can easily change.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n        if \"dash_snapshots\" in sys.modules:\n            raise RuntimeError(\n                f\"\"\"\n                Attempting to use an auto-generated ID in an app with `dash_snapshots`.\n                This is prohibited because snapshots saves the whole app layout,\n                including component IDs, and auto-generated IDs can easily change.\n                Callbacks referencing the new IDs will not work with old snapshots.\n\n                Please assign an explicit ID to this {kind} component.\n                \"\"\"\n            )\n\n        v = str(uuid.UUID(int=rd.randint(0, 2**128)))\n        setattr(self, \"id\", v)\n        return v\n\n###The function: to_plotly_json###\n    # pylint: disable=too-many-branches, too-many-return-statements\n    # pylint: disable=redefined-builtin, inconsistent-return-statements\n    def _get_set_or_delete(self, id, operation, new_item=None):\n        _check_if_has_indexable_children(self)\n\n        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            if getattr(self.children, \"id\", None) is not None:\n                # Woohoo! It's the item that we're looking for\n                if self.children.id == id:\n                    if operation == \"get\":\n                        return self.children\n                    if operation == \"set\":\n                        self.children = new_item\n                        return\n                    if operation == \"delete\":\n                        self.children = None\n                        return\n\n            # Recursively dig into its subtree\n            try:\n                if operation == \"get\":\n                    return self.children.__getitem__(id)\n                if operation == \"set\":\n                    self.children.__setitem__(id, new_item)\n                    return\n                if operation == \"delete\":\n                    self.children.__delitem__(id)\n                    return\n            except KeyError:\n                pass\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for i, item in enumerate(self.children):\n                # If the item itself is the one we're looking for\n                if getattr(item, \"id\", None) == id:\n                    if operation == \"get\":\n                        return item\n                    if operation == \"set\":\n                        self.children[i] = new_item\n                        return\n                    if operation == \"delete\":\n                        del self.children[i]\n                        return\n\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, Component):\n                    try:\n                        if operation == \"get\":\n                            return item.__getitem__(id)\n                        if operation == \"set\":\n                            item.__setitem__(id, new_item)\n                            return\n                        if operation == \"delete\":\n                            item.__delitem__(id)\n                            return\n                    except KeyError:\n                        pass\n\n        # The end of our branch\n        # If we were in a list, then this exception will get caught\n        raise KeyError(id)\n\n    # Magic methods for a mapping interface:\n    # - __getitem__\n    # - __setitem__\n    # - __delitem__\n    # - __iter__\n    # - __len__\n\n    def __getitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Recursively find the element with the given ID through the tree of\n        children.\"\"\"\n\n        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        return self._get_set_or_delete(id, \"get\")\n\n    def __setitem__(self, id, item):  # pylint: disable=redefined-builtin\n        \"\"\"Set an element by its ID.\"\"\"\n        return self._get_set_or_delete(id, \"set\", item)\n\n    def __delitem__(self, id):  # pylint: disable=redefined-builtin\n        \"\"\"Delete items by ID in the tree of children.\"\"\"\n        return self._get_set_or_delete(id, \"delete\")\n\n    def _traverse(self):\n        \"\"\"Yield each item in the tree.\"\"\"\n        for t in self._traverse_with_paths():\n            yield t[1]\n\n    @staticmethod\n    def _id_str(component):\n        id_ = stringify_id(getattr(component, \"id\", \"\"))\n        return id_ and f\" (id={id_:s})\"\n\n    def _traverse_with_paths(self):\n        \"\"\"Yield each item with its path in the tree.\"\"\"\n        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield \"[*] \" + children_string, children\n            # pylint: disable=protected-access\n            for p, t in children._traverse_with_paths():\n                yield \"\\n\".join([\"[*] \" + children_string, p]), t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for idx, i in enumerate(children):\n                list_path = f\"[{idx:d}] {type(i).__name__:s}{self._id_str(i)}\"\n                yield list_path, i\n\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for p, t in i._traverse_with_paths():\n                        yield \"\\n\".join([list_path, p]), t\n\n    def _traverse_ids(self):\n        \"\"\"Yield components with IDs in the tree of children.\"\"\"\n        for t in self._traverse():\n            if isinstance(t, Component) and getattr(t, \"id\", None) is not None:\n                yield t\n\n    def __iter__(self):\n        \"\"\"Yield IDs in the tree of children.\"\"\"\n        for t in self._traverse_ids():\n            yield t.id\n\n    def __len__(self):\n        \"\"\"Return the number of items in the tree.\"\"\"\n        # TODO - Should we return the number of items that have IDs\n        # or just the number of items?\n        # The number of items is more intuitive but returning the number\n        # of IDs matches __iter__ better.\n        length = 0\n        if getattr(self, \"children\", None) is None:\n            length = 0\n        elif isinstance(self.children, Component):\n            length = 1\n            length += len(self.children)\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for c in self.children:\n                length += 1\n                if isinstance(c, Component):\n                    length += len(c)\n        else:\n            # string or number\n            length = 1\n        return length\n\n    def __repr__(self):\n        # pylint: disable=no-member\n        props_with_values = [\n            c for c in self._prop_names if getattr(self, c, None) is not None\n        ] + [\n            c\n            for c in self.__dict__\n            if any(c.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes)\n        ]\n        if any(p != \"children\" for p in props_with_values):\n            props_string = \", \".join(\n                f\"{p}={getattr(self, p)!r}\" for p in props_with_values\n            )\n        else:\n            props_string = repr(getattr(self, \"children\", None))\n        return f\"{self._type}({props_string})\"\n", "test_list": ["def test_debc022_to_plotly_json_with_children():\n    c = Component(id='a', children='Hello World')\n    c._prop_names = ('id', 'children')\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a', 'children': 'Hello World'}, 'type': 'MyComponent'}", "def test_debc012_to_plotly_json_full_tree():\n    c = nested_tree()[0]\n    Component._namespace\n    Component._type\n    expected = {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': [{'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'id': '0.0'}}, {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': [10, None, 'wrap string', {'type': 'TestComponent', 'namespace': 'test_namespace', 'props': {'children': 'string', 'id': '0.1.x.x.0'}}, 'another string', 4.51], 'id': '0.1.x.x'}}, 'id': '0.1.x'}}, 'id': '0.1'}}], 'id': '0'}}\n    res = json.loads(json.dumps(c.to_plotly_json(), cls=plotly.utils.PlotlyJSONEncoder))\n    assert res == expected", "def test_debc020_to_plotly_json_without_children():\n    c = Component(id='a')\n    c._prop_names = ('id',)\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a'}, 'type': 'MyComponent'}", "def test_debc023_to_plotly_json_with_wildcards():\n    c = Component(id='a', **{'aria-expanded': 'true', 'data-toggle': 'toggled', 'data-none': None})\n    c._prop_names = ('id',)\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'aria-expanded': 'true', 'data-toggle': 'toggled', 'data-none': None, 'id': 'a'}, 'type': 'MyComponent'}", "def test_debc021_to_plotly_json_with_null_arguments():\n    c = Component(id='a')\n    c._prop_names = ('id', 'style')\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a'}, 'type': 'MyComponent'}\n    c = Component(id='a', style=None)\n    c._prop_names = ('id', 'style')\n    c._type = 'MyComponent'\n    c._namespace = 'basic'\n    assert c.to_plotly_json() == {'namespace': 'basic', 'props': {'id': 'a', 'style': None}, 'type': 'MyComponent'}"], "requirements": {"Input-Output Conditions": {"requirement": "The 'to_plotly_json' function should correctly convert a Component instance to a JSON object, ensuring that all properties, including wildcard properties, are accurately represented in the output.", "unit_test": ["def test_to_plotly_json_output_structure():\n    c = Component(id='test', **{'data-test': 'value'})\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert 'namespace' in result\n    assert 'props' in result\n    assert 'type' in result\n    assert result['props']['id'] == 'test'\n    assert result['props']['data-test'] == 'value'"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_output_structure"}, "Exception Handling": {"requirement": "The 'to_plotly_json' function should raise a TypeError if a Component instance has properties not defined in '_prop_names' or not matching wildcard attributes.", "unit_test": ["def test_to_plotly_json_unexpected_property():\n    c = Component(id='test', unexpected_prop='value')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    try:\n        c.to_plotly_json()\n    except TypeError as e:\n        assert 'unexpected keyword argument' in str(e)"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_unexpected_property"}, "Edge Case Handling": {"requirement": "The 'to_plotly_json' function should handle cases where the Component instance has no properties set, returning a JSON object with only 'type' and 'namespace'.", "unit_test": ["def test_to_plotly_json_no_properties():\n    c = Component()\n    c._prop_names = ()\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert result == {'namespace': 'test_namespace', 'props': {}, 'type': 'TestComponent'}"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_no_properties"}, "Functionality Extension": {"requirement": "Extend the 'to_plotly_json' function to include a 'version' key in the JSON object, representing the version of the component's namespace.", "unit_test": ["def test_to_plotly_json_with_version():\n    c = Component(id='test')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    c._version = '1.0.0'\n    result = c.to_plotly_json()\n    assert result['version'] == '1.0.0'"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_with_version"}, "Annotation Coverage": {"requirement": "Ensure that the 'to_plotly_json' function includes comprehensive docstrings and type annotations for all parameters and return types.", "unit_test": ["def test_to_plotly_json_annotations():\n    import inspect\n    annotations = inspect.getfullargspec(Component.to_plotly_json).annotations\n    assert 'self' in annotations\n    assert annotations['return'] == dict"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_annotations"}, "Code Complexity": {"requirement": "The 'to_plotly_json' function should maintain a cyclomatic complexity of 10 or lower to ensure readability and maintainability.", "unit_test": ["def test_to_plotly_json_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(Component.to_plotly_json)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 10"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_complexity"}, "Code Standard": {"requirement": "The 'to_plotly_json' function should adhere to PEP 8 standards, ensuring proper formatting and style.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(Component.to_plotly_json)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/unit/development/test_base_component.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'to_plotly_json' function should utilize the '_namespace', '_prop_names', and '_type' attributes of the Component class.", "unit_test": ["def test_to_plotly_json_context_usage():\n    c = Component(id='test')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert result['namespace'] == 'test_namespace'\n    assert result['type'] == 'TestComponent'"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'to_plotly_json' function correctly uses the '_namespace', '_prop_names', and '_type' attributes to construct the JSON object.", "unit_test": ["def test_to_plotly_json_context_correctness():\n    c = Component(id='test')\n    c._prop_names = ('id',)\n    c._type = 'TestComponent'\n    c._namespace = 'test_namespace'\n    result = c.to_plotly_json()\n    assert result['namespace'] == c._namespace\n    assert result['type'] == c._type"], "test": "tests/unit/development/test_base_component.py::test_to_plotly_json_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "type": "method", "project_path": "Scientific-Engineering/lux", "completion_path": "Scientific-Engineering/lux/lux/vis/Vis.py", "signature_position": [142, 142], "body_position": [143, 149], "dependency": {"intra_class": ["lux.vis.Vis.Vis._inferred_intent"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function retrieves the attribute based on the given channel from the inferred intent list. It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.", "Arguments": ":param self: Vis. An instance of the Vis class.\n:param channel: The channel to filter the inferred intent list.\n:return: List. The filtered list of objects from the inferred intent list."}, "tests": ["tests/test_dates.py::test_refresh_inplace", "tests/test_columns.py::test_special_char"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def get_attr_by_channel(self, channel):\n        spec_obj = list(\n            filter(\n                lambda x: x.channel == channel and x.value == \"\" if hasattr(x, \"channel\") else False,\n                self._inferred_intent,\n            )\n        )\n        return spec_obj\n", "context": "class Vis:\n    \"\"\"\n    Vis Object represents a collection of fully fleshed out specifications required for data fetching and visualization.\n    \"\"\"\n\n    def __init__(self, intent, source=None, title=\"\", score=0.0):\n        self._intent = intent  # user's original intent to Vis\n        self._inferred_intent = intent  # re-written, expanded version of user's original intent\n        self._source = source  # original data attached to the Vis\n        self._vis_data = None  # processed data for Vis (e.g., selected, aggregated, binned)\n        self._code = None\n        self._mark = \"\"\n        self._min_max = {}\n        self._postbin = None\n        self.title = title\n        self.score = score\n        self._all_column = False\n        self.approx = False\n        self.refresh_source(self._source)\n\n    def __repr__(self):\n        all_clause = all([isinstance(unit, lux.Clause) for unit in self._inferred_intent])\n        if all_clause:\n            filter_intents = None\n            channels, additional_channels = [], []\n            for clause in self._inferred_intent:\n\n                if hasattr(clause, \"value\"):\n                    if clause.value != \"\":\n                        filter_intents = clause\n                if hasattr(clause, \"attribute\"):\n                    if clause.attribute != \"\":\n                        if clause.aggregation != \"\" and clause.aggregation is not None:\n                            attribute = f\"{clause._aggregation_name.upper()}({clause.attribute})\"\n                        elif clause.bin_size > 0:\n                            attribute = f\"BIN({clause.attribute})\"\n                        else:\n                            attribute = clause.attribute\n                        if clause.channel == \"x\":\n                            channels.insert(0, [clause.channel, attribute])\n                        elif clause.channel == \"y\":\n                            channels.insert(1, [clause.channel, attribute])\n                        elif clause.channel != \"\":\n                            additional_channels.append([clause.channel, attribute])\n\n            channels.extend(additional_channels)\n            str_channels = \"\"\n            for channel in channels:\n                str_channels += f\"{channel[0]}: {channel[1]}, \"\n\n            if filter_intents:\n                return f\"<Vis  ({str_channels[:-2]} -- [{filter_intents.attribute}{filter_intents.filter_op}{filter_intents.value}]) mark: {self._mark}, score: {self.score} >\"\n            else:\n                return f\"<Vis  ({str_channels[:-2]}) mark: {self._mark}, score: {self.score} >\"\n        else:\n            # When Vis not compiled (e.g., when self._source not populated), print original intent\n            return f\"<Vis  ({str(self._intent)}) mark: {self._mark}, score: {self.score} >\"\n\n    @property\n    def data(self):\n        return self._vis_data\n\n    @property\n    def code(self):\n        return self._code\n\n    @property\n    def mark(self):\n        return self._mark\n\n    @property\n    def min_max(self):\n        return self._min_max\n\n    @property\n    def intent(self):\n        return self._intent\n\n    @intent.setter\n    def intent(self, intent: List[Clause]) -> None:\n        self.set_intent(intent)\n\n    def set_intent(self, intent: List[Clause]) -> None:\n        \"\"\"\n        Sets the intent of the Vis and refresh the source based on the new intent\n\n        Parameters\n        ----------\n        intent : List[Clause]\n                Query specifying the desired VisList\n        \"\"\"\n        self._intent = intent\n        self.refresh_source(self._source)\n\n    def _ipython_display_(self):\n        from lux.utils.utils import check_import_lux_widget\n        from IPython.display import display\n\n        check_import_lux_widget()\n        import luxwidget\n\n        if self.data is None:\n            raise Exception(\n                \"No data is populated in Vis. In order to generate data required for the vis, use the 'refresh_source' function to populate the Vis with a data source (e.g., vis.refresh_source(df)).\"\n            )\n        else:\n            from lux.core.frame import LuxDataFrame\n\n            widget = luxwidget.LuxWidget(\n                currentVis=LuxDataFrame.current_vis_to_JSON([self]),\n                recommendations=[],\n                intent=\"\",\n                message=\"\",\n                config={\"plottingScale\": lux.config.plotting_scale},\n            )\n            display(widget)\n\n    def get_attr_by_attr_name(self, attr_name):\n        return list(filter(lambda x: x.attribute == attr_name, self._inferred_intent))\n\n###The function: get_attr_by_channel###\n    def get_attr_by_data_model(self, dmodel, exclude_record=False):\n        if exclude_record:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if x.attribute != \"Record\" and hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n        else:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n\n    def get_attr_by_data_type(self, dtype):\n        return list(\n            filter(\n                lambda x: x.data_type == dtype and x.value == \"\" if hasattr(x, \"data_type\") else False,\n                self._inferred_intent,\n            )\n        )\n\n    def remove_filter_from_spec(self, value):\n        new_intent = list(filter(lambda x: x.value != value, self._inferred_intent))\n        self.set_intent(new_intent)\n\n    def remove_column_from_spec(self, attribute, remove_first: bool = False):\n        \"\"\"\n        Removes an attribute from the Vis's clause\n\n        Parameters\n        ----------\n        attribute : str\n                attribute to be removed\n        remove_first : bool, optional\n                Boolean flag to determine whether to remove all instances of the attribute or only one (first) instance, by default False\n        \"\"\"\n        if not remove_first:\n            new_inferred = list(filter(lambda x: x.attribute != attribute, self._inferred_intent))\n            self._inferred_intent = new_inferred\n            self._intent = new_inferred\n        elif remove_first:\n            new_inferred = []\n            skip_check = False\n            for i in range(0, len(self._inferred_intent)):\n                if self._inferred_intent[i].value == \"\":  # clause is type attribute\n                    column_spec = []\n                    column_names = self._inferred_intent[i].attribute\n                    # if only one variable in a column, columnName results in a string and not a list so\n                    # you need to differentiate the cases\n                    if isinstance(column_names, list):\n                        for column in column_names:\n                            if (column != attribute) or skip_check:\n                                column_spec.append(column)\n                            elif remove_first:\n                                remove_first = True\n                        new_inferred.append(Clause(column_spec))\n                    else:\n                        if column_names != attribute or skip_check:\n                            new_inferred.append(Clause(attribute=column_names))\n                        elif remove_first:\n                            skip_check = True\n                else:\n                    new_inferred.append(self._inferred_intent[i])\n            self._intent = new_inferred\n            self._inferred_intent = new_inferred\n\n    def to_altair(self, standalone=False) -> str:\n        \"\"\"\n        Generate minimal Altair code to visualize the Vis\n\n        Parameters\n        ----------\n        standalone : bool, optional\n                Flag to determine if outputted code uses user-defined variable names or can be run independently, by default False\n\n        Returns\n        -------\n        str\n                String version of the Altair code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"Altair\")\n        self._code = renderer.create_vis(self, standalone)\n\n        if lux.config.executor.name == \"PandasExecutor\":\n            function_code = \"def plot_data(source_df, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(source_df, vis)\\n\"\n        else:\n            function_code = \"def plot_data(tbl, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(tbl, vis)\\n\"\n\n        vis_code_lines = self._code.split(\"\\n\")\n        for i in range(2, len(vis_code_lines) - 1):\n            function_code += \"\\t\" + vis_code_lines[i] + \"\\n\"\n        function_code += \"\\treturn chart\\n#plot_data(your_df, vis) this creates an Altair plot using your source data and vis specification\"\n        function_code = function_code.replace(\"alt.Chart(tbl)\", \"alt.Chart(visData)\")\n\n        if \"mark_circle\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_scatterplot\")\n        elif \"mark_bar\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_barchart\")\n        elif \"mark_line\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_linechart\")\n        elif \"mark_rect\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_heatmap\")\n        return function_code\n\n    def to_matplotlib(self) -> str:\n        \"\"\"\n        Generate minimal Matplotlib code to visualize the Vis\n\n        Returns\n        -------\n        str\n                String version of the Matplotlib code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def _to_matplotlib_svg(self) -> str:\n        \"\"\"\n        Private method to render Vis as SVG with Matplotlib\n\n        Returns\n        -------\n        str\n                String version of the SVG.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib_svg\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def to_vegalite(self, prettyOutput=True) -> Union[dict, str]:\n        \"\"\"\n        Generate minimal Vega-Lite code to visualize the Vis\n\n        Returns\n        -------\n        Union[dict,str]\n                String or Dictionary of the VegaLite JSON specification\n        \"\"\"\n        import json\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"VegaLite\")\n        self._code = renderer.create_vis(self)\n        if prettyOutput:\n            return (\n                \"** Remove this comment -- Copy Text Below to Vega Editor(vega.github.io/editor) to visualize and edit **\\n\"\n                + json.dumps(self._code, indent=2)\n            )\n        else:\n            return self._code\n\n    def to_code(self, language=\"vegalite\", **kwargs):\n        \"\"\"\n        Export Vis object to code specification\n\n        Parameters\n        ----------\n        language : str, optional\n            choice of target language to produce the visualization code in, by default \"vegalite\"\n\n        Returns\n        -------\n        spec:\n            visualization specification corresponding to the Vis object\n        \"\"\"\n        if language == \"vegalite\":\n            return self.to_vegalite(**kwargs)\n        elif language == \"altair\":\n            return self.to_altair(**kwargs)\n        elif language == \"matplotlib\":\n            return self.to_matplotlib()\n        elif language == \"matplotlib_svg\":\n            return self._to_matplotlib_svg()\n        elif language == \"python\":\n            lux.config.tracer.start_tracing()\n            lux.config.executor.execute(lux.vis.VisList.VisList(input_lst=[self]), self._source)\n            lux.config.tracer.stop_tracing()\n            self._trace_code = lux.config.tracer.process_executor_code(lux.config.tracer_relevant_lines)\n            lux.config.tracer_relevant_lines = []\n            return self._trace_code\n        elif language == \"SQL\":\n            if self._query:\n                return self._query\n            else:\n                warnings.warn(\n                    \"The data for this Vis was not collected via a SQL database. Use the 'python' parameter to view the code used to generate the data.\",\n                    stacklevel=2,\n                )\n        else:\n            warnings.warn(\n                \"Unsupported plotting backend. Lux currently only support 'altair', 'vegalite', or 'matplotlib'\",\n                stacklevel=2,\n            )\n\n    def refresh_source(self, ldf):  # -> Vis:\n        \"\"\"\n        Loading the source data into the Vis by instantiating the specification and\n        populating the Vis based on the source data, effectively \"materializing\" the Vis.\n\n        Parameters\n        ----------\n        ldf : LuxDataframe\n                Input Dataframe to be attached to the Vis\n\n        Returns\n        -------\n        Vis\n                Complete Vis with fully-specified fields\n\n        See Also\n        --------\n        lux.Vis.VisList.refresh_source\n\n        Note\n        ----\n        Function derives a new _inferred_intent by instantiating the intent specification on the new data\n        \"\"\"\n        if ldf is not None:\n            from lux.processor.Parser import Parser\n            from lux.processor.Validator import Validator\n            from lux.processor.Compiler import Compiler\n\n            self.check_not_vislist_intent()\n\n            ldf.maintain_metadata()\n            self._source = ldf\n            self._inferred_intent = Parser.parse(self._intent)\n            Validator.validate_intent(self._inferred_intent, ldf)\n\n            Compiler.compile_vis(ldf, self)\n            lux.config.executor.execute([self], ldf)\n\n    def check_not_vislist_intent(self):\n\n        syntaxMsg = (\n            \"The intent that you specified corresponds to more than one visualization. \"\n            \"Please replace the Vis constructor with VisList to generate a list of visualizations. \"\n            \"For more information, see: https://lux-api.readthedocs.io/en/latest/source/guide/vis.html#working-with-collections-of-visualization-with-vislist\"\n        )\n\n        for i in range(len(self._intent)):\n            clause = self._intent[i]\n            if isinstance(clause, str):\n                if \"|\" in clause or \"?\" in clause:\n                    raise TypeError(syntaxMsg)\n            if isinstance(clause, list):\n                raise TypeError(syntaxMsg)\n", "prompt": "Please write a python function called 'get_attr_by_channel' base the context. This function retrieves the attribute based on the given channel from the inferred intent list. It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.:param self: Vis. An instance of the Vis class.\n:param channel: The channel to filter the inferred intent list.\n:return: List. The filtered list of objects from the inferred intent list..\n        The context you need to refer to is as follows: class Vis:\n    \"\"\"\n    Vis Object represents a collection of fully fleshed out specifications required for data fetching and visualization.\n    \"\"\"\n\n    def __init__(self, intent, source=None, title=\"\", score=0.0):\n        self._intent = intent  # user's original intent to Vis\n        self._inferred_intent = intent  # re-written, expanded version of user's original intent\n        self._source = source  # original data attached to the Vis\n        self._vis_data = None  # processed data for Vis (e.g., selected, aggregated, binned)\n        self._code = None\n        self._mark = \"\"\n        self._min_max = {}\n        self._postbin = None\n        self.title = title\n        self.score = score\n        self._all_column = False\n        self.approx = False\n        self.refresh_source(self._source)\n\n    def __repr__(self):\n        all_clause = all([isinstance(unit, lux.Clause) for unit in self._inferred_intent])\n        if all_clause:\n            filter_intents = None\n            channels, additional_channels = [], []\n            for clause in self._inferred_intent:\n\n                if hasattr(clause, \"value\"):\n                    if clause.value != \"\":\n                        filter_intents = clause\n                if hasattr(clause, \"attribute\"):\n                    if clause.attribute != \"\":\n                        if clause.aggregation != \"\" and clause.aggregation is not None:\n                            attribute = f\"{clause._aggregation_name.upper()}({clause.attribute})\"\n                        elif clause.bin_size > 0:\n                            attribute = f\"BIN({clause.attribute})\"\n                        else:\n                            attribute = clause.attribute\n                        if clause.channel == \"x\":\n                            channels.insert(0, [clause.channel, attribute])\n                        elif clause.channel == \"y\":\n                            channels.insert(1, [clause.channel, attribute])\n                        elif clause.channel != \"\":\n                            additional_channels.append([clause.channel, attribute])\n\n            channels.extend(additional_channels)\n            str_channels = \"\"\n            for channel in channels:\n                str_channels += f\"{channel[0]}: {channel[1]}, \"\n\n            if filter_intents:\n                return f\"<Vis  ({str_channels[:-2]} -- [{filter_intents.attribute}{filter_intents.filter_op}{filter_intents.value}]) mark: {self._mark}, score: {self.score} >\"\n            else:\n                return f\"<Vis  ({str_channels[:-2]}) mark: {self._mark}, score: {self.score} >\"\n        else:\n            # When Vis not compiled (e.g., when self._source not populated), print original intent\n            return f\"<Vis  ({str(self._intent)}) mark: {self._mark}, score: {self.score} >\"\n\n    @property\n    def data(self):\n        return self._vis_data\n\n    @property\n    def code(self):\n        return self._code\n\n    @property\n    def mark(self):\n        return self._mark\n\n    @property\n    def min_max(self):\n        return self._min_max\n\n    @property\n    def intent(self):\n        return self._intent\n\n    @intent.setter\n    def intent(self, intent: List[Clause]) -> None:\n        self.set_intent(intent)\n\n    def set_intent(self, intent: List[Clause]) -> None:\n        \"\"\"\n        Sets the intent of the Vis and refresh the source based on the new intent\n\n        Parameters\n        ----------\n        intent : List[Clause]\n                Query specifying the desired VisList\n        \"\"\"\n        self._intent = intent\n        self.refresh_source(self._source)\n\n    def _ipython_display_(self):\n        from lux.utils.utils import check_import_lux_widget\n        from IPython.display import display\n\n        check_import_lux_widget()\n        import luxwidget\n\n        if self.data is None:\n            raise Exception(\n                \"No data is populated in Vis. In order to generate data required for the vis, use the 'refresh_source' function to populate the Vis with a data source (e.g., vis.refresh_source(df)).\"\n            )\n        else:\n            from lux.core.frame import LuxDataFrame\n\n            widget = luxwidget.LuxWidget(\n                currentVis=LuxDataFrame.current_vis_to_JSON([self]),\n                recommendations=[],\n                intent=\"\",\n                message=\"\",\n                config={\"plottingScale\": lux.config.plotting_scale},\n            )\n            display(widget)\n\n    def get_attr_by_attr_name(self, attr_name):\n        return list(filter(lambda x: x.attribute == attr_name, self._inferred_intent))\n\n###The function: get_attr_by_channel###\n    def get_attr_by_data_model(self, dmodel, exclude_record=False):\n        if exclude_record:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if x.attribute != \"Record\" and hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n        else:\n            return list(\n                filter(\n                    lambda x: x.data_model == dmodel and x.value == \"\"\n                    if hasattr(x, \"data_model\")\n                    else False,\n                    self._inferred_intent,\n                )\n            )\n\n    def get_attr_by_data_type(self, dtype):\n        return list(\n            filter(\n                lambda x: x.data_type == dtype and x.value == \"\" if hasattr(x, \"data_type\") else False,\n                self._inferred_intent,\n            )\n        )\n\n    def remove_filter_from_spec(self, value):\n        new_intent = list(filter(lambda x: x.value != value, self._inferred_intent))\n        self.set_intent(new_intent)\n\n    def remove_column_from_spec(self, attribute, remove_first: bool = False):\n        \"\"\"\n        Removes an attribute from the Vis's clause\n\n        Parameters\n        ----------\n        attribute : str\n                attribute to be removed\n        remove_first : bool, optional\n                Boolean flag to determine whether to remove all instances of the attribute or only one (first) instance, by default False\n        \"\"\"\n        if not remove_first:\n            new_inferred = list(filter(lambda x: x.attribute != attribute, self._inferred_intent))\n            self._inferred_intent = new_inferred\n            self._intent = new_inferred\n        elif remove_first:\n            new_inferred = []\n            skip_check = False\n            for i in range(0, len(self._inferred_intent)):\n                if self._inferred_intent[i].value == \"\":  # clause is type attribute\n                    column_spec = []\n                    column_names = self._inferred_intent[i].attribute\n                    # if only one variable in a column, columnName results in a string and not a list so\n                    # you need to differentiate the cases\n                    if isinstance(column_names, list):\n                        for column in column_names:\n                            if (column != attribute) or skip_check:\n                                column_spec.append(column)\n                            elif remove_first:\n                                remove_first = True\n                        new_inferred.append(Clause(column_spec))\n                    else:\n                        if column_names != attribute or skip_check:\n                            new_inferred.append(Clause(attribute=column_names))\n                        elif remove_first:\n                            skip_check = True\n                else:\n                    new_inferred.append(self._inferred_intent[i])\n            self._intent = new_inferred\n            self._inferred_intent = new_inferred\n\n    def to_altair(self, standalone=False) -> str:\n        \"\"\"\n        Generate minimal Altair code to visualize the Vis\n\n        Parameters\n        ----------\n        standalone : bool, optional\n                Flag to determine if outputted code uses user-defined variable names or can be run independently, by default False\n\n        Returns\n        -------\n        str\n                String version of the Altair code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"Altair\")\n        self._code = renderer.create_vis(self, standalone)\n\n        if lux.config.executor.name == \"PandasExecutor\":\n            function_code = \"def plot_data(source_df, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(source_df, vis)\\n\"\n        else:\n            function_code = \"def plot_data(tbl, vis):\\n\"\n            function_code += \"\\timport altair as alt\\n\"\n            function_code += \"\\tvisData = create_chart_data(tbl, vis)\\n\"\n\n        vis_code_lines = self._code.split(\"\\n\")\n        for i in range(2, len(vis_code_lines) - 1):\n            function_code += \"\\t\" + vis_code_lines[i] + \"\\n\"\n        function_code += \"\\treturn chart\\n#plot_data(your_df, vis) this creates an Altair plot using your source data and vis specification\"\n        function_code = function_code.replace(\"alt.Chart(tbl)\", \"alt.Chart(visData)\")\n\n        if \"mark_circle\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_scatterplot\")\n        elif \"mark_bar\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_barchart\")\n        elif \"mark_line\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_linechart\")\n        elif \"mark_rect\" in function_code:\n            function_code = function_code.replace(\"plot_data\", \"plot_heatmap\")\n        return function_code\n\n    def to_matplotlib(self) -> str:\n        \"\"\"\n        Generate minimal Matplotlib code to visualize the Vis\n\n        Returns\n        -------\n        str\n                String version of the Matplotlib code. Need to print out the string to apply formatting.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def _to_matplotlib_svg(self) -> str:\n        \"\"\"\n        Private method to render Vis as SVG with Matplotlib\n\n        Returns\n        -------\n        str\n                String version of the SVG.\n        \"\"\"\n        from lux.vislib.matplotlib.MatplotlibRenderer import MatplotlibRenderer\n\n        renderer = MatplotlibRenderer(output_type=\"matplotlib_svg\")\n        self._code = renderer.create_vis(self)\n        return self._code\n\n    def to_vegalite(self, prettyOutput=True) -> Union[dict, str]:\n        \"\"\"\n        Generate minimal Vega-Lite code to visualize the Vis\n\n        Returns\n        -------\n        Union[dict,str]\n                String or Dictionary of the VegaLite JSON specification\n        \"\"\"\n        import json\n        from lux.vislib.altair.AltairRenderer import AltairRenderer\n\n        renderer = AltairRenderer(output_type=\"VegaLite\")\n        self._code = renderer.create_vis(self)\n        if prettyOutput:\n            return (\n                \"** Remove this comment -- Copy Text Below to Vega Editor(vega.github.io/editor) to visualize and edit **\\n\"\n                + json.dumps(self._code, indent=2)\n            )\n        else:\n            return self._code\n\n    def to_code(self, language=\"vegalite\", **kwargs):\n        \"\"\"\n        Export Vis object to code specification\n\n        Parameters\n        ----------\n        language : str, optional\n            choice of target language to produce the visualization code in, by default \"vegalite\"\n\n        Returns\n        -------\n        spec:\n            visualization specification corresponding to the Vis object\n        \"\"\"\n        if language == \"vegalite\":\n            return self.to_vegalite(**kwargs)\n        elif language == \"altair\":\n            return self.to_altair(**kwargs)\n        elif language == \"matplotlib\":\n            return self.to_matplotlib()\n        elif language == \"matplotlib_svg\":\n            return self._to_matplotlib_svg()\n        elif language == \"python\":\n            lux.config.tracer.start_tracing()\n            lux.config.executor.execute(lux.vis.VisList.VisList(input_lst=[self]), self._source)\n            lux.config.tracer.stop_tracing()\n            self._trace_code = lux.config.tracer.process_executor_code(lux.config.tracer_relevant_lines)\n            lux.config.tracer_relevant_lines = []\n            return self._trace_code\n        elif language == \"SQL\":\n            if self._query:\n                return self._query\n            else:\n                warnings.warn(\n                    \"The data for this Vis was not collected via a SQL database. Use the 'python' parameter to view the code used to generate the data.\",\n                    stacklevel=2,\n                )\n        else:\n            warnings.warn(\n                \"Unsupported plotting backend. Lux currently only support 'altair', 'vegalite', or 'matplotlib'\",\n                stacklevel=2,\n            )\n\n    def refresh_source(self, ldf):  # -> Vis:\n        \"\"\"\n        Loading the source data into the Vis by instantiating the specification and\n        populating the Vis based on the source data, effectively \"materializing\" the Vis.\n\n        Parameters\n        ----------\n        ldf : LuxDataframe\n                Input Dataframe to be attached to the Vis\n\n        Returns\n        -------\n        Vis\n                Complete Vis with fully-specified fields\n\n        See Also\n        --------\n        lux.Vis.VisList.refresh_source\n\n        Note\n        ----\n        Function derives a new _inferred_intent by instantiating the intent specification on the new data\n        \"\"\"\n        if ldf is not None:\n            from lux.processor.Parser import Parser\n            from lux.processor.Validator import Validator\n            from lux.processor.Compiler import Compiler\n\n            self.check_not_vislist_intent()\n\n            ldf.maintain_metadata()\n            self._source = ldf\n            self._inferred_intent = Parser.parse(self._intent)\n            Validator.validate_intent(self._inferred_intent, ldf)\n\n            Compiler.compile_vis(ldf, self)\n            lux.config.executor.execute([self], ldf)\n\n    def check_not_vislist_intent(self):\n\n        syntaxMsg = (\n            \"The intent that you specified corresponds to more than one visualization. \"\n            \"Please replace the Vis constructor with VisList to generate a list of visualizations. \"\n            \"For more information, see: https://lux-api.readthedocs.io/en/latest/source/guide/vis.html#working-with-collections-of-visualization-with-vislist\"\n        )\n\n        for i in range(len(self._intent)):\n            clause = self._intent[i]\n            if isinstance(clause, str):\n                if \"|\" in clause or \"?\" in clause:\n                    raise TypeError(syntaxMsg)\n            if isinstance(clause, list):\n                raise TypeError(syntaxMsg)\n", "test_list": ["def test_refresh_inplace():\n    df = pd.DataFrame({'date': ['2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01'], 'value': [10.5, 15.2, 20.3, 25.2]})\n    with pytest.warns(UserWarning, match=\"Lux detects that the attribute 'date' may be temporal.\"):\n        df._ipython_display_()\n    assert df.data_type['date'] == 'temporal'\n    from lux.vis.Vis import Vis\n    vis = Vis(['date', 'value'], df)\n    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n    df.maintain_metadata()\n    inverted_data_type = lux.config.executor.invert_data_type(df.data_type)\n    assert inverted_data_type['temporal'][0] == 'date'\n    vis.refresh_source(df)\n    assert vis.mark == 'line'\n    assert vis.get_attr_by_channel('x')[0].attribute == 'date'\n    assert vis.get_attr_by_channel('y')[0].attribute == 'value'", "def test_special_char():\n    dataset = [{'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 5}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 3}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 6}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 7}, {'special.char': 1, 'normal': 2}, {'special.char': 3, 'normal': 10}, {'special.char': 1, 'normal': 1}, {'special.char': 5, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}, {'special.char': 1, 'normal': 2}]\n    test = pd.DataFrame(dataset)\n    from lux.vis.Vis import Vis\n    vis = Vis(['special.char'], test)\n    assert vis.mark == 'bar'\n    assert vis.intent == ['special.char']\n    assert vis.get_attr_by_channel('x')[0].attribute == 'Record'\n    assert vis.get_attr_by_channel('y')[0].attribute == 'special.char'\n    vis = vis.to_altair()\n    assert \"alt.Y('specialchar', type= 'nominal', axis=alt.Axis(labelOverlap=True, title='special.char'))\" in vis\n    assert \"alt.X('Record', type= 'quantitative', title='Number of Records', axis=alt.Axis(title='Number of Records')\" in vis\n    test = test.rename(columns={'special.char': 'special..char..'})\n    vis = Vis(['special..char..'], test)\n    assert vis.mark == 'bar'\n    assert vis.intent == ['special..char..']\n    assert vis.get_attr_by_channel('x')[0].attribute == 'Record'\n    assert vis.get_attr_by_channel('y')[0].attribute == 'special..char..'\n    vis = vis.to_altair()\n    assert \"alt.Y('specialchar', type= 'nominal', axis=alt.Axis(labelOverlap=True, title='special..char..')\" in vis\n    assert \"alt.X('Record', type= 'quantitative', title='Number of Records', axis=alt.Axis(title='Number of Records')\" in vis"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'get_attr_by_channel' should accept a string as the 'channel' parameter and return a list of objects from '_inferred_intent' that have a matching 'channel' attribute.", "unit_test": ["def test_get_attr_by_channel_input_output():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [\n        lux.Clause(channel='x', attribute='date'),\n        lux.Clause(channel='y', attribute='value'),\n        lux.Clause(channel='color', attribute='category')\n    ]\n    result = vis.get_attr_by_channel('x')\n    assert len(result) == 1\n    assert result[0].attribute == 'date'"], "test": "tests/test_dates.py::test_get_attr_by_channel_input_output"}, "Exception Handling": {"requirement": "The function 'get_attr_by_channel' should raise a TypeError if the 'channel' parameter is not a string.", "unit_test": ["def test_get_attr_by_channel_exception_handling():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [lux.Clause(channel='x', attribute='date')]\n    try:\n        vis.get_attr_by_channel(123)\n    except TypeError as e:\n        assert str(e) == \"Channel must be a string\""], "test": "tests/test_dates.py::test_get_attr_by_channel_exception_handling"}, "Edge Case Handling": {"requirement": "The function 'get_attr_by_channel' should return an empty list if no objects in '_inferred_intent' match the given 'channel'.", "unit_test": ["def test_get_attr_by_channel_edge_case():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [lux.Clause(channel='x', attribute='date')]\n    result = vis.get_attr_by_channel('z')\n    assert result == []"], "test": "tests/test_dates.py::test_get_attr_by_channel_edge_case"}, "Functionality Extension": {"requirement": "Extend 'get_attr_by_channel' to optionally filter by a secondary 'attribute' parameter, returning only objects that match both 'channel' and 'attribute'.", "unit_test": ["def test_get_attr_by_channel_functionality_extension():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [\n        lux.Clause(channel='x', attribute='date'),\n        lux.Clause(channel='x', attribute='value')\n    ]\n    result = vis.get_attr_by_channel('x', 'date')\n    assert len(result) == 1\n    assert result[0].attribute == 'date'"], "test": "tests/test_dates.py::test_get_attr_by_channel_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that 'get_attr_by_channel' has complete parameter and return type annotations, besides the parameter self, the other parameter is 'channel' : str, and the return type is list.", "unit_test": ["def test_get_attr_by_channel_annotation_coverage():\n    import inspect\n    sig = inspect.signature(Vis.get_attr_by_channel)\n    assert sig.parameters['channel'].annotation == str\n    assert sig.return_annotation == list"], "test": "tests/test_dates.py::test_get_attr_by_channel_annotation_coverage"}, "Code Complexity": {"requirement": "The function 'get_attr_by_channel' should have a cyclomatic complexity of 3, indicating a simple function with no branching.", "unit_test": ["def test_get_attr_by_channel_code_complexity():\n    from lux.vis.Vis import Vis\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    code = inspect.getsource(Vis.get_attr_by_channel)\n    complexity = cc_visit(textwrap.dedent(code))\n    assert complexity[0].complexity <=3"], "test": "tests/test_dates.py::test_get_attr_by_channel_code_complexity"}, "Code Standard": {"requirement": "Ensure 'get_attr_by_channel' adheres to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    import os\n    import inspect\n    from lux.vis.Vis import Vis\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(Vis.get_attr_by_channel)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_dates.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that 'get_attr_by_channel' utilizes the '_inferred_intent' attribute from the Vis class context.", "unit_test": ["def test_get_attr_by_channel_context_usage():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [lux.Clause(channel='x', attribute='date')]\n    result = vis.get_attr_by_channel('x')\n    assert result == vis._inferred_intent"], "test": "tests/test_dates.py::test_get_attr_by_channel_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure 'get_attr_by_channel' correctly filters '_inferred_intent' based on the 'channel' attribute of each object.", "unit_test": ["def test_get_attr_by_channel_context_usage_correctness():\n    vis = Vis(intent=[])\n    vis._inferred_intent = [\n        lux.Clause(channel='x', attribute='date'),\n        lux.Clause(channel='y', attribute='value')\n    ]\n    result = vis.get_attr_by_channel('x')\n    assert len(result) == 1\n    assert result[0].channel == 'x'"], "test": "tests/test_dates.py::test_get_attr_by_channel_context_usage_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "type": "method", "project_path": "Scientific-Engineering/folium", "completion_path": "Scientific-Engineering/folium/folium/features.py", "signature_position": [323, 323], "body_position": [324, 329], "dependency": {"intra_class": ["folium.features.VegaLite.data"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function returns the major version number of the Vega-Lite schema used in the VegaLite instance. It extracts the major version number from the \"$schema\" attribute in the instance's data.", "Arguments": ":param self: VegaLite. An instance of the VegaLite class.\n:return: int. The major version number of the Vega-Lite schema used in the instance."}, "tests": ["tests/test_features.py::test_vegalite_major_version"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def vegalite_major_version(self) -> int:\n        if \"$schema\" not in self.data:\n            return None\n\n        schema = self.data[\"$schema\"]\n\n        return int(schema.split(\"/\")[-1].split(\".\")[0].lstrip(\"v\"))\n", "context": "class VegaLite(Element):\n    \"\"\"\n    Creates a Vega-Lite chart element.\n\n    Parameters\n    ----------\n    data: JSON-like str or object\n        The Vega-Lite description of the chart.\n        It can also be any object that has a method `to_json`,\n        so that you can (for instance) provide an `Altair` chart.\n    width: int or str, default None\n        The width of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    height: int or str, default None\n        The height of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    left: int or str, default '0%'\n        The horizontal distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    top: int or str, default '0%'\n        The vertical distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    position: str, default 'relative'\n        The `position` argument that the CSS shall contain.\n        Ex: 'relative', 'absolute'\n\n    \"\"\"\n\n    _template = Template(\"\")\n\n    def __init__(\n        self, data, width=None, height=None, left=\"0%\", top=\"0%\", position=\"relative\"\n    ):\n        super(self.__class__, self).__init__()\n        self._name = \"VegaLite\"\n        self.data = data.to_json() if hasattr(data, \"to_json\") else data\n        if isinstance(self.data, str):\n            self.data = json.loads(self.data)\n\n        self.json = json.dumps(self.data)\n\n        # Size Parameters.\n        self.width = _parse_size(\n            self.data.get(\"width\", \"100%\") if width is None else width\n        )\n        self.height = _parse_size(\n            self.data.get(\"height\", \"100%\") if height is None else height\n        )\n        self.left = _parse_size(left)\n        self.top = _parse_size(top)\n        self.position = position\n\n    def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        self._parent.html.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <div id=\"{{this.get_name()}}\"></div>\n            \"\"\"\n                ).render(this=self, kwargs=kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        figure = self.get_root()\n        assert isinstance(\n            figure, Figure\n        ), \"You cannot render this Element if it is not in a Figure.\"\n\n        figure.header.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <style> #{{this.get_name()}} {\n                position : {{this.position}};\n                width : {{this.width[0]}}{{this.width[1]}};\n                height: {{this.height[0]}}{{this.height[1]}};\n                left: {{this.left[0]}}{{this.left[1]}};\n                top: {{this.top[0]}}{{this.top[1]}};\n            </style>\n            \"\"\"\n                ).render(this=self, **kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        embed_mapping = {\n            1: self._embed_vegalite_v1,\n            2: self._embed_vegalite_v2,\n            3: self._embed_vegalite_v3,\n            4: self._embed_vegalite_v4,\n            5: self._embed_vegalite_v5,\n        }\n\n        # Version 2 is assumed as the default, if no version is given in the schema.\n        embed_vegalite = embed_mapping.get(\n            self.vegalite_major_version, self._embed_vegalite_v2\n        )\n        embed_vegalite(figure)\n\n    @property\n###The function: vegalite_major_version###\n    def _embed_vegalite_v5(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@5\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v4(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@4\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v3(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@4\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@3\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v2(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@3\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@2\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _vega_embed(self):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    vegaEmbed({{this.get_name()}}, {{this.json}})\n                        .then(function(result) {})\n                        .catch(console.error);\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n    def _embed_vegalite_v1(self, figure):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    var embedSpec = {\n                        mode: \"vega-lite\",\n                        spec: {{this.json}}\n                    };\n                    vg.embed(\n                        {{this.get_name()}}, embedSpec, function(error, result) {}\n                    );\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n        figure.header.add_child(\n            JavascriptLink(\"https://d3js.org/d3.v3.min.js\"), name=\"d3\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdnjs.cloudflare.com/ajax/libs/vega/2.6.5/vega.js\"),\n            name=\"vega\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-lite/1.3.1/vega-lite.js\"\n            ),\n            name=\"vega-lite\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-embed/2.2.0/vega-embed.js\"\n            ),\n            name=\"vega-embed\",\n        )  # noqa\n", "prompt": "Please write a python function called 'vegalite_major_version' base the context. This function returns the major version number of the Vega-Lite schema used in the VegaLite instance. It extracts the major version number from the \"$schema\" attribute in the instance's data.:param self: VegaLite. An instance of the VegaLite class.\n:return: int. The major version number of the Vega-Lite schema used in the instance..\n        The context you need to refer to is as follows: class VegaLite(Element):\n    \"\"\"\n    Creates a Vega-Lite chart element.\n\n    Parameters\n    ----------\n    data: JSON-like str or object\n        The Vega-Lite description of the chart.\n        It can also be any object that has a method `to_json`,\n        so that you can (for instance) provide an `Altair` chart.\n    width: int or str, default None\n        The width of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    height: int or str, default None\n        The height of the output element.\n        If None, either data['width'] (if available) or '100%' will be used.\n        Ex: 120, '120px', '80%'\n    left: int or str, default '0%'\n        The horizontal distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    top: int or str, default '0%'\n        The vertical distance of the output with respect to the parent\n        HTML object. Ex: 120, '120px', '80%'\n    position: str, default 'relative'\n        The `position` argument that the CSS shall contain.\n        Ex: 'relative', 'absolute'\n\n    \"\"\"\n\n    _template = Template(\"\")\n\n    def __init__(\n        self, data, width=None, height=None, left=\"0%\", top=\"0%\", position=\"relative\"\n    ):\n        super(self.__class__, self).__init__()\n        self._name = \"VegaLite\"\n        self.data = data.to_json() if hasattr(data, \"to_json\") else data\n        if isinstance(self.data, str):\n            self.data = json.loads(self.data)\n\n        self.json = json.dumps(self.data)\n\n        # Size Parameters.\n        self.width = _parse_size(\n            self.data.get(\"width\", \"100%\") if width is None else width\n        )\n        self.height = _parse_size(\n            self.data.get(\"height\", \"100%\") if height is None else height\n        )\n        self.left = _parse_size(left)\n        self.top = _parse_size(top)\n        self.position = position\n\n    def render(self, **kwargs):\n        \"\"\"Renders the HTML representation of the element.\"\"\"\n        self._parent.html.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <div id=\"{{this.get_name()}}\"></div>\n            \"\"\"\n                ).render(this=self, kwargs=kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        figure = self.get_root()\n        assert isinstance(\n            figure, Figure\n        ), \"You cannot render this Element if it is not in a Figure.\"\n\n        figure.header.add_child(\n            Element(\n                Template(\n                    \"\"\"\n            <style> #{{this.get_name()}} {\n                position : {{this.position}};\n                width : {{this.width[0]}}{{this.width[1]}};\n                height: {{this.height[0]}}{{this.height[1]}};\n                left: {{this.left[0]}}{{this.left[1]}};\n                top: {{this.top[0]}}{{this.top[1]}};\n            </style>\n            \"\"\"\n                ).render(this=self, **kwargs)\n            ),\n            name=self.get_name(),\n        )\n\n        embed_mapping = {\n            1: self._embed_vegalite_v1,\n            2: self._embed_vegalite_v2,\n            3: self._embed_vegalite_v3,\n            4: self._embed_vegalite_v4,\n            5: self._embed_vegalite_v5,\n        }\n\n        # Version 2 is assumed as the default, if no version is given in the schema.\n        embed_vegalite = embed_mapping.get(\n            self.vegalite_major_version, self._embed_vegalite_v2\n        )\n        embed_vegalite(figure)\n\n    @property\n###The function: vegalite_major_version###\n    def _embed_vegalite_v5(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@5\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v4(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm//vega@5\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@4\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@6\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v3(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@4\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@3\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _embed_vegalite_v2(self, figure):\n        self._vega_embed()\n\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega@3\"), name=\"vega\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-lite@2\"), name=\"vega-lite\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdn.jsdelivr.net/npm/vega-embed@3\"),\n            name=\"vega-embed\",\n        )\n\n    def _vega_embed(self):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    vegaEmbed({{this.get_name()}}, {{this.json}})\n                        .then(function(result) {})\n                        .catch(console.error);\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n    def _embed_vegalite_v1(self, figure):\n        self._parent.script.add_child(\n            Element(\n                Template(\n                    \"\"\"\n                    var embedSpec = {\n                        mode: \"vega-lite\",\n                        spec: {{this.json}}\n                    };\n                    vg.embed(\n                        {{this.get_name()}}, embedSpec, function(error, result) {}\n                    );\n                \"\"\"\n                ).render(this=self)\n            ),\n            name=self.get_name(),\n        )\n\n        figure.header.add_child(\n            JavascriptLink(\"https://d3js.org/d3.v3.min.js\"), name=\"d3\"\n        )\n        figure.header.add_child(\n            JavascriptLink(\"https://cdnjs.cloudflare.com/ajax/libs/vega/2.6.5/vega.js\"),\n            name=\"vega\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-lite/1.3.1/vega-lite.js\"\n            ),\n            name=\"vega-lite\",\n        )  # noqa\n        figure.header.add_child(\n            JavascriptLink(\n                \"https://cdnjs.cloudflare.com/ajax/libs/vega-embed/2.2.0/vega-embed.js\"\n            ),\n            name=\"vega-embed\",\n        )  # noqa\n", "test_list": ["@pytest.mark.parametrize('version', [1, 2, 3, 4, 5, None])\ndef test_vegalite_major_version(vegalite_spec, version):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    if version is None:\n        assert vegalite.vegalite_major_version is None\n    else:\n        assert vegalite.vegalite_major_version == version"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'vegalite_major_version' should return an integer representing the major version number extracted from the '$schema' attribute of the Vega-Lite instance's data. If the '$schema' attribute is missing or not a valid URL, the function should return None.", "unit_test": ["@pytest.mark.parametrize('vegalite_spec, expected_version', [({'$schema': 'https://vega.github.io/schema/vega-lite/v4.json'}, 4), ({'$schema': 'https://vega.github.io/schema/vega-lite/v3.json'}, 3), ({'$schema': 'https://vega.github.io/schema/vega-lite/v2.json'}, 2), ({'$schema': 'https://vega.github.io/schema/vega-lite/v1.json'}, 1), ({'$schema': 'https://vega.github.io/schema/vega-lite/v5.json'}, 5), ({'$schema': 'invalid_url'}, None), ({}, None)])\ndef test_vegalite_major_version_output(vegalite_spec, expected_version):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version == expected_version"], "test": "tests/test_features.py::test_vegalite_major_version_output"}, "Exception Handling": {"requirement": "The function 'vegalite_major_version' should handle exceptions gracefully and return None if the '$schema' attribute is not a string or if any error occurs during the extraction of the major version number.", "unit_test": ["@pytest.mark.parametrize('vegalite_spec', [{'data': {'$schema': 123}}, {'data': {'$schema': None}}, {'data': {'$schema': {}}}, {'data': {'$schema': []}}])\ndef test_vegalite_major_version_exception_handling(vegalite_spec):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version is None"], "test": "tests/test_features.py::test_vegalite_major_version_exception_handling"}, "Edge Case Handling": {"requirement": "The function 'vegalite_major_version' should correctly handle edge cases where the '$schema' attribute is present but does not conform to the expected URL format, returning None in such cases.", "unit_test": ["@pytest.mark.parametrize('vegalite_spec', [{'data': {'$schema': 'https://vega.github.io/schema/vega-lite/v.json'}}, {'data': {'$schema': 'https://vega.github.io/schema/vega-lite/vx.json'}}, {'data': {'$schema': 'https://vega.github.io/schema/vega-lite/5.json'}}])\ndef test_vegalite_major_version_edge_cases(vegalite_spec):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version is None"], "test": "tests/test_features.py::test_vegalite_major_version_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'vegalite_major_version' function to also return the minor version number as a tuple (major, minor) if the minor version is present in the '$schema' URL.", "unit_test": ["@pytest.mark.parametrize('vegalite_spec, expected_version', [({'$schema': 'https://vega.github.io/schema/vega-lite/v4.1.json'}, (4, 1)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v3.2.json'}, (3, 2)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v2.0.json'}, (2, 0)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v1.5.json'}, (1, 5)), ({'$schema': 'https://vega.github.io/schema/vega-lite/v5.json'}, (5, None))])\ndef test_vegalite_major_minor_version(vegalite_spec, expected_version):\n    vegalite = folium.features.VegaLite(vegalite_spec)\n    assert vegalite.vegalite_major_version == expected_version"], "test": "tests/test_features.py::test_vegalite_major_minor_version"}, "Annotation Coverage": {"requirement": "Ensure that the 'vegalite_major_version' function is fully documented with appropriate docstrings, including parameter types and return types.", "unit_test": ["def test_vegalite_major_version_annotations():\n    assert 'vegalite_major_version' in folium.features.VegaLite.__dict__\n    assert folium.features.VegaLite.vegalite_major_version.__doc__ is not None\n    assert ':param self: VegaLite' in folium.features.VegaLite.vegalite_major_version.__doc__\n    assert ':return: int' in folium.features.VegaLite.vegalite_major_version.__doc__"], "test": "tests/test_features.py::test_vegalite_major_version_annotations"}, "Code Complexity": {"requirement": "The 'vegalite_major_version' function should maintain a cyclomatic complexity of 2 or less to ensure readability and maintainability.", "unit_test": ["def test_code_complexity():\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(vegalite.vegalite_major_version)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 2"], "test": "tests/test_features.py::test_code_complexity"}, "Code Standard": {"requirement": "The 'vegalite_major_version' function should adhere to PEP 8 standards, including proper naming conventions and spacing.", "unit_test": ["def test_check_code_style(s:\n    import pycodestyle\n    import os\n    import inspect\n    code_string = inspect.getsource(folium.features.VegaLite.vegalite_major_version)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/test_features.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'vegalite_major_version' function utilizes the 'data' attribute from the 'VegaLite' class context to extract the '$schema' attribute.", "unit_test": ["def test_vegalite_major_version_context_usage():\n    vegalite = folium.features.VegaLite({'$schema': 'https://vega.github.io/schema/vega-lite/v4.json'})\n    assert vegalite.data['$schema'] == 'https://vega.github.io/schema/vega-lite/v4.json'\n    assert vegalite.vegalite_major_version == 4"], "test": "tests/test_features.py::test_vegalite_major_version_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'vegalite_major_version' function correctly uses the 'data' attribute from the 'VegaLite' class to access and parse the '$schema' attribute.", "unit_test": ["def test_vegalite_major_version_context_correctness():\n    vegalite = folium.features.VegaLite({'$schema': 'https://vega.github.io/schema/vega-lite/v4.json'})\n    assert vegalite.data['$schema'] == 'https://vega.github.io/schema/vega-lite/v4.json'\n    assert isinstance(vegalite.vegalite_major_version, int)\n    assert vegalite.vegalite_major_version == 4"], "test": "tests/test_features.py::test_vegalite_major_version_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "type": "method", "project_path": "Text-Processing/pycorrector", "completion_path": "Text-Processing/pycorrector/pycorrector/en_spell.py", "signature_position": [90, 90], "body_position": [96, 97], "dependency": {"intra_class": ["pycorrector.en_spell.EnSpell.check_init", "pycorrector.en_spell.EnSpell.edits1", "pycorrector.en_spell.EnSpell.edits2", "pycorrector.en_spell.EnSpell.known"], "intra_file": [], "cross_file": []}, "requirement": {"Functionality": "This function generates possible spelling corrections for a given word. It checks whether zero, one, or two edits are needed to correct the word. If zero edit is needed, it returns the set of the given words. If one edit is needed, it returns the set of known words by applying one edit. If two edits are needed, it returns the set of known words by applying two edits. If no corrections are found, it returns the original word. It checks if the EnSpell instance has been initialized before performing the operation.", "Arguments": ":param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word for which spelling corrections need to be generated.\n:return: Set of strings. The set of possible spelling corrections for the word."}, "tests": ["tests/en_spell_bug_fix_test.py::EnBugTestCase::test_en_bug_correct2", "tests/en_spell_dict_test.py::TestEnSpell::test_candidates"], "indent": 4, "domain": "Text-Processing", "code": "    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n", "context": "class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n###The function: candidates###\n    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :param mini_prob:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "prompt": "Please write a python function called 'candidates' base the context. This function generates possible spelling corrections for a given word. It checks whether zero, one, or two edits are needed to correct the word. If zero edit is needed, it returns the set of the given words. If one edit is needed, it returns the set of known words by applying one edit. If two edits are needed, it returns the set of known words by applying two edits. If no corrections are found, it returns the original word. It checks if the EnSpell instance has been initialized before performing the operation.:param self: EnSpell. An instance of the EnSpell class.\n:param word: String. The word for which spelling corrections need to be generated.\n:return: Set of strings. The set of possible spelling corrections for the word..\n        The context you need to refer to is as follows: class EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n###The function: candidates###\n    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :param mini_prob:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "test_list": ["def test_en_bug_correct2(self):\n    \"\"\"\u6d4b\u8bd5\u82f1\u6587\u7ea0\u9519bug\"\"\"\n    print(spell.word_freq_dict.get('whould'))\n    print(spell.candidates('whould'))\n    a = spell.correct_word('whould')\n    print(a)\n    r = spell.correct('contend proble poety adress whould niether  quaties')\n    print(r)\n    assert spell.correct('whould')[0] == 'would'", "def test_candidates(self):\n    \"\"\" test spell checker candidates \"\"\"\n    spell = EnSpell()\n    spell.check_init()\n    print(spell.word_freq_dict.get('ths'), spell.candidates('ths'))\n    self.assertEqual(len(spell.candidates('ths')) > 0, True)\n    self.assertEqual(spell.candidates('the'), {'the'})\n    self.assertEqual(spell.candidates('hi'), {'hi'})\n    self.assertEqual(''.join(spell.candidates('manasaeds')), 'manasaeds')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'candidates' function should accept a string input and return a set of strings as output. It should handle both valid and invalid word inputs gracefully.", "unit_test": ["def test_candidates_input_output(self):\n    spell = EnSpell()\n    spell.check_init()\n    result = spell.candidates('example')\n    self.assertIsInstance(result, set)\n    self.assertTrue(all(isinstance(word, str) for word in result))\n    result_invalid = spell.candidates('')\n    self.assertEqual(result_invalid, set())"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_input_output"}, "Exception Handling": {"requirement": "The 'candidates' function should raise a ValueError if the input word is not a string.", "unit_test": ["def test_candidates_exception_handling(self):\n    spell = EnSpell()\n    spell.check_init()\n    with self.assertRaises(ValueError):\n        spell.candidates(123)"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_input_output"}, "Edge Case Handling": {"requirement": "The 'candidates' function should correctly handle edge cases such as empty strings and single-character words.", "unit_test": ["def test_candidates_edge_cases(self):\n    spell = EnSpell()\n    spell.check_init()\n    self.assertEqual(spell.candidates(''), set())\n    self.assertEqual(spell.candidates('a'), {'a'})"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'candidates' function to include a parameter that limits the number of suggestions returned.", "unit_test": ["def test_candidates_functionality_extension(self):\n    spell = EnSpell()\n    spell.check_init()\n    result = spell.candidates('example', limit=5)\n    self.assertLessEqual(len(result), 5)"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_candidates_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that all functions, including 'candidates', have complete and accurate docstrings describing parameters, return types, and functionality.", "unit_test": ["def test_annotation_coverage(self):\n    import inspect\n    spell = EnSpell()\n    docstring = inspect.getdoc(spell.candidates)\n    self.assertIsNotNone(docstring)\n    self.assertIn(':param', docstring)\n    self.assertIn(':return:', docstring)"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'candidates' function should maintain a cyclomatic complexity of 10 or less to ensure readability and maintainability.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    import inspect\n    import textwrap\n    source = inspect.getsource(EnSpell.candidates)\n    complexity = cc_visit(textwrap.dedent(source))\n    assert complexity[0].complexity <= 10"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_code_complexity"}, "Code Standard": {"requirement": "The 'candidates' function should adhere to PEP 8 standards, including proper naming conventions and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    import os\n    import inspect\n    from pyramid.registry import Introspector\n    code_string = inspect.getsource(EnSpell.candidates)\n    filename = \"temp.py\"\n    with open(filename, \"w\") as file:\n        file.write(code_string)    \n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files([filename])\n    os.remove(filename)\n    assert result.total_errors==0"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'candidates' function should utilize the 'edits1', 'edits2', and 'known' methods from the EnSpell class.", "unit_test": ["def test_context_usage_verification(self):\n    spell = EnSpell()\n    spell.check_init()\n    with unittest.mock.patch.object(spell, 'edits1', wraps=spell.edits1) as mock_edits1,\n         unittest.mock.patch.object(spell, 'edits2', wraps=spell.edits2) as mock_edits2,\n         unittest.mock.patch.object(spell, 'known', wraps=spell.known) as mock_known:\n        spell.candidates('example')\n        mock_edits1.assert_called()\n        mock_edits2.assert_called()\n        mock_known.assert_called()"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'candidates' function should correctly apply the 'edits1', 'edits2', and 'known' methods to generate spelling corrections.", "unit_test": ["def test_context_usage_correctness(self):\n    spell = EnSpell()\n    spell.check_init()\n    known_words = spell.known(spell.edits1('example'))\n    self.assertEqual(spell.candidates('example'), known_words)\n    known_words_two_edits = spell.known(spell.edits2('example'))\n    self.assertEqual(spell.candidates('example'), known_words.union(known_words_two_edits))"], "test": "tests/en_spell_dict_test.py::TestEnSpell::test_context_usage_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "type": "method", "project_path": "System/prometheus-client", "completion_path": "System/prometheus-client/prometheus_client/mmap_dict.py", "signature_position": [121, 121], "body_position": [122, 125], "dependency": {"intra_class": ["prometheus_client.mmap_dict.MmapedDict._init_value", "prometheus_client.mmap_dict.MmapedDict._m", "prometheus_client.mmap_dict.MmapedDict._positions"], "intra_file": ["prometheus_client.mmap_dict._unpack_two_doubles"], "cross_file": []}, "requirement": {"Functionality": "Read the value corresponding to the given key from the MmapedDict instance. If the key is not found in the instance, it initializes the value and then returns it.", "Arguments": ":param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to read the value from the instance.\n:return: The value corresponding to the key."}, "tests": ["tests/test_multiprocess.py::TestMmapedDict::test_process_restart"], "indent": 4, "domain": "System", "code": "    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n", "context": "import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n###The function: read_value###\n    def write_value(self, key, value, timestamp):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "prompt": "Please write a python function called 'read_value' base the context. Read the value corresponding to the given key from the MmapedDict instance. If the key is not found in the instance, it initializes the value and then returns it.:param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to read the value from the instance.\n:return: The value corresponding to the key..\n        The context you need to refer to is as follows: import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n###The function: read_value###\n    def write_value(self, key, value, timestamp):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "test_list": ["def test_process_restart(self):\n    self.d.write_value('abc', 123.0, 987.0)\n    self.d.close()\n    self.d = mmap_dict.MmapedDict(self.tempfile)\n    self.assertEqual((123, 987.0), self.d.read_value('abc'))\n    self.assertEqual([('abc', 123.0, 987.0)], list(self.d.read_all_values()))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'read_value' function should return a tuple of (value, timestamp) when a valid key is provided. If the key does not exist, it should initialize the value to 0.0 and timestamp to 0.0, then return this tuple.", "unit_test": ["def test_read_value_initialization(self):\n    result = self.d.read_value('nonexistent_key')\n    self.assertEqual((0.0, 0.0), result)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_initialization"}, "Exception Handling": {"requirement": "The 'read_value' function should raise a KeyError with a descriptive message if the key is malformed or cannot be decoded.", "unit_test": ["def test_read_value_malformed_key(self):\n    with self.assertRaises(KeyError) as context:\n        self.d.read_value(b'\\x80\\x81\\x82')\n    self.assertIn('Malformed key', str(context.exception))"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_malformed_key"}, "Edge Case Handling": {"requirement": "The 'read_value' function should handle the case where the mmap file is empty and return (0.0, 0.0) for any key.", "unit_test": ["def test_read_value_empty_mmap(self):\n    empty_dict = mmap_dict.MmapedDict(self.tempfile, read_mode=True)\n    result = empty_dict.read_value('any_key')\n    self.assertEqual((0.0, 0.0), result)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_empty_mmap"}, "Functionality Extension": {"requirement": "Extend 'read_value' to accept an optional default value and timestamp, which are returned if the key is not found.", "unit_test": ["def test_read_value_with_default(self):\n    result = self.d.read_value('nonexistent_key', default_value=1.0, default_timestamp=100.0)\n    self.assertEqual((1.0, 100.0), result)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_with_default"}, "Annotation Coverage": {"requirement": "Ensure that the 'read_value' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_read_value_annotations(self):\n    annotations = self.d.read_value.__annotations__\n    self.assertEqual(annotations['key'], str)\n    self.assertEqual(annotations['return'], tuple)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_annotations"}, "Code Complexity": {"requirement": "The 'read_value' function should have a cyclomatic complexity of 5 or less.", "unit_test": ["def test_read_value_complexity(self):\n    complexity = calculate_cyclomatic_complexity(self.d.read_value)\n    self.assertLessEqual(complexity, 5)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_code_complexity"}, "Code Standard": {"requirement": "The 'read_value' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_read_value_pep8(self):\n    pep8_checker = pep8.Checker(self.d.read_value)\n    result = pep8_checker.check_all()\n    self.assertEqual(result, 0)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'read_value' function should utilize the '_init_value' method from the MmapedDict class when initializing a new key.", "unit_test": ["def test_read_value_uses_init_value(self):\n    with mock.patch.object(self.d, '_init_value', wraps=self.d._init_value) as mock_init:\n        self.d.read_value('new_key')\n        mock_init.assert_called_once_with('new_key')"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_uses_init_value"}, "Context Usage Correctness Verification": {"requirement": "The 'read_value' function should correctly update the '_positions' dictionary when a new key is initialized.", "unit_test": ["def test_read_value_updates_positions(self):\n    self.d.read_value('new_key')\n    self.assertIn('new_key', self.d._positions)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_read_value_updates_positions"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "type": "method", "project_path": "Internet/boto", "completion_path": "Internet/boto/boto/ec2/securitygroup.py", "signature_position": [97, 99], "body_position": [105, 116], "dependency": {"intra_class": ["boto.ec2.securitygroup.SecurityGroup.rules"], "intra_file": ["boto.ec2.securitygroup.IPPermissions", "boto.ec2.securitygroup.IPPermissions.__init__", "boto.ec2.securitygroup.IPPermissions.add_grant", "boto.ec2.securitygroup.IPPermissions.from_port", "boto.ec2.securitygroup.IPPermissions.ip_protocol", "boto.ec2.securitygroup.IPPermissions.to_port"], "cross_file": []}, "requirement": {"Functionality": "Add a rule to a SecurityGroup instance. Note that this method only changes the local version of the instance. No information is sent to EC2.", "Arguments": ":param self: SecurityGroup. An instance of the SecurityGroup class.\n:param ip_protocol: String. The IP protocol for the rule.\n:param from_port: Integer. The starting port range for the rule.\n:param to_port: Integer. The ending port range for the rule.\n:param src_group_name: String. The name of the source security group.\n:param src_group_owner_id: String. The ID of the owner of the source security group.\n:param cidr_ip: String. The CIDR IP range for the rule.\n:param src_group_group_id: String. The ID of the source security group.\n:param dry_run: Bool. Whether to perform a dry run. Defaults to False.\n:return: No return values."}, "tests": ["tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule"], "indent": 4, "domain": "Internet", "code": "    def add_rule(self, ip_protocol, from_port, to_port,\n                 src_group_name, src_group_owner_id, cidr_ip,\n                 src_group_group_id, dry_run=False):\n        \"\"\"\n        Add a rule to the SecurityGroup object.  Note that this method\n        only changes the local version of the object.  No information\n        is sent to EC2.\n        \"\"\"\n        rule = IPPermissions(self)\n        rule.ip_protocol = ip_protocol\n        rule.from_port = from_port\n        rule.to_port = to_port\n        self.rules.append(rule)\n        rule.add_grant(\n            src_group_name,\n            src_group_owner_id,\n            cidr_ip,\n            src_group_group_id,\n            dry_run=dry_run\n        )\n", "context": "# Copyright (c) 2006-2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2011, Eucalyptus Systems, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents an EC2 Security Group\n\"\"\"\nfrom boto.ec2.ec2object import TaggedEC2Object\nfrom boto.exception import BotoClientError\n\n\nclass SecurityGroup(TaggedEC2Object):\n\n    def __init__(self, connection=None, owner_id=None,\n                 name=None, description=None, id=None):\n        super(SecurityGroup, self).__init__(connection)\n        self.id = id\n        self.owner_id = owner_id\n        self.name = name\n        self.description = description\n        self.vpc_id = None\n        self.rules = IPPermissionsList()\n        self.rules_egress = IPPermissionsList()\n\n    def __repr__(self):\n        return 'SecurityGroup:%s' % self.name\n\n    def startElement(self, name, attrs, connection):\n        retval = super(SecurityGroup, self).startElement(name, attrs, connection)\n        if retval is not None:\n            return retval\n        if name == 'ipPermissions':\n            return self.rules\n        elif name == 'ipPermissionsEgress':\n            return self.rules_egress\n        else:\n            return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ownerId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.id = value\n        elif name == 'groupName':\n            self.name = value\n        elif name == 'vpcId':\n            self.vpc_id = value\n        elif name == 'groupDescription':\n            self.description = value\n        elif name == 'ipRanges':\n            pass\n        elif name == 'return':\n            if value == 'false':\n                self.status = False\n            elif value == 'true':\n                self.status = True\n            else:\n                raise Exception(\n                    'Unexpected value of status %s for group %s' % (\n                        value,\n                        self.name\n                    )\n                )\n        else:\n            setattr(self, name, value)\n\n    def delete(self, dry_run=False):\n        if self.vpc_id:\n            return self.connection.delete_security_group(\n                group_id=self.id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.delete_security_group(\n                self.name,\n                dry_run=dry_run\n            )\n\n###The function: add_rule###\n    def remove_rule(self, ip_protocol, from_port, to_port,\n                    src_group_name, src_group_owner_id, cidr_ip,\n                    src_group_group_id, dry_run=False):\n        \"\"\"\n        Remove a rule to the SecurityGroup object.  Note that this method\n        only changes the local version of the object.  No information\n        is sent to EC2.\n        \"\"\"\n        if not self.rules:\n            raise ValueError(\"The security group has no rules\")\n\n        target_rule = None\n        for rule in self.rules:\n            if rule.ip_protocol == ip_protocol:\n                if rule.from_port == from_port:\n                    if rule.to_port == to_port:\n                        target_rule = rule\n                        target_grant = None\n                        for grant in rule.grants:\n                            if grant.name == src_group_name or grant.group_id == src_group_group_id:\n                                if grant.owner_id == src_group_owner_id:\n                                    if grant.cidr_ip == cidr_ip:\n                                        target_grant = grant\n                        if target_grant:\n                            rule.grants.remove(target_grant)\n            if len(rule.grants) == 0:\n                self.rules.remove(target_rule)\n\n    def authorize(self, ip_protocol=None, from_port=None, to_port=None,\n                  cidr_ip=None, src_group=None, dry_run=False):\n        \"\"\"\n        Add a new rule to this security group.\n        You need to pass in either src_group_name\n        OR ip_protocol, from_port, to_port,\n        and cidr_ip.  In other words, either you are authorizing another\n        group or you are authorizing some ip-based rule.\n\n        :type ip_protocol: string\n        :param ip_protocol: Either tcp | udp | icmp\n\n        :type from_port: int\n        :param from_port: The beginning port number you are enabling\n\n        :type to_port: int\n        :param to_port: The ending port number you are enabling\n\n        :type cidr_ip: string or list of strings\n        :param cidr_ip: The CIDR block you are providing access to.\n                        See http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing\n\n        :type src_group: :class:`boto.ec2.securitygroup.SecurityGroup` or\n                         :class:`boto.ec2.securitygroup.GroupOrCIDR`\n        :param src_group: The Security Group you are granting access to.\n\n        :rtype: bool\n        :return: True if successful.\n        \"\"\"\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.authorize_security_group(group_name,\n                                                          src_group_name,\n                                                          src_group_owner_id,\n                                                          ip_protocol,\n                                                          from_port,\n                                                          to_port,\n                                                          cidr_ip,\n                                                          group_id,\n                                                          src_group_group_id,\n                                                          dry_run=dry_run)\n        if status:\n            if not isinstance(cidr_ip, list):\n                cidr_ip = [cidr_ip]\n            for single_cidr_ip in cidr_ip:\n                self.add_rule(ip_protocol, from_port, to_port, src_group_name,\n                              src_group_owner_id, single_cidr_ip,\n                              src_group_group_id, dry_run=dry_run)\n        return status\n\n    def revoke(self, ip_protocol=None, from_port=None, to_port=None,\n               cidr_ip=None, src_group=None, dry_run=False):\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.revoke_security_group(group_name,\n                                                       src_group_name,\n                                                       src_group_owner_id,\n                                                       ip_protocol,\n                                                       from_port,\n                                                       to_port,\n                                                       cidr_ip,\n                                                       group_id,\n                                                       src_group_group_id,\n                                                       dry_run=dry_run)\n        if status:\n            self.remove_rule(ip_protocol, from_port, to_port, src_group_name,\n                             src_group_owner_id, cidr_ip, src_group_group_id,\n                             dry_run=dry_run)\n        return status\n\n    def copy_to_region(self, region, name=None, dry_run=False):\n        \"\"\"\n        Create a copy of this security group in another region.\n        Note that the new security group will be a separate entity\n        and will not stay in sync automatically after the copy\n        operation.\n\n        :type region: :class:`boto.ec2.regioninfo.RegionInfo`\n        :param region: The region to which this security group will be copied.\n\n        :type name: string\n        :param name: The name of the copy.  If not supplied, the copy\n                     will have the same name as this security group.\n\n        :rtype: :class:`boto.ec2.securitygroup.SecurityGroup`\n        :return: The new security group.\n        \"\"\"\n        if region.name == self.region:\n            raise BotoClientError('Unable to copy to the same Region')\n        conn_params = self.connection.get_params()\n        rconn = region.connect(**conn_params)\n        sg = rconn.create_security_group(\n            name or self.name,\n            self.description,\n            dry_run=dry_run\n        )\n        source_groups = []\n        for rule in self.rules:\n            for grant in rule.grants:\n                grant_nom = grant.name or grant.group_id\n                if grant_nom:\n                    if grant_nom not in source_groups:\n                        source_groups.append(grant_nom)\n                        sg.authorize(None, None, None, None, grant,\n                                     dry_run=dry_run)\n                else:\n                    sg.authorize(rule.ip_protocol, rule.from_port, rule.to_port,\n                                 grant.cidr_ip, dry_run=dry_run)\n        return sg\n\n    def instances(self, dry_run=False):\n        \"\"\"\n        Find all of the current instances that are running within this\n        security group.\n\n        :rtype: list of :class:`boto.ec2.instance.Instance`\n        :return: A list of Instance objects\n        \"\"\"\n        rs = []\n        if self.vpc_id:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'instance.group-id': self.id},\n                dry_run=dry_run\n            ))\n        else:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'group-id': self.id},\n                dry_run=dry_run\n            ))\n        instances = [i for r in rs for i in r.instances]\n        return instances\n\n\nclass IPPermissionsList(list):\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.append(IPPermissions(self))\n            return self[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        pass\n\n\nclass IPPermissions(object):\n\n    def __init__(self, parent=None):\n        self.parent = parent\n        self.ip_protocol = None\n        self.from_port = None\n        self.to_port = None\n        self.grants = []\n\n    def __repr__(self):\n        return 'IPPermissions:%s(%s-%s)' % (self.ip_protocol,\n                                            self.from_port, self.to_port)\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.grants.append(GroupOrCIDR(self))\n            return self.grants[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ipProtocol':\n            self.ip_protocol = value\n        elif name == 'fromPort':\n            self.from_port = value\n        elif name == 'toPort':\n            self.to_port = value\n        else:\n            setattr(self, name, value)\n\n    def add_grant(self, name=None, owner_id=None, cidr_ip=None, group_id=None,\n                  dry_run=False):\n        grant = GroupOrCIDR(self)\n        grant.owner_id = owner_id\n        grant.group_id = group_id\n        grant.name = name\n        grant.cidr_ip = cidr_ip\n        self.grants.append(grant)\n        return grant\n\n\nclass GroupOrCIDR(object):\n\n    def __init__(self, parent=None):\n        self.owner_id = None\n        self.group_id = None\n        self.name = None\n        self.cidr_ip = None\n\n    def __repr__(self):\n        if self.cidr_ip:\n            return '%s' % self.cidr_ip\n        else:\n            return '%s-%s' % (self.name or self.group_id, self.owner_id)\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'userId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.group_id = value\n        elif name == 'groupName':\n            self.name = value\n        if name == 'cidrIp':\n            self.cidr_ip = value\n        else:\n            setattr(self, name, value)\n", "prompt": "Please write a python function called 'add_rule' base the context. Add a rule to a SecurityGroup instance. Note that this method only changes the local version of the instance. No information is sent to EC2.:param self: SecurityGroup. An instance of the SecurityGroup class.\n:param ip_protocol: String. The IP protocol for the rule.\n:param from_port: Integer. The starting port range for the rule.\n:param to_port: Integer. The ending port range for the rule.\n:param src_group_name: String. The name of the source security group.\n:param src_group_owner_id: String. The ID of the owner of the source security group.\n:param cidr_ip: String. The CIDR IP range for the rule.\n:param src_group_group_id: String. The ID of the source security group.\n:param dry_run: Bool. Whether to perform a dry run. Defaults to False.\n:return: No return values..\n        The context you need to refer to is as follows: # Copyright (c) 2006-2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2011, Eucalyptus Systems, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\"\"\"\nRepresents an EC2 Security Group\n\"\"\"\nfrom boto.ec2.ec2object import TaggedEC2Object\nfrom boto.exception import BotoClientError\n\n\nclass SecurityGroup(TaggedEC2Object):\n\n    def __init__(self, connection=None, owner_id=None,\n                 name=None, description=None, id=None):\n        super(SecurityGroup, self).__init__(connection)\n        self.id = id\n        self.owner_id = owner_id\n        self.name = name\n        self.description = description\n        self.vpc_id = None\n        self.rules = IPPermissionsList()\n        self.rules_egress = IPPermissionsList()\n\n    def __repr__(self):\n        return 'SecurityGroup:%s' % self.name\n\n    def startElement(self, name, attrs, connection):\n        retval = super(SecurityGroup, self).startElement(name, attrs, connection)\n        if retval is not None:\n            return retval\n        if name == 'ipPermissions':\n            return self.rules\n        elif name == 'ipPermissionsEgress':\n            return self.rules_egress\n        else:\n            return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ownerId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.id = value\n        elif name == 'groupName':\n            self.name = value\n        elif name == 'vpcId':\n            self.vpc_id = value\n        elif name == 'groupDescription':\n            self.description = value\n        elif name == 'ipRanges':\n            pass\n        elif name == 'return':\n            if value == 'false':\n                self.status = False\n            elif value == 'true':\n                self.status = True\n            else:\n                raise Exception(\n                    'Unexpected value of status %s for group %s' % (\n                        value,\n                        self.name\n                    )\n                )\n        else:\n            setattr(self, name, value)\n\n    def delete(self, dry_run=False):\n        if self.vpc_id:\n            return self.connection.delete_security_group(\n                group_id=self.id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.delete_security_group(\n                self.name,\n                dry_run=dry_run\n            )\n\n###The function: add_rule###\n    def remove_rule(self, ip_protocol, from_port, to_port,\n                    src_group_name, src_group_owner_id, cidr_ip,\n                    src_group_group_id, dry_run=False):\n        \"\"\"\n        Remove a rule to the SecurityGroup object.  Note that this method\n        only changes the local version of the object.  No information\n        is sent to EC2.\n        \"\"\"\n        if not self.rules:\n            raise ValueError(\"The security group has no rules\")\n\n        target_rule = None\n        for rule in self.rules:\n            if rule.ip_protocol == ip_protocol:\n                if rule.from_port == from_port:\n                    if rule.to_port == to_port:\n                        target_rule = rule\n                        target_grant = None\n                        for grant in rule.grants:\n                            if grant.name == src_group_name or grant.group_id == src_group_group_id:\n                                if grant.owner_id == src_group_owner_id:\n                                    if grant.cidr_ip == cidr_ip:\n                                        target_grant = grant\n                        if target_grant:\n                            rule.grants.remove(target_grant)\n            if len(rule.grants) == 0:\n                self.rules.remove(target_rule)\n\n    def authorize(self, ip_protocol=None, from_port=None, to_port=None,\n                  cidr_ip=None, src_group=None, dry_run=False):\n        \"\"\"\n        Add a new rule to this security group.\n        You need to pass in either src_group_name\n        OR ip_protocol, from_port, to_port,\n        and cidr_ip.  In other words, either you are authorizing another\n        group or you are authorizing some ip-based rule.\n\n        :type ip_protocol: string\n        :param ip_protocol: Either tcp | udp | icmp\n\n        :type from_port: int\n        :param from_port: The beginning port number you are enabling\n\n        :type to_port: int\n        :param to_port: The ending port number you are enabling\n\n        :type cidr_ip: string or list of strings\n        :param cidr_ip: The CIDR block you are providing access to.\n                        See http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing\n\n        :type src_group: :class:`boto.ec2.securitygroup.SecurityGroup` or\n                         :class:`boto.ec2.securitygroup.GroupOrCIDR`\n        :param src_group: The Security Group you are granting access to.\n\n        :rtype: bool\n        :return: True if successful.\n        \"\"\"\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.authorize_security_group(group_name,\n                                                          src_group_name,\n                                                          src_group_owner_id,\n                                                          ip_protocol,\n                                                          from_port,\n                                                          to_port,\n                                                          cidr_ip,\n                                                          group_id,\n                                                          src_group_group_id,\n                                                          dry_run=dry_run)\n        if status:\n            if not isinstance(cidr_ip, list):\n                cidr_ip = [cidr_ip]\n            for single_cidr_ip in cidr_ip:\n                self.add_rule(ip_protocol, from_port, to_port, src_group_name,\n                              src_group_owner_id, single_cidr_ip,\n                              src_group_group_id, dry_run=dry_run)\n        return status\n\n    def revoke(self, ip_protocol=None, from_port=None, to_port=None,\n               cidr_ip=None, src_group=None, dry_run=False):\n        group_name = None\n        if not self.vpc_id:\n            group_name = self.name\n        group_id = None\n        if self.vpc_id:\n            group_id = self.id\n        src_group_name = None\n        src_group_owner_id = None\n        src_group_group_id = None\n        if src_group:\n            cidr_ip = None\n            src_group_owner_id = src_group.owner_id\n            if not self.vpc_id:\n                src_group_name = src_group.name\n            else:\n                if hasattr(src_group, 'group_id'):\n                    src_group_group_id = src_group.group_id\n                else:\n                    src_group_group_id = src_group.id\n        status = self.connection.revoke_security_group(group_name,\n                                                       src_group_name,\n                                                       src_group_owner_id,\n                                                       ip_protocol,\n                                                       from_port,\n                                                       to_port,\n                                                       cidr_ip,\n                                                       group_id,\n                                                       src_group_group_id,\n                                                       dry_run=dry_run)\n        if status:\n            self.remove_rule(ip_protocol, from_port, to_port, src_group_name,\n                             src_group_owner_id, cidr_ip, src_group_group_id,\n                             dry_run=dry_run)\n        return status\n\n    def copy_to_region(self, region, name=None, dry_run=False):\n        \"\"\"\n        Create a copy of this security group in another region.\n        Note that the new security group will be a separate entity\n        and will not stay in sync automatically after the copy\n        operation.\n\n        :type region: :class:`boto.ec2.regioninfo.RegionInfo`\n        :param region: The region to which this security group will be copied.\n\n        :type name: string\n        :param name: The name of the copy.  If not supplied, the copy\n                     will have the same name as this security group.\n\n        :rtype: :class:`boto.ec2.securitygroup.SecurityGroup`\n        :return: The new security group.\n        \"\"\"\n        if region.name == self.region:\n            raise BotoClientError('Unable to copy to the same Region')\n        conn_params = self.connection.get_params()\n        rconn = region.connect(**conn_params)\n        sg = rconn.create_security_group(\n            name or self.name,\n            self.description,\n            dry_run=dry_run\n        )\n        source_groups = []\n        for rule in self.rules:\n            for grant in rule.grants:\n                grant_nom = grant.name or grant.group_id\n                if grant_nom:\n                    if grant_nom not in source_groups:\n                        source_groups.append(grant_nom)\n                        sg.authorize(None, None, None, None, grant,\n                                     dry_run=dry_run)\n                else:\n                    sg.authorize(rule.ip_protocol, rule.from_port, rule.to_port,\n                                 grant.cidr_ip, dry_run=dry_run)\n        return sg\n\n    def instances(self, dry_run=False):\n        \"\"\"\n        Find all of the current instances that are running within this\n        security group.\n\n        :rtype: list of :class:`boto.ec2.instance.Instance`\n        :return: A list of Instance objects\n        \"\"\"\n        rs = []\n        if self.vpc_id:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'instance.group-id': self.id},\n                dry_run=dry_run\n            ))\n        else:\n            rs.extend(self.connection.get_all_reservations(\n                filters={'group-id': self.id},\n                dry_run=dry_run\n            ))\n        instances = [i for r in rs for i in r.instances]\n        return instances\n\n\nclass IPPermissionsList(list):\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.append(IPPermissions(self))\n            return self[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        pass\n\n\nclass IPPermissions(object):\n\n    def __init__(self, parent=None):\n        self.parent = parent\n        self.ip_protocol = None\n        self.from_port = None\n        self.to_port = None\n        self.grants = []\n\n    def __repr__(self):\n        return 'IPPermissions:%s(%s-%s)' % (self.ip_protocol,\n                                            self.from_port, self.to_port)\n\n    def startElement(self, name, attrs, connection):\n        if name == 'item':\n            self.grants.append(GroupOrCIDR(self))\n            return self.grants[-1]\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'ipProtocol':\n            self.ip_protocol = value\n        elif name == 'fromPort':\n            self.from_port = value\n        elif name == 'toPort':\n            self.to_port = value\n        else:\n            setattr(self, name, value)\n\n    def add_grant(self, name=None, owner_id=None, cidr_ip=None, group_id=None,\n                  dry_run=False):\n        grant = GroupOrCIDR(self)\n        grant.owner_id = owner_id\n        grant.group_id = group_id\n        grant.name = name\n        grant.cidr_ip = cidr_ip\n        self.grants.append(grant)\n        return grant\n\n\nclass GroupOrCIDR(object):\n\n    def __init__(self, parent=None):\n        self.owner_id = None\n        self.group_id = None\n        self.name = None\n        self.cidr_ip = None\n\n    def __repr__(self):\n        if self.cidr_ip:\n            return '%s' % self.cidr_ip\n        else:\n            return '%s-%s' % (self.name or self.group_id, self.owner_id)\n\n    def startElement(self, name, attrs, connection):\n        return None\n\n    def endElement(self, name, value, connection):\n        if name == 'userId':\n            self.owner_id = value\n        elif name == 'groupId':\n            self.group_id = value\n        elif name == 'groupName':\n            self.name = value\n        if name == 'cidrIp':\n            self.cidr_ip = value\n        else:\n            setattr(self, name, value)\n", "test_list": ["def test_add_rule(self):\n    sg = SecurityGroup()\n    self.assertEqual(len(sg.rules), 0)\n    sg.add_rule(ip_protocol='http', from_port='80', to_port='8080', src_group_name='groupy', src_group_owner_id='12345', cidr_ip='10.0.0.1', src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'add_rule' function should accept valid input types for each parameter and ensure that the rule is correctly added to the 'rules' list of the SecurityGroup instance.", "unit_test": ["def test_add_rule_input_output_conditions(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)\n    rule = sg.rules[0]\n    self.assertEqual(rule.ip_protocol, 'tcp')\n    self.assertEqual(rule.from_port, 22)\n    self.assertEqual(rule.to_port, 22)\n    self.assertEqual(rule.grants[0].cidr_ip, '192.168.1.0/24')"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_input_output_conditions"}, "Exception Handling": {"requirement": "The 'add_rule' function should raise a ValueError if any of the required parameters are missing or invalid.", "unit_test": ["def test_add_rule_exception_handling(self):\n    sg = SecurityGroup()\n    with self.assertRaises(ValueError):\n        sg.add_rule(ip_protocol=None, from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_exception_handling"}, "Edge Case Handling": {"requirement": "The 'add_rule' function should handle edge cases such as adding a rule with the same parameters multiple times without duplication.", "unit_test": ["def test_add_rule_edge_case_handling(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'add_rule' function to allow adding multiple CIDR IP ranges in a single call.", "unit_test": ["def test_add_rule_functionality_extension(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip=['192.168.1.0/24', '10.0.0.0/24'], src_group_group_id='54321', dry_run=False)\n    self.assertEqual(len(sg.rules), 1)\n    self.assertEqual(len(sg.rules[0].grants), 2)"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'add_rule' function has complete parameter type annotations and a detailed docstring explaining its behavior.", "unit_test": ["def test_add_rule_annotation_coverage(self):\n    sg = SecurityGroup()\n    self.assertTrue(hasattr(sg.add_rule, '__annotations__'))\n    self.assertIn('ip_protocol', sg.add_rule.__annotations__)\n    self.assertIn('from_port', sg.add_rule.__annotations__)\n    self.assertIn('to_port', sg.add_rule.__annotations__)\n    self.assertIn('src_group_name', sg.add_rule.__annotations__)\n    self.assertIn('src_group_owner_id', sg.add_rule.__annotations__)\n    self.assertIn('cidr_ip', sg.add_rule.__annotations__)\n    self.assertIn('src_group_group_id', sg.add_rule.__annotations__)\n    self.assertIn('dry_run', sg.add_rule.__annotations__)"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_annotation_coverage"}, "Code Complexity": {"requirement": "The 'add_rule' function should maintain a cyclomatic complexity of no more than 5 to ensure readability and maintainability.", "unit_test": ["def test_add_rule_code_complexity(self):\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(SecurityGroup.add_rule)\n    complexity = cc_visit(source_code)\n    self.assertTrue(all(c.complexity <= 5 for c in complexity))"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_code_complexity"}, "Code Standard": {"requirement": "The 'add_rule' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_add_rule_code_standard(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_your_module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'add_rule' function correctly utilizes the 'rules' attribute of the SecurityGroup class.", "unit_test": ["def test_add_rule_context_usage_verification(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    self.assertIsInstance(sg.rules, IPPermissionsList)\n    self.assertEqual(len(sg.rules), 1)"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'add_rule' function correctly adds an IPPermissions object to the 'rules' list with the specified parameters.", "unit_test": ["def test_add_rule_context_usage_correctness_verification(self):\n    sg = SecurityGroup()\n    sg.add_rule(ip_protocol='tcp', from_port=22, to_port=22, src_group_name='groupA', src_group_owner_id='12345', cidr_ip='192.168.1.0/24', src_group_group_id='54321', dry_run=False)\n    rule = sg.rules[0]\n    self.assertEqual(rule.ip_protocol, 'tcp')\n    self.assertEqual(rule.from_port, 22)\n    self.assertEqual(rule.to_port, 22)\n    self.assertEqual(rule.grants[0].cidr_ip, '192.168.1.0/24')"], "test": "tests/unit/ec2/test_securitygroup.py::SecurityGroupTest::test_add_rule_context_usage_correctness_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "type": "method", "project_path": "Scientific-Engineering/bentoml", "completion_path": "Scientific-Engineering/bentoml/src/bentoml/_internal/runner/container.py", "signature_position": [550, 554], "body_position": [555, 556], "dependency": {"intra_class": ["bentoml._internal.runner.container.DefaultContainer.batches_to_batch"], "intra_file": ["bentoml._internal.runner.container.Payload"], "cross_file": []}, "requirement": {"Functionality": "This function takes a sequence of payloads and converts them into batches. It creates a list of batches on each payload in the sequence. Then, it combines the batches into a single batch along the specified batch dimension.", "Arguments": ":param cls: DefaultContainer. The class itself.\n:param payloads: Sequence of Payload. The payloads to be converted into batches.\n:param batch_dim: int. The dimension along which the batches will be combined. Defaults to 0.\n:return: tuple[list[Any], list[int]]. A tuple containing the list of batches and a list of integers representing the batch sizes."}, "tests": ["tests/unit/_internal/runner/test_container.py::test_default_container"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[list[t.Any], list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n", "context": "from __future__ import annotations\n\nimport abc\nimport base64\nimport io\nimport itertools\nimport pickle\nimport typing as t\n\nfrom ..types import LazyType\nfrom ..utils import LazyLoader\nfrom ..utils.pickle import fixed_torch_loads\nfrom ..utils.pickle import pep574_dumps\nfrom ..utils.pickle import pep574_loads\n\nSingleType = t.TypeVar(\"SingleType\")\nBatchType = t.TypeVar(\"BatchType\")\n\nTRITON_EXC_MSG = \"tritonclient is required to use triton with BentoML. Install with 'pip install \\\"tritonclient[all]>=2.29.0\\\"'.\"\n\nif t.TYPE_CHECKING:\n    import tritonclient.grpc.aio as tritongrpcclient\n    import tritonclient.http.aio as tritonhttpclient\n\n    from .. import external_typing as ext\nelse:\n    np = LazyLoader(\"np\", globals(), \"numpy\")\n    tritongrpcclient = LazyLoader(\n        \"tritongrpcclient\", globals(), \"tritonclient.grpc.aio\", exc_msg=TRITON_EXC_MSG\n    )\n    tritonhttpclient = LazyLoader(\n        \"tritonhttpclient\", globals(), \"tritonclient.http.aio\", exc_msg=TRITON_EXC_MSG\n    )\n\n\nclass Payload(t.NamedTuple):\n    data: bytes\n    meta: dict[str, bool | int | float | str | list[int]]\n    container: str\n    batch_size: int = -1\n\n\nclass DataContainer(t.Generic[SingleType, BatchType]):\n    @classmethod\n    def create_payload(\n        cls,\n        data: bytes,\n        batch_size: int,\n        meta: dict[str, bool | int | float | str | list[int]] | None = None,\n    ) -> Payload:\n        return Payload(data, meta or {}, container=cls.__name__, batch_size=batch_size)\n\n    @classmethod\n    @abc.abstractmethod\n    def to_payload(cls, batch: BatchType, batch_dim: int) -> Payload:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_payload(cls, payload: Payload) -> BatchType:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: SingleType, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return cls.to_triton_http_payload(inp, meta)\n        else:\n            return cls.to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: SingleType,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via gRPC client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: SingleType,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via HTTP client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[BatchType]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n\nclass TritonInferInputDataContainer(\n    DataContainer[\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n    ]\n):\n    @classmethod\n    def to_payload(cls, batch: tritongrpcclient.InferInput, batch_dim: int) -> Payload:\n        raise NotImplementedError\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> tritongrpcclient.InferInput:\n        raise NotImplementedError\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        return t.cast(\"tritongrpcclient.InferInput\", inp)\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        return t.cast(\"tritonhttpclient.InferInput\", inp)\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[tritongrpcclient.InferInput | tritonhttpclient.InferInput],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[tritongrpcclient.InferInput | tritonhttpclient.InferInput]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[Payload]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n\nclass NdarrayContainer(DataContainer[\"ext.NpNDArray\", \"ext.NpNDArray\"]):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.NpNDArray = np.concatenate(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.NpNDArray]:\n        return np.split(batch, indices[1:-1], axis=batch_dim)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        InferInput = tritongrpcclient.InferInput(\n            meta.name, inp.shape, tritongrpcclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        InferInput = tritonhttpclient.InferInput(\n            meta[\"name\"], inp.shape, tritonhttpclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array\n        if batch.shape:\n            if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n                # TODO: use fortan contiguous if it's faster\n                batch = np.ascontiguousarray(batch)\n\n            bs: bytes\n            concat_buffer_bs: bytes\n            indices: list[int]\n            bs, concat_buffer_bs, indices = pep574_dumps(batch)\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n            return cls.create_payload(\n                concat_buffer_bs,\n                batch.shape[batch_dim],\n                {\n                    \"format\": \"pickle5\",\n                    \"pickle_bytes_str\": bs_str,\n                    \"indices\": indices,\n                },\n            )\n\n        return cls.create_payload(\n            pickle.dumps(batch),\n            batch.shape[batch_dim],\n            {\"format\": \"default\"},\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.NpNDArray\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PandasDataFrameContainer(\n    DataContainer[t.Union[\"ext.PdDataFrame\", \"ext.PdSeries\"], \"ext.PdDataFrame\"]\n):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return pd.concat(batches, ignore_index=True), indices  # type: ignore (incomplete panadas types)\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        return [\n            batch.iloc[indices[i] : indices[i + 1]].reset_index(drop=True)\n            for i in range(len(indices) - 1)\n        ]\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame([batch])\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[0],\n            meta=meta,\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:\n        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pep574_loads(payload.data, b\"\", [])\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PILImageContainer(DataContainer[\"ext.PILImage\", \"ext.PILImage\"]):\n    _error = (\n        \"PIL.Image doesn't support batch inference.\"\n        \"You can convert it to numpy.ndarray before passing to the runner.\"\n    )\n\n    @classmethod\n    def to_payload(cls, batch: ext.PILImage, batch_dim: int) -> Payload:\n        buffer = io.BytesIO()\n        batch.save(buffer, format=batch.format)\n        return cls.create_payload(buffer.getvalue(), batch_size=1)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> ext.PILImage:\n        from ..io_descriptors.image import PIL\n\n        return PIL.Image.open(io.BytesIO(payload.data))\n\n    @classmethod\n    def batch_to_payloads(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[ext.PILImage], batch_dim: int\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[ext.PILImage]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n\nclass DefaultContainer(DataContainer[t.Any, t.List[t.Any]]):\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch: list[t.Any] = []\n        for subbatch in batches:\n            batch.extend(subbatch)\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        if isinstance(batch, t.Generator):  # Generators can't be pickled\n            batch = list(t.cast(t.Generator[t.Any, t.Any, t.Any], batch))\n\n        data = pickle.dumps(batch)\n\n        if isinstance(batch, list):\n            batch_size = len(t.cast(t.List[t.Any], batch))\n        else:\n            batch_size = 1\n\n        return cls.create_payload(data=data, batch_size=batch_size)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        return fixed_torch_loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n###The function: from_batch_payloads###\n\nclass DataContainerRegistry:\n    CONTAINER_SINGLE_TYPE_MAP: t.Dict[\n        LazyType[t.Any], t.Type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n    CONTAINER_BATCH_TYPE_MAP: t.Dict[\n        LazyType[t.Any], type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n\n    @classmethod\n    def register_container(\n        cls,\n        single_type: LazyType[t.Any] | type,\n        batch_type: LazyType[t.Any] | type,\n        container_cls: t.Type[DataContainer[t.Any, t.Any]],\n    ):\n        single_type = LazyType.from_type(single_type)\n        batch_type = LazyType.from_type(batch_type)\n\n        cls.CONTAINER_BATCH_TYPE_MAP[batch_type] = container_cls\n        cls.CONTAINER_SINGLE_TYPE_MAP[single_type] = container_cls\n\n    @classmethod\n    def find_by_single_type(\n        cls, type_: t.Type[SingleType] | LazyType[SingleType]\n    ) -> t.Type[DataContainer[SingleType, t.Any]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_SINGLE_TYPE_MAP:\n            return cls.CONTAINER_SINGLE_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_SINGLE_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_SINGLE_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_batch_type(\n        cls, type_: t.Type[BatchType] | LazyType[BatchType]\n    ) -> t.Type[DataContainer[t.Any, BatchType]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_BATCH_TYPE_MAP:\n            return cls.CONTAINER_BATCH_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_BATCH_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_BATCH_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_name(cls, name: str) -> t.Type[DataContainer[t.Any, t.Any]]:\n        for container_cls in cls.CONTAINER_BATCH_TYPE_MAP.values():\n            if container_cls.__name__ == name:\n                return container_cls\n        if name == DefaultContainer.__name__:\n            return DefaultContainer\n        raise ValueError(f\"can not find specified container class by name {name}\")\n\n\ndef register_builtin_containers():\n    DataContainerRegistry.register_container(\n        LazyType(\"numpy\", \"ndarray\"), LazyType(\"numpy\", \"ndarray\"), NdarrayContainer\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.series\", \"Series\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"PIL.Image\", \"Image\"),\n        LazyType(\"PIL.Image\", \"Image\"),\n        PILImageContainer,\n    )\n\n\nregister_builtin_containers()\n\n\nclass AutoContainer(DataContainer[t.Any, t.Any]):\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.to_payload(batch, batch_dim)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        container_cls = DataContainerRegistry.find_by_name(payload.container)\n        return container_cls.from_payload(payload)\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: t.Any, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_http_payload(inp, meta)\n        else:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int = 0\n    ) -> tuple[BatchType, list[int]]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batches[0]))\n        return container_cls.batches_to_batch(batches, batch_dim)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[BatchType]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_batches(batch, indices, batch_dim)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: t.Any,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_payloads(batch, indices, batch_dim)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[t.Any, list[int]]:\n        container_cls = DataContainerRegistry.find_by_name(payloads[0].container)\n        return container_cls.from_batch_payloads(payloads, batch_dim)\n", "prompt": "Please write a python function called 'from_batch_payloads' base the context. This function takes a sequence of payloads and converts them into batches. It creates a list of batches on each payload in the sequence. Then, it combines the batches into a single batch along the specified batch dimension.:param cls: DefaultContainer. The class itself.\n:param payloads: Sequence of Payload. The payloads to be converted into batches.\n:param batch_dim: int. The dimension along which the batches will be combined. Defaults to 0.\n:return: tuple[list[Any], list[int]]. A tuple containing the list of batches and a list of integers representing the batch sizes..\n        The context you need to refer to is as follows: from __future__ import annotations\n\nimport abc\nimport base64\nimport io\nimport itertools\nimport pickle\nimport typing as t\n\nfrom ..types import LazyType\nfrom ..utils import LazyLoader\nfrom ..utils.pickle import fixed_torch_loads\nfrom ..utils.pickle import pep574_dumps\nfrom ..utils.pickle import pep574_loads\n\nSingleType = t.TypeVar(\"SingleType\")\nBatchType = t.TypeVar(\"BatchType\")\n\nTRITON_EXC_MSG = \"tritonclient is required to use triton with BentoML. Install with 'pip install \\\"tritonclient[all]>=2.29.0\\\"'.\"\n\nif t.TYPE_CHECKING:\n    import tritonclient.grpc.aio as tritongrpcclient\n    import tritonclient.http.aio as tritonhttpclient\n\n    from .. import external_typing as ext\nelse:\n    np = LazyLoader(\"np\", globals(), \"numpy\")\n    tritongrpcclient = LazyLoader(\n        \"tritongrpcclient\", globals(), \"tritonclient.grpc.aio\", exc_msg=TRITON_EXC_MSG\n    )\n    tritonhttpclient = LazyLoader(\n        \"tritonhttpclient\", globals(), \"tritonclient.http.aio\", exc_msg=TRITON_EXC_MSG\n    )\n\n\nclass Payload(t.NamedTuple):\n    data: bytes\n    meta: dict[str, bool | int | float | str | list[int]]\n    container: str\n    batch_size: int = -1\n\n\nclass DataContainer(t.Generic[SingleType, BatchType]):\n    @classmethod\n    def create_payload(\n        cls,\n        data: bytes,\n        batch_size: int,\n        meta: dict[str, bool | int | float | str | list[int]] | None = None,\n    ) -> Payload:\n        return Payload(data, meta or {}, container=cls.__name__, batch_size=batch_size)\n\n    @classmethod\n    @abc.abstractmethod\n    def to_payload(cls, batch: BatchType, batch_dim: int) -> Payload:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_payload(cls, payload: Payload) -> BatchType:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: SingleType, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return cls.to_triton_http_payload(inp, meta)\n        else:\n            return cls.to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: SingleType,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via gRPC client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: SingleType,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        \"\"\"\n        Convert given input types to a Triton payload via HTTP client.\n\n        Implementation of this method should always return a InferInput.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} doesn't support converting to Triton payload.\"\n        )\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[BatchType]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        ...\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int\n    ) -> tuple[BatchType, list[int]]:\n        ...\n\n\nclass TritonInferInputDataContainer(\n    DataContainer[\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n        t.Union[\"tritongrpcclient.InferInput\", \"tritonhttpclient.InferInput\"],\n    ]\n):\n    @classmethod\n    def to_payload(cls, batch: tritongrpcclient.InferInput, batch_dim: int) -> Payload:\n        raise NotImplementedError\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> tritongrpcclient.InferInput:\n        raise NotImplementedError\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        return t.cast(\"tritongrpcclient.InferInput\", inp)\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        return t.cast(\"tritonhttpclient.InferInput\", inp)\n\n    @classmethod\n    @abc.abstractmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[tritongrpcclient.InferInput | tritonhttpclient.InferInput],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_batches(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[tritongrpcclient.InferInput | tritonhttpclient.InferInput]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def batch_to_payloads(\n        cls,\n        batch: tritongrpcclient.InferInput | tritonhttpclient.InferInput,\n        indices: t.Sequence[int],\n        batch_dim: int,\n    ) -> list[Payload]:\n        raise NotImplementedError\n\n    @classmethod\n    @abc.abstractmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int,\n    ) -> tuple[tritongrpcclient.InferInput | tritonhttpclient.InferInput, list[int]]:\n        raise NotImplementedError\n\n\nclass NdarrayContainer(DataContainer[\"ext.NpNDArray\", \"ext.NpNDArray\"]):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.NpNDArray = np.concatenate(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.NpNDArray]:\n        return np.split(batch, indices[1:-1], axis=batch_dim)\n\n    @classmethod\n    def to_triton_grpc_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n    ) -> tritongrpcclient.InferInput:\n        InferInput = tritongrpcclient.InferInput(\n            meta.name, inp.shape, tritongrpcclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_triton_http_payload(\n        cls,\n        inp: ext.NpNDArray,\n        meta: dict[str, t.Any],\n    ) -> tritonhttpclient.InferInput:\n        InferInput = tritonhttpclient.InferInput(\n            meta[\"name\"], inp.shape, tritonhttpclient.np_to_triton_dtype(inp.dtype)\n        )\n        InferInput.set_data_from_numpy(inp)\n        return InferInput\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array\n        if batch.shape:\n            if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n                # TODO: use fortan contiguous if it's faster\n                batch = np.ascontiguousarray(batch)\n\n            bs: bytes\n            concat_buffer_bs: bytes\n            indices: list[int]\n            bs, concat_buffer_bs, indices = pep574_dumps(batch)\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n            return cls.create_payload(\n                concat_buffer_bs,\n                batch.shape[batch_dim],\n                {\n                    \"format\": \"pickle5\",\n                    \"pickle_bytes_str\": bs_str,\n                    \"indices\": indices,\n                },\n            )\n\n        return cls.create_payload(\n            pickle.dumps(batch),\n            batch.shape[batch_dim],\n            {\"format\": \"default\"},\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.NpNDArray\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PandasDataFrameContainer(\n    DataContainer[t.Union[\"ext.PdDataFrame\", \"ext.PdSeries\"], \"ext.PdDataFrame\"]\n):\n    @classmethod\n    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return pd.concat(batches, ignore_index=True), indices  # type: ignore (incomplete panadas types)\n\n    @classmethod\n    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        return [\n            batch.iloc[indices[i] : indices[i + 1]].reset_index(drop=True)\n            for i in range(len(indices) - 1)\n        ]\n\n    @classmethod\n    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:\n        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame([batch])\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[0],\n            meta=meta,\n        )\n\n    @classmethod\n    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:\n        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pep574_loads(payload.data, b\"\", [])\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n\nclass PILImageContainer(DataContainer[\"ext.PILImage\", \"ext.PILImage\"]):\n    _error = (\n        \"PIL.Image doesn't support batch inference.\"\n        \"You can convert it to numpy.ndarray before passing to the runner.\"\n    )\n\n    @classmethod\n    def to_payload(cls, batch: ext.PILImage, batch_dim: int) -> Payload:\n        buffer = io.BytesIO()\n        batch.save(buffer, format=batch.format)\n        return cls.create_payload(buffer.getvalue(), batch_size=1)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> ext.PILImage:\n        from ..io_descriptors.image import PIL\n\n        return PIL.Image.open(io.BytesIO(payload.data))\n\n    @classmethod\n    def batch_to_payloads(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[Payload]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[ext.PILImage], batch_dim: int\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: ext.PILImage, indices: t.Sequence[int], batch_dim: int\n    ) -> list[ext.PILImage]:\n        raise NotImplementedError(cls._error)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[ext.PILImage, list[int]]:\n        raise NotImplementedError(cls._error)\n\n\nclass DefaultContainer(DataContainer[t.Any, t.List[t.Any]]):\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch: list[t.Any] = []\n        for subbatch in batches:\n            batch.extend(subbatch)\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:\n        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        if isinstance(batch, t.Generator):  # Generators can't be pickled\n            batch = list(t.cast(t.Generator[t.Any, t.Any, t.Any], batch))\n\n        data = pickle.dumps(batch)\n\n        if isinstance(batch, list):\n            batch_size = len(t.cast(t.List[t.Any], batch))\n        else:\n            batch_size = 1\n\n        return cls.create_payload(data=data, batch_size=batch_size)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        return fixed_torch_loads(payload.data)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    @classmethod\n###The function: from_batch_payloads###\n\nclass DataContainerRegistry:\n    CONTAINER_SINGLE_TYPE_MAP: t.Dict[\n        LazyType[t.Any], t.Type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n    CONTAINER_BATCH_TYPE_MAP: t.Dict[\n        LazyType[t.Any], type[DataContainer[t.Any, t.Any]]\n    ] = dict()\n\n    @classmethod\n    def register_container(\n        cls,\n        single_type: LazyType[t.Any] | type,\n        batch_type: LazyType[t.Any] | type,\n        container_cls: t.Type[DataContainer[t.Any, t.Any]],\n    ):\n        single_type = LazyType.from_type(single_type)\n        batch_type = LazyType.from_type(batch_type)\n\n        cls.CONTAINER_BATCH_TYPE_MAP[batch_type] = container_cls\n        cls.CONTAINER_SINGLE_TYPE_MAP[single_type] = container_cls\n\n    @classmethod\n    def find_by_single_type(\n        cls, type_: t.Type[SingleType] | LazyType[SingleType]\n    ) -> t.Type[DataContainer[SingleType, t.Any]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_SINGLE_TYPE_MAP:\n            return cls.CONTAINER_SINGLE_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_SINGLE_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_SINGLE_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_batch_type(\n        cls, type_: t.Type[BatchType] | LazyType[BatchType]\n    ) -> t.Type[DataContainer[t.Any, BatchType]]:\n        typeref = LazyType.from_type(type_)\n        if typeref in cls.CONTAINER_BATCH_TYPE_MAP:\n            return cls.CONTAINER_BATCH_TYPE_MAP[typeref]\n        for klass in cls.CONTAINER_BATCH_TYPE_MAP:\n            if klass.issubclass(type_):\n                return cls.CONTAINER_BATCH_TYPE_MAP[klass]\n        return DefaultContainer\n\n    @classmethod\n    def find_by_name(cls, name: str) -> t.Type[DataContainer[t.Any, t.Any]]:\n        for container_cls in cls.CONTAINER_BATCH_TYPE_MAP.values():\n            if container_cls.__name__ == name:\n                return container_cls\n        if name == DefaultContainer.__name__:\n            return DefaultContainer\n        raise ValueError(f\"can not find specified container class by name {name}\")\n\n\ndef register_builtin_containers():\n    DataContainerRegistry.register_container(\n        LazyType(\"numpy\", \"ndarray\"), LazyType(\"numpy\", \"ndarray\"), NdarrayContainer\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.series\", \"Series\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        LazyType(\"pandas.core.frame\", \"DataFrame\"),\n        PandasDataFrameContainer,\n    )\n\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.grpc.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        LazyType(\"tritonclient.http\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        LazyType(\"tritonclient.http.aio\", \"InferInput\"),\n        TritonInferInputDataContainer,\n    )\n    DataContainerRegistry.register_container(\n        LazyType(\"PIL.Image\", \"Image\"),\n        LazyType(\"PIL.Image\", \"Image\"),\n        PILImageContainer,\n    )\n\n\nregister_builtin_containers()\n\n\nclass AutoContainer(DataContainer[t.Any, t.Any]):\n    @classmethod\n    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.to_payload(batch, batch_dim)\n\n    @classmethod\n    def from_payload(cls, payload: Payload) -> t.Any:\n        container_cls = DataContainerRegistry.find_by_name(payload.container)\n        return container_cls.from_payload(payload)\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritongrpcclient.InferInput,\n        meta: tritongrpcclient.service_pb2.ModelMetadataResponse.TensorMetadata,\n        _use_http_client: t.Literal[False] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @t.overload\n    @classmethod\n    def to_triton_payload(\n        cls,\n        inp: tritonhttpclient.InferInput,\n        meta: dict[str, t.Any],\n        _use_http_client: t.Literal[True] = ...,\n    ) -> tritongrpcclient.InferInput:\n        ...\n\n    @classmethod\n    def to_triton_payload(\n        cls, inp: t.Any, meta: t.Any, _use_http_client: bool = False\n    ) -> t.Any:\n        \"\"\"\n        Convert given input types to a Triton payload.\n\n        Implementation of this method should always return a InferInput that has\n        data of a Numpy array.\n        \"\"\"\n        if _use_http_client:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_http_payload(inp, meta)\n        else:\n            return DataContainerRegistry.find_by_single_type(\n                type(inp)\n            ).to_triton_grpc_payload(inp, meta)\n\n    @classmethod\n    def batches_to_batch(\n        cls, batches: t.Sequence[BatchType], batch_dim: int = 0\n    ) -> tuple[BatchType, list[int]]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batches[0]))\n        return container_cls.batches_to_batch(batches, batch_dim)\n\n    @classmethod\n    def batch_to_batches(\n        cls, batch: BatchType, indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[BatchType]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_batches(batch, indices, batch_dim)\n\n    @classmethod\n    def batch_to_payloads(\n        cls,\n        batch: t.Any,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:\n        container_cls: t.Type[\n            DataContainer[t.Any, t.Any]\n        ] = DataContainerRegistry.find_by_batch_type(type(batch))\n        return container_cls.batch_to_payloads(batch, indices, batch_dim)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[t.Any, list[int]]:\n        container_cls = DataContainerRegistry.find_by_name(payloads[0].container)\n        return container_cls.from_batch_payloads(payloads, batch_dim)\n", "test_list": ["@pytest.mark.parametrize('batch_dim_exc', [AssertionError])\n@pytest.mark.parametrize('wrong_batch_dim', [1, 19])\ndef test_default_container(batch_dim_exc: t.Type[Exception], wrong_batch_dim: int):\n    l1 = [1, 2, 3]\n    l2 = [3, 4, 5, 6]\n    batch, indices = c.DefaultContainer.batches_to_batch([l1, l2])\n    assert batch == l1 + l2\n    assert indices == [0, 3, 7]\n    restored_l1, restored_l2 = c.DefaultContainer.batch_to_batches(batch, indices)\n    assert restored_l1 == l1\n    assert restored_l2 == l2\n    with pytest.raises(batch_dim_exc):\n        c.DefaultContainer.batches_to_batch([l1, l2], batch_dim=wrong_batch_dim)\n    with pytest.raises(batch_dim_exc):\n        c.DefaultContainer.batch_to_batches(batch, indices, batch_dim=wrong_batch_dim)\n\n    def _generator():\n        yield 'apple'\n        yield 'banana'\n        yield 'cherry'\n    assert c.DefaultContainer.from_payload(c.DefaultContainer.to_payload(_generator(), batch_dim=0)) == list(_generator())\n    assert c.DefaultContainer.from_batch_payloads(c.DefaultContainer.batch_to_payloads(batch, indices)) == (batch, indices)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'from_batch_payloads' should correctly convert a sequence of Payload objects into a single batch and a list of batch sizes, ensuring the output types match the expected return types.", "unit_test": ["def test_from_batch_payloads_output_type():\n    payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                Payload(data=b'data2', meta={}, container='DefaultContainer', batch_size=4)]\n    batch, batch_sizes = DefaultContainer.from_batch_payloads(payloads, batch_dim=0)\n    assert isinstance(batch, list)\n    assert isinstance(batch_sizes, list)\n    assert all(isinstance(size, int) for size in batch_sizes)"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_output_type"}, "Exception Handling": {"requirement": "The function 'from_batch_payloads' should raise a ValueError if the payloads have inconsistent container types.", "unit_test": ["def test_from_batch_payloads_inconsistent_container():\n    payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                Payload(data=b'data2', meta={}, container='AnotherContainer', batch_size=4)]\n    with pytest.raises(ValueError):\n        DefaultContainer.from_batch_payloads(payloads, batch_dim=0)"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_inconsistent_container"}, "Edge Case Handling": {"requirement": "The function 'from_batch_payloads' should handle an empty sequence of payloads gracefully, returning an empty batch and an empty list of batch sizes.", "unit_test": ["def test_from_batch_payloads_empty_sequence():\n    payloads = []\n    batch, batch_sizes = DefaultContainer.from_batch_payloads(payloads, batch_dim=0)\n    assert batch == []\n    assert batch_sizes == []"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_empty_sequence"}, "Functionality Extension": {"requirement": "Extend the 'from_batch_payloads' function to support an optional parameter 'validate' that, when set to True, checks if all payloads have the same batch size and raises an AssertionError if not.", "unit_test": ["def test_from_batch_payloads_validate_batch_size():\n    payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                Payload(data=b'data2', meta={}, container='DefaultContainer', batch_size=3)]\n    batch, batch_sizes = DefaultContainer.from_batch_payloads(payloads, batch_dim=0, validate=True)\n    assert batch_sizes == [3, 3]\n\n    payloads_mismatch = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3),\n                         Payload(data=b'data2', meta={}, container='DefaultContainer', batch_size=4)]\n    with pytest.raises(AssertionError):\n        DefaultContainer.from_batch_payloads(payloads_mismatch, batch_dim=0, validate=True)"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_validate_batch_size"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'from_batch_payloads' function are annotated with type hints.", "unit_test": ["def test_from_batch_payloads_annotations():\n    annotations = DefaultContainer.from_batch_payloads.__annotations__\n    assert 'payloads' in annotations\n    assert annotations['payloads'] == t.Sequence[Payload]\n    assert 'batch_dim' in annotations\n    assert annotations['batch_dim'] == int\n    assert 'return' in annotations\n    assert annotations['return'] == t.Tuple[t.Any, list[int]]"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'from_batch_payloads' function should not exceed 5.", "unit_test": ["def test_from_batch_payloads_complexity():\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(DefaultContainer.from_batch_payloads)\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 10"], "test": "tests/unit/_internal/runner/test_container.py::test_code_complexity"}, "Code Standard": {"requirement": "The 'from_batch_payloads' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_from_batch_payloads_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/module.py'])\n    assert result.total_errors == 0"], "test": "tests/unit/_internal/runner/test_container.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'from_batch_payloads' function should utilize the 'batches_to_batch' method from the 'DefaultContainer' class to combine batches.", "unit_test": ["def test_from_batch_payloads_uses_batches_to_batch():\n    with mock.patch('bentoml._internal.runner.container.DefaultContainer.batches_to_batch') as mock_batches_to_batch:\n        payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3)]\n        DefaultContainer.from_batch_payloads(payloads, batch_dim=0)\n        mock_batches_to_batch.assert_called_once()"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_uses_batches_to_batch"}, "Context Usage Correctness Verification": {"requirement": "The 'from_batch_payloads' function should correctly pass the batch dimension parameter to the 'batches_to_batch' method.", "unit_test": ["def test_from_batch_payloads_correct_batch_dim_usage():\n    with mock.patch('bentoml._internal.runner.container.DefaultContainer.batches_to_batch') as mock_batches_to_batch:\n        payloads = [Payload(data=b'data1', meta={}, container='DefaultContainer', batch_size=3)]\n        DefaultContainer.from_batch_payloads(payloads, batch_dim=2)\n        mock_batches_to_batch.assert_called_with(mock.ANY, 2)"], "test": "tests/unit/_internal/runner/test_container.py::test_from_batch_payloads_correct_batch_dim_usage"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "sqlitedict.SqliteDict.commit", "type": "method", "project_path": "Database/sqlitedict", "completion_path": "Database/sqlitedict/sqlitedict.py", "signature_position": [370, 370], "body_position": [377, 378], "dependency": {"intra_class": ["sqlitedict.SqliteDict.conn"], "intra_file": ["sqlitedict.SqliteMultithread.commit"], "cross_file": []}, "requirement": {"Functionality": "This function is used to persist all data in the SqliteDict instance to disk. It commits the changes made to the database. If `blocking` is set to False, the commit command is queued but the data is not guaranteed to be persisted immediately.", "Arguments": ":param self: SqliteDict. An instance of the SqliteDict class.\n:param blocking: Bool. Whether to block until the commit is complete. Defaults to True.\n:return: No return values."}, "tests": ["tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_overwrite_using_flag_w", "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_readonly", "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_default_reuse_existing_flag_c"], "indent": 4, "domain": "Database", "code": "    def commit(self, blocking=True):\n        \"\"\"\n        Persist all data to disk.\n\n        When `blocking` is False, the commit command is queued, but the data is\n        not guaranteed persisted (default implication when autocommit=True).\n        \"\"\"\n        if self.conn is not None:\n            self.conn.commit(blocking)\n", "context": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# This code is distributed under the terms and conditions\n# from the Apache License, Version 2.0\n#\n# http://opensource.org/licenses/apache2.0.php\n#\n# This code was inspired by:\n#  * http://code.activestate.com/recipes/576638-draft-for-an-sqlite3-based-dbm/\n#  * http://code.activestate.com/recipes/526618/\n\n\"\"\"\nA lightweight wrapper around Python's sqlite3 database, with a dict-like interface\nand multi-thread access support::\n\n>>> mydict = SqliteDict('some.db', autocommit=True) # the mapping will be persisted to file `some.db`\n>>> mydict['some_key'] = any_picklable_object\n>>> print mydict['some_key']\n>>> print len(mydict) # etc... all dict functions work\n\nPickle is used internally to serialize the values. Keys are strings.\n\nIf you don't use autocommit (default is no autocommit for performance), then\ndon't forget to call `mydict.commit()` when done with a transaction.\n\n\"\"\"\n\nimport sqlite3\nimport os\nimport sys\nimport tempfile\nimport threading\nimport logging\nimport traceback\nfrom base64 import b64decode, b64encode\nimport weakref\n\n__version__ = '2.1.0'\n\n\ndef reraise(tp, value, tb=None):\n    if value is None:\n        value = tp()\n    if value.__traceback__ is not tb:\n        raise value.with_traceback(tb)\n    raise value\n\n\ntry:\n    from cPickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\nexcept ImportError:\n    from pickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\n\n# some Python 3 vs 2 imports\ntry:\n    from collections import UserDict as DictClass\nexcept ImportError:\n    from UserDict import DictMixin as DictClass\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\n\nlogger = logging.getLogger(__name__)\n\n#\n# There's a thread that holds the actual SQL connection (SqliteMultithread).\n# We communicate with this thread via queues (request and responses).\n# The requests can either be SQL commands or one of the \"special\" commands\n# below:\n#\n# _REQUEST_CLOSE: request that the SQL connection be closed\n# _REQUEST_COMMIT: request that any changes be committed to the DB\n#\n# Responses are either SQL records (e.g. results of a SELECT) or the magic\n# _RESPONSE_NO_MORE command, which indicates nothing else will ever be written\n# to the response queue.\n#\n_REQUEST_CLOSE = '--close--'\n_REQUEST_COMMIT = '--commit--'\n_RESPONSE_NO_MORE = '--no more--'\n\n#\n# We work with weak references for better memory efficiency.\n# Dereferencing, checking the referent queue still exists, and putting to it\n# is boring and repetitive, so we have a _put function to handle it for us.\n#\n_PUT_OK, _PUT_REFERENT_DESTROYED, _PUT_NOOP = 0, 1, 2\n\n\ndef _put(queue_reference, item):\n    if queue_reference is not None:\n        queue = queue_reference()\n        if queue is None:\n            #\n            # We got a reference to a queue, but that queue no longer exists\n            #\n            retval = _PUT_REFERENT_DESTROYED\n        else:\n            queue.put(item)\n            retval = _PUT_OK\n\n        del queue\n        return retval\n\n    #\n    # We didn't get a reference to a queue, so do nothing (no-op).\n    #\n    return _PUT_NOOP\n\n\ndef open(*args, **kwargs):\n    \"\"\"See documentation of the SqliteDict class.\"\"\"\n    return SqliteDict(*args, **kwargs)\n\n\ndef encode(obj):\n    \"\"\"Serialize an object using pickle to a binary format accepted by SQLite.\"\"\"\n    return sqlite3.Binary(dumps(obj, protocol=PICKLE_PROTOCOL))\n\n\ndef decode(obj):\n    \"\"\"Deserialize objects retrieved from SQLite.\"\"\"\n    return loads(bytes(obj))\n\n\ndef encode_key(key):\n    \"\"\"Serialize a key using pickle + base64 encoding to text accepted by SQLite.\"\"\"\n    return b64encode(dumps(key, protocol=PICKLE_PROTOCOL)).decode(\"ascii\")\n\n\ndef decode_key(key):\n    \"\"\"Deserialize a key retrieved from SQLite.\"\"\"\n    return loads(b64decode(key.encode(\"ascii\")))\n\n\ndef identity(obj):\n    \"\"\"Identity f(x) = x function for encoding/decoding.\"\"\"\n    return obj\n\n\nclass SqliteDict(DictClass):\n    VALID_FLAGS = ['c', 'r', 'w', 'n']\n\n    def __init__(self, filename=None, tablename='unnamed', flag='c',\n                 autocommit=False, journal_mode=\"DELETE\", encode=encode,\n                 decode=decode, encode_key=identity, decode_key=identity,\n                 timeout=5, outer_stack=True):\n        \"\"\"\n        Initialize a thread-safe sqlite-backed dictionary. The dictionary will\n        be a table `tablename` in database file `filename`. A single file (=database)\n        may contain multiple tables.\n\n        If no `filename` is given, a random file in temp will be used (and deleted\n        from temp once the dict is closed/deleted).\n\n        If you enable `autocommit`, changes will be committed after each operation\n        (more inefficient but safer). Otherwise, changes are committed on `self.commit()`,\n        `self.clear()` and `self.close()`.\n\n        Set `journal_mode` to 'OFF' if you're experiencing sqlite I/O problems\n        or if you need performance and don't care about crash-consistency.\n\n        Set `outer_stack` to False to disable the output of the outer exception\n        to the error logs.  This may improve the efficiency of sqlitedict\n        operation at the expense of a detailed exception trace.\n\n        The `flag` parameter. Exactly one of:\n          'c': default mode, open for read/write, creating the db/table if necessary.\n          'w': open for r/w, but drop `tablename` contents first (start with empty table)\n          'r': open as read-only\n          'n': create a new database (erasing any existing tables, not just `tablename`!).\n\n        The `encode` and `decode` parameters are used to customize how the values\n        are serialized and deserialized.\n        The `encode` parameter must be a function that takes a single Python\n        object and returns a serialized representation.\n        The `decode` function must be a function that takes the serialized\n        representation produced by `encode` and returns a deserialized Python\n        object.\n        The default is to use pickle.\n\n        The `timeout` defines the maximum time (in seconds) to wait for initial Thread startup.\n\n        \"\"\"\n        self.in_temp = filename is None\n        if self.in_temp:\n            fd, filename = tempfile.mkstemp(prefix='sqldict')\n            os.close(fd)\n\n        if flag not in SqliteDict.VALID_FLAGS:\n            raise RuntimeError(\"Unrecognized flag: %s\" % flag)\n        self.flag = flag\n\n        if flag == 'n':\n            if os.path.exists(filename):\n                os.remove(filename)\n\n        dirname = os.path.dirname(filename)\n        if dirname:\n            if not os.path.exists(dirname):\n                raise RuntimeError('Error! The directory does not exist, %s' % dirname)\n\n        self.filename = filename\n\n        # Use standard SQL escaping of double quote characters in identifiers, by doubling them.\n        # See https://github.com/RaRe-Technologies/sqlitedict/pull/113\n        self.tablename = tablename.replace('\"', '\"\"')\n\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        self.encode = encode\n        self.decode = decode\n        self.encode_key = encode_key\n        self.decode_key = decode_key\n        self._outer_stack = outer_stack\n\n        logger.debug(\"opening Sqlite table %r in %r\" % (tablename, filename))\n        self.conn = self._new_conn()\n        if self.flag == 'r':\n            if self.tablename not in SqliteDict.get_tablenames(self.filename):\n                msg = 'Refusing to create a new table \"%s\" in read-only DB mode' % tablename\n                raise RuntimeError(msg)\n        else:\n            MAKE_TABLE = 'CREATE TABLE IF NOT EXISTS \"%s\" (key TEXT PRIMARY KEY, value BLOB)' % self.tablename\n            self.conn.execute(MAKE_TABLE)\n            self.conn.commit()\n        if flag == 'w':\n            self.clear()\n\n    def _new_conn(self):\n        return SqliteMultithread(\n            self.filename,\n            autocommit=self.autocommit,\n            journal_mode=self.journal_mode,\n            outer_stack=self._outer_stack,\n        )\n\n    def __enter__(self):\n        if not hasattr(self, 'conn') or self.conn is None:\n            self.conn = self._new_conn()\n        return self\n\n    def __exit__(self, *exc_info):\n        self.close()\n\n    def __str__(self):\n        return \"SqliteDict(%s)\" % (self.filename)\n\n    def __repr__(self):\n        return str(self)  # no need of something complex\n\n    def __len__(self):\n        # `select count (*)` is super slow in sqlite (does a linear scan!!)\n        # As a result, len() is very slow too once the table size grows beyond trivial.\n        # We could keep the total count of rows ourselves, by means of triggers,\n        # but that seems too complicated and would slow down normal operation\n        # (insert/delete etc).\n        GET_LEN = 'SELECT COUNT(*) FROM \"%s\"' % self.tablename\n        rows = self.conn.select_one(GET_LEN)[0]\n        return rows if rows is not None else 0\n\n    def __bool__(self):\n        # No elements is False, otherwise True\n        GET_MAX = 'SELECT MAX(ROWID) FROM \"%s\"' % self.tablename\n        m = self.conn.select_one(GET_MAX)[0]\n        # Explicit better than implicit and bla bla\n        return True if m is not None else False\n\n    def iterkeys(self):\n        GET_KEYS = 'SELECT key FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key in self.conn.select(GET_KEYS):\n            yield self.decode_key(key[0])\n\n    def itervalues(self):\n        GET_VALUES = 'SELECT value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for value in self.conn.select(GET_VALUES):\n            yield self.decode(value[0])\n\n    def iteritems(self):\n        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n\n    def keys(self):\n        return self.iterkeys()\n\n    def values(self):\n        return self.itervalues()\n\n    def items(self):\n        return self.iteritems()\n\n    def __contains__(self, key):\n        HAS_ITEM = 'SELECT 1 FROM \"%s\" WHERE key = ?' % self.tablename\n        return self.conn.select_one(HAS_ITEM, (self.encode_key(key),)) is not None\n\n    def __getitem__(self, key):\n        GET_ITEM = 'SELECT value FROM \"%s\" WHERE key = ?' % self.tablename\n        item = self.conn.select_one(GET_ITEM, (self.encode_key(key),))\n        if item is None:\n            raise KeyError(key)\n        return self.decode(item[0])\n\n    def __setitem__(self, key, value):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        ADD_ITEM = 'REPLACE INTO \"%s\" (key, value) VALUES (?,?)' % self.tablename\n        self.conn.execute(ADD_ITEM, (self.encode_key(key), self.encode(value)))\n        if self.autocommit:\n            self.commit()\n\n    def __delitem__(self, key):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete from read-only SqliteDict')\n\n        if key not in self:\n            raise KeyError(key)\n        DEL_ITEM = 'DELETE FROM \"%s\" WHERE key = ?' % self.tablename\n        self.conn.execute(DEL_ITEM, (self.encode_key(key),))\n        if self.autocommit:\n            self.commit()\n\n    def update(self, items=(), **kwds):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n\n    def __iter__(self):\n        return self.iterkeys()\n\n    def clear(self):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        # avoid VACUUM, as it gives \"OperationalError: database schema has changed\"\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()\n\n    @staticmethod\n    def get_tablenames(filename):\n        \"\"\"get the names of the tables in an sqlite db as a list\"\"\"\n        if not os.path.isfile(filename):\n            raise IOError('file %s does not exist' % (filename))\n        GET_TABLENAMES = 'SELECT name FROM sqlite_master WHERE type=\"table\"'\n        with sqlite3.connect(filename) as conn:\n            cursor = conn.execute(GET_TABLENAMES)\n            res = cursor.fetchall()\n\n        return [name[0] for name in res]\n\n###The function: commit###    sync = commit\n\n    def close(self, do_log=True, force=False):\n        if do_log:\n            logger.debug(\"closing %s\" % self)\n        if hasattr(self, 'conn') and self.conn is not None:\n            if self.conn.autocommit and not force:\n                # typically calls to commit are non-blocking when autocommit is\n                # used.  However, we need to block on close() to ensure any\n                # awaiting exceptions are handled and that all data is\n                # persisted to disk before returning.\n                self.conn.commit(blocking=True)\n            self.conn.close(force=force)\n            self.conn = None\n        if self.in_temp:\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n\n    def terminate(self):\n        \"\"\"Delete the underlying database file. Use with care.\"\"\"\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close()\n\n        if self.filename == ':memory:':\n            return\n\n        logger.info(\"deleting %s\" % self.filename)\n        try:\n            if os.path.isfile(self.filename):\n                os.remove(self.filename)\n        except (OSError, IOError):\n            logger.exception(\"failed to delete %s\" % (self.filename))\n\n    def __del__(self):\n        # like close(), but assume globals are gone by now (do not log!)\n        try:\n            self.close(do_log=False, force=True)\n        except Exception:\n            # prevent error log flood in case of multiple SqliteDicts\n            # closed after connection lost (exceptions are always ignored\n            # in __del__ method.\n            pass\n\n\nclass SqliteMultithread(threading.Thread):\n    \"\"\"\n    Wrap sqlite connection in a way that allows concurrent requests from multiple threads.\n\n    This is done by internally queueing the requests and processing them sequentially\n    in a separate thread (in the same order they arrived).\n\n    \"\"\"\n    def __init__(self, filename, autocommit, journal_mode, outer_stack=True):\n        super(SqliteMultithread, self).__init__()\n        self.filename = filename\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        # use request queue of unlimited size\n        self.reqs = Queue()\n        self.daemon = True\n        self._outer_stack = outer_stack\n        self.log = logging.getLogger('sqlitedict.SqliteMultithread')\n\n        #\n        # Parts of this object's state get accessed from different threads, so\n        # we use synchronization to avoid race conditions.  For example,\n        # .exception gets set inside the new daemon thread that we spawned, but\n        # gets read from the main thread.  This is particularly important\n        # during initialization: the Thread needs some time to actually start\n        # working, and until this happens, any calls to e.g.\n        # check_raise_error() will prematurely return None, meaning all is\n        # well.  If the that connection happens to fail, we'll never know about\n        # it, and instead wait for a result that never arrives (effectively,\n        # deadlocking).  Locking solves this problem by eliminating the race\n        # condition.\n        #\n        self._lock = threading.Lock()\n        self._lock.acquire()\n        self.exception = None\n\n        self.start()\n\n    def _connect(self):\n        \"\"\"Connect to the underlying database.\n\n        Raises an exception on failure.  Returns the connection and cursor on success.\n        \"\"\"\n        try:\n            if self.autocommit:\n                conn = sqlite3.connect(self.filename, isolation_level=None, check_same_thread=False)\n            else:\n                conn = sqlite3.connect(self.filename, check_same_thread=False)\n        except Exception:\n            self.log.exception(\"Failed to initialize connection for filename: %s\" % self.filename)\n            self.exception = sys.exc_info()\n            raise\n\n        try:\n            conn.execute('PRAGMA journal_mode = %s' % self.journal_mode)\n            conn.text_factory = str\n            cursor = conn.cursor()\n            conn.commit()\n            cursor.execute('PRAGMA synchronous=OFF')\n        except Exception:\n            self.log.exception(\"Failed to execute PRAGMA statements.\")\n            self.exception = sys.exc_info()\n            raise\n\n        return conn, cursor\n\n    def run(self):\n        #\n        # Nb. this is what actually runs inside the new daemon thread.\n        # self._lock is locked at this stage - see the initializer function.\n        #\n        try:\n            conn, cursor = self._connect()\n        finally:\n            self._lock.release()\n\n        res_ref = None\n        while True:\n            #\n            # req: an SQL command or one of the --magic-- commands we use internally\n            # arg: arguments for the command\n            # res_ref: a weak reference to the queue into which responses must be placed\n            # outer_stack: the outer stack, for producing more informative traces in case of error\n            #\n            req, arg, res_ref, outer_stack = self.reqs.get()\n\n            if req == _REQUEST_CLOSE:\n                assert res_ref, ('--close-- without return queue', res_ref)\n                break\n            elif req == _REQUEST_COMMIT:\n                conn.commit()\n                _put(res_ref, _RESPONSE_NO_MORE)\n            else:\n                try:\n                    cursor.execute(req, arg)\n                except Exception:\n                    with self._lock:\n                        self.exception = (e_type, e_value, e_tb) = sys.exc_info()\n\n                    inner_stack = traceback.extract_stack()\n\n                    # An exception occurred in our thread, but we may not\n                    # immediately able to throw it in our calling thread, if it has\n                    # no return `res` queue: log as level ERROR both the inner and\n                    # outer exception immediately.\n                    #\n                    # Any iteration of res.get() or any next call will detect the\n                    # inner exception and re-raise it in the calling Thread; though\n                    # it may be confusing to see an exception for an unrelated\n                    # statement, an ERROR log statement from the 'sqlitedict.*'\n                    # namespace contains the original outer stack location.\n                    self.log.error('Inner exception:')\n                    for item in traceback.format_list(inner_stack):\n                        self.log.error(item)\n                    self.log.error('')  # deliniate traceback & exception w/blank line\n                    for item in traceback.format_exception_only(e_type, e_value):\n                        self.log.error(item)\n\n                    self.log.error('')  # exception & outer stack w/blank line\n\n                    if self._outer_stack:\n                        self.log.error('Outer stack:')\n                        for item in traceback.format_list(outer_stack):\n                            self.log.error(item)\n                        self.log.error('Exception will be re-raised at next call.')\n                    else:\n                        self.log.error(\n                            'Unable to show the outer stack. Pass '\n                            'outer_stack=True when initializing the '\n                            'SqliteDict instance to show the outer stack.'\n                        )\n\n                if res_ref:\n                    for rec in cursor:\n                        if _put(res_ref, rec) == _PUT_REFERENT_DESTROYED:\n                            #\n                            # The queue we are sending responses to got garbage\n                            # collected.  Nobody is listening anymore, so we\n                            # stop sending responses.\n                            #\n                            break\n\n                    _put(res_ref, _RESPONSE_NO_MORE)\n\n                if self.autocommit:\n                    conn.commit()\n\n        self.log.debug('received: %s, send: --no more--', req)\n        conn.close()\n\n        _put(res_ref, _RESPONSE_NO_MORE)\n\n    def check_raise_error(self):\n        \"\"\"\n        Check for and raise exception for any previous sqlite query.\n\n        For the `execute*` family of method calls, such calls are non-blocking and any\n        exception raised in the thread cannot be handled by the calling Thread (usually\n        MainThread).  This method is called on `close`, and prior to any subsequent\n        calls to the `execute*` methods to check for and raise an exception in a\n        previous call to the MainThread.\n        \"\"\"\n        with self._lock:\n            if self.exception:\n                e_type, e_value, e_tb = self.exception\n\n                # clear self.exception, if the caller decides to handle such\n                # exception, we should not repeatedly re-raise it.\n                self.exception = None\n\n                self.log.error('An exception occurred from a previous statement, view '\n                               'the logging namespace \"sqlitedict\" for outer stack.')\n\n                # The third argument to raise is the traceback object, and it is\n                # substituted instead of the current location as the place where\n                # the exception occurred, this is so that when using debuggers such\n                # as `pdb', or simply evaluating the naturally raised traceback, we\n                # retain the original (inner) location of where the exception\n                # occurred.\n                reraise(e_type, e_value, e_tb)\n\n    def execute(self, req, arg=None, res=None):\n        \"\"\"\n        `execute` calls are non-blocking: just queue up the request and return immediately.\n\n        :param req: The request (an SQL command)\n        :param arg: Arguments to the SQL command\n        :param res: A queue in which to place responses as they become available\n        \"\"\"\n        self.check_raise_error()\n        stack = None\n\n        if self._outer_stack:\n            # NOTE: This might be a lot of information to pump into an input\n            # queue, affecting performance.  I've also seen earlier versions of\n            # jython take a severe performance impact for throwing exceptions\n            # so often.\n            stack = traceback.extract_stack()[:-1]\n\n        #\n        # We pass a weak reference to the response queue instead of a regular\n        # reference, because we want the queues to be garbage-collected\n        # more aggressively.\n        #\n        res_ref = None\n        if res:\n            res_ref = weakref.ref(res)\n\n        self.reqs.put((req, arg or tuple(), res_ref, stack))\n\n    def executemany(self, req, items):\n        for item in items:\n            self.execute(req, item)\n        self.check_raise_error()\n\n    def select(self, req, arg=None):\n        \"\"\"\n        Unlike sqlite's native select, this select doesn't handle iteration efficiently.\n\n        The result of `select` starts filling up with values as soon as the\n        request is dequeued, and although you can iterate over the result normally\n        (`for res in self.select(): ...`), the entire result will be in memory.\n        \"\"\"\n        res = Queue()  # results of the select will appear as items in this queue\n        self.execute(req, arg, res)\n        while True:\n            rec = res.get()\n            self.check_raise_error()\n            if rec == _RESPONSE_NO_MORE:\n                break\n            yield rec\n\n    def select_one(self, req, arg=None):\n        \"\"\"Return only the first row of the SELECT, or None if there are no matching rows.\"\"\"\n        try:\n            return next(iter(self.select(req, arg)))\n        except StopIteration:\n            return None\n\n    def commit(self, blocking=True):\n        if blocking:\n            # by default, we await completion of commit() unless\n            # blocking=False.  This ensures any available exceptions for any\n            # previous statement are thrown before returning, and that the\n            # data has actually persisted to disk!\n            self.select_one(_REQUEST_COMMIT)\n        else:\n            # otherwise, we fire and forget as usual.\n            self.execute(_REQUEST_COMMIT)\n\n    def close(self, force=False):\n        if force:\n            # If a SqliteDict is being killed or garbage-collected, then select_one()\n            # could hang forever because run() might already have exited and therefore\n            # can't process the request. Instead, push the close command to the requests\n            # queue directly. If run() is still alive, it will exit gracefully. If not,\n            # then there's nothing we can do anyway.\n            self.reqs.put((_REQUEST_CLOSE, None, weakref.ref(Queue()), None))\n        else:\n            # we abuse 'select' to \"iter\" over a \"--close--\" statement so that we\n            # can confirm the completion of close before joining the thread and\n            # returning (by semaphore '--no more--'\n            self.select_one(_REQUEST_CLOSE)\n            self.join()\n\n\n#\n# This is here for .github/workflows/release.yml\n#\nif __name__ == '__main__':\n    print(__version__)\n", "prompt": "Please write a python function called 'commit' base the context. This function is used to persist all data in the SqliteDict instance to disk. It commits the changes made to the database. If `blocking` is set to False, the commit command is queued but the data is not guaranteed to be persisted immediately.:param self: SqliteDict. An instance of the SqliteDict class.\n:param blocking: Bool. Whether to block until the commit is complete. Defaults to True.\n:return: No return values..\n        The context you need to refer to is as follows: #!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# This code is distributed under the terms and conditions\n# from the Apache License, Version 2.0\n#\n# http://opensource.org/licenses/apache2.0.php\n#\n# This code was inspired by:\n#  * http://code.activestate.com/recipes/576638-draft-for-an-sqlite3-based-dbm/\n#  * http://code.activestate.com/recipes/526618/\n\n\"\"\"\nA lightweight wrapper around Python's sqlite3 database, with a dict-like interface\nand multi-thread access support::\n\n>>> mydict = SqliteDict('some.db', autocommit=True) # the mapping will be persisted to file `some.db`\n>>> mydict['some_key'] = any_picklable_object\n>>> print mydict['some_key']\n>>> print len(mydict) # etc... all dict functions work\n\nPickle is used internally to serialize the values. Keys are strings.\n\nIf you don't use autocommit (default is no autocommit for performance), then\ndon't forget to call `mydict.commit()` when done with a transaction.\n\n\"\"\"\n\nimport sqlite3\nimport os\nimport sys\nimport tempfile\nimport threading\nimport logging\nimport traceback\nfrom base64 import b64decode, b64encode\nimport weakref\n\n__version__ = '2.1.0'\n\n\ndef reraise(tp, value, tb=None):\n    if value is None:\n        value = tp()\n    if value.__traceback__ is not tb:\n        raise value.with_traceback(tb)\n    raise value\n\n\ntry:\n    from cPickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\nexcept ImportError:\n    from pickle import dumps, loads, HIGHEST_PROTOCOL as PICKLE_PROTOCOL\n\n# some Python 3 vs 2 imports\ntry:\n    from collections import UserDict as DictClass\nexcept ImportError:\n    from UserDict import DictMixin as DictClass\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\n\nlogger = logging.getLogger(__name__)\n\n#\n# There's a thread that holds the actual SQL connection (SqliteMultithread).\n# We communicate with this thread via queues (request and responses).\n# The requests can either be SQL commands or one of the \"special\" commands\n# below:\n#\n# _REQUEST_CLOSE: request that the SQL connection be closed\n# _REQUEST_COMMIT: request that any changes be committed to the DB\n#\n# Responses are either SQL records (e.g. results of a SELECT) or the magic\n# _RESPONSE_NO_MORE command, which indicates nothing else will ever be written\n# to the response queue.\n#\n_REQUEST_CLOSE = '--close--'\n_REQUEST_COMMIT = '--commit--'\n_RESPONSE_NO_MORE = '--no more--'\n\n#\n# We work with weak references for better memory efficiency.\n# Dereferencing, checking the referent queue still exists, and putting to it\n# is boring and repetitive, so we have a _put function to handle it for us.\n#\n_PUT_OK, _PUT_REFERENT_DESTROYED, _PUT_NOOP = 0, 1, 2\n\n\ndef _put(queue_reference, item):\n    if queue_reference is not None:\n        queue = queue_reference()\n        if queue is None:\n            #\n            # We got a reference to a queue, but that queue no longer exists\n            #\n            retval = _PUT_REFERENT_DESTROYED\n        else:\n            queue.put(item)\n            retval = _PUT_OK\n\n        del queue\n        return retval\n\n    #\n    # We didn't get a reference to a queue, so do nothing (no-op).\n    #\n    return _PUT_NOOP\n\n\ndef open(*args, **kwargs):\n    \"\"\"See documentation of the SqliteDict class.\"\"\"\n    return SqliteDict(*args, **kwargs)\n\n\ndef encode(obj):\n    \"\"\"Serialize an object using pickle to a binary format accepted by SQLite.\"\"\"\n    return sqlite3.Binary(dumps(obj, protocol=PICKLE_PROTOCOL))\n\n\ndef decode(obj):\n    \"\"\"Deserialize objects retrieved from SQLite.\"\"\"\n    return loads(bytes(obj))\n\n\ndef encode_key(key):\n    \"\"\"Serialize a key using pickle + base64 encoding to text accepted by SQLite.\"\"\"\n    return b64encode(dumps(key, protocol=PICKLE_PROTOCOL)).decode(\"ascii\")\n\n\ndef decode_key(key):\n    \"\"\"Deserialize a key retrieved from SQLite.\"\"\"\n    return loads(b64decode(key.encode(\"ascii\")))\n\n\ndef identity(obj):\n    \"\"\"Identity f(x) = x function for encoding/decoding.\"\"\"\n    return obj\n\n\nclass SqliteDict(DictClass):\n    VALID_FLAGS = ['c', 'r', 'w', 'n']\n\n    def __init__(self, filename=None, tablename='unnamed', flag='c',\n                 autocommit=False, journal_mode=\"DELETE\", encode=encode,\n                 decode=decode, encode_key=identity, decode_key=identity,\n                 timeout=5, outer_stack=True):\n        \"\"\"\n        Initialize a thread-safe sqlite-backed dictionary. The dictionary will\n        be a table `tablename` in database file `filename`. A single file (=database)\n        may contain multiple tables.\n\n        If no `filename` is given, a random file in temp will be used (and deleted\n        from temp once the dict is closed/deleted).\n\n        If you enable `autocommit`, changes will be committed after each operation\n        (more inefficient but safer). Otherwise, changes are committed on `self.commit()`,\n        `self.clear()` and `self.close()`.\n\n        Set `journal_mode` to 'OFF' if you're experiencing sqlite I/O problems\n        or if you need performance and don't care about crash-consistency.\n\n        Set `outer_stack` to False to disable the output of the outer exception\n        to the error logs.  This may improve the efficiency of sqlitedict\n        operation at the expense of a detailed exception trace.\n\n        The `flag` parameter. Exactly one of:\n          'c': default mode, open for read/write, creating the db/table if necessary.\n          'w': open for r/w, but drop `tablename` contents first (start with empty table)\n          'r': open as read-only\n          'n': create a new database (erasing any existing tables, not just `tablename`!).\n\n        The `encode` and `decode` parameters are used to customize how the values\n        are serialized and deserialized.\n        The `encode` parameter must be a function that takes a single Python\n        object and returns a serialized representation.\n        The `decode` function must be a function that takes the serialized\n        representation produced by `encode` and returns a deserialized Python\n        object.\n        The default is to use pickle.\n\n        The `timeout` defines the maximum time (in seconds) to wait for initial Thread startup.\n\n        \"\"\"\n        self.in_temp = filename is None\n        if self.in_temp:\n            fd, filename = tempfile.mkstemp(prefix='sqldict')\n            os.close(fd)\n\n        if flag not in SqliteDict.VALID_FLAGS:\n            raise RuntimeError(\"Unrecognized flag: %s\" % flag)\n        self.flag = flag\n\n        if flag == 'n':\n            if os.path.exists(filename):\n                os.remove(filename)\n\n        dirname = os.path.dirname(filename)\n        if dirname:\n            if not os.path.exists(dirname):\n                raise RuntimeError('Error! The directory does not exist, %s' % dirname)\n\n        self.filename = filename\n\n        # Use standard SQL escaping of double quote characters in identifiers, by doubling them.\n        # See https://github.com/RaRe-Technologies/sqlitedict/pull/113\n        self.tablename = tablename.replace('\"', '\"\"')\n\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        self.encode = encode\n        self.decode = decode\n        self.encode_key = encode_key\n        self.decode_key = decode_key\n        self._outer_stack = outer_stack\n\n        logger.debug(\"opening Sqlite table %r in %r\" % (tablename, filename))\n        self.conn = self._new_conn()\n        if self.flag == 'r':\n            if self.tablename not in SqliteDict.get_tablenames(self.filename):\n                msg = 'Refusing to create a new table \"%s\" in read-only DB mode' % tablename\n                raise RuntimeError(msg)\n        else:\n            MAKE_TABLE = 'CREATE TABLE IF NOT EXISTS \"%s\" (key TEXT PRIMARY KEY, value BLOB)' % self.tablename\n            self.conn.execute(MAKE_TABLE)\n            self.conn.commit()\n        if flag == 'w':\n            self.clear()\n\n    def _new_conn(self):\n        return SqliteMultithread(\n            self.filename,\n            autocommit=self.autocommit,\n            journal_mode=self.journal_mode,\n            outer_stack=self._outer_stack,\n        )\n\n    def __enter__(self):\n        if not hasattr(self, 'conn') or self.conn is None:\n            self.conn = self._new_conn()\n        return self\n\n    def __exit__(self, *exc_info):\n        self.close()\n\n    def __str__(self):\n        return \"SqliteDict(%s)\" % (self.filename)\n\n    def __repr__(self):\n        return str(self)  # no need of something complex\n\n    def __len__(self):\n        # `select count (*)` is super slow in sqlite (does a linear scan!!)\n        # As a result, len() is very slow too once the table size grows beyond trivial.\n        # We could keep the total count of rows ourselves, by means of triggers,\n        # but that seems too complicated and would slow down normal operation\n        # (insert/delete etc).\n        GET_LEN = 'SELECT COUNT(*) FROM \"%s\"' % self.tablename\n        rows = self.conn.select_one(GET_LEN)[0]\n        return rows if rows is not None else 0\n\n    def __bool__(self):\n        # No elements is False, otherwise True\n        GET_MAX = 'SELECT MAX(ROWID) FROM \"%s\"' % self.tablename\n        m = self.conn.select_one(GET_MAX)[0]\n        # Explicit better than implicit and bla bla\n        return True if m is not None else False\n\n    def iterkeys(self):\n        GET_KEYS = 'SELECT key FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key in self.conn.select(GET_KEYS):\n            yield self.decode_key(key[0])\n\n    def itervalues(self):\n        GET_VALUES = 'SELECT value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for value in self.conn.select(GET_VALUES):\n            yield self.decode(value[0])\n\n    def iteritems(self):\n        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n\n    def keys(self):\n        return self.iterkeys()\n\n    def values(self):\n        return self.itervalues()\n\n    def items(self):\n        return self.iteritems()\n\n    def __contains__(self, key):\n        HAS_ITEM = 'SELECT 1 FROM \"%s\" WHERE key = ?' % self.tablename\n        return self.conn.select_one(HAS_ITEM, (self.encode_key(key),)) is not None\n\n    def __getitem__(self, key):\n        GET_ITEM = 'SELECT value FROM \"%s\" WHERE key = ?' % self.tablename\n        item = self.conn.select_one(GET_ITEM, (self.encode_key(key),))\n        if item is None:\n            raise KeyError(key)\n        return self.decode(item[0])\n\n    def __setitem__(self, key, value):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        ADD_ITEM = 'REPLACE INTO \"%s\" (key, value) VALUES (?,?)' % self.tablename\n        self.conn.execute(ADD_ITEM, (self.encode_key(key), self.encode(value)))\n        if self.autocommit:\n            self.commit()\n\n    def __delitem__(self, key):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete from read-only SqliteDict')\n\n        if key not in self:\n            raise KeyError(key)\n        DEL_ITEM = 'DELETE FROM \"%s\" WHERE key = ?' % self.tablename\n        self.conn.execute(DEL_ITEM, (self.encode_key(key),))\n        if self.autocommit:\n            self.commit()\n\n    def update(self, items=(), **kwds):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n\n    def __iter__(self):\n        return self.iterkeys()\n\n    def clear(self):\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        # avoid VACUUM, as it gives \"OperationalError: database schema has changed\"\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()\n\n    @staticmethod\n    def get_tablenames(filename):\n        \"\"\"get the names of the tables in an sqlite db as a list\"\"\"\n        if not os.path.isfile(filename):\n            raise IOError('file %s does not exist' % (filename))\n        GET_TABLENAMES = 'SELECT name FROM sqlite_master WHERE type=\"table\"'\n        with sqlite3.connect(filename) as conn:\n            cursor = conn.execute(GET_TABLENAMES)\n            res = cursor.fetchall()\n\n        return [name[0] for name in res]\n\n###The function: commit###    sync = commit\n\n    def close(self, do_log=True, force=False):\n        if do_log:\n            logger.debug(\"closing %s\" % self)\n        if hasattr(self, 'conn') and self.conn is not None:\n            if self.conn.autocommit and not force:\n                # typically calls to commit are non-blocking when autocommit is\n                # used.  However, we need to block on close() to ensure any\n                # awaiting exceptions are handled and that all data is\n                # persisted to disk before returning.\n                self.conn.commit(blocking=True)\n            self.conn.close(force=force)\n            self.conn = None\n        if self.in_temp:\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n\n    def terminate(self):\n        \"\"\"Delete the underlying database file. Use with care.\"\"\"\n        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close()\n\n        if self.filename == ':memory:':\n            return\n\n        logger.info(\"deleting %s\" % self.filename)\n        try:\n            if os.path.isfile(self.filename):\n                os.remove(self.filename)\n        except (OSError, IOError):\n            logger.exception(\"failed to delete %s\" % (self.filename))\n\n    def __del__(self):\n        # like close(), but assume globals are gone by now (do not log!)\n        try:\n            self.close(do_log=False, force=True)\n        except Exception:\n            # prevent error log flood in case of multiple SqliteDicts\n            # closed after connection lost (exceptions are always ignored\n            # in __del__ method.\n            pass\n\n\nclass SqliteMultithread(threading.Thread):\n    \"\"\"\n    Wrap sqlite connection in a way that allows concurrent requests from multiple threads.\n\n    This is done by internally queueing the requests and processing them sequentially\n    in a separate thread (in the same order they arrived).\n\n    \"\"\"\n    def __init__(self, filename, autocommit, journal_mode, outer_stack=True):\n        super(SqliteMultithread, self).__init__()\n        self.filename = filename\n        self.autocommit = autocommit\n        self.journal_mode = journal_mode\n        # use request queue of unlimited size\n        self.reqs = Queue()\n        self.daemon = True\n        self._outer_stack = outer_stack\n        self.log = logging.getLogger('sqlitedict.SqliteMultithread')\n\n        #\n        # Parts of this object's state get accessed from different threads, so\n        # we use synchronization to avoid race conditions.  For example,\n        # .exception gets set inside the new daemon thread that we spawned, but\n        # gets read from the main thread.  This is particularly important\n        # during initialization: the Thread needs some time to actually start\n        # working, and until this happens, any calls to e.g.\n        # check_raise_error() will prematurely return None, meaning all is\n        # well.  If the that connection happens to fail, we'll never know about\n        # it, and instead wait for a result that never arrives (effectively,\n        # deadlocking).  Locking solves this problem by eliminating the race\n        # condition.\n        #\n        self._lock = threading.Lock()\n        self._lock.acquire()\n        self.exception = None\n\n        self.start()\n\n    def _connect(self):\n        \"\"\"Connect to the underlying database.\n\n        Raises an exception on failure.  Returns the connection and cursor on success.\n        \"\"\"\n        try:\n            if self.autocommit:\n                conn = sqlite3.connect(self.filename, isolation_level=None, check_same_thread=False)\n            else:\n                conn = sqlite3.connect(self.filename, check_same_thread=False)\n        except Exception:\n            self.log.exception(\"Failed to initialize connection for filename: %s\" % self.filename)\n            self.exception = sys.exc_info()\n            raise\n\n        try:\n            conn.execute('PRAGMA journal_mode = %s' % self.journal_mode)\n            conn.text_factory = str\n            cursor = conn.cursor()\n            conn.commit()\n            cursor.execute('PRAGMA synchronous=OFF')\n        except Exception:\n            self.log.exception(\"Failed to execute PRAGMA statements.\")\n            self.exception = sys.exc_info()\n            raise\n\n        return conn, cursor\n\n    def run(self):\n        #\n        # Nb. this is what actually runs inside the new daemon thread.\n        # self._lock is locked at this stage - see the initializer function.\n        #\n        try:\n            conn, cursor = self._connect()\n        finally:\n            self._lock.release()\n\n        res_ref = None\n        while True:\n            #\n            # req: an SQL command or one of the --magic-- commands we use internally\n            # arg: arguments for the command\n            # res_ref: a weak reference to the queue into which responses must be placed\n            # outer_stack: the outer stack, for producing more informative traces in case of error\n            #\n            req, arg, res_ref, outer_stack = self.reqs.get()\n\n            if req == _REQUEST_CLOSE:\n                assert res_ref, ('--close-- without return queue', res_ref)\n                break\n            elif req == _REQUEST_COMMIT:\n                conn.commit()\n                _put(res_ref, _RESPONSE_NO_MORE)\n            else:\n                try:\n                    cursor.execute(req, arg)\n                except Exception:\n                    with self._lock:\n                        self.exception = (e_type, e_value, e_tb) = sys.exc_info()\n\n                    inner_stack = traceback.extract_stack()\n\n                    # An exception occurred in our thread, but we may not\n                    # immediately able to throw it in our calling thread, if it has\n                    # no return `res` queue: log as level ERROR both the inner and\n                    # outer exception immediately.\n                    #\n                    # Any iteration of res.get() or any next call will detect the\n                    # inner exception and re-raise it in the calling Thread; though\n                    # it may be confusing to see an exception for an unrelated\n                    # statement, an ERROR log statement from the 'sqlitedict.*'\n                    # namespace contains the original outer stack location.\n                    self.log.error('Inner exception:')\n                    for item in traceback.format_list(inner_stack):\n                        self.log.error(item)\n                    self.log.error('')  # deliniate traceback & exception w/blank line\n                    for item in traceback.format_exception_only(e_type, e_value):\n                        self.log.error(item)\n\n                    self.log.error('')  # exception & outer stack w/blank line\n\n                    if self._outer_stack:\n                        self.log.error('Outer stack:')\n                        for item in traceback.format_list(outer_stack):\n                            self.log.error(item)\n                        self.log.error('Exception will be re-raised at next call.')\n                    else:\n                        self.log.error(\n                            'Unable to show the outer stack. Pass '\n                            'outer_stack=True when initializing the '\n                            'SqliteDict instance to show the outer stack.'\n                        )\n\n                if res_ref:\n                    for rec in cursor:\n                        if _put(res_ref, rec) == _PUT_REFERENT_DESTROYED:\n                            #\n                            # The queue we are sending responses to got garbage\n                            # collected.  Nobody is listening anymore, so we\n                            # stop sending responses.\n                            #\n                            break\n\n                    _put(res_ref, _RESPONSE_NO_MORE)\n\n                if self.autocommit:\n                    conn.commit()\n\n        self.log.debug('received: %s, send: --no more--', req)\n        conn.close()\n\n        _put(res_ref, _RESPONSE_NO_MORE)\n\n    def check_raise_error(self):\n        \"\"\"\n        Check for and raise exception for any previous sqlite query.\n\n        For the `execute*` family of method calls, such calls are non-blocking and any\n        exception raised in the thread cannot be handled by the calling Thread (usually\n        MainThread).  This method is called on `close`, and prior to any subsequent\n        calls to the `execute*` methods to check for and raise an exception in a\n        previous call to the MainThread.\n        \"\"\"\n        with self._lock:\n            if self.exception:\n                e_type, e_value, e_tb = self.exception\n\n                # clear self.exception, if the caller decides to handle such\n                # exception, we should not repeatedly re-raise it.\n                self.exception = None\n\n                self.log.error('An exception occurred from a previous statement, view '\n                               'the logging namespace \"sqlitedict\" for outer stack.')\n\n                # The third argument to raise is the traceback object, and it is\n                # substituted instead of the current location as the place where\n                # the exception occurred, this is so that when using debuggers such\n                # as `pdb', or simply evaluating the naturally raised traceback, we\n                # retain the original (inner) location of where the exception\n                # occurred.\n                reraise(e_type, e_value, e_tb)\n\n    def execute(self, req, arg=None, res=None):\n        \"\"\"\n        `execute` calls are non-blocking: just queue up the request and return immediately.\n\n        :param req: The request (an SQL command)\n        :param arg: Arguments to the SQL command\n        :param res: A queue in which to place responses as they become available\n        \"\"\"\n        self.check_raise_error()\n        stack = None\n\n        if self._outer_stack:\n            # NOTE: This might be a lot of information to pump into an input\n            # queue, affecting performance.  I've also seen earlier versions of\n            # jython take a severe performance impact for throwing exceptions\n            # so often.\n            stack = traceback.extract_stack()[:-1]\n\n        #\n        # We pass a weak reference to the response queue instead of a regular\n        # reference, because we want the queues to be garbage-collected\n        # more aggressively.\n        #\n        res_ref = None\n        if res:\n            res_ref = weakref.ref(res)\n\n        self.reqs.put((req, arg or tuple(), res_ref, stack))\n\n    def executemany(self, req, items):\n        for item in items:\n            self.execute(req, item)\n        self.check_raise_error()\n\n    def select(self, req, arg=None):\n        \"\"\"\n        Unlike sqlite's native select, this select doesn't handle iteration efficiently.\n\n        The result of `select` starts filling up with values as soon as the\n        request is dequeued, and although you can iterate over the result normally\n        (`for res in self.select(): ...`), the entire result will be in memory.\n        \"\"\"\n        res = Queue()  # results of the select will appear as items in this queue\n        self.execute(req, arg, res)\n        while True:\n            rec = res.get()\n            self.check_raise_error()\n            if rec == _RESPONSE_NO_MORE:\n                break\n            yield rec\n\n    def select_one(self, req, arg=None):\n        \"\"\"Return only the first row of the SELECT, or None if there are no matching rows.\"\"\"\n        try:\n            return next(iter(self.select(req, arg)))\n        except StopIteration:\n            return None\n\n    def commit(self, blocking=True):\n        if blocking:\n            # by default, we await completion of commit() unless\n            # blocking=False.  This ensures any available exceptions for any\n            # previous statement are thrown before returning, and that the\n            # data has actually persisted to disk!\n            self.select_one(_REQUEST_COMMIT)\n        else:\n            # otherwise, we fire and forget as usual.\n            self.execute(_REQUEST_COMMIT)\n\n    def close(self, force=False):\n        if force:\n            # If a SqliteDict is being killed or garbage-collected, then select_one()\n            # could hang forever because run() might already have exited and therefore\n            # can't process the request. Instead, push the close command to the requests\n            # queue directly. If run() is still alive, it will exit gracefully. If not,\n            # then there's nothing we can do anyway.\n            self.reqs.put((_REQUEST_CLOSE, None, weakref.ref(Queue()), None))\n        else:\n            # we abuse 'select' to \"iter\" over a \"--close--\" statement so that we\n            # can confirm the completion of close before joining the thread and\n            # returning (by semaphore '--no more--'\n            self.select_one(_REQUEST_CLOSE)\n            self.join()\n\n\n#\n# This is here for .github/workflows/release.yml\n#\nif __name__ == '__main__':\n    print(__version__)\n", "test_list": ["def test_overwrite_using_flag_w(self):\n    \"\"\"Re-opening of a database with flag='w' destroys only the target table.\"\"\"\n    fname = norm_file('tests/db/sqlitedict-override-test.sqlite')\n    orig_db_1 = SqliteDict(filename=fname, tablename='one')\n    orig_db_1['key'] = 'value'\n    orig_db_1.commit()\n    orig_db_1.close()\n    orig_db_2 = SqliteDict(filename=fname, tablename='two')\n    orig_db_2['key'] = 'value'\n    orig_db_2.commit()\n    orig_db_2.close()\n    next_db_1 = SqliteDict(filename=fname, tablename='one', flag='w')\n    self.assertNotIn('key', next_db_1.keys())\n    next_db_2 = SqliteDict(filename=fname, tablename='two')\n    self.assertIn('key', next_db_2.keys())", "def test_readonly(self):\n    fname = norm_file('tests/db/sqlitedict-override-test.sqlite')\n    orig_db = SqliteDict(filename=fname)\n    orig_db['key'] = 'value'\n    orig_db['key_two'] = 2\n    orig_db.commit()\n    orig_db.close()\n    readonly_db = SqliteDict(filename=fname, flag='r')\n    self.assertTrue(readonly_db['key'] == 'value')\n    self.assertTrue(readonly_db['key_two'] == 2)\n\n    def attempt_write():\n        readonly_db['key'] = ['new_value']\n\n    def attempt_update():\n        readonly_db.update(key='value2', key_two=2.1)\n\n    def attempt_delete():\n        del readonly_db['key']\n\n    def attempt_clear():\n        readonly_db.clear()\n\n    def attempt_terminate():\n        readonly_db.terminate()\n    attempt_funcs = [attempt_write, attempt_update, attempt_delete, attempt_clear, attempt_terminate]\n    for func in attempt_funcs:\n        with self.assertRaises(RuntimeError):\n            func()", "def test_default_reuse_existing_flag_c(self):\n    \"\"\"Re-opening of a database does not destroy it.\"\"\"\n    fname = norm_file('tests/db/sqlitedict-override-test.sqlite')\n    orig_db = SqliteDict(filename=fname)\n    orig_db['key'] = 'value'\n    orig_db.commit()\n    orig_db.close()\n    next_db = SqliteDict(filename=fname)\n    self.assertIn('key', next_db.keys())\n    self.assertEqual(next_db['key'], 'value')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'commit' function should ensure that the 'blocking' parameter is a boolean and defaults to True if not provided.", "unit_test": ["def test_commit_input_output_conditions():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.commit(blocking='not_a_boolean')\n    # Expecting a TypeError because 'blocking' should be a boolean\n    with self.assertRaises(TypeError):\n        db.commit(blocking='not_a_boolean')\n    db.close()"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_input_output_conditions"}, "Exception Handling": {"requirement": "The 'commit' function should raise RuntimeError if the database connection is closed before committing.", "unit_test": ["def test_commit_exception_handling():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.close()\n    with self.assertRaises(RuntimeError):\n        db.commit()"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_exception_handling"}, "Edge Case Handling": {"requirement": "The 'commit' function should handle the case where no changes have been made since the last commit gracefully without errors.", "unit_test": ["def test_commit_edge_case_handling():\n    db = SqliteDict(filename='test.db')\n    db.commit()  # No changes made\n    # Expecting no exception to be raised\n    try:\n        db.commit()\n    except Exception as e:\n        self.fail(f'Commit raised an exception unexpectedly: {e}')\n    db.close()"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'commit' function to return a boolean indicating whether the commit was successful.", "unit_test": ["def test_commit_functionality_extension():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    success = db.commit()\n    self.assertTrue(success)\n    db.close()"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'commit' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_commit_annotation_coverage():\n    import inspect\n    signature = inspect.signature(SqliteDict.commit)\n    self.assertEqual(signature.parameters['self'].annotation, SqliteDict)\n    self.assertEqual(signature.parameters['blocking'].annotation, bool)\n    self.assertEqual(signature.return_annotation, None)"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_annotation_coverage"}, "Code Complexity": {"requirement": "The 'commit' function should maintain a cyclomatic complexity of 2 to ensure simplicity.", "unit_test": ["def test_commit_code_complexity():\n    from radon.complexity import cc_visit\n    code = inspect.getsource(SqliteDict.commit)\n    complexity = cc_visit(code)\n    self.assertEqual(complexity[0].complexity, 2)"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_code_complexity"}, "Code Standard": {"requirement": "The 'commit' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_commit_code_standard():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_sqlitedict.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'commit' function should utilize the 'conn' attribute of the SqliteDict class to perform the commit operation.", "unit_test": ["def test_commit_context_usage_verification():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.commit()\n    # Verify that the commit operation uses the 'conn' attribute\n    self.assertIsNotNone(db.conn)\n    db.close()"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'commit' function should correctly use the 'conn' attribute to ensure data is persisted to disk.", "unit_test": ["def test_commit_context_usage_correctness_verification():\n    db = SqliteDict(filename='test.db')\n    db['key'] = 'value'\n    db.commit()\n    db.close()\n    db_reopen = SqliteDict(filename='test.db')\n    self.assertEqual(db_reopen['key'], 'value')\n    db_reopen.close()"], "test": "tests/test_core.py::NamedSqliteDictCreateOrReuseTest::test_commit_context_usage_correctness_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "type": "method", "project_path": "Database/alembic", "completion_path": "Database/alembic/alembic/operations/ops.py", "signature_position": [2202, 2207], "body_position": [2208, 2213], "dependency": {"intra_class": ["alembic.operations.ops.DropColumnOp.__init__"], "intra_file": ["alembic.operations.ops.AddColumnOp", "alembic.operations.ops.AddColumnOp.from_column_and_tablename"], "cross_file": []}, "requirement": {"Functionality": "This function creates an instance of the class based on the given parameters.", "Arguments": ":param cls: A class.\n:param schema: Optional string. The schema of the table.\n:param tname: String. The name of the table.\n:param col: Column. The column to be dropped.\n:return: The created instance."}, "tests": ["tests/test_autogen_render.py::AutogenRenderTest::test_render_drop_column_w_schema", "tests/test_autogen_render.py::AutogenRenderTest::test_render_drop_column", "tests/test_autogen_diffs.py::OrigObjectTest::test_drop_column"], "indent": 4, "domain": "Database", "code": "    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:\n        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )\n", "context": "from __future__ import annotations\n\nfrom abc import abstractmethod\nimport re\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import FrozenSet\nfrom typing import Iterator\nfrom typing import List\nfrom typing import MutableMapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom sqlalchemy.types import NULLTYPE\n\nfrom . import schemaobj\nfrom .base import BatchOperations\nfrom .base import Operations\nfrom .. import util\nfrom ..util import sqla_compat\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from sqlalchemy.sql import Executable\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.elements import conv\n    from sqlalchemy.sql.elements import quoted_name\n    from sqlalchemy.sql.elements import TextClause\n    from sqlalchemy.sql.functions import Function\n    from sqlalchemy.sql.schema import CheckConstraint\n    from sqlalchemy.sql.schema import Column\n    from sqlalchemy.sql.schema import Computed\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import ForeignKeyConstraint\n    from sqlalchemy.sql.schema import Identity\n    from sqlalchemy.sql.schema import Index\n    from sqlalchemy.sql.schema import MetaData\n    from sqlalchemy.sql.schema import PrimaryKeyConstraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.schema import Table\n    from sqlalchemy.sql.schema import UniqueConstraint\n    from sqlalchemy.sql.selectable import TableClause\n    from sqlalchemy.sql.type_api import TypeEngine\n\n    from ..autogenerate.rewriter import Rewriter\n    from ..runtime.migration import MigrationContext\n    from ..script.revision import _RevIdType\n\n\nclass MigrateOperation:\n    \"\"\"base class for migration command and organization objects.\n\n    This system is part of the operation extensibility API.\n\n    .. seealso::\n\n        :ref:`operation_objects`\n\n        :ref:`operation_plugins`\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    @util.memoized_property\n    def info(self):\n        \"\"\"A dictionary that may be used to store arbitrary information\n        along with this :class:`.MigrateOperation` object.\n\n        \"\"\"\n        return {}\n\n    _mutations: FrozenSet[Rewriter] = frozenset()\n\n    def reverse(self) -> MigrateOperation:\n        raise NotImplementedError\n\n    def to_diff_tuple(self) -> Tuple[Any, ...]:\n        raise NotImplementedError\n\n\nclass AddConstraintOp(MigrateOperation):\n    \"\"\"Represent an add constraint operation.\"\"\"\n\n    add_constraint_ops = util.Dispatcher()\n\n    @property\n    def constraint_type(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def register_add_constraint(cls, type_: str) -> Callable:\n        def go(klass):\n            cls.add_constraint_ops.dispatch_for(type_)(klass.from_constraint)\n            return klass\n\n        return go\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> AddConstraintOp:\n        return cls.add_constraint_ops.dispatch(constraint.__visit_name__)(\n            constraint\n        )\n\n    @abstractmethod\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Constraint:\n        pass\n\n    def reverse(self) -> DropConstraintOp:\n        return DropConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(self) -> Tuple[str, Constraint]:\n        return (\"add_constraint\", self.to_constraint())\n\n\n@Operations.register_operation(\"drop_constraint\")\n@BatchOperations.register_operation(\"drop_constraint\", \"batch_drop_constraint\")\nclass DropConstraintOp(MigrateOperation):\n    \"\"\"Represent a drop constraint operation.\"\"\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddConstraintOp] = None,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.constraint_type = type_\n        self.schema = schema\n        self._reverse = _reverse\n\n    def reverse(self) -> AddConstraintOp:\n        return AddConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, SchemaItem]:\n        if self.constraint_type == \"foreignkey\":\n            return (\"remove_fk\", self.to_constraint())\n        else:\n            return (\"remove_constraint\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:\n        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n\n    def to_constraint(self) -> Constraint:\n        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )\n\n    @classmethod\n    def drop_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: str,\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        r\"\"\"Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param constraint_name: name of the constraint.\n        :param table_name: table name.\n        :param type\\_: optional, required on MySQL.  can be\n         'foreignkey', 'primary', 'unique', or 'check'.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, type_=type_, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        type_: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            type_=type_,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_primary_key\")\n@BatchOperations.register_operation(\n    \"create_primary_key\", \"batch_create_primary_key\"\n)\n@AddConstraintOp.register_add_constraint(\"primary_key_constraint\")\nclass CreatePrimaryKeyOp(AddConstraintOp):\n    \"\"\"Represent a create primary key operation.\"\"\"\n\n    constraint_type = \"primarykey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreatePrimaryKeyOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        pk_constraint = cast(\"PrimaryKeyConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(pk_constraint.name),\n            constraint_table.name,\n            pk_constraint.columns.keys(),\n            schema=constraint_table.schema,\n            **pk_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_primary_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: List[str],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_primary_key(\"pk_my_table\", \"my_table\", [\"id\", \"version\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the primary key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param columns: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, columns, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_primary_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: List[str],\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_primary_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_unique_constraint\")\n@BatchOperations.register_operation(\n    \"create_unique_constraint\", \"batch_create_unique_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"unique_constraint\")\nclass CreateUniqueConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create unique constraint operation.\"\"\"\n\n    constraint_type = \"unique\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateUniqueConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        uq_constraint = cast(\"UniqueConstraint\", constraint)\n\n        kw: dict = {}\n        if uq_constraint.deferrable:\n            kw[\"deferrable\"] = uq_constraint.deferrable\n        if uq_constraint.initially:\n            kw[\"initially\"] = uq_constraint.initially\n        kw.update(uq_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(uq_constraint.name),\n            constraint_table.name,\n            [c.name for c in uq_constraint.columns],\n            schema=constraint_table.schema,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> UniqueConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.unique_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_unique_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param columns: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, columns, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_unique_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: Sequence[str],\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_unique_constraint`\n\n        \"\"\"\n        kw[\"schema\"] = operations.impl.schema\n        op = cls(constraint_name, operations.impl.table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_foreign_key\")\n@BatchOperations.register_operation(\n    \"create_foreign_key\", \"batch_create_foreign_key\"\n)\n@AddConstraintOp.register_add_constraint(\"foreign_key_constraint\")\nclass CreateForeignKeyOp(AddConstraintOp):\n    \"\"\"Represent a create foreign key constraint operation.\"\"\"\n\n    constraint_type = \"foreignkey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.source_table = source_table\n        self.referent_table = referent_table\n        self.local_cols = local_cols\n        self.remote_cols = remote_cols\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, ForeignKeyConstraint]:\n        return (\"add_fk\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreateForeignKeyOp:\n        fk_constraint = cast(\"ForeignKeyConstraint\", constraint)\n        kw: dict = {}\n        if fk_constraint.onupdate:\n            kw[\"onupdate\"] = fk_constraint.onupdate\n        if fk_constraint.ondelete:\n            kw[\"ondelete\"] = fk_constraint.ondelete\n        if fk_constraint.initially:\n            kw[\"initially\"] = fk_constraint.initially\n        if fk_constraint.deferrable:\n            kw[\"deferrable\"] = fk_constraint.deferrable\n        if fk_constraint.use_alter:\n            kw[\"use_alter\"] = fk_constraint.use_alter\n        if fk_constraint.match:\n            kw[\"match\"] = fk_constraint.match\n\n        (\n            source_schema,\n            source_table,\n            source_columns,\n            target_schema,\n            target_table,\n            target_columns,\n            onupdate,\n            ondelete,\n            deferrable,\n            initially,\n        ) = sqla_compat._fk_spec(fk_constraint)\n\n        kw[\"source_schema\"] = source_schema\n        kw[\"referent_schema\"] = target_schema\n        kw.update(fk_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(fk_constraint.name),\n            source_table,\n            target_table,\n            source_columns,\n            target_columns,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> ForeignKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.foreign_key_constraint(\n            self.constraint_name,\n            self.source_table,\n            self.referent_table,\n            self.local_cols,\n            self.remote_cols,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_foreign_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        source_schema: Optional[str] = None,\n        referent_schema: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_foreign_key(\n                \"fk_user_address\",\n                \"address\",\n                \"user\",\n                [\"user_id\"],\n                [\"id\"],\n            )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the foreign key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source_table: String name of the source table.\n        :param referent_table: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        op = cls(\n            constraint_name,\n            source_table,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=source_schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_foreign_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        referent_schema: Optional[str] = None,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``source_schema``\n        arguments from the call.\n\n        e.g.::\n\n            with batch_alter_table(\"address\") as batch_op:\n                batch_op.create_foreign_key(\n                    \"fk_user_address\",\n                    \"user\",\n                    [\"user_id\"],\n                    [\"id\"],\n                )\n\n        .. seealso::\n\n            :meth:`.Operations.create_foreign_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=operations.impl.schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_check_constraint\")\n@BatchOperations.register_operation(\n    \"create_check_constraint\", \"batch_create_check_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"check_constraint\")\n@AddConstraintOp.register_add_constraint(\"table_or_column_check_constraint\")\n@AddConstraintOp.register_add_constraint(\"column_check_constraint\")\nclass CreateCheckConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create check constraint operation.\"\"\"\n\n    constraint_type = \"check\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        condition: Union[str, TextClause, ColumnElement[Any]],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.condition = condition\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateCheckConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        ck_constraint = cast(\"CheckConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(ck_constraint.name),\n            constraint_table.name,\n            cast(\"ColumnElement[Any]\", ck_constraint.sqltext),\n            schema=constraint_table.schema,\n            **ck_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> CheckConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.check_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.condition,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_check_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column(\"name\")) > 5,\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param condition: SQL expression that's the condition of the\n         constraint. Can be a string or SQLAlchemy expression language\n         structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, condition, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_check_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_check_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            condition,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_index\")\n@BatchOperations.register_operation(\"create_index\", \"batch_create_index\")\nclass CreateIndexOp(MigrateOperation):\n    \"\"\"Represent a create index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.unique = unique\n        self.if_not_exists = if_not_exists\n        self.kw = kw\n\n    def reverse(self) -> DropIndexOp:\n        return DropIndexOp.from_index(self.to_index())\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"add_index\", self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> CreateIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n\n    @classmethod\n    def create_index(\n        cls,\n        operations: Operations,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, Function[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_index(\"ik_test\", \"t1\", [\"foo\", \"bar\"])\n\n        Functional indexes can be produced by using the\n        :func:`sqlalchemy.sql.expression.text` construct::\n\n            from alembic import op\n            from sqlalchemy import text\n\n            op.create_index(\"ik_test\", \"t1\", [text(\"lower(foo)\")])\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.\n        :param columns: a list consisting of string column names and/or\n         :func:`~sqlalchemy.sql.expression.text` constructs.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param unique: If True, create a unique index.\n\n        :param quote: Force quoting of this column's name on or off,\n         corresponding to ``True`` or ``False``. When left at its default\n         of ``None``, the column identifier will be quoted according to\n         whether the name is case sensitive (identifiers with at least one\n         upper case character are treated as case sensitive), or if it's a\n         reserved word. This flag is only needed to force quoting of a\n         reserved word which is not known by the SQLAlchemy dialect.\n\n        :param if_not_exists: If True, adds IF NOT EXISTS operator when\n         creating the new index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_index(\n        cls,\n        operations: BatchOperations,\n        index_name: str,\n        columns: List[str],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.create_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_index\")\n@BatchOperations.register_operation(\"drop_index\", \"batch_drop_index\")\nclass DropIndexOp(MigrateOperation):\n    \"\"\"Represent a drop index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Union[quoted_name, str, conv],\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        _reverse: Optional[CreateIndexOp] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.schema = schema\n        self.if_exists = if_exists\n        self._reverse = _reverse\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"remove_index\", self.to_index())\n\n    def reverse(self) -> CreateIndexOp:\n        return CreateIndexOp.from_index(self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> DropIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def drop_index(\n        cls,\n        operations: Operations,\n        index_name: str,\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        :param if_exists: If True, adds IF EXISTS operator when\n         dropping the index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_index(\n        cls, operations: BatchOperations, index_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            table_name=operations.impl.table_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table\")\nclass CreateTableOp(MigrateOperation):\n    \"\"\"Represent a create table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        columns: Sequence[SchemaItem],\n        *,\n        schema: Optional[str] = None,\n        _namespace_metadata: Optional[MetaData] = None,\n        _constraints_included: bool = False,\n        **kw: Any,\n    ) -> None:\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.info = kw.pop(\"info\", {})\n        self.comment = kw.pop(\"comment\", None)\n        self.prefixes = kw.pop(\"prefixes\", None)\n        self.kw = kw\n        self._namespace_metadata = _namespace_metadata\n        self._constraints_included = _constraints_included\n\n    def reverse(self) -> DropTableOp:\n        return DropTableOp.from_table(\n            self.to_table(), _namespace_metadata=self._namespace_metadata\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"add_table\", self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:\n        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=self._constraints_included,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *columns: SchemaItem,\n        **kw: Any,\n    ) -> Table:\n        r\"\"\"Issue a \"create table\" instruction using the current migration\n        context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        Note that :meth:`.create_table` accepts\n        :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        The function also returns a newly created\n        :class:`~sqlalchemy.schema.Table` object, corresponding to the table\n        specification given, which is suitable for\n        immediate SQL operations, in particular\n        :meth:`.Operations.bulk_insert`::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            account_table = op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n            op.bulk_insert(\n                account_table,\n                [\n                    {\"name\": \"A1\", \"description\": \"account 1\"},\n                    {\"name\": \"A2\", \"description\": \"account 2\"},\n                ],\n            )\n\n        :param table_name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        :return: the :class:`~sqlalchemy.schema.Table` object corresponding\n         to the parameters given.\n\n        \"\"\"\n        op = cls(table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_table\")\nclass DropTableOp(MigrateOperation):\n    \"\"\"Represent a drop table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        table_kw: Optional[MutableMapping[Any, Any]] = None,\n        _reverse: Optional[CreateTableOp] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n        self.table_kw = table_kw or {}\n        self.comment = self.table_kw.pop(\"comment\", None)\n        self.info = self.table_kw.pop(\"info\", None)\n        self.prefixes = self.table_kw.pop(\"prefixes\", None)\n        self._reverse = _reverse\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"remove_table\", self.to_table())\n\n    def reverse(self) -> CreateTableOp:\n        return CreateTableOp.from_table(self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n\n    @classmethod\n    def drop_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param table_name: Name of the table\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        op = cls(table_name, schema=schema, table_kw=kw)\n        operations.invoke(op)\n\n\nclass AlterTableOp(MigrateOperation):\n    \"\"\"Represent an alter table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n\n\n@Operations.register_operation(\"rename_table\")\nclass RenameTableOp(AlterTableOp):\n    \"\"\"Represent a rename table operation.\"\"\"\n\n    def __init__(\n        self,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(old_table_name, schema=schema)\n        self.new_table_name = new_table_name\n\n    @classmethod\n    def rename_table(\n        cls,\n        operations: Operations,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(old_table_name, new_table_name, schema=schema)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table_comment\")\n@BatchOperations.register_operation(\n    \"create_table_comment\", \"batch_create_table_comment\"\n)\nclass CreateTableCommentOp(AlterTableOp):\n    \"\"\"Represent a COMMENT ON `table` operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.comment = comment\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def create_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table.\n\n        :param table_name: string name of the target table.\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(\n            table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=schema,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_table_comment(\n        cls,\n        operations: BatchOperations,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table\n        using the current batch migration context.\n\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        if self.existing_comment is None:\n            return DropTableCommentOp(\n                self.table_name,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n        else:\n            return CreateTableCommentOp(\n                self.table_name,\n                self.existing_comment,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name, schema=self.schema, comment=self.comment\n        )\n\n    def to_diff_tuple(self):\n        return (\"add_table_comment\", self.to_table(), self.existing_comment)\n\n\n@Operations.register_operation(\"drop_table_comment\")\n@BatchOperations.register_operation(\n    \"drop_table_comment\", \"batch_drop_table_comment\"\n)\nclass DropTableCommentOp(AlterTableOp):\n    \"\"\"Represent an operation to remove the comment from a table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def drop_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table.\n\n        :param table_name: string name of the target table.\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        .. seealso::\n\n            :meth:`.Operations.create_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(table_name, existing_comment=existing_comment, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_table_comment(\n        cls,\n        operations: BatchOperations,\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table using the current\n        batch operations context.\n\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        return CreateTableCommentOp(\n            self.table_name, self.existing_comment, schema=self.schema\n        )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(self.table_name, schema=self.schema)\n\n    def to_diff_tuple(self):\n        return (\"remove_table_comment\", self.to_table())\n\n\n@Operations.register_operation(\"alter_column\")\n@BatchOperations.register_operation(\"alter_column\", \"batch_alter_column\")\nclass AlterColumnOp(AlterTableOp):\n    \"\"\"Represent an alter column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_type: Optional[Any] = None,\n        existing_server_default: Any = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        modify_nullable: Optional[bool] = None,\n        modify_comment: Optional[Union[str, Literal[False]]] = False,\n        modify_server_default: Any = False,\n        modify_name: Optional[str] = None,\n        modify_type: Optional[Any] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.existing_type = existing_type\n        self.existing_server_default = existing_server_default\n        self.existing_nullable = existing_nullable\n        self.existing_comment = existing_comment\n        self.modify_nullable = modify_nullable\n        self.modify_comment = modify_comment\n        self.modify_server_default = modify_server_default\n        self.modify_name = modify_name\n        self.modify_type = modify_type\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Any:\n        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n\n    def has_changes(self) -> bool:\n        hc1 = (\n            self.modify_nullable is not None\n            or self.modify_server_default is not False\n            or self.modify_type is not None\n            or self.modify_comment is not False\n        )\n        if hc1:\n            return True\n        for kw in self.kw:\n            if kw.startswith(\"modify_\"):\n                return True\n        else:\n            return False\n\n    def reverse(self) -> AlterColumnOp:\n        kw = self.kw.copy()\n        kw[\"existing_type\"] = self.existing_type\n        kw[\"existing_nullable\"] = self.existing_nullable\n        kw[\"existing_server_default\"] = self.existing_server_default\n        kw[\"existing_comment\"] = self.existing_comment\n        if self.modify_type is not None:\n            kw[\"modify_type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            kw[\"modify_nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            kw[\"modify_server_default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            kw[\"modify_comment\"] = self.modify_comment\n\n        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return self.__class__(\n            self.table_name, self.column_name, schema=self.schema, **kw\n        )\n\n    @classmethod\n    def alter_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_type`` argument is\n        used in this case to identify and remove a previous\n        constraint that was bound to the type object.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param comment: optional string text of a new comment to add to the\n         column.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type\\_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLAlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param existing_comment: string text of the existing comment on the\n         column to be maintained.  Required on MySQL if the existing comment\n         on the column is not being changed.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param postgresql_using: String argument which will indicate a\n         SQL expression to render within the Postgresql-specific USING clause\n         within ALTER COLUMN.    This string is taken directly as raw SQL which\n         must explicitly include any necessary quoting or escaping of tokens\n         within the expression.\n\n        \"\"\"\n\n        alt = cls(\n            table_name,\n            column_name,\n            schema=schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n    @classmethod\n    def batch_alter_column(\n        cls,\n        operations: BatchOperations,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue an \"alter column\" instruction using the current\n        batch migration context.\n\n        Parameters are the same as that of :meth:`.Operations.alter_column`,\n        as well as the following option(s):\n\n        :param insert_before: String name of an existing column which this\n         column should be placed before, when creating the new table.\n\n        :param insert_after: String name of an existing column which this\n         column should be placed after, when creating the new table.  If\n         both :paramref:`.BatchOperations.alter_column.insert_before`\n         and :paramref:`.BatchOperations.alter_column.insert_after` are\n         omitted, the column is inserted after the last existing column\n         in the table.\n\n        .. seealso::\n\n            :meth:`.Operations.alter_column`\n\n\n        \"\"\"\n        alt = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            insert_before=insert_before,\n            insert_after=insert_after,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n\n@Operations.register_operation(\"add_column\")\n@BatchOperations.register_operation(\"add_column\", \"batch_add_column\")\nclass AddColumnOp(AlterTableOp):\n    \"\"\"Represent an add column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column = column\n        self.kw = kw\n\n    def reverse(self) -> DropColumnOp:\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\"add_column\", self.schema, self.table_name, self.column)\n\n    def to_column(self) -> Column:\n        return self.column\n\n    @classmethod\n    def from_column(cls, col: Column) -> AddColumnOp:\n        return cls(col.table.name, col, schema=col.table.schema)\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> AddColumnOp:\n        return cls(tname, col, schema=schema)\n\n    @classmethod\n    def add_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\"organization\", Column(\"name\", String()))\n\n        The :meth:`.Operations.add_column` method typically corresponds\n        to the SQL command \"ALTER TABLE... ADD COLUMN\".    Within the scope\n        of this command, the column's name, datatype, nullability,\n        and optional server-generated defaults may be indicated.\n\n        .. note::\n\n            With the exception of NOT NULL constraints or single-column FOREIGN\n            KEY constraints, other kinds of constraints such as PRIMARY KEY,\n            UNIQUE or CHECK constraints **cannot** be generated using this\n            method; for these constraints, refer to operations such as\n            :meth:`.Operations.create_primary_key` and\n            :meth:`.Operations.create_check_constraint`. In particular, the\n            following :class:`~sqlalchemy.schema.Column` parameters are\n            **ignored**:\n\n            * :paramref:`~sqlalchemy.schema.Column.primary_key` - SQL databases\n              typically do not support an ALTER operation that can add\n              individual columns one at a time to an existing primary key\n              constraint, therefore it's less ambiguous to use the\n              :meth:`.Operations.create_primary_key` method, which assumes no\n              existing primary key constraint is present.\n            * :paramref:`~sqlalchemy.schema.Column.unique` - use the\n              :meth:`.Operations.create_unique_constraint` method\n            * :paramref:`~sqlalchemy.schema.Column.index` - use the\n              :meth:`.Operations.create_index` method\n\n\n        The provided :class:`~sqlalchemy.schema.Column` object may include a\n        :class:`~sqlalchemy.schema.ForeignKey` constraint directive,\n        referencing a remote table name. For this specific type of constraint,\n        Alembic will automatically emit a second ALTER statement in order to\n        add the single-column FOREIGN KEY constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\n                \"organization\",\n                Column(\"account_id\", INTEGER, ForeignKey(\"accounts.id\")),\n            )\n\n        The column argument passed to :meth:`.Operations.add_column` is a\n        :class:`~sqlalchemy.schema.Column` construct, used in the same way it's\n        used in SQLAlchemy. In particular, values or functions to be indicated\n        as producing the column's default value on the database side are\n        specified using the ``server_default`` parameter, and not ``default``\n        which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column(\n                \"account\",\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(table_name, column, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_add_column(\n        cls,\n        operations: BatchOperations,\n        column: Column[Any],\n        *,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.add_column`\n\n        \"\"\"\n\n        kw = {}\n        if insert_before:\n            kw[\"insert_before\"] = insert_before\n        if insert_after:\n            kw[\"insert_after\"] = insert_after\n\n        op = cls(\n            operations.impl.table_name,\n            column,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_column\")\n@BatchOperations.register_operation(\"drop_column\", \"batch_drop_column\")\nclass DropColumnOp(AlterTableOp):\n    \"\"\"Represent a drop column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddColumnOp] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.kw = kw\n        self._reverse = _reverse\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\n            \"remove_column\",\n            self.schema,\n            self.table_name,\n            self.to_column(),\n        )\n\n    def reverse(self) -> AddColumnOp:\n        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n\n    @classmethod\n###The function: from_column_and_tablename###\n    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:\n        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n\n    @classmethod\n    def drop_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\"organization\", \"account_id\")\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec's a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n        \"\"\"\n\n        op = cls(table_name, column_name, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_column(\n        cls, operations: BatchOperations, column_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_column`\n\n        \"\"\"\n        op = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"bulk_insert\")\nclass BulkInsertOp(MigrateOperation):\n    \"\"\"Represent a bulk insert operation.\"\"\"\n\n    def __init__(\n        self,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        self.table = table\n        self.rows = rows\n        self.multiinsert = multiinsert\n\n    @classmethod\n    def bulk_insert(\n        cls,\n        operations: Operations,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        \"\"\"Issue a \"bulk insert\" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\n                \"account\",\n                column(\"id\", Integer),\n                column(\"name\", String),\n                column(\"create_date\", Date),\n            )\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": date(2010, 10, 5),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": date(2007, 5, 27),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": date(2008, 8, 15),\n                    },\n                ],\n            )\n\n        When using --sql mode, some datatypes may not render inline\n        automatically, such as dates and other special types.   When this\n        issue is present, :meth:`.Operations.inline_literal` may be used::\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": op.inline_literal(\"2010-10-05\"),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": op.inline_literal(\"2007-05-27\"),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": op.inline_literal(\"2008-08-15\"),\n                    },\n                ],\n                multiinsert=False,\n            )\n\n        When using :meth:`.Operations.inline_literal` in conjunction with\n        :meth:`.Operations.bulk_insert`, in order for the statement to work\n        in \"online\" (e.g. non --sql) mode, the\n        :paramref:`~.Operations.bulk_insert.multiinsert`\n        flag should be set to ``False``, which will have the effect of\n        individual INSERT statements being emitted to the database, each\n        with a distinct VALUES clause, so that the \"inline\" values can\n        still be rendered, rather than attempting to pass the values\n        as bound parameters.\n\n        :param table: a table object which represents the target of the INSERT.\n\n        :param rows: a list of dictionaries indicating rows.\n\n        :param multiinsert: when at its default of True and --sql mode is not\n           enabled, the INSERT statement will be executed using\n           \"executemany()\" style, where all elements in the list of\n           dictionaries are passed as bound parameters in a single\n           list.   Setting this to False results in individual INSERT\n           statements being emitted per parameter set, and is needed\n           in those cases where non-literal values are present in the\n           parameter sets.\n\n        \"\"\"\n\n        op = cls(table, rows, multiinsert=multiinsert)\n        operations.invoke(op)\n\n\n@Operations.register_operation(\"execute\")\n@BatchOperations.register_operation(\"execute\", \"batch_execute\")\nclass ExecuteSQLOp(MigrateOperation):\n    \"\"\"Represent an execute SQL operation.\"\"\"\n\n    def __init__(\n        self,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.sqltext = sqltext\n        self.execution_options = execution_options\n\n    @classmethod\n    def execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        r\"\"\"Execute the given SQL using the current migration context.\n\n        The given SQL can be a plain string, e.g.::\n\n            op.execute(\"INSERT INTO table (foo) VALUES ('some value')\")\n\n        Or it can be any kind of Core SQL Expression construct, such as\n        below where we use an update construct::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\"account\", column(\"name\", String))\n            op.execute(\n                account.update()\n                .where(account.c.name == op.inline_literal(\"account 1\"))\n                .values({\"name\": op.inline_literal(\"account 2\")})\n            )\n\n        Above, we made use of the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table` and\n        :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it's a recommended practice to at least\n        ensure the definition of a table is self-contained within the migration\n        script, rather than imported from a module that may break compatibility\n        with older migrations.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in \"offline\" mode.     Additionally, parameterized\n        statements are discouraged here, as they *will not work* in offline\n        mode.  Above, we use :meth:`.inline_literal` where parameters are\n        to be used.\n\n        For full interaction with a connected database where parameters can\n        also be used normally, use the \"bind\" available from the context::\n\n            from alembic import op\n\n            connection = op.get_bind()\n\n            connection.execute(\n                account.update()\n                .where(account.c.name == \"account 1\")\n                .values({\"name\": \"account 2\"})\n            )\n\n        Additionally, when passing the statement as a plain string, it is first\n        coerced into a :func:`sqlalchemy.sql.expression.text` construct\n        before being passed along.  In the less likely case that the\n        literal SQL string contains a colon, it must be escaped with a\n        backslash, as::\n\n           op.execute(r\"INSERT INTO table (foo) VALUES ('\\:colon_value')\")\n\n\n        :param sqltext: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update` construct.\n        * a :func:`sqlalchemy.sql.expression.delete` construct.\n        * Any \"executable\" described in SQLAlchemy Core documentation,\n          noting that no result set is returned.\n\n        .. note::  when passing a plain string, the statement is coerced into\n           a :func:`sqlalchemy.sql.expression.text` construct. This construct\n           considers symbols with colons, e.g. ``:foo`` to be bound parameters.\n           To avoid this, ensure that colon symbols are escaped, e.g.\n           ``\\:foo``.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        \"\"\"\n        op = cls(sqltext, execution_options=execution_options)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Execute the given SQL using the current migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.execute`\n\n        \"\"\"\n        return cls.execute(\n            operations, sqltext, execution_options=execution_options\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Union[Executable, str]]:\n        return (\"execute\", self.sqltext)\n\n\nclass OpContainer(MigrateOperation):\n    \"\"\"Represent a sequence of operations operation.\"\"\"\n\n    def __init__(self, ops: Sequence[MigrateOperation] = ()) -> None:\n        self.ops = list(ops)\n\n    def is_empty(self) -> bool:\n        return not self.ops\n\n    def as_diffs(self) -> Any:\n        return list(OpContainer._ops_as_diffs(self))\n\n    @classmethod\n    def _ops_as_diffs(\n        cls, migrations: OpContainer\n    ) -> Iterator[Tuple[Any, ...]]:\n        for op in migrations.ops:\n            if hasattr(op, \"ops\"):\n                yield from cls._ops_as_diffs(cast(\"OpContainer\", op))\n            else:\n                yield op.to_diff_tuple()\n\n\nclass ModifyTableOps(OpContainer):\n    \"\"\"Contains a sequence of operations that all apply to a single Table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        ops: Sequence[MigrateOperation],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(ops)\n        self.table_name = table_name\n        self.schema = schema\n\n    def reverse(self) -> ModifyTableOps:\n        return ModifyTableOps(\n            self.table_name,\n            ops=list(reversed([op.reverse() for op in self.ops])),\n            schema=self.schema,\n        )\n\n\nclass UpgradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'upgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        upgrade_token: str = \"upgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.upgrade_token = upgrade_token\n\n    def reverse_into(self, downgrade_ops: DowngradeOps) -> DowngradeOps:\n        downgrade_ops.ops[:] = list(  # type:ignore[index]\n            reversed([op.reverse() for op in self.ops])\n        )\n        return downgrade_ops\n\n    def reverse(self) -> DowngradeOps:\n        return self.reverse_into(DowngradeOps(ops=[]))\n\n\nclass DowngradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'downgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        downgrade_token: str = \"downgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.downgrade_token = downgrade_token\n\n    def reverse(self):\n        return UpgradeOps(\n            ops=list(reversed([op.reverse() for op in self.ops]))\n        )\n\n\nclass MigrationScript(MigrateOperation):\n    \"\"\"represents a migration script.\n\n    E.g. when autogenerate encounters this object, this corresponds to the\n    production of an actual script file.\n\n    A normal :class:`.MigrationScript` object would contain a single\n    :class:`.UpgradeOps` and a single :class:`.DowngradeOps` directive.\n    These are accessible via the ``.upgrade_ops`` and ``.downgrade_ops``\n    attributes.\n\n    In the case of an autogenerate operation that runs multiple times,\n    such as the multiple database example in the \"multidb\" template,\n    the ``.upgrade_ops`` and ``.downgrade_ops`` attributes are disabled,\n    and instead these objects should be accessed via the ``.upgrade_ops_list``\n    and ``.downgrade_ops_list`` list-based attributes.  These latter\n    attributes are always available at the very least as single-element lists.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    _needs_render: Optional[bool]\n\n    def __init__(\n        self,\n        rev_id: Optional[str],\n        upgrade_ops: UpgradeOps,\n        downgrade_ops: DowngradeOps,\n        *,\n        message: Optional[str] = None,\n        imports: Set[str] = set(),\n        head: Optional[str] = None,\n        splice: Optional[bool] = None,\n        branch_label: Optional[_RevIdType] = None,\n        version_path: Optional[str] = None,\n        depends_on: Optional[_RevIdType] = None,\n    ) -> None:\n        self.rev_id = rev_id\n        self.message = message\n        self.imports = imports\n        self.head = head\n        self.splice = splice\n        self.branch_label = branch_label\n        self.version_path = version_path\n        self.depends_on = depends_on\n        self.upgrade_ops = upgrade_ops\n        self.downgrade_ops = downgrade_ops\n\n    @property\n    def upgrade_ops(self):\n        \"\"\"An instance of :class:`.UpgradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.upgrade_ops_list`\n        \"\"\"\n        if len(self._upgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for UpgradeOps; please use the \"\n                \"upgrade_ops_list attribute.\"\n            )\n        elif not self._upgrade_ops:\n            return None\n        else:\n            return self._upgrade_ops[0]\n\n    @upgrade_ops.setter\n    def upgrade_ops(self, upgrade_ops):\n        self._upgrade_ops = util.to_list(upgrade_ops)\n        for elem in self._upgrade_ops:\n            assert isinstance(elem, UpgradeOps)\n\n    @property\n    def downgrade_ops(self):\n        \"\"\"An instance of :class:`.DowngradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.downgrade_ops_list`\n        \"\"\"\n        if len(self._downgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for DowngradeOps; please use the \"\n                \"downgrade_ops_list attribute.\"\n            )\n        elif not self._downgrade_ops:\n            return None\n        else:\n            return self._downgrade_ops[0]\n\n    @downgrade_ops.setter\n    def downgrade_ops(self, downgrade_ops):\n        self._downgrade_ops = util.to_list(downgrade_ops)\n        for elem in self._downgrade_ops:\n            assert isinstance(elem, DowngradeOps)\n\n    @property\n    def upgrade_ops_list(self) -> List[UpgradeOps]:\n        \"\"\"A list of :class:`.UpgradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.upgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._upgrade_ops\n\n    @property\n    def downgrade_ops_list(self) -> List[DowngradeOps]:\n        \"\"\"A list of :class:`.DowngradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.downgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._downgrade_ops\n", "prompt": "Please write a python function called 'from_column_and_tablename' base the context. This function creates an instance of the class based on the given parameters.:param cls: A class.\n:param schema: Optional string. The schema of the table.\n:param tname: String. The name of the table.\n:param col: Column. The column to be dropped.\n:return: The created instance..\n        The context you need to refer to is as follows: from __future__ import annotations\n\nfrom abc import abstractmethod\nimport re\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import FrozenSet\nfrom typing import Iterator\nfrom typing import List\nfrom typing import MutableMapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom sqlalchemy.types import NULLTYPE\n\nfrom . import schemaobj\nfrom .base import BatchOperations\nfrom .base import Operations\nfrom .. import util\nfrom ..util import sqla_compat\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from sqlalchemy.sql import Executable\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.elements import conv\n    from sqlalchemy.sql.elements import quoted_name\n    from sqlalchemy.sql.elements import TextClause\n    from sqlalchemy.sql.functions import Function\n    from sqlalchemy.sql.schema import CheckConstraint\n    from sqlalchemy.sql.schema import Column\n    from sqlalchemy.sql.schema import Computed\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import ForeignKeyConstraint\n    from sqlalchemy.sql.schema import Identity\n    from sqlalchemy.sql.schema import Index\n    from sqlalchemy.sql.schema import MetaData\n    from sqlalchemy.sql.schema import PrimaryKeyConstraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.schema import Table\n    from sqlalchemy.sql.schema import UniqueConstraint\n    from sqlalchemy.sql.selectable import TableClause\n    from sqlalchemy.sql.type_api import TypeEngine\n\n    from ..autogenerate.rewriter import Rewriter\n    from ..runtime.migration import MigrationContext\n    from ..script.revision import _RevIdType\n\n\nclass MigrateOperation:\n    \"\"\"base class for migration command and organization objects.\n\n    This system is part of the operation extensibility API.\n\n    .. seealso::\n\n        :ref:`operation_objects`\n\n        :ref:`operation_plugins`\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    @util.memoized_property\n    def info(self):\n        \"\"\"A dictionary that may be used to store arbitrary information\n        along with this :class:`.MigrateOperation` object.\n\n        \"\"\"\n        return {}\n\n    _mutations: FrozenSet[Rewriter] = frozenset()\n\n    def reverse(self) -> MigrateOperation:\n        raise NotImplementedError\n\n    def to_diff_tuple(self) -> Tuple[Any, ...]:\n        raise NotImplementedError\n\n\nclass AddConstraintOp(MigrateOperation):\n    \"\"\"Represent an add constraint operation.\"\"\"\n\n    add_constraint_ops = util.Dispatcher()\n\n    @property\n    def constraint_type(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def register_add_constraint(cls, type_: str) -> Callable:\n        def go(klass):\n            cls.add_constraint_ops.dispatch_for(type_)(klass.from_constraint)\n            return klass\n\n        return go\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> AddConstraintOp:\n        return cls.add_constraint_ops.dispatch(constraint.__visit_name__)(\n            constraint\n        )\n\n    @abstractmethod\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Constraint:\n        pass\n\n    def reverse(self) -> DropConstraintOp:\n        return DropConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(self) -> Tuple[str, Constraint]:\n        return (\"add_constraint\", self.to_constraint())\n\n\n@Operations.register_operation(\"drop_constraint\")\n@BatchOperations.register_operation(\"drop_constraint\", \"batch_drop_constraint\")\nclass DropConstraintOp(MigrateOperation):\n    \"\"\"Represent a drop constraint operation.\"\"\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddConstraintOp] = None,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.constraint_type = type_\n        self.schema = schema\n        self._reverse = _reverse\n\n    def reverse(self) -> AddConstraintOp:\n        return AddConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, SchemaItem]:\n        if self.constraint_type == \"foreignkey\":\n            return (\"remove_fk\", self.to_constraint())\n        else:\n            return (\"remove_constraint\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:\n        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n\n    def to_constraint(self) -> Constraint:\n        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )\n\n    @classmethod\n    def drop_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: str,\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        r\"\"\"Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param constraint_name: name of the constraint.\n        :param table_name: table name.\n        :param type\\_: optional, required on MySQL.  can be\n         'foreignkey', 'primary', 'unique', or 'check'.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, type_=type_, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        type_: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            type_=type_,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_primary_key\")\n@BatchOperations.register_operation(\n    \"create_primary_key\", \"batch_create_primary_key\"\n)\n@AddConstraintOp.register_add_constraint(\"primary_key_constraint\")\nclass CreatePrimaryKeyOp(AddConstraintOp):\n    \"\"\"Represent a create primary key operation.\"\"\"\n\n    constraint_type = \"primarykey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreatePrimaryKeyOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        pk_constraint = cast(\"PrimaryKeyConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(pk_constraint.name),\n            constraint_table.name,\n            pk_constraint.columns.keys(),\n            schema=constraint_table.schema,\n            **pk_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_primary_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: List[str],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_primary_key(\"pk_my_table\", \"my_table\", [\"id\", \"version\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the primary key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param columns: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, columns, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_primary_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: List[str],\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_primary_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_unique_constraint\")\n@BatchOperations.register_operation(\n    \"create_unique_constraint\", \"batch_create_unique_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"unique_constraint\")\nclass CreateUniqueConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create unique constraint operation.\"\"\"\n\n    constraint_type = \"unique\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateUniqueConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        uq_constraint = cast(\"UniqueConstraint\", constraint)\n\n        kw: dict = {}\n        if uq_constraint.deferrable:\n            kw[\"deferrable\"] = uq_constraint.deferrable\n        if uq_constraint.initially:\n            kw[\"initially\"] = uq_constraint.initially\n        kw.update(uq_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(uq_constraint.name),\n            constraint_table.name,\n            [c.name for c in uq_constraint.columns],\n            schema=constraint_table.schema,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> UniqueConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.unique_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_unique_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param columns: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, columns, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_unique_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: Sequence[str],\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_unique_constraint`\n\n        \"\"\"\n        kw[\"schema\"] = operations.impl.schema\n        op = cls(constraint_name, operations.impl.table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_foreign_key\")\n@BatchOperations.register_operation(\n    \"create_foreign_key\", \"batch_create_foreign_key\"\n)\n@AddConstraintOp.register_add_constraint(\"foreign_key_constraint\")\nclass CreateForeignKeyOp(AddConstraintOp):\n    \"\"\"Represent a create foreign key constraint operation.\"\"\"\n\n    constraint_type = \"foreignkey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.source_table = source_table\n        self.referent_table = referent_table\n        self.local_cols = local_cols\n        self.remote_cols = remote_cols\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, ForeignKeyConstraint]:\n        return (\"add_fk\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreateForeignKeyOp:\n        fk_constraint = cast(\"ForeignKeyConstraint\", constraint)\n        kw: dict = {}\n        if fk_constraint.onupdate:\n            kw[\"onupdate\"] = fk_constraint.onupdate\n        if fk_constraint.ondelete:\n            kw[\"ondelete\"] = fk_constraint.ondelete\n        if fk_constraint.initially:\n            kw[\"initially\"] = fk_constraint.initially\n        if fk_constraint.deferrable:\n            kw[\"deferrable\"] = fk_constraint.deferrable\n        if fk_constraint.use_alter:\n            kw[\"use_alter\"] = fk_constraint.use_alter\n        if fk_constraint.match:\n            kw[\"match\"] = fk_constraint.match\n\n        (\n            source_schema,\n            source_table,\n            source_columns,\n            target_schema,\n            target_table,\n            target_columns,\n            onupdate,\n            ondelete,\n            deferrable,\n            initially,\n        ) = sqla_compat._fk_spec(fk_constraint)\n\n        kw[\"source_schema\"] = source_schema\n        kw[\"referent_schema\"] = target_schema\n        kw.update(fk_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(fk_constraint.name),\n            source_table,\n            target_table,\n            source_columns,\n            target_columns,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> ForeignKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.foreign_key_constraint(\n            self.constraint_name,\n            self.source_table,\n            self.referent_table,\n            self.local_cols,\n            self.remote_cols,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_foreign_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        source_schema: Optional[str] = None,\n        referent_schema: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_foreign_key(\n                \"fk_user_address\",\n                \"address\",\n                \"user\",\n                [\"user_id\"],\n                [\"id\"],\n            )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the foreign key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source_table: String name of the source table.\n        :param referent_table: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        op = cls(\n            constraint_name,\n            source_table,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=source_schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_foreign_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        referent_schema: Optional[str] = None,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``source_schema``\n        arguments from the call.\n\n        e.g.::\n\n            with batch_alter_table(\"address\") as batch_op:\n                batch_op.create_foreign_key(\n                    \"fk_user_address\",\n                    \"user\",\n                    [\"user_id\"],\n                    [\"id\"],\n                )\n\n        .. seealso::\n\n            :meth:`.Operations.create_foreign_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=operations.impl.schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_check_constraint\")\n@BatchOperations.register_operation(\n    \"create_check_constraint\", \"batch_create_check_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"check_constraint\")\n@AddConstraintOp.register_add_constraint(\"table_or_column_check_constraint\")\n@AddConstraintOp.register_add_constraint(\"column_check_constraint\")\nclass CreateCheckConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create check constraint operation.\"\"\"\n\n    constraint_type = \"check\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        condition: Union[str, TextClause, ColumnElement[Any]],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.condition = condition\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateCheckConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        ck_constraint = cast(\"CheckConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(ck_constraint.name),\n            constraint_table.name,\n            cast(\"ColumnElement[Any]\", ck_constraint.sqltext),\n            schema=constraint_table.schema,\n            **ck_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> CheckConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.check_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.condition,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_check_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column(\"name\")) > 5,\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param condition: SQL expression that's the condition of the\n         constraint. Can be a string or SQLAlchemy expression language\n         structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, condition, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_check_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_check_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            condition,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_index\")\n@BatchOperations.register_operation(\"create_index\", \"batch_create_index\")\nclass CreateIndexOp(MigrateOperation):\n    \"\"\"Represent a create index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.unique = unique\n        self.if_not_exists = if_not_exists\n        self.kw = kw\n\n    def reverse(self) -> DropIndexOp:\n        return DropIndexOp.from_index(self.to_index())\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"add_index\", self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> CreateIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n\n    @classmethod\n    def create_index(\n        cls,\n        operations: Operations,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, Function[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_index(\"ik_test\", \"t1\", [\"foo\", \"bar\"])\n\n        Functional indexes can be produced by using the\n        :func:`sqlalchemy.sql.expression.text` construct::\n\n            from alembic import op\n            from sqlalchemy import text\n\n            op.create_index(\"ik_test\", \"t1\", [text(\"lower(foo)\")])\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.\n        :param columns: a list consisting of string column names and/or\n         :func:`~sqlalchemy.sql.expression.text` constructs.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param unique: If True, create a unique index.\n\n        :param quote: Force quoting of this column's name on or off,\n         corresponding to ``True`` or ``False``. When left at its default\n         of ``None``, the column identifier will be quoted according to\n         whether the name is case sensitive (identifiers with at least one\n         upper case character are treated as case sensitive), or if it's a\n         reserved word. This flag is only needed to force quoting of a\n         reserved word which is not known by the SQLAlchemy dialect.\n\n        :param if_not_exists: If True, adds IF NOT EXISTS operator when\n         creating the new index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_index(\n        cls,\n        operations: BatchOperations,\n        index_name: str,\n        columns: List[str],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.create_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_index\")\n@BatchOperations.register_operation(\"drop_index\", \"batch_drop_index\")\nclass DropIndexOp(MigrateOperation):\n    \"\"\"Represent a drop index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Union[quoted_name, str, conv],\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        _reverse: Optional[CreateIndexOp] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.schema = schema\n        self.if_exists = if_exists\n        self._reverse = _reverse\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"remove_index\", self.to_index())\n\n    def reverse(self) -> CreateIndexOp:\n        return CreateIndexOp.from_index(self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> DropIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def drop_index(\n        cls,\n        operations: Operations,\n        index_name: str,\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        :param if_exists: If True, adds IF EXISTS operator when\n         dropping the index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_index(\n        cls, operations: BatchOperations, index_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            table_name=operations.impl.table_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table\")\nclass CreateTableOp(MigrateOperation):\n    \"\"\"Represent a create table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        columns: Sequence[SchemaItem],\n        *,\n        schema: Optional[str] = None,\n        _namespace_metadata: Optional[MetaData] = None,\n        _constraints_included: bool = False,\n        **kw: Any,\n    ) -> None:\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.info = kw.pop(\"info\", {})\n        self.comment = kw.pop(\"comment\", None)\n        self.prefixes = kw.pop(\"prefixes\", None)\n        self.kw = kw\n        self._namespace_metadata = _namespace_metadata\n        self._constraints_included = _constraints_included\n\n    def reverse(self) -> DropTableOp:\n        return DropTableOp.from_table(\n            self.to_table(), _namespace_metadata=self._namespace_metadata\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"add_table\", self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:\n        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=self._constraints_included,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *columns: SchemaItem,\n        **kw: Any,\n    ) -> Table:\n        r\"\"\"Issue a \"create table\" instruction using the current migration\n        context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        Note that :meth:`.create_table` accepts\n        :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        The function also returns a newly created\n        :class:`~sqlalchemy.schema.Table` object, corresponding to the table\n        specification given, which is suitable for\n        immediate SQL operations, in particular\n        :meth:`.Operations.bulk_insert`::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            account_table = op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n            op.bulk_insert(\n                account_table,\n                [\n                    {\"name\": \"A1\", \"description\": \"account 1\"},\n                    {\"name\": \"A2\", \"description\": \"account 2\"},\n                ],\n            )\n\n        :param table_name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        :return: the :class:`~sqlalchemy.schema.Table` object corresponding\n         to the parameters given.\n\n        \"\"\"\n        op = cls(table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_table\")\nclass DropTableOp(MigrateOperation):\n    \"\"\"Represent a drop table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        table_kw: Optional[MutableMapping[Any, Any]] = None,\n        _reverse: Optional[CreateTableOp] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n        self.table_kw = table_kw or {}\n        self.comment = self.table_kw.pop(\"comment\", None)\n        self.info = self.table_kw.pop(\"info\", None)\n        self.prefixes = self.table_kw.pop(\"prefixes\", None)\n        self._reverse = _reverse\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"remove_table\", self.to_table())\n\n    def reverse(self) -> CreateTableOp:\n        return CreateTableOp.from_table(self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n\n    @classmethod\n    def drop_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param table_name: Name of the table\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        op = cls(table_name, schema=schema, table_kw=kw)\n        operations.invoke(op)\n\n\nclass AlterTableOp(MigrateOperation):\n    \"\"\"Represent an alter table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n\n\n@Operations.register_operation(\"rename_table\")\nclass RenameTableOp(AlterTableOp):\n    \"\"\"Represent a rename table operation.\"\"\"\n\n    def __init__(\n        self,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(old_table_name, schema=schema)\n        self.new_table_name = new_table_name\n\n    @classmethod\n    def rename_table(\n        cls,\n        operations: Operations,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(old_table_name, new_table_name, schema=schema)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table_comment\")\n@BatchOperations.register_operation(\n    \"create_table_comment\", \"batch_create_table_comment\"\n)\nclass CreateTableCommentOp(AlterTableOp):\n    \"\"\"Represent a COMMENT ON `table` operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.comment = comment\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def create_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table.\n\n        :param table_name: string name of the target table.\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(\n            table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=schema,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_table_comment(\n        cls,\n        operations: BatchOperations,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table\n        using the current batch migration context.\n\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        if self.existing_comment is None:\n            return DropTableCommentOp(\n                self.table_name,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n        else:\n            return CreateTableCommentOp(\n                self.table_name,\n                self.existing_comment,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name, schema=self.schema, comment=self.comment\n        )\n\n    def to_diff_tuple(self):\n        return (\"add_table_comment\", self.to_table(), self.existing_comment)\n\n\n@Operations.register_operation(\"drop_table_comment\")\n@BatchOperations.register_operation(\n    \"drop_table_comment\", \"batch_drop_table_comment\"\n)\nclass DropTableCommentOp(AlterTableOp):\n    \"\"\"Represent an operation to remove the comment from a table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def drop_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table.\n\n        :param table_name: string name of the target table.\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        .. seealso::\n\n            :meth:`.Operations.create_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(table_name, existing_comment=existing_comment, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_table_comment(\n        cls,\n        operations: BatchOperations,\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table using the current\n        batch operations context.\n\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        return CreateTableCommentOp(\n            self.table_name, self.existing_comment, schema=self.schema\n        )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(self.table_name, schema=self.schema)\n\n    def to_diff_tuple(self):\n        return (\"remove_table_comment\", self.to_table())\n\n\n@Operations.register_operation(\"alter_column\")\n@BatchOperations.register_operation(\"alter_column\", \"batch_alter_column\")\nclass AlterColumnOp(AlterTableOp):\n    \"\"\"Represent an alter column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_type: Optional[Any] = None,\n        existing_server_default: Any = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        modify_nullable: Optional[bool] = None,\n        modify_comment: Optional[Union[str, Literal[False]]] = False,\n        modify_server_default: Any = False,\n        modify_name: Optional[str] = None,\n        modify_type: Optional[Any] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.existing_type = existing_type\n        self.existing_server_default = existing_server_default\n        self.existing_nullable = existing_nullable\n        self.existing_comment = existing_comment\n        self.modify_nullable = modify_nullable\n        self.modify_comment = modify_comment\n        self.modify_server_default = modify_server_default\n        self.modify_name = modify_name\n        self.modify_type = modify_type\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Any:\n        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n\n    def has_changes(self) -> bool:\n        hc1 = (\n            self.modify_nullable is not None\n            or self.modify_server_default is not False\n            or self.modify_type is not None\n            or self.modify_comment is not False\n        )\n        if hc1:\n            return True\n        for kw in self.kw:\n            if kw.startswith(\"modify_\"):\n                return True\n        else:\n            return False\n\n    def reverse(self) -> AlterColumnOp:\n        kw = self.kw.copy()\n        kw[\"existing_type\"] = self.existing_type\n        kw[\"existing_nullable\"] = self.existing_nullable\n        kw[\"existing_server_default\"] = self.existing_server_default\n        kw[\"existing_comment\"] = self.existing_comment\n        if self.modify_type is not None:\n            kw[\"modify_type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            kw[\"modify_nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            kw[\"modify_server_default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            kw[\"modify_comment\"] = self.modify_comment\n\n        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return self.__class__(\n            self.table_name, self.column_name, schema=self.schema, **kw\n        )\n\n    @classmethod\n    def alter_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_type`` argument is\n        used in this case to identify and remove a previous\n        constraint that was bound to the type object.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param comment: optional string text of a new comment to add to the\n         column.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type\\_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLAlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param existing_comment: string text of the existing comment on the\n         column to be maintained.  Required on MySQL if the existing comment\n         on the column is not being changed.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param postgresql_using: String argument which will indicate a\n         SQL expression to render within the Postgresql-specific USING clause\n         within ALTER COLUMN.    This string is taken directly as raw SQL which\n         must explicitly include any necessary quoting or escaping of tokens\n         within the expression.\n\n        \"\"\"\n\n        alt = cls(\n            table_name,\n            column_name,\n            schema=schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n    @classmethod\n    def batch_alter_column(\n        cls,\n        operations: BatchOperations,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue an \"alter column\" instruction using the current\n        batch migration context.\n\n        Parameters are the same as that of :meth:`.Operations.alter_column`,\n        as well as the following option(s):\n\n        :param insert_before: String name of an existing column which this\n         column should be placed before, when creating the new table.\n\n        :param insert_after: String name of an existing column which this\n         column should be placed after, when creating the new table.  If\n         both :paramref:`.BatchOperations.alter_column.insert_before`\n         and :paramref:`.BatchOperations.alter_column.insert_after` are\n         omitted, the column is inserted after the last existing column\n         in the table.\n\n        .. seealso::\n\n            :meth:`.Operations.alter_column`\n\n\n        \"\"\"\n        alt = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            insert_before=insert_before,\n            insert_after=insert_after,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n\n@Operations.register_operation(\"add_column\")\n@BatchOperations.register_operation(\"add_column\", \"batch_add_column\")\nclass AddColumnOp(AlterTableOp):\n    \"\"\"Represent an add column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column = column\n        self.kw = kw\n\n    def reverse(self) -> DropColumnOp:\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\"add_column\", self.schema, self.table_name, self.column)\n\n    def to_column(self) -> Column:\n        return self.column\n\n    @classmethod\n    def from_column(cls, col: Column) -> AddColumnOp:\n        return cls(col.table.name, col, schema=col.table.schema)\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> AddColumnOp:\n        return cls(tname, col, schema=schema)\n\n    @classmethod\n    def add_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\"organization\", Column(\"name\", String()))\n\n        The :meth:`.Operations.add_column` method typically corresponds\n        to the SQL command \"ALTER TABLE... ADD COLUMN\".    Within the scope\n        of this command, the column's name, datatype, nullability,\n        and optional server-generated defaults may be indicated.\n\n        .. note::\n\n            With the exception of NOT NULL constraints or single-column FOREIGN\n            KEY constraints, other kinds of constraints such as PRIMARY KEY,\n            UNIQUE or CHECK constraints **cannot** be generated using this\n            method; for these constraints, refer to operations such as\n            :meth:`.Operations.create_primary_key` and\n            :meth:`.Operations.create_check_constraint`. In particular, the\n            following :class:`~sqlalchemy.schema.Column` parameters are\n            **ignored**:\n\n            * :paramref:`~sqlalchemy.schema.Column.primary_key` - SQL databases\n              typically do not support an ALTER operation that can add\n              individual columns one at a time to an existing primary key\n              constraint, therefore it's less ambiguous to use the\n              :meth:`.Operations.create_primary_key` method, which assumes no\n              existing primary key constraint is present.\n            * :paramref:`~sqlalchemy.schema.Column.unique` - use the\n              :meth:`.Operations.create_unique_constraint` method\n            * :paramref:`~sqlalchemy.schema.Column.index` - use the\n              :meth:`.Operations.create_index` method\n\n\n        The provided :class:`~sqlalchemy.schema.Column` object may include a\n        :class:`~sqlalchemy.schema.ForeignKey` constraint directive,\n        referencing a remote table name. For this specific type of constraint,\n        Alembic will automatically emit a second ALTER statement in order to\n        add the single-column FOREIGN KEY constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\n                \"organization\",\n                Column(\"account_id\", INTEGER, ForeignKey(\"accounts.id\")),\n            )\n\n        The column argument passed to :meth:`.Operations.add_column` is a\n        :class:`~sqlalchemy.schema.Column` construct, used in the same way it's\n        used in SQLAlchemy. In particular, values or functions to be indicated\n        as producing the column's default value on the database side are\n        specified using the ``server_default`` parameter, and not ``default``\n        which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column(\n                \"account\",\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(table_name, column, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_add_column(\n        cls,\n        operations: BatchOperations,\n        column: Column[Any],\n        *,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.add_column`\n\n        \"\"\"\n\n        kw = {}\n        if insert_before:\n            kw[\"insert_before\"] = insert_before\n        if insert_after:\n            kw[\"insert_after\"] = insert_after\n\n        op = cls(\n            operations.impl.table_name,\n            column,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_column\")\n@BatchOperations.register_operation(\"drop_column\", \"batch_drop_column\")\nclass DropColumnOp(AlterTableOp):\n    \"\"\"Represent a drop column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddColumnOp] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.kw = kw\n        self._reverse = _reverse\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\n            \"remove_column\",\n            self.schema,\n            self.table_name,\n            self.to_column(),\n        )\n\n    def reverse(self) -> AddColumnOp:\n        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n\n    @classmethod\n###The function: from_column_and_tablename###\n    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:\n        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n\n    @classmethod\n    def drop_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\"organization\", \"account_id\")\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec's a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n        \"\"\"\n\n        op = cls(table_name, column_name, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_column(\n        cls, operations: BatchOperations, column_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_column`\n\n        \"\"\"\n        op = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"bulk_insert\")\nclass BulkInsertOp(MigrateOperation):\n    \"\"\"Represent a bulk insert operation.\"\"\"\n\n    def __init__(\n        self,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        self.table = table\n        self.rows = rows\n        self.multiinsert = multiinsert\n\n    @classmethod\n    def bulk_insert(\n        cls,\n        operations: Operations,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        \"\"\"Issue a \"bulk insert\" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\n                \"account\",\n                column(\"id\", Integer),\n                column(\"name\", String),\n                column(\"create_date\", Date),\n            )\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": date(2010, 10, 5),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": date(2007, 5, 27),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": date(2008, 8, 15),\n                    },\n                ],\n            )\n\n        When using --sql mode, some datatypes may not render inline\n        automatically, such as dates and other special types.   When this\n        issue is present, :meth:`.Operations.inline_literal` may be used::\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": op.inline_literal(\"2010-10-05\"),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": op.inline_literal(\"2007-05-27\"),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": op.inline_literal(\"2008-08-15\"),\n                    },\n                ],\n                multiinsert=False,\n            )\n\n        When using :meth:`.Operations.inline_literal` in conjunction with\n        :meth:`.Operations.bulk_insert`, in order for the statement to work\n        in \"online\" (e.g. non --sql) mode, the\n        :paramref:`~.Operations.bulk_insert.multiinsert`\n        flag should be set to ``False``, which will have the effect of\n        individual INSERT statements being emitted to the database, each\n        with a distinct VALUES clause, so that the \"inline\" values can\n        still be rendered, rather than attempting to pass the values\n        as bound parameters.\n\n        :param table: a table object which represents the target of the INSERT.\n\n        :param rows: a list of dictionaries indicating rows.\n\n        :param multiinsert: when at its default of True and --sql mode is not\n           enabled, the INSERT statement will be executed using\n           \"executemany()\" style, where all elements in the list of\n           dictionaries are passed as bound parameters in a single\n           list.   Setting this to False results in individual INSERT\n           statements being emitted per parameter set, and is needed\n           in those cases where non-literal values are present in the\n           parameter sets.\n\n        \"\"\"\n\n        op = cls(table, rows, multiinsert=multiinsert)\n        operations.invoke(op)\n\n\n@Operations.register_operation(\"execute\")\n@BatchOperations.register_operation(\"execute\", \"batch_execute\")\nclass ExecuteSQLOp(MigrateOperation):\n    \"\"\"Represent an execute SQL operation.\"\"\"\n\n    def __init__(\n        self,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.sqltext = sqltext\n        self.execution_options = execution_options\n\n    @classmethod\n    def execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        r\"\"\"Execute the given SQL using the current migration context.\n\n        The given SQL can be a plain string, e.g.::\n\n            op.execute(\"INSERT INTO table (foo) VALUES ('some value')\")\n\n        Or it can be any kind of Core SQL Expression construct, such as\n        below where we use an update construct::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\"account\", column(\"name\", String))\n            op.execute(\n                account.update()\n                .where(account.c.name == op.inline_literal(\"account 1\"))\n                .values({\"name\": op.inline_literal(\"account 2\")})\n            )\n\n        Above, we made use of the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table` and\n        :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it's a recommended practice to at least\n        ensure the definition of a table is self-contained within the migration\n        script, rather than imported from a module that may break compatibility\n        with older migrations.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in \"offline\" mode.     Additionally, parameterized\n        statements are discouraged here, as they *will not work* in offline\n        mode.  Above, we use :meth:`.inline_literal` where parameters are\n        to be used.\n\n        For full interaction with a connected database where parameters can\n        also be used normally, use the \"bind\" available from the context::\n\n            from alembic import op\n\n            connection = op.get_bind()\n\n            connection.execute(\n                account.update()\n                .where(account.c.name == \"account 1\")\n                .values({\"name\": \"account 2\"})\n            )\n\n        Additionally, when passing the statement as a plain string, it is first\n        coerced into a :func:`sqlalchemy.sql.expression.text` construct\n        before being passed along.  In the less likely case that the\n        literal SQL string contains a colon, it must be escaped with a\n        backslash, as::\n\n           op.execute(r\"INSERT INTO table (foo) VALUES ('\\:colon_value')\")\n\n\n        :param sqltext: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update` construct.\n        * a :func:`sqlalchemy.sql.expression.delete` construct.\n        * Any \"executable\" described in SQLAlchemy Core documentation,\n          noting that no result set is returned.\n\n        .. note::  when passing a plain string, the statement is coerced into\n           a :func:`sqlalchemy.sql.expression.text` construct. This construct\n           considers symbols with colons, e.g. ``:foo`` to be bound parameters.\n           To avoid this, ensure that colon symbols are escaped, e.g.\n           ``\\:foo``.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        \"\"\"\n        op = cls(sqltext, execution_options=execution_options)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Execute the given SQL using the current migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.execute`\n\n        \"\"\"\n        return cls.execute(\n            operations, sqltext, execution_options=execution_options\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Union[Executable, str]]:\n        return (\"execute\", self.sqltext)\n\n\nclass OpContainer(MigrateOperation):\n    \"\"\"Represent a sequence of operations operation.\"\"\"\n\n    def __init__(self, ops: Sequence[MigrateOperation] = ()) -> None:\n        self.ops = list(ops)\n\n    def is_empty(self) -> bool:\n        return not self.ops\n\n    def as_diffs(self) -> Any:\n        return list(OpContainer._ops_as_diffs(self))\n\n    @classmethod\n    def _ops_as_diffs(\n        cls, migrations: OpContainer\n    ) -> Iterator[Tuple[Any, ...]]:\n        for op in migrations.ops:\n            if hasattr(op, \"ops\"):\n                yield from cls._ops_as_diffs(cast(\"OpContainer\", op))\n            else:\n                yield op.to_diff_tuple()\n\n\nclass ModifyTableOps(OpContainer):\n    \"\"\"Contains a sequence of operations that all apply to a single Table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        ops: Sequence[MigrateOperation],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(ops)\n        self.table_name = table_name\n        self.schema = schema\n\n    def reverse(self) -> ModifyTableOps:\n        return ModifyTableOps(\n            self.table_name,\n            ops=list(reversed([op.reverse() for op in self.ops])),\n            schema=self.schema,\n        )\n\n\nclass UpgradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'upgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        upgrade_token: str = \"upgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.upgrade_token = upgrade_token\n\n    def reverse_into(self, downgrade_ops: DowngradeOps) -> DowngradeOps:\n        downgrade_ops.ops[:] = list(  # type:ignore[index]\n            reversed([op.reverse() for op in self.ops])\n        )\n        return downgrade_ops\n\n    def reverse(self) -> DowngradeOps:\n        return self.reverse_into(DowngradeOps(ops=[]))\n\n\nclass DowngradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'downgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        downgrade_token: str = \"downgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.downgrade_token = downgrade_token\n\n    def reverse(self):\n        return UpgradeOps(\n            ops=list(reversed([op.reverse() for op in self.ops]))\n        )\n\n\nclass MigrationScript(MigrateOperation):\n    \"\"\"represents a migration script.\n\n    E.g. when autogenerate encounters this object, this corresponds to the\n    production of an actual script file.\n\n    A normal :class:`.MigrationScript` object would contain a single\n    :class:`.UpgradeOps` and a single :class:`.DowngradeOps` directive.\n    These are accessible via the ``.upgrade_ops`` and ``.downgrade_ops``\n    attributes.\n\n    In the case of an autogenerate operation that runs multiple times,\n    such as the multiple database example in the \"multidb\" template,\n    the ``.upgrade_ops`` and ``.downgrade_ops`` attributes are disabled,\n    and instead these objects should be accessed via the ``.upgrade_ops_list``\n    and ``.downgrade_ops_list`` list-based attributes.  These latter\n    attributes are always available at the very least as single-element lists.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    _needs_render: Optional[bool]\n\n    def __init__(\n        self,\n        rev_id: Optional[str],\n        upgrade_ops: UpgradeOps,\n        downgrade_ops: DowngradeOps,\n        *,\n        message: Optional[str] = None,\n        imports: Set[str] = set(),\n        head: Optional[str] = None,\n        splice: Optional[bool] = None,\n        branch_label: Optional[_RevIdType] = None,\n        version_path: Optional[str] = None,\n        depends_on: Optional[_RevIdType] = None,\n    ) -> None:\n        self.rev_id = rev_id\n        self.message = message\n        self.imports = imports\n        self.head = head\n        self.splice = splice\n        self.branch_label = branch_label\n        self.version_path = version_path\n        self.depends_on = depends_on\n        self.upgrade_ops = upgrade_ops\n        self.downgrade_ops = downgrade_ops\n\n    @property\n    def upgrade_ops(self):\n        \"\"\"An instance of :class:`.UpgradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.upgrade_ops_list`\n        \"\"\"\n        if len(self._upgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for UpgradeOps; please use the \"\n                \"upgrade_ops_list attribute.\"\n            )\n        elif not self._upgrade_ops:\n            return None\n        else:\n            return self._upgrade_ops[0]\n\n    @upgrade_ops.setter\n    def upgrade_ops(self, upgrade_ops):\n        self._upgrade_ops = util.to_list(upgrade_ops)\n        for elem in self._upgrade_ops:\n            assert isinstance(elem, UpgradeOps)\n\n    @property\n    def downgrade_ops(self):\n        \"\"\"An instance of :class:`.DowngradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.downgrade_ops_list`\n        \"\"\"\n        if len(self._downgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for DowngradeOps; please use the \"\n                \"downgrade_ops_list attribute.\"\n            )\n        elif not self._downgrade_ops:\n            return None\n        else:\n            return self._downgrade_ops[0]\n\n    @downgrade_ops.setter\n    def downgrade_ops(self, downgrade_ops):\n        self._downgrade_ops = util.to_list(downgrade_ops)\n        for elem in self._downgrade_ops:\n            assert isinstance(elem, DowngradeOps)\n\n    @property\n    def upgrade_ops_list(self) -> List[UpgradeOps]:\n        \"\"\"A list of :class:`.UpgradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.upgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._upgrade_ops\n\n    @property\n    def downgrade_ops_list(self) -> List[DowngradeOps]:\n        \"\"\"A list of :class:`.DowngradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.downgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._downgrade_ops\n", "test_list": ["def test_render_drop_column_w_schema(self):\n    op_obj = ops.DropColumnOp.from_column_and_tablename('foo', 'bar', Column('x', Integer, server_default='5'))\n    eq_ignore_whitespace(autogenerate.render_op_text(self.autogen_context, op_obj), \"op.drop_column('bar', 'x', schema='foo')\")", "def test_render_drop_column(self):\n    op_obj = ops.DropColumnOp.from_column_and_tablename(None, 'foo', Column('x', Integer, server_default='5'))\n    eq_ignore_whitespace(autogenerate.render_op_text(self.autogen_context, op_obj), \"op.drop_column('foo', 'x')\")", "def test_drop_column(self):\n    t = self.table\n    op = ops.DropColumnOp.from_column_and_tablename(None, 't', t.c.x)\n    is_(op.to_column(), t.c.x)\n    is_(op.reverse().to_column(), t.c.x)\n    is_not_(None, op.to_column().table)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'from_column_and_tablename' should correctly instantiate a 'DropColumnOp' object with the given schema, table name, and column, ensuring that the column is of type 'Column'.", "unit_test": ["def test_from_column_and_tablename_input_output():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column)\n    assert op_obj.schema == 'foo'\n    assert op_obj.table_name == 'bar'\n    assert isinstance(op_obj.to_column(), Column)"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_input_output"}, "Exception Handling": {"requirement": "The function 'from_column_and_tablename' should raise a TypeError if the 'col' parameter is not an instance of 'Column'.", "unit_test": ["def test_from_column_and_tablename_type_error():\n    try:\n        DropColumnOp.from_column_and_tablename('foo', 'bar', 'not_a_column')\n    except TypeError as e:\n        assert str(e) == 'The col parameter must be an instance of Column.'"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_type_error"}, "Edge Case Handling": {"requirement": "The function 'from_column_and_tablename' should handle the case where the 'schema' parameter is None, defaulting to a schema-less operation.", "unit_test": ["def test_from_column_and_tablename_no_schema():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename(None, 'bar', column)\n    assert op_obj.schema is None\n    assert op_obj.table_name == 'bar'"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_no_schema"}, "Functionality Extension": {"requirement": "Extend the function 'from_column_and_tablename' to accept an optional 'if_exists' parameter, which defaults to False, to conditionally drop the column only if it exists.", "unit_test": ["def test_from_column_and_tablename_if_exists():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column, if_exists=True)\n    assert op_obj.kw.get('if_exists') is True"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_if_exists"}, "Annotation Coverage": {"requirement": "Ensure that the function 'from_column_and_tablename' includes type annotations for all parameters and the return type.", "unit_test": ["def test_from_column_and_tablename_annotations():\n    from inspect import signature\n    sig = signature(DropColumnOp.from_column_and_tablename)\n    assert sig.parameters['schema'].annotation == Optional[str]\n    assert sig.parameters['tname'].annotation == str\n    assert sig.parameters['col'].annotation == Column\n    assert sig.return_annotation == DropColumnOp"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_annotations"}, "Code Complexity": {"requirement": "The function 'from_column_and_tablename' should maintain a cyclomatic complexity of 1, ensuring straightforward logic without branching.", "unit_test": ["def test_from_column_and_tablename_complexity():\n    import radon.complexity as rc\n    code = '''\n    def from_column_and_tablename(cls, schema: Optional[str], tname: str, col: Column) -> DropColumnOp:\n        return cls(tname, col, schema=schema)\n    '''\n    complexity = rc.cc_visit(code)\n    assert complexity[0].complexity == 1"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_code_complexity"}, "Code Standard": {"requirement": "Ensure that the function 'from_column_and_tablename' adheres to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_from_column_and_tablename_pep8():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_function.py'])\n    assert result.total_errors == 0"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the function 'from_column_and_tablename' uses the 'DropColumnOp' class from the 'alembic.operations.ops' module.", "unit_test": ["def test_from_column_and_tablename_context_usage():\n    from alembic.operations.ops import DropColumnOp\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column)\n    assert isinstance(op_obj, DropColumnOp)"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the function 'from_column_and_tablename' correctly initializes a 'DropColumnOp' object using the '__init__' method of the 'DropColumnOp' class.", "unit_test": ["def test_from_column_and_tablename_context_correctness():\n    column = Column('x', Integer, server_default='5')\n    op_obj = DropColumnOp.from_column_and_tablename('foo', 'bar', column)\n    assert op_obj.schema == 'foo'\n    assert op_obj.table_name == 'bar'\n    assert op_obj.column_name == 'x'"], "test": "tests/test_autogen_render.py::AutogenRenderTest::test_from_column_and_tablename_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pyramid.i18n.Translations.add", "type": "method", "project_path": "Internet/pyramid", "completion_path": "Internet/pyramid/src/pyramid/i18n.py", "signature_position": [277, 277], "body_position": [293, 307], "dependency": {"intra_class": ["pyramid.i18n.Translations.DEFAULT_DOMAIN", "pyramid.i18n.Translations._domains", "pyramid.i18n.Translations.domain", "pyramid.i18n.Translations.merge", "pyramid.i18n.Translations.plural"], "intra_file": ["pyramid.i18n.DEFAULT_PLURAL"], "cross_file": []}, "requirement": {"Functionality": "This function adds the given translations to the catalog. If the domain of the translations is different from the current catalog, they are added as a separate catalog. It also provides the option to merge translations for message domains that have already been added.", "Arguments": ":param self: Translations. An instance of the Translations class.\n:param translations: Translations. The Translations instance with the messages to add.\n:param merge: Bool. Whether translations for message domains that have already been added should be merged with the existing translations. Defaults to True.\n:return: Translations. The Translations instance (self) so that merge calls can be easily chained."}, "tests": ["tests/test_i18n.py::TestTranslations::test_add_default_domain_replaces_plural_first_time", "tests/test_i18n.py::TestTranslations::test_add_different_domain_merge_true_notexisting", "tests/test_i18n.py::TestTranslations::test_add_same_domain_merge_true", "tests/test_i18n.py::TestTranslations::test_add_different_domain_merge_true_existing"], "indent": 4, "domain": "Internet", "code": "    def add(self, translations, merge=True):\n        \"\"\"Add the given translations to the catalog.\n\n        If the domain of the translations is different than that of the\n        current catalog, they are added as a catalog that is only accessible\n        by the various ``d*gettext`` functions.\n\n        :param translations: the `Translations` instance with the messages to\n                             add\n        :param merge: whether translations for message domains that have\n                      already been added should be merged with the existing\n                      translations\n        :return: the `Translations` instance (``self``) so that `merge` calls\n                 can be easily chained\n        :rtype: `Translations`\n        \"\"\"\n        domain = getattr(translations, 'domain', self.DEFAULT_DOMAIN)\n        if domain == self.DEFAULT_DOMAIN and self.plural is DEFAULT_PLURAL:\n            self.plural = translations.plural\n\n        if merge and domain == self.domain:\n            return self.merge(translations)\n\n        existing = self._domains.get(domain)\n        if merge and existing is not None:\n            existing.merge(translations)\n        else:\n            translations.add_fallback(self)\n            self._domains[domain] = translations\n\n        return self\n", "context": "import gettext\nimport os\nfrom translationstring import Pluralizer, Translator\nfrom translationstring import TranslationString  # API\nfrom translationstring import TranslationStringFactory  # API\n\nfrom pyramid.decorator import reify\nfrom pyramid.interfaces import ILocaleNegotiator\nfrom pyramid.threadlocal import get_current_registry\n\nTranslationString = TranslationString  # PyFlakes\nTranslationStringFactory = TranslationStringFactory  # PyFlakes\n\nDEFAULT_PLURAL = lambda n: int(n != 1)\n\n\nclass Localizer:\n    \"\"\"\n    An object providing translation and pluralizations related to\n    the current request's locale name.  A\n    :class:`pyramid.i18n.Localizer` object is created using the\n    :func:`pyramid.i18n.get_localizer` function.\n    \"\"\"\n\n    def __init__(self, locale_name, translations):\n        self.locale_name = locale_name\n        self.translations = translations\n        self.pluralizer = None\n        self.translator = None\n\n    def translate(self, tstring, domain=None, mapping=None):\n        \"\"\"\n        Translate a :term:`translation string` to the current language\n        and interpolate any *replacement markers* in the result.  The\n        ``translate`` method accepts three arguments: ``tstring``\n        (required), ``domain`` (optional) and ``mapping`` (optional).\n        When called, it will translate the ``tstring`` translation\n        string using the current locale.  If the current locale could not be\n        determined, the result of interpolation of the default value is\n        returned.  The optional ``domain`` argument can be used to specify\n        or override the domain of the ``tstring`` (useful when ``tstring``\n        is a normal string rather than a translation string).  The optional\n        ``mapping`` argument can specify or override the ``tstring``\n        interpolation mapping, useful when the ``tstring`` argument is\n        a simple string instead of a translation string.\n\n        Example::\n\n           from pyramid.i18n import TranslationString\n           ts = TranslationString('Add ${item}', domain='mypackage',\n                                  mapping={'item':'Item'})\n           translated = localizer.translate(ts)\n\n        Example::\n\n           translated = localizer.translate('Add ${item}', domain='mypackage',\n                                            mapping={'item':'Item'})\n\n        \"\"\"\n        if self.translator is None:\n            self.translator = Translator(self.translations)\n        return self.translator(tstring, domain=domain, mapping=mapping)\n\n    def pluralize(self, singular, plural, n, domain=None, mapping=None):\n        \"\"\"\n        Return a string translation by using two\n        :term:`message identifier` objects as a singular/plural pair\n        and an ``n`` value representing the number that appears in the\n        message using gettext plural forms support.  The ``singular``\n        and ``plural`` objects should be strings. There is no\n        reason to use translation string objects as arguments as all\n        metadata is ignored.\n\n        ``n`` represents the number of elements. ``domain`` is the\n        translation domain to use to do the pluralization, and ``mapping``\n        is the interpolation mapping that should be used on the result. If\n        the ``domain`` is not supplied, a default domain is used (usually\n        ``messages``).\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('Add ${num} item',\n                                            'Add ${num} items',\n                                            num,\n                                            mapping={'num':num})\n\n        If using the gettext plural support, which is required for\n        languages that have pluralisation rules other than n != 1, the\n        ``singular`` argument must be the message_id defined in the\n        translation file. The plural argument is not used in this case.\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('item_plural',\n                                            '',\n                                            num,\n                                            mapping={'num':num})\n\n\n        \"\"\"\n        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )\n\n\ndef default_locale_negotiator(request):\n    \"\"\"The default :term:`locale negotiator`.  Returns a locale name\n    or ``None``.\n\n    - First, the negotiator looks for the ``_LOCALE_`` attribute of\n      the request object (possibly set by a view or a listener for an\n      :term:`event`). If the attribute exists and it is not ``None``,\n      its value will be used.\n\n    - Then it looks for the ``request.params['_LOCALE_']`` value.\n\n    - Then it looks for the ``request.cookies['_LOCALE_']`` value.\n\n    - Finally, the negotiator returns ``None`` if the locale could not\n      be determined via any of the previous checks (when a locale\n      negotiator returns ``None``, it signifies that the\n      :term:`default locale name` should be used.)\n    \"\"\"\n    name = '_LOCALE_'\n    locale_name = getattr(request, name, None)\n    if locale_name is None:\n        locale_name = request.params.get(name)\n        if locale_name is None:\n            locale_name = request.cookies.get(name)\n    return locale_name\n\n\ndef negotiate_locale_name(request):\n    \"\"\"Negotiate and return the :term:`locale name` associated with\n    the current request.\"\"\"\n    try:\n        registry = request.registry\n    except AttributeError:\n        registry = get_current_registry()\n    negotiator = registry.queryUtility(\n        ILocaleNegotiator, default=default_locale_negotiator\n    )\n    locale_name = negotiator(request)\n\n    if locale_name is None:\n        settings = registry.settings or {}\n        locale_name = settings.get('default_locale_name', 'en')\n\n    return locale_name\n\n\ndef get_locale_name(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use :attr:`pyramid.request.Request.locale_name` directly instead.\n        Return the :term:`locale name` associated with the current request.\n    \"\"\"\n    return request.locale_name\n\n\ndef make_localizer(current_locale_name, translation_directories):\n    \"\"\"Create a :class:`pyramid.i18n.Localizer` object\n    corresponding to the provided locale name from the\n    translations found in the list of translation directories.\"\"\"\n    translations = Translations()\n    translations._catalog = {}\n\n    locales_to_try = []\n    if '_' in current_locale_name:\n        locales_to_try = [current_locale_name.split('_')[0]]\n    locales_to_try.append(current_locale_name)\n\n    # intent: order locales left to right in least specific to most specific,\n    # e.g. ['de', 'de_DE'].  This services the intent of creating a\n    # translations object that returns a \"more specific\" translation for a\n    # region, but will fall back to a \"less specific\" translation for the\n    # locale if necessary.  Ordering from least specific to most specific\n    # allows us to call translations.add in the below loop to get this\n    # behavior.\n\n    for tdir in translation_directories:\n        locale_dirs = []\n        for lname in locales_to_try:\n            ldir = os.path.realpath(os.path.join(tdir, lname))\n            if os.path.isdir(ldir):\n                locale_dirs.append(ldir)\n\n        for locale_dir in locale_dirs:\n            messages_dir = os.path.join(locale_dir, 'LC_MESSAGES')\n            if not os.path.isdir(os.path.realpath(messages_dir)):\n                continue\n            for mofile in os.listdir(messages_dir):\n                mopath = os.path.realpath(os.path.join(messages_dir, mofile))\n                if mofile.endswith('.mo') and os.path.isfile(mopath):\n                    with open(mopath, 'rb') as mofp:\n                        domain = mofile[:-3]\n                        dtrans = Translations(mofp, domain)\n                        translations.add(dtrans)\n\n    return Localizer(\n        locale_name=current_locale_name, translations=translations\n    )\n\n\ndef get_localizer(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use the :attr:`pyramid.request.Request.localizer` attribute directly\n        instead.  Retrieve a :class:`pyramid.i18n.Localizer` object\n        corresponding to the current request's locale name.\n    \"\"\"\n    return request.localizer\n\n\nclass Translations(gettext.GNUTranslations):\n    \"\"\"An extended translation catalog class (ripped off from Babel)\"\"\"\n\n    DEFAULT_DOMAIN = 'messages'\n\n    def __init__(self, fileobj=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Initialize the translations catalog.\n\n        :param fileobj: the file-like object the translation should be read\n                        from\n        \"\"\"\n        # germanic plural by default; self.plural will be overwritten by\n        # GNUTranslations._parse (called as a side effect if fileobj is\n        # passed to GNUTranslations.__init__) with a \"real\" self.plural for\n        # this domain; see https://github.com/Pylons/pyramid/issues/235\n        # It is only overridden the first time a new message file is found\n        # for a given domain, so all message files must have matching plural\n        # rules if they are in the same domain. We keep track of if we have\n        # overridden so we can special case the default domain, which is always\n        # instantiated before a message file is read.\n        # See also https://github.com/Pylons/pyramid/pull/2102\n        self.plural = DEFAULT_PLURAL\n        gettext.GNUTranslations.__init__(self, fp=fileobj)\n        self.files = list(filter(None, [getattr(fileobj, 'name', None)]))\n        self.domain = domain\n        self._domains = {}\n\n    @classmethod\n    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Load translations from the given directory.\n\n        :param dirname: the directory containing the ``MO`` files\n        :param locales: the list of locales in order of preference (items in\n                        this list can be either `Locale` objects or locale\n                        strings)\n        :param domain: the message domain\n        :return: the loaded catalog, or a ``NullTranslations`` instance if no\n                 matching translations were found\n        :rtype: `Translations`\n        \"\"\"\n        if locales is not None:\n            if not isinstance(locales, (list, tuple)):\n                locales = [locales]\n            locales = [str(locale) for locale in locales]\n        if not domain:\n            domain = cls.DEFAULT_DOMAIN\n        filename = gettext.find(domain, dirname, locales)\n        if not filename:\n            return gettext.NullTranslations()\n        with open(filename, 'rb') as fp:\n            return cls(fileobj=fp, domain=domain)\n\n    def __repr__(self):\n        return '<%s: \"%s\">' % (\n            type(self).__name__,\n            self._info.get('project-id-version'),\n        )\n\n###The function: add###\n    def merge(self, translations):\n        \"\"\"Merge the given translations into the catalog.\n\n        Message translations in the specified catalog override any messages\n        with the same identifier in the existing catalog.\n\n        :param translations: the `Translations` instance with the messages to\n                             merge\n        :return: the `Translations` instance (``self``) so that `merge` calls\n                 can be easily chained\n        :rtype: `Translations`\n        \"\"\"\n        if isinstance(translations, gettext.GNUTranslations):\n            self._catalog.update(translations._catalog)\n            if isinstance(translations, Translations):\n                self.files.extend(translations.files)\n\n        return self\n\n    def dgettext(self, domain, message):\n        \"\"\"Like ``gettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dugettext(self, domain, message):\n        \"\"\"Like ``ugettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dngettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ngettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n    def dungettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ungettext()`` but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n\nclass LocalizerRequestMixin:\n    @reify\n    def localizer(self):\n        \"\"\"Convenience property to return a localizer\"\"\"\n        from pyramid.interfaces import ILocalizer\n        from pyramid.interfaces import ITranslationDirectories\n        registry = self.registry\n\n        current_locale_name = self.locale_name\n        localizer = registry.queryUtility(ILocalizer, name=current_locale_name)\n\n        if localizer is None:\n            # no localizer utility registered yet\n            tdirs = registry.queryUtility(ITranslationDirectories, default=[])\n            localizer = make_localizer(current_locale_name, tdirs)\n\n            registry.registerUtility(\n                localizer, ILocalizer, name=current_locale_name\n            )\n\n        return localizer\n\n    @reify\n    def locale_name(self):\n        locale_name = negotiate_locale_name(self)\n        return locale_name\n", "prompt": "Please write a python function called 'add' base the context. This function adds the given translations to the catalog. If the domain of the translations is different from the current catalog, they are added as a separate catalog. It also provides the option to merge translations for message domains that have already been added.:param self: Translations. An instance of the Translations class.\n:param translations: Translations. The Translations instance with the messages to add.\n:param merge: Bool. Whether translations for message domains that have already been added should be merged with the existing translations. Defaults to True.\n:return: Translations. The Translations instance (self) so that merge calls can be easily chained..\n        The context you need to refer to is as follows: import gettext\nimport os\nfrom translationstring import Pluralizer, Translator\nfrom translationstring import TranslationString  # API\nfrom translationstring import TranslationStringFactory  # API\n\nfrom pyramid.decorator import reify\nfrom pyramid.interfaces import ILocaleNegotiator\nfrom pyramid.threadlocal import get_current_registry\n\nTranslationString = TranslationString  # PyFlakes\nTranslationStringFactory = TranslationStringFactory  # PyFlakes\n\nDEFAULT_PLURAL = lambda n: int(n != 1)\n\n\nclass Localizer:\n    \"\"\"\n    An object providing translation and pluralizations related to\n    the current request's locale name.  A\n    :class:`pyramid.i18n.Localizer` object is created using the\n    :func:`pyramid.i18n.get_localizer` function.\n    \"\"\"\n\n    def __init__(self, locale_name, translations):\n        self.locale_name = locale_name\n        self.translations = translations\n        self.pluralizer = None\n        self.translator = None\n\n    def translate(self, tstring, domain=None, mapping=None):\n        \"\"\"\n        Translate a :term:`translation string` to the current language\n        and interpolate any *replacement markers* in the result.  The\n        ``translate`` method accepts three arguments: ``tstring``\n        (required), ``domain`` (optional) and ``mapping`` (optional).\n        When called, it will translate the ``tstring`` translation\n        string using the current locale.  If the current locale could not be\n        determined, the result of interpolation of the default value is\n        returned.  The optional ``domain`` argument can be used to specify\n        or override the domain of the ``tstring`` (useful when ``tstring``\n        is a normal string rather than a translation string).  The optional\n        ``mapping`` argument can specify or override the ``tstring``\n        interpolation mapping, useful when the ``tstring`` argument is\n        a simple string instead of a translation string.\n\n        Example::\n\n           from pyramid.i18n import TranslationString\n           ts = TranslationString('Add ${item}', domain='mypackage',\n                                  mapping={'item':'Item'})\n           translated = localizer.translate(ts)\n\n        Example::\n\n           translated = localizer.translate('Add ${item}', domain='mypackage',\n                                            mapping={'item':'Item'})\n\n        \"\"\"\n        if self.translator is None:\n            self.translator = Translator(self.translations)\n        return self.translator(tstring, domain=domain, mapping=mapping)\n\n    def pluralize(self, singular, plural, n, domain=None, mapping=None):\n        \"\"\"\n        Return a string translation by using two\n        :term:`message identifier` objects as a singular/plural pair\n        and an ``n`` value representing the number that appears in the\n        message using gettext plural forms support.  The ``singular``\n        and ``plural`` objects should be strings. There is no\n        reason to use translation string objects as arguments as all\n        metadata is ignored.\n\n        ``n`` represents the number of elements. ``domain`` is the\n        translation domain to use to do the pluralization, and ``mapping``\n        is the interpolation mapping that should be used on the result. If\n        the ``domain`` is not supplied, a default domain is used (usually\n        ``messages``).\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('Add ${num} item',\n                                            'Add ${num} items',\n                                            num,\n                                            mapping={'num':num})\n\n        If using the gettext plural support, which is required for\n        languages that have pluralisation rules other than n != 1, the\n        ``singular`` argument must be the message_id defined in the\n        translation file. The plural argument is not used in this case.\n\n        Example::\n\n           num = 1\n           translated = localizer.pluralize('item_plural',\n                                            '',\n                                            num,\n                                            mapping={'num':num})\n\n\n        \"\"\"\n        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )\n\n\ndef default_locale_negotiator(request):\n    \"\"\"The default :term:`locale negotiator`.  Returns a locale name\n    or ``None``.\n\n    - First, the negotiator looks for the ``_LOCALE_`` attribute of\n      the request object (possibly set by a view or a listener for an\n      :term:`event`). If the attribute exists and it is not ``None``,\n      its value will be used.\n\n    - Then it looks for the ``request.params['_LOCALE_']`` value.\n\n    - Then it looks for the ``request.cookies['_LOCALE_']`` value.\n\n    - Finally, the negotiator returns ``None`` if the locale could not\n      be determined via any of the previous checks (when a locale\n      negotiator returns ``None``, it signifies that the\n      :term:`default locale name` should be used.)\n    \"\"\"\n    name = '_LOCALE_'\n    locale_name = getattr(request, name, None)\n    if locale_name is None:\n        locale_name = request.params.get(name)\n        if locale_name is None:\n            locale_name = request.cookies.get(name)\n    return locale_name\n\n\ndef negotiate_locale_name(request):\n    \"\"\"Negotiate and return the :term:`locale name` associated with\n    the current request.\"\"\"\n    try:\n        registry = request.registry\n    except AttributeError:\n        registry = get_current_registry()\n    negotiator = registry.queryUtility(\n        ILocaleNegotiator, default=default_locale_negotiator\n    )\n    locale_name = negotiator(request)\n\n    if locale_name is None:\n        settings = registry.settings or {}\n        locale_name = settings.get('default_locale_name', 'en')\n\n    return locale_name\n\n\ndef get_locale_name(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use :attr:`pyramid.request.Request.locale_name` directly instead.\n        Return the :term:`locale name` associated with the current request.\n    \"\"\"\n    return request.locale_name\n\n\ndef make_localizer(current_locale_name, translation_directories):\n    \"\"\"Create a :class:`pyramid.i18n.Localizer` object\n    corresponding to the provided locale name from the\n    translations found in the list of translation directories.\"\"\"\n    translations = Translations()\n    translations._catalog = {}\n\n    locales_to_try = []\n    if '_' in current_locale_name:\n        locales_to_try = [current_locale_name.split('_')[0]]\n    locales_to_try.append(current_locale_name)\n\n    # intent: order locales left to right in least specific to most specific,\n    # e.g. ['de', 'de_DE'].  This services the intent of creating a\n    # translations object that returns a \"more specific\" translation for a\n    # region, but will fall back to a \"less specific\" translation for the\n    # locale if necessary.  Ordering from least specific to most specific\n    # allows us to call translations.add in the below loop to get this\n    # behavior.\n\n    for tdir in translation_directories:\n        locale_dirs = []\n        for lname in locales_to_try:\n            ldir = os.path.realpath(os.path.join(tdir, lname))\n            if os.path.isdir(ldir):\n                locale_dirs.append(ldir)\n\n        for locale_dir in locale_dirs:\n            messages_dir = os.path.join(locale_dir, 'LC_MESSAGES')\n            if not os.path.isdir(os.path.realpath(messages_dir)):\n                continue\n            for mofile in os.listdir(messages_dir):\n                mopath = os.path.realpath(os.path.join(messages_dir, mofile))\n                if mofile.endswith('.mo') and os.path.isfile(mopath):\n                    with open(mopath, 'rb') as mofp:\n                        domain = mofile[:-3]\n                        dtrans = Translations(mofp, domain)\n                        translations.add(dtrans)\n\n    return Localizer(\n        locale_name=current_locale_name, translations=translations\n    )\n\n\ndef get_localizer(request):\n    \"\"\"\n    .. deprecated:: 1.5\n        Use the :attr:`pyramid.request.Request.localizer` attribute directly\n        instead.  Retrieve a :class:`pyramid.i18n.Localizer` object\n        corresponding to the current request's locale name.\n    \"\"\"\n    return request.localizer\n\n\nclass Translations(gettext.GNUTranslations):\n    \"\"\"An extended translation catalog class (ripped off from Babel)\"\"\"\n\n    DEFAULT_DOMAIN = 'messages'\n\n    def __init__(self, fileobj=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Initialize the translations catalog.\n\n        :param fileobj: the file-like object the translation should be read\n                        from\n        \"\"\"\n        # germanic plural by default; self.plural will be overwritten by\n        # GNUTranslations._parse (called as a side effect if fileobj is\n        # passed to GNUTranslations.__init__) with a \"real\" self.plural for\n        # this domain; see https://github.com/Pylons/pyramid/issues/235\n        # It is only overridden the first time a new message file is found\n        # for a given domain, so all message files must have matching plural\n        # rules if they are in the same domain. We keep track of if we have\n        # overridden so we can special case the default domain, which is always\n        # instantiated before a message file is read.\n        # See also https://github.com/Pylons/pyramid/pull/2102\n        self.plural = DEFAULT_PLURAL\n        gettext.GNUTranslations.__init__(self, fp=fileobj)\n        self.files = list(filter(None, [getattr(fileobj, 'name', None)]))\n        self.domain = domain\n        self._domains = {}\n\n    @classmethod\n    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):\n        \"\"\"Load translations from the given directory.\n\n        :param dirname: the directory containing the ``MO`` files\n        :param locales: the list of locales in order of preference (items in\n                        this list can be either `Locale` objects or locale\n                        strings)\n        :param domain: the message domain\n        :return: the loaded catalog, or a ``NullTranslations`` instance if no\n                 matching translations were found\n        :rtype: `Translations`\n        \"\"\"\n        if locales is not None:\n            if not isinstance(locales, (list, tuple)):\n                locales = [locales]\n            locales = [str(locale) for locale in locales]\n        if not domain:\n            domain = cls.DEFAULT_DOMAIN\n        filename = gettext.find(domain, dirname, locales)\n        if not filename:\n            return gettext.NullTranslations()\n        with open(filename, 'rb') as fp:\n            return cls(fileobj=fp, domain=domain)\n\n    def __repr__(self):\n        return '<%s: \"%s\">' % (\n            type(self).__name__,\n            self._info.get('project-id-version'),\n        )\n\n###The function: add###\n    def merge(self, translations):\n        \"\"\"Merge the given translations into the catalog.\n\n        Message translations in the specified catalog override any messages\n        with the same identifier in the existing catalog.\n\n        :param translations: the `Translations` instance with the messages to\n                             merge\n        :return: the `Translations` instance (``self``) so that `merge` calls\n                 can be easily chained\n        :rtype: `Translations`\n        \"\"\"\n        if isinstance(translations, gettext.GNUTranslations):\n            self._catalog.update(translations._catalog)\n            if isinstance(translations, Translations):\n                self.files.extend(translations.files)\n\n        return self\n\n    def dgettext(self, domain, message):\n        \"\"\"Like ``gettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dugettext(self, domain, message):\n        \"\"\"Like ``ugettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).gettext(message)\n\n    def dngettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ngettext()``, but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n    def dungettext(self, domain, singular, plural, num):\n        \"\"\"Like ``ungettext()`` but look the message up in the specified\n        domain.\n        \"\"\"\n        return self._domains.get(domain, self).ngettext(singular, plural, num)\n\n\nclass LocalizerRequestMixin:\n    @reify\n    def localizer(self):\n        \"\"\"Convenience property to return a localizer\"\"\"\n        from pyramid.interfaces import ILocalizer\n        from pyramid.interfaces import ITranslationDirectories\n        registry = self.registry\n\n        current_locale_name = self.locale_name\n        localizer = registry.queryUtility(ILocalizer, name=current_locale_name)\n\n        if localizer is None:\n            # no localizer utility registered yet\n            tdirs = registry.queryUtility(ITranslationDirectories, default=[])\n            localizer = make_localizer(current_locale_name, tdirs)\n\n            registry.registerUtility(\n                localizer, ILocalizer, name=current_locale_name\n            )\n\n        return localizer\n\n    @reify\n    def locale_name(self):\n        locale_name = negotiate_locale_name(self)\n        return locale_name\n", "test_list": ["def test_add_default_domain_replaces_plural_first_time(self):\n    inst = self._getTargetClass()(None, domain='messages')\n    inst2 = self._getTargetClass()(None, domain='messages')\n    inst3 = self._getTargetClass()(None, domain='messages')\n    inst._catalog = {}\n    inst2._catalog = {}\n    inst3._catalog = {}\n    self.assertEqual(inst.plural(0), 1)\n    self.assertEqual(inst.plural(1), 0)\n    self.assertEqual(inst.plural(2), 1)\n    inst2.plural = lambda n: n > 1\n    inst.add(inst2)\n    self.assertEqual(inst.plural(0), 0)\n    self.assertEqual(inst.plural(1), 0)\n    self.assertEqual(inst.plural(2), 1)\n    inst3.plural = lambda n: n > 0\n    inst.add(inst3)\n    self.assertEqual(inst.plural(0), 0)\n    self.assertEqual(inst.plural(1), 0)\n    self.assertEqual(inst.plural(2), 1)", "def test_add_different_domain_merge_true_notexisting(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst.add(inst2)\n    self.assertEqual(inst._domains['domain2'], inst2)", "def test_add_same_domain_merge_true(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2._catalog['a'] = 'b'\n    inst.add(inst2)\n    self.assertEqual(inst._catalog['a'], 'b')", "def test_add_different_domain_merge_true_existing(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst3 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst2._catalog['a'] = 'b'\n    inst3.domain = 'domain2'\n    inst._domains['domain2'] = inst3\n    inst.add(inst2)\n    self.assertEqual(inst._domains['domain2'], inst3)\n    self.assertEqual(inst3._catalog['a'], 'b')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'add' function should ensure that the 'translations' parameter is an instance of the Translations class and that the 'merge' parameter is a boolean. If the input is not legal, please raise a TypeError.", "unit_test": ["def test_add_input_output_conditions(self):\n    inst = self._makeOne()\n    with self.assertRaises(TypeError):\n        inst.add('not a Translations instance')\n    with self.assertRaises(TypeError):\n        inst.add(self._makeOne(), merge='not a boolean')"], "test": "tests/test_i18n.py::TestTranslations::test_add_input_output_conditions"}, "Exception Handling": {"requirement": "The 'add' function should raise a ValueError if the 'translations' parameter is None.", "unit_test": ["def test_add_exception_handling(self):\n    inst = self._makeOne()\n    with self.assertRaises(ValueError):\n        inst.add(None)"], "test": "tests/test_i18n.py::TestTranslations::test_add_exception_handling"}, "Edge Case Handling": {"requirement": "The 'add' function should handle the case where the 'translations' parameter has an empty catalog gracefully.", "unit_test": ["def test_add_edge_case_handling_empty_catalog(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2._catalog = {}\n    inst.add(inst2)\n    self.assertEqual(inst._catalog, {})"], "test": "tests/test_i18n.py::TestTranslations::test_add_edge_case_handling_empty_catalog"}, "Functionality Extension": {"requirement": "Extend the 'add' function to allow adding multiple Translations instances at once by accepting a list of Translations objects.", "unit_test": ["def test_add_functionality_extension_multiple_translations(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst3 = self._makeOne()\n    inst2._catalog['a'] = 'b'\n    inst3._catalog['c'] = 'd'\n    inst.add([inst2, inst3])\n    self.assertEqual(inst._catalog['a'], 'b')\n    self.assertEqual(inst._catalog['c'], 'd')"], "test": "tests/test_i18n.py::TestTranslations::test_add_functionality_extension_multiple_translations"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'add' function are properly annotated with type hints.", "unit_test": ["def test_add_annotation_coverage(self):\n    from typing import get_type_hints\n    hints = get_type_hints(self._makeOne().add)\n    self.assertEqual(hints['translations'], Translations)\n    self.assertEqual(hints['merge'], bool)\n    self.assertEqual(hints['return'], Translations)"], "test": "tests/test_i18n.py::TestTranslations::test_add_annotation_coverage"}, "Code Complexity": {"requirement": "The 'add' function should maintain a cyclomatic complexity of 7 or less.", "unit_test": ["def test_add_code_complexity(self):\n    from radon.complexity import cc_visit\n    code = inspect.getsource(self._makeOne().add)\n    complexity = cc_visit(code)\n    self.assertTrue(all(c.complexity <= 5 for c in complexity))"], "test": "tests/test_i18n.py::TestTranslations::test_code_complexity"}, "Code Standard": {"requirement": "The 'add' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_add_code_standard(self):\n    import pep8\n    style = pep8.StyleGuide(quiet=True)\n    result = style.check_files([inspect.getfile(self._makeOne().add)])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/test_i18n.py::TestTranslations::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'add' function should utilize the '_domains' attributes from the Translations class.", "unit_test": ["def test_add_context_usage_verification(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst.add(inst2)\n    self.assertIn('domain2', inst._domains)"], "test": "tests/test_i18n.py::TestTranslations::test_add_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'add' function should correctly update the '_domains' dictionary when adding translations with a different domain.", "unit_test": ["def test_add_context_usage_correctness_verification(self):\n    inst = self._makeOne()\n    inst2 = self._makeOne()\n    inst2.domain = 'domain2'\n    inst2._catalog['a'] = 'b'\n    inst.add(inst2)\n    self.assertEqual(inst._domains['domain2']._catalog['a'], 'b')"], "test": "tests/test_i18n.py::TestTranslations::test_add_context_usage_correctness_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "type": "method", "project_path": "Utilities/python-for-android", "completion_path": "Utilities/python-for-android/pythonforandroid/prerequisites.py", "signature_position": [269, 269], "body_position": [270, 275], "dependency": {"intra_class": ["pythonforandroid.prerequisites.OpenSSLPrerequisite.homebrew_formula_name"], "intra_file": ["pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix"], "cross_file": []}, "requirement": {"Functionality": "Check if the OpenSSL prerequisite is met on a Darwin (MacOS) system. It checks if the Homebrew formula for OpenSSL is installed.", "Arguments": ":param self: OpenSSLPrerequisite. An instance of the OpenSSLPrerequisite class.\n:return: bool. True if the OpenSSL prerequisite is met, False otherwise."}, "tests": ["tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker"], "indent": 4, "domain": "Utilities", "code": "    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            )\n            is not None\n        )\n", "context": "#!/usr/bin/env python3\n\nimport os\nimport platform\nimport shutil\nimport subprocess\nimport sys\n\nfrom pythonforandroid.logger import info, warning, error\nfrom pythonforandroid.util import ensure_dir\n\n\nclass Prerequisite(object):\n    name = \"Default\"\n    homebrew_formula_name = \"\"\n    mandatory = dict(linux=False, darwin=False)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def is_valid(self):\n        if self.checker():\n            info(f\"Prerequisite {self.name} is met\")\n            return (True, \"\")\n        elif not self.mandatory[sys.platform]:\n            warning(\n                f\"Prerequisite {self.name} is not met, but is marked as non-mandatory\"\n            )\n        else:\n            error(f\"Prerequisite {self.name} is not met\")\n\n    def checker(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_checker()\n        elif sys.platform == \"linux\":\n            return self.linux_checker()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def ask_to_install(self):\n        if (\n            os.environ.get(\"PYTHONFORANDROID_PREREQUISITES_INSTALL_INTERACTIVE\", \"1\")\n            == \"1\"\n        ):\n            res = input(\n                f\"Do you want automatically install prerequisite {self.name}? [y/N] \"\n            )\n            if res.lower() == \"y\":\n                return True\n            else:\n                return False\n        else:\n            info(\n                \"Session is not interactive (usually this happens during a CI run), so let's consider it as a YES\"\n            )\n            return True\n\n    def install(self):\n        info(f\"python-for-android can automatically install prerequisite: {self.name}\")\n        if self.ask_to_install():\n            if sys.platform == \"darwin\":\n                self.darwin_installer()\n            elif sys.platform == \"linux\":\n                self.linux_installer()\n            else:\n                raise Exception(\"Unsupported platform\")\n        else:\n            info(\n                f\"Skipping installation of prerequisite {self.name} as per user request\"\n            )\n\n    def show_helper(self):\n        if sys.platform == \"darwin\":\n            self.darwin_helper()\n        elif sys.platform == \"linux\":\n            self.linux_helper()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def install_is_supported(self):\n        return self.installer_is_supported[sys.platform]\n\n    def linux_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on linux for {self.name}\")\n\n    def darwin_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on macOS for {self.name}\")\n\n    def linux_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on linux for {self.name}\")\n\n    def darwin_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on macOS for {self.name}\")\n\n    def darwin_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on macOS\")\n\n    def linux_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on linux\")\n\n    def _darwin_get_brew_formula_location_prefix(self, formula, installed=False):\n        opts = [\"--installed\"] if installed else []\n        p = subprocess.Popen(\n            [\"brew\", \"--prefix\", formula, *opts],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(_stderr_res.decode(\"utf-8\").strip())\n            return None\n        else:\n            return _stdout_res.decode(\"utf-8\").strip()\n\n    def darwin_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on macOS for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    def linux_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on linux for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    @property\n    def pkg_config_location(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_pkg_config_location()\n        elif sys.platform == \"linux\":\n            return self.linux_pkg_config_location()\n\n\nclass HomebrewPrerequisite(Prerequisite):\n    name = \"homebrew\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def darwin_checker(self):\n        return shutil.which(\"brew\") is not None\n\n    def darwin_helper(self):\n        info(\n            \"Installer for homebrew is not yet supported on macOS,\"\n            \"the nice news is that the installation process is easy!\"\n            \"See: https://brew.sh for further instructions.\"\n        )\n\n\nclass JDKPrerequisite(Prerequisite):\n    name = \"JDK\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n    min_supported_version = 11\n\n    def darwin_checker(self):\n        if \"JAVA_HOME\" in os.environ:\n            info(\"Found JAVA_HOME environment variable, using it\")\n            jdk_path = os.environ[\"JAVA_HOME\"]\n        else:\n            jdk_path = self._darwin_get_libexec_jdk_path(version=None)\n        return self._darwin_jdk_is_supported(jdk_path)\n\n    def _darwin_get_libexec_jdk_path(self, version=None):\n        version_args = []\n        if version is not None:\n            version_args = [\"-v\", version]\n        return (\n            subprocess.run(\n                [\"/usr/libexec/java_home\", *version_args],\n                stdout=subprocess.PIPE,\n            )\n            .stdout.strip()\n            .decode()\n        )\n\n    def _darwin_jdk_is_supported(self, jdk_path):\n        if not jdk_path:\n            return False\n\n        javac_bin = os.path.join(jdk_path, \"bin\", \"javac\")\n        if not os.path.exists(javac_bin):\n            return False\n\n        p = subprocess.Popen(\n            [javac_bin, \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(\"Failed to run javac to check JDK version\")\n            return False\n\n        if not _stdout_res:\n            _stdout_res = _stderr_res\n\n        res = _stdout_res.strip().decode()\n\n        major_version = int(res.split(\" \")[-1].split(\".\")[0])\n        if major_version >= self.min_supported_version:\n            info(f\"Found a valid JDK at {jdk_path}\")\n            return True\n        else:\n            error(f\"JDK {self.min_supported_version} or higher is required\")\n            return False\n\n    def darwin_helper(self):\n        info(\n            \"python-for-android requires a JDK 11 or higher to be installed on macOS,\"\n            \"but seems like you don't have one installed.\"\n        )\n        info(\n            \"If you think that a valid JDK is already installed, please verify that \"\n            \"you have a JDK 11 or higher installed and that `/usr/libexec/java_home` \"\n            \"shows the correct path.\"\n        )\n        info(\n            \"If you have multiple JDK installations, please make sure that you have \"\n            \"`JAVA_HOME` environment variable set to the correct JDK installation.\"\n        )\n\n    def darwin_installer(self):\n        info(\n            \"Looking for a JDK 11 or higher installation which is not the default one ...\"\n        )\n        jdk_path = self._darwin_get_libexec_jdk_path(version=\"11+\")\n\n        if not self._darwin_jdk_is_supported(jdk_path):\n            info(\"We're unlucky, there's no JDK 11 or higher installation available\")\n\n            base_url = \"https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.2%2B8/\"\n            if platform.machine() == \"arm64\":\n                filename = \"OpenJDK17U-jdk_aarch64_mac_hotspot_17.0.2_8.tar.gz\"\n            else:\n                filename = \"OpenJDK17U-jdk_x64_mac_hotspot_17.0.2_8.tar.gz\"\n\n            info(f\"Downloading {filename} from {base_url}\")\n            subprocess.check_output(\n                [\n                    \"curl\",\n                    \"-L\",\n                    f\"{base_url}{filename}\",\n                    \"-o\",\n                    f\"/tmp/{filename}\",\n                ]\n            )\n\n            user_library_java_path = os.path.expanduser(\n                \"~/Library/Java/JavaVirtualMachines\"\n            )\n            info(f\"Extracting {filename} to {user_library_java_path}\")\n            ensure_dir(user_library_java_path)\n            subprocess.check_output(\n                [\"tar\", \"xzf\", f\"/tmp/{filename}\", \"-C\", user_library_java_path],\n            )\n\n            jdk_path = self._darwin_get_libexec_jdk_path(version=\"17.0.2+8\")\n\n        info(f\"Setting JAVA_HOME to {jdk_path}\")\n        os.environ[\"JAVA_HOME\"] = jdk_path\n\n\nclass OpenSSLPrerequisite(Prerequisite):\n    name = \"openssl\"\n    homebrew_formula_name = \"openssl@1.1\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n###The function: darwin_checker###\n    def darwin_pkg_config_location(self):\n        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )\n\n    def darwin_installer(self):\n        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n\nclass AutoconfPrerequisite(Prerequisite):\n    name = \"autoconf\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"autoconf\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n\nclass AutomakePrerequisite(Prerequisite):\n    name = \"automake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n\nclass LibtoolPrerequisite(Prerequisite):\n    name = \"libtool\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n\nclass PkgConfigPrerequisite(Prerequisite):\n    name = \"pkg-config\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n\nclass CmakePrerequisite(Prerequisite):\n    name = \"cmake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n\ndef get_required_prerequisites(platform=\"linux\"):\n    return [\n        prerequisite_cls()\n        for prerequisite_cls in [\n            HomebrewPrerequisite,\n            AutoconfPrerequisite,\n            AutomakePrerequisite,\n            LibtoolPrerequisite,\n            PkgConfigPrerequisite,\n            CmakePrerequisite,\n            OpenSSLPrerequisite,\n            JDKPrerequisite,\n        ] if prerequisite_cls.mandatory.get(platform, False)\n    ]\n\n\ndef check_and_install_default_prerequisites():\n\n    prerequisites_not_met = []\n\n    warning(\n        \"prerequisites.py is experimental and does not support all prerequisites yet.\"\n    )\n    warning(\"Please report any issues to the python-for-android issue tracker.\")\n\n    # Phase 1: Check if all prerequisites are met and add the ones\n    # which are not to `prerequisites_not_met`\n    for prerequisite in get_required_prerequisites(sys.platform):\n        if not prerequisite.is_valid():\n            prerequisites_not_met.append(prerequisite)\n\n    # Phase 2: Setup/Install all prerequisites that are not met\n    # (where possible), otherwise show an helper.\n    for prerequisite in prerequisites_not_met:\n        prerequisite.show_helper()\n        if prerequisite.install_is_supported():\n            prerequisite.install()\n\n\nif __name__ == \"__main__\":\n    check_and_install_default_prerequisites()\n", "prompt": "Please write a python function called 'darwin_checker' base the context. Check if the OpenSSL prerequisite is met on a Darwin (MacOS) system. It checks if the Homebrew formula for OpenSSL is installed.:param self: OpenSSLPrerequisite. An instance of the OpenSSLPrerequisite class.\n:return: bool. True if the OpenSSL prerequisite is met, False otherwise..\n        The context you need to refer to is as follows: #!/usr/bin/env python3\n\nimport os\nimport platform\nimport shutil\nimport subprocess\nimport sys\n\nfrom pythonforandroid.logger import info, warning, error\nfrom pythonforandroid.util import ensure_dir\n\n\nclass Prerequisite(object):\n    name = \"Default\"\n    homebrew_formula_name = \"\"\n    mandatory = dict(linux=False, darwin=False)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def is_valid(self):\n        if self.checker():\n            info(f\"Prerequisite {self.name} is met\")\n            return (True, \"\")\n        elif not self.mandatory[sys.platform]:\n            warning(\n                f\"Prerequisite {self.name} is not met, but is marked as non-mandatory\"\n            )\n        else:\n            error(f\"Prerequisite {self.name} is not met\")\n\n    def checker(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_checker()\n        elif sys.platform == \"linux\":\n            return self.linux_checker()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def ask_to_install(self):\n        if (\n            os.environ.get(\"PYTHONFORANDROID_PREREQUISITES_INSTALL_INTERACTIVE\", \"1\")\n            == \"1\"\n        ):\n            res = input(\n                f\"Do you want automatically install prerequisite {self.name}? [y/N] \"\n            )\n            if res.lower() == \"y\":\n                return True\n            else:\n                return False\n        else:\n            info(\n                \"Session is not interactive (usually this happens during a CI run), so let's consider it as a YES\"\n            )\n            return True\n\n    def install(self):\n        info(f\"python-for-android can automatically install prerequisite: {self.name}\")\n        if self.ask_to_install():\n            if sys.platform == \"darwin\":\n                self.darwin_installer()\n            elif sys.platform == \"linux\":\n                self.linux_installer()\n            else:\n                raise Exception(\"Unsupported platform\")\n        else:\n            info(\n                f\"Skipping installation of prerequisite {self.name} as per user request\"\n            )\n\n    def show_helper(self):\n        if sys.platform == \"darwin\":\n            self.darwin_helper()\n        elif sys.platform == \"linux\":\n            self.linux_helper()\n        else:\n            raise Exception(\"Unsupported platform\")\n\n    def install_is_supported(self):\n        return self.installer_is_supported[sys.platform]\n\n    def linux_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on linux for {self.name}\")\n\n    def darwin_checker(self):\n        raise Exception(f\"Unsupported prerequisite check on macOS for {self.name}\")\n\n    def linux_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on linux for {self.name}\")\n\n    def darwin_installer(self):\n        raise Exception(f\"Unsupported prerequisite installer on macOS for {self.name}\")\n\n    def darwin_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on macOS\")\n\n    def linux_helper(self):\n        info(f\"No helper available for prerequisite: {self.name} on linux\")\n\n    def _darwin_get_brew_formula_location_prefix(self, formula, installed=False):\n        opts = [\"--installed\"] if installed else []\n        p = subprocess.Popen(\n            [\"brew\", \"--prefix\", formula, *opts],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(_stderr_res.decode(\"utf-8\").strip())\n            return None\n        else:\n            return _stdout_res.decode(\"utf-8\").strip()\n\n    def darwin_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on macOS for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    def linux_pkg_config_location(self):\n        warning(\n            f\"pkg-config location is not supported on linux for prerequisite: {self.name}\"\n        )\n        return \"\"\n\n    @property\n    def pkg_config_location(self):\n        if sys.platform == \"darwin\":\n            return self.darwin_pkg_config_location()\n        elif sys.platform == \"linux\":\n            return self.linux_pkg_config_location()\n\n\nclass HomebrewPrerequisite(Prerequisite):\n    name = \"homebrew\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=False)\n\n    def darwin_checker(self):\n        return shutil.which(\"brew\") is not None\n\n    def darwin_helper(self):\n        info(\n            \"Installer for homebrew is not yet supported on macOS,\"\n            \"the nice news is that the installation process is easy!\"\n            \"See: https://brew.sh for further instructions.\"\n        )\n\n\nclass JDKPrerequisite(Prerequisite):\n    name = \"JDK\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n    min_supported_version = 11\n\n    def darwin_checker(self):\n        if \"JAVA_HOME\" in os.environ:\n            info(\"Found JAVA_HOME environment variable, using it\")\n            jdk_path = os.environ[\"JAVA_HOME\"]\n        else:\n            jdk_path = self._darwin_get_libexec_jdk_path(version=None)\n        return self._darwin_jdk_is_supported(jdk_path)\n\n    def _darwin_get_libexec_jdk_path(self, version=None):\n        version_args = []\n        if version is not None:\n            version_args = [\"-v\", version]\n        return (\n            subprocess.run(\n                [\"/usr/libexec/java_home\", *version_args],\n                stdout=subprocess.PIPE,\n            )\n            .stdout.strip()\n            .decode()\n        )\n\n    def _darwin_jdk_is_supported(self, jdk_path):\n        if not jdk_path:\n            return False\n\n        javac_bin = os.path.join(jdk_path, \"bin\", \"javac\")\n        if not os.path.exists(javac_bin):\n            return False\n\n        p = subprocess.Popen(\n            [javac_bin, \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        _stdout_res, _stderr_res = p.communicate()\n\n        if p.returncode != 0:\n            error(\"Failed to run javac to check JDK version\")\n            return False\n\n        if not _stdout_res:\n            _stdout_res = _stderr_res\n\n        res = _stdout_res.strip().decode()\n\n        major_version = int(res.split(\" \")[-1].split(\".\")[0])\n        if major_version >= self.min_supported_version:\n            info(f\"Found a valid JDK at {jdk_path}\")\n            return True\n        else:\n            error(f\"JDK {self.min_supported_version} or higher is required\")\n            return False\n\n    def darwin_helper(self):\n        info(\n            \"python-for-android requires a JDK 11 or higher to be installed on macOS,\"\n            \"but seems like you don't have one installed.\"\n        )\n        info(\n            \"If you think that a valid JDK is already installed, please verify that \"\n            \"you have a JDK 11 or higher installed and that `/usr/libexec/java_home` \"\n            \"shows the correct path.\"\n        )\n        info(\n            \"If you have multiple JDK installations, please make sure that you have \"\n            \"`JAVA_HOME` environment variable set to the correct JDK installation.\"\n        )\n\n    def darwin_installer(self):\n        info(\n            \"Looking for a JDK 11 or higher installation which is not the default one ...\"\n        )\n        jdk_path = self._darwin_get_libexec_jdk_path(version=\"11+\")\n\n        if not self._darwin_jdk_is_supported(jdk_path):\n            info(\"We're unlucky, there's no JDK 11 or higher installation available\")\n\n            base_url = \"https://github.com/adoptium/temurin17-binaries/releases/download/jdk-17.0.2%2B8/\"\n            if platform.machine() == \"arm64\":\n                filename = \"OpenJDK17U-jdk_aarch64_mac_hotspot_17.0.2_8.tar.gz\"\n            else:\n                filename = \"OpenJDK17U-jdk_x64_mac_hotspot_17.0.2_8.tar.gz\"\n\n            info(f\"Downloading {filename} from {base_url}\")\n            subprocess.check_output(\n                [\n                    \"curl\",\n                    \"-L\",\n                    f\"{base_url}{filename}\",\n                    \"-o\",\n                    f\"/tmp/{filename}\",\n                ]\n            )\n\n            user_library_java_path = os.path.expanduser(\n                \"~/Library/Java/JavaVirtualMachines\"\n            )\n            info(f\"Extracting {filename} to {user_library_java_path}\")\n            ensure_dir(user_library_java_path)\n            subprocess.check_output(\n                [\"tar\", \"xzf\", f\"/tmp/{filename}\", \"-C\", user_library_java_path],\n            )\n\n            jdk_path = self._darwin_get_libexec_jdk_path(version=\"17.0.2+8\")\n\n        info(f\"Setting JAVA_HOME to {jdk_path}\")\n        os.environ[\"JAVA_HOME\"] = jdk_path\n\n\nclass OpenSSLPrerequisite(Prerequisite):\n    name = \"openssl\"\n    homebrew_formula_name = \"openssl@1.1\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n###The function: darwin_checker###\n    def darwin_pkg_config_location(self):\n        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )\n\n    def darwin_installer(self):\n        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n\nclass AutoconfPrerequisite(Prerequisite):\n    name = \"autoconf\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"autoconf\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n\nclass AutomakePrerequisite(Prerequisite):\n    name = \"automake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n\nclass LibtoolPrerequisite(Prerequisite):\n    name = \"libtool\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n\nclass PkgConfigPrerequisite(Prerequisite):\n    name = \"pkg-config\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n\nclass CmakePrerequisite(Prerequisite):\n    name = \"cmake\"\n    mandatory = dict(linux=False, darwin=True)\n    installer_is_supported = dict(linux=False, darwin=True)\n\n    def darwin_checker(self):\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n\n    def darwin_installer(self):\n        info(\"Installing cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n\ndef get_required_prerequisites(platform=\"linux\"):\n    return [\n        prerequisite_cls()\n        for prerequisite_cls in [\n            HomebrewPrerequisite,\n            AutoconfPrerequisite,\n            AutomakePrerequisite,\n            LibtoolPrerequisite,\n            PkgConfigPrerequisite,\n            CmakePrerequisite,\n            OpenSSLPrerequisite,\n            JDKPrerequisite,\n        ] if prerequisite_cls.mandatory.get(platform, False)\n    ]\n\n\ndef check_and_install_default_prerequisites():\n\n    prerequisites_not_met = []\n\n    warning(\n        \"prerequisites.py is experimental and does not support all prerequisites yet.\"\n    )\n    warning(\"Please report any issues to the python-for-android issue tracker.\")\n\n    # Phase 1: Check if all prerequisites are met and add the ones\n    # which are not to `prerequisites_not_met`\n    for prerequisite in get_required_prerequisites(sys.platform):\n        if not prerequisite.is_valid():\n            prerequisites_not_met.append(prerequisite)\n\n    # Phase 2: Setup/Install all prerequisites that are not met\n    # (where possible), otherwise show an helper.\n    for prerequisite in prerequisites_not_met:\n        prerequisite.show_helper()\n        if prerequisite.install_is_supported():\n            prerequisite.install()\n\n\nif __name__ == \"__main__\":\n    check_and_install_default_prerequisites()\n", "test_list": ["@mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix')\ndef test_darwin_checker(self, _darwin_get_brew_formula_location_prefix):\n    _darwin_get_brew_formula_location_prefix.return_value = None\n    self.assertFalse(self.prerequisite.darwin_checker())\n    _darwin_get_brew_formula_location_prefix.return_value = self.expected_homebrew_location_prefix\n    self.assertTrue(self.prerequisite.darwin_checker())\n    _darwin_get_brew_formula_location_prefix.assert_called_with(self.expected_homebrew_formula_name, installed=True)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'darwin_checker' function should return a boolean value indicating whether the OpenSSL prerequisite is met on a Darwin system. It should return True if the Homebrew formula for OpenSSL is installed, and False otherwise.", "unit_test": ["@mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix')\ndef test_darwin_checker_output(self, _darwin_get_brew_formula_location_prefix):\n    _darwin_get_brew_formula_location_prefix.return_value = None\n    self.assertFalse(self.prerequisite.darwin_checker())\n    _darwin_get_brew_formula_location_prefix.return_value = '/usr/local/opt/openssl@1.1'\n    self.assertTrue(self.prerequisite.darwin_checker())"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_output"}, "Exception Handling": {"requirement": "The 'darwin_checker' function should handle exceptions gracefully, logging an error message if the subprocess call to check the Homebrew formula fails.", "unit_test": ["@mock.patch('subprocess.Popen')\ndef test_darwin_checker_exception_handling(self, mock_popen):\n    process_mock = mock.Mock()\n    attrs = {'communicate.return_value': (b'', b'Error'), 'returncode': 1}\n    process_mock.configure_mock(**attrs)\n    mock_popen.return_value = process_mock\n    with self.assertLogs('pythonforandroid.logger', level='ERROR') as log:\n        self.assertFalse(self.prerequisite.darwin_checker())\n        self.assertIn('Error', log.output[0])"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_exception_handling"}, "Edge Case Handling": {"requirement": "The 'darwin_checker' function should handle edge cases such as an empty or malformed response from the subprocess call.", "unit_test": ["@mock.patch('subprocess.Popen')\ndef test_darwin_checker_edge_cases(self, mock_popen):\n    process_mock = mock.Mock()\n    attrs = {'communicate.return_value': (b'', b''), 'returncode': 0}\n    process_mock.configure_mock(**attrs)\n    mock_popen.return_value = process_mock\n    self.assertFalse(self.prerequisite.darwin_checker())"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_edge_cases"}, "Functionality Extension": {"requirement": "Extend the 'darwin_checker' function to also verify the version of OpenSSL installed, ensuring it meets a minimum version requirement.", "unit_test": ["@mock.patch('subprocess.Popen')\ndef test_darwin_checker_version_check(self, mock_popen):\n    process_mock = mock.Mock()\n    attrs = {'communicate.return_value': (b'OpenSSL 1.1.1', b''), 'returncode': 0}\n    process_mock.configure_mock(**attrs)\n    mock_popen.return_value = process_mock\n    self.assertTrue(self.prerequisite.darwin_checker())\n    attrs = {'communicate.return_value': (b'OpenSSL 1.0.2', b''), 'returncode': 0}\n    process_mock.configure_mock(**attrs)\n    self.assertFalse(self.prerequisite.darwin_checker())"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_version_check"}, "Annotation Coverage": {"requirement": "Ensure that the 'darwin_checker' function is fully annotated with type hints for parameters and return types.", "unit_test": ["def test_darwin_checker_annotations(self):\n    annotations = self.prerequisite.darwin_checker.__annotations__\n    self.assertEqual(annotations['return'], bool)"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_annotations"}, "Code Complexity": {"requirement": "The 'darwin_checker' function should maintain a cyclomatic complexity of 5 or lower to ensure readability and maintainability.", "unit_test": ["def test_darwin_checker_complexity(self):\n    complexity = get_cyclomatic_complexity(self.prerequisite.darwin_checker)\n    self.assertLessEqual(complexity, 5)"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_code_complexity"}, "Code Standard": {"requirement": "The 'darwin_checker' function should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": ["def test_darwin_checker_pep8(self):\n    pep8style = pep8.StyleGuide(quiet=True)\n    result = pep8style.check_files(['path/to/your/module.py'])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'darwin_checker' function should utilize the 'homebrew_formula_name' attribute from the OpenSSLPrerequisite class to determine the correct formula to check.", "unit_test": ["def test_darwin_checker_context_usage(self):\n    self.assertEqual(self.prerequisite.homebrew_formula_name, 'openssl@1.1')\n    with mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix') as mocked_method:\n        mocked_method.return_value = '/usr/local/opt/openssl@1.1'\n        self.prerequisite.darwin_checker()\n        mocked_method.assert_called_with('openssl@1.1', installed=True)"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'darwin_checker' function correctly uses the 'homebrew_formula_name' attribute to check for the installation of the OpenSSL formula.", "unit_test": ["def test_darwin_checker_correct_context_usage(self):\n    with mock.patch('pythonforandroid.prerequisites.Prerequisite._darwin_get_brew_formula_location_prefix') as mocked_method:\n        mocked_method.return_value = '/usr/local/opt/openssl@1.1'\n        self.prerequisite.darwin_checker()\n        mocked_method.assert_called_with(self.prerequisite.homebrew_formula_name, installed=True)"], "test": "tests/test_prerequisites.py::TestOpenSSLPrerequisite::test_darwin_checker_correct_context_usage"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "boltons.cacheutils.LRI.pop", "type": "method", "project_path": "Utilities/boltons", "completion_path": "Utilities/boltons/boltons/cacheutils.py", "signature_position": [268, 269], "body_position": [270, 279], "dependency": {"intra_class": ["boltons.cacheutils.LRI._lock", "boltons.cacheutils.LRI._remove_from_ll"], "intra_file": ["boltons.cacheutils._MISSING"], "cross_file": []}, "requirement": {"Functionality": "Pop the key in the LRI instance and return the corresponding value. If the key is not found and the default value is not passed, the exception is re-raised. This function bypasses the hit count and miss count.\n", "Arguments": ":param self: LRI, an instance of the LRI class.\n:param key: The key to remove in the instance.\n:param default: The value to return if the key is not found in the instance. Defaults to _UNSET.\n:return: The value corresponding to the key.\n"}, "tests": ["tests/test_cacheutils.py::test_lru_basic"], "indent": 4, "domain": "Utilities", "code": "    def pop(self, key, default=_MISSING):\n        # NB: hit/miss counts are bypassed for pop()\n        with self._lock:\n            try:\n                ret = super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                ret = default\n            else:\n                self._remove_from_ll(key)\n            return ret\n", "context": "# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"``cacheutils`` contains consistent implementations of fundamental\ncache types. Currently there are two to choose from:\n\n  * :class:`LRI` - Least-recently inserted\n  * :class:`LRU` - Least-recently used\n\nBoth caches are :class:`dict` subtypes, designed to be as\ninterchangeable as possible, to facilitate experimentation. A key\npractice with performance enhancement with caching is ensuring that\nthe caching strategy is working. If the cache is constantly missing,\nit is just adding more overhead and code complexity. The standard\nstatistics are:\n\n  * ``hit_count`` - the number of times the queried key has been in\n    the cache\n  * ``miss_count`` - the number of times a key has been absent and/or\n    fetched by the cache\n  * ``soft_miss_count`` - the number of times a key has been absent,\n    but a default has been provided by the caller, as with\n    :meth:`dict.get` and :meth:`dict.setdefault`. Soft misses are a\n    subset of misses, so this number is always less than or equal to\n    ``miss_count``.\n\nAdditionally, ``cacheutils`` provides :class:`ThresholdCounter`, a\ncache-like bounded counter useful for online statistics collection.\n\nLearn more about `caching algorithms on Wikipedia\n<https://en.wikipedia.org/wiki/Cache_algorithms#Examples>`_.\n\n\"\"\"\n\n# TODO: TimedLRI\n# TODO: support 0 max_size?\n\n\nimport heapq\nimport weakref\nimport itertools\nfrom operator import attrgetter\n\ntry:\n    from threading import RLock\nexcept Exception:\n    class RLock(object):\n        'Dummy reentrant lock for builds without threads'\n        def __enter__(self):\n            pass\n\n        def __exit__(self, exctype, excinst, exctb):\n            pass\n\ntry:\n    from .typeutils import make_sentinel\n    _MISSING = make_sentinel(var_name='_MISSING')\n    _KWARG_MARK = make_sentinel(var_name='_KWARG_MARK')\nexcept ImportError:\n    _MISSING = object()\n    _KWARG_MARK = object()\n\ntry:\n    xrange\nexcept NameError:\n    # py3\n    xrange = range\n    unicode, str, bytes, basestring = str, bytes, bytes, (str, bytes)\n\nPREV, NEXT, KEY, VALUE = range(4)   # names for the link fields\nDEFAULT_MAX_SIZE = 128\n\n\nclass LRI(dict):\n    \"\"\"The ``LRI`` implements the basic *Least Recently Inserted* strategy to\n    caching. One could also think of this as a ``SizeLimitedDefaultDict``.\n\n    *on_miss* is a callable that accepts the missing key (as opposed\n    to :class:`collections.defaultdict`'s \"default_factory\", which\n    accepts no arguments.) Also note that, like the :class:`LRI`,\n    the ``LRI`` is instrumented with statistics tracking.\n\n    >>> cap_cache = LRI(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n    \"\"\"\n    def __init__(self, max_size=DEFAULT_MAX_SIZE, values=None,\n                 on_miss=None):\n        if max_size <= 0:\n            raise ValueError('expected max_size > 0, not %r' % max_size)\n        self.hit_count = self.miss_count = self.soft_miss_count = 0\n        self.max_size = max_size\n        self._lock = RLock()\n        self._init_ll()\n\n        if on_miss is not None and not callable(on_miss):\n            raise TypeError('expected on_miss to be a callable'\n                            ' (or None), not %r' % on_miss)\n        self.on_miss = on_miss\n\n        if values:\n            self.update(values)\n\n    # TODO: fromkeys()?\n\n    # linked list manipulation methods.\n    #\n    # invariants:\n    # 1) 'anchor' is the sentinel node in the doubly linked list.  there is\n    #    always only one, and its KEY and VALUE are both _MISSING.\n    # 2) the most recently accessed node comes immediately before 'anchor'.\n    # 3) the least recently accessed node comes immediately after 'anchor'.\n    def _init_ll(self):\n        anchor = []\n        anchor[:] = [anchor, anchor, _MISSING, _MISSING]\n        # a link lookup table for finding linked list links in O(1)\n        # time.\n        self._link_lookup = {}\n        self._anchor = anchor\n\n    def _print_ll(self):\n        print('***')\n        for (key, val) in self._get_flattened_ll():\n            print(key, val)\n        print('***')\n        return\n\n    def _get_flattened_ll(self):\n        flattened_list = []\n        link = self._anchor\n        while True:\n            flattened_list.append((link[KEY], link[VALUE]))\n            link = link[NEXT]\n            if link is self._anchor:\n                break\n        return flattened_list\n\n    def _get_link_and_move_to_front_of_ll(self, key):\n        # find what will become the newest link. this may raise a\n        # KeyError, which is useful to __getitem__ and __setitem__\n        newest = self._link_lookup[key]\n\n        # splice out what will become the newest link.\n        newest[PREV][NEXT] = newest[NEXT]\n        newest[NEXT][PREV] = newest[PREV]\n\n        # move what will become the newest link immediately before\n        # anchor (invariant 2)\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        second_newest[NEXT] = anchor[PREV] = newest\n        newest[PREV] = second_newest\n        newest[NEXT] = anchor\n        return newest\n\n    def _set_key_and_add_to_front_of_ll(self, key, value):\n        # create a new link and place it immediately before anchor\n        # (invariant 2).\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        newest = [second_newest, anchor, key, value]\n        second_newest[NEXT] = anchor[PREV] = newest\n        self._link_lookup[key] = newest\n\n    def _set_key_and_evict_last_in_ll(self, key, value):\n        # the link after anchor is the oldest in the linked list\n        # (invariant 3).  the current anchor becomes a link that holds\n        # the newest key, and the oldest link becomes the new anchor\n        # (invariant 1).  now the newest link comes before anchor\n        # (invariant 2).  no links are moved; only their keys\n        # and values are changed.\n        oldanchor = self._anchor\n        oldanchor[KEY] = key\n        oldanchor[VALUE] = value\n\n        self._anchor = anchor = oldanchor[NEXT]\n        evicted = anchor[KEY]\n        anchor[KEY] = anchor[VALUE] = _MISSING\n        del self._link_lookup[evicted]\n        self._link_lookup[key] = oldanchor\n        return evicted\n\n    def _remove_from_ll(self, key):\n        # splice a link out of the list and drop it from our lookup\n        # table.\n        link = self._link_lookup.pop(key)\n        link[PREV][NEXT] = link[NEXT]\n        link[NEXT][PREV] = link[PREV]\n\n    def __setitem__(self, key, value):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                if len(self) < self.max_size:\n                    self._set_key_and_add_to_front_of_ll(key, value)\n                else:\n                    evicted = self._set_key_and_evict_last_in_ll(key, value)\n                    super(LRI, self).__delitem__(evicted)\n                super(LRI, self).__setitem__(key, value)\n            else:\n                link[VALUE] = value\n\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self.soft_miss_count += 1\n            return default\n\n    def __delitem__(self, key):\n        with self._lock:\n            super(LRI, self).__delitem__(key)\n            self._remove_from_ll(key)\n\n###The function: pop###\n    def popitem(self):\n        with self._lock:\n            item = super(LRI, self).popitem()\n            self._remove_from_ll(item[0])\n            return item\n\n    def clear(self):\n        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n\n    def copy(self):\n        return self.__class__(max_size=self.max_size, values=self)\n\n    def setdefault(self, key, default=None):\n        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n\n    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__\n        with self._lock:\n            if E is self:\n                return\n            setitem = self.__setitem__\n            if callable(getattr(E, 'keys', None)):\n                for k in E.keys():\n                    setitem(k, E[k])\n            else:\n                for k, v in E:\n                    setitem(k, v)\n            for k in F:\n                setitem(k, F[k])\n            return\n\n    def __eq__(self, other):\n        with self._lock:\n            if self is other:\n                return True\n            if len(other) != len(self):\n                return False\n            if not isinstance(other, LRI):\n                return other == self\n            return super(LRI, self).__eq__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        val_map = super(LRI, self).__repr__()\n        return ('%s(max_size=%r, on_miss=%r, values=%s)'\n                % (cn, self.max_size, self.on_miss, val_map))\n\n\nclass LRU(LRI):\n    \"\"\"The ``LRU`` is :class:`dict` subtype implementation of the\n    *Least-Recently Used* caching strategy.\n\n    Args:\n        max_size (int): Max number of items to cache. Defaults to ``128``.\n        values (iterable): Initial values for the cache. Defaults to ``None``.\n        on_miss (callable): a callable which accepts a single argument, the\n            key not present in the cache, and returns the value to be cached.\n\n    >>> cap_cache = LRU(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n\n    This cache is also instrumented with statistics\n    collection. ``hit_count``, ``miss_count``, and ``soft_miss_count``\n    are all integer members that can be used to introspect the\n    performance of the cache. (\"Soft\" misses are misses that did not\n    raise :exc:`KeyError`, e.g., ``LRU.get()`` or ``on_miss`` was used to\n    cache a default.\n\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n\n    Other than the size-limiting caching behavior and statistics,\n    ``LRU`` acts like its parent class, the built-in Python :class:`dict`.\n    \"\"\"\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n\n### Cached decorator\n# Key-making technique adapted from Python 3.4's functools\n\nclass _HashedKey(list):\n    \"\"\"The _HashedKey guarantees that hash() will be called no more than once\n    per cached function invocation.\n    \"\"\"\n    __slots__ = 'hash_value'\n\n    def __init__(self, key):\n        self[:] = key\n        self.hash_value = hash(tuple(key))\n\n    def __hash__(self):\n        return self.hash_value\n\n    def __repr__(self):\n        return '%s(%s)' % (self.__class__.__name__, list.__repr__(self))\n\n\ndef make_cache_key(args, kwargs, typed=False,\n                   kwarg_mark=_KWARG_MARK,\n                   fasttypes=frozenset([int, str, frozenset, type(None)])):\n    \"\"\"Make a generic key from a function's positional and keyword\n    arguments, suitable for use in caches. Arguments within *args* and\n    *kwargs* must be `hashable`_. If *typed* is ``True``, ``3`` and\n    ``3.0`` will be treated as separate keys.\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    >>> tuple(make_cache_key(('a', 'b'), {'c': ('d')}))\n    ('a', 'b', _KWARG_MARK, ('c', 'd'))\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n    \"\"\"\n\n    # key = [func_name] if func_name else []\n    # key.extend(args)\n    key = list(args)\n    if kwargs:\n        sorted_items = sorted(kwargs.items())\n        key.append(kwarg_mark)\n        key.extend(sorted_items)\n    if typed:\n        key.extend([type(v) for v in args])\n        if kwargs:\n            key.extend([type(v) for k, v in sorted_items])\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedKey(key)\n\n# for backwards compatibility in case someone was importing it\n_make_cache_key = make_cache_key\n\n\nclass CachedFunction(object):\n    \"\"\"This type is used by :func:`cached`, below. Instances of this\n    class are used to wrap functions in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        if callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be a dict-like object,'\n                            ' or callable returning a dict-like object, not %r'\n                            % cache)\n        else:\n            def _get_cache():\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n\n    def __call__(self, *args, **kwargs):\n        cache = self.get_cache()\n        key = self.key_func(args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\"\n                    % (cn, self.func, self.scoped, self.typed))\n        return \"%s(func=%r)\" % (cn, self.func)\n\n\nclass CachedMethod(object):\n    \"\"\"Similar to :class:`CachedFunction`, this type is used by\n    :func:`cachedmethod` to wrap methods in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        if isinstance(cache, basestring):\n            self.get_cache = attrgetter(cache)\n        elif callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be an attribute name,'\n                            ' dict-like object, or callable returning'\n                            ' a dict-like object, not %r' % cache)\n        else:\n            def _get_cache(obj):\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n        self.bound_to = None\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        cls = self.__class__\n        ret = cls(self.func, self.get_cache, typed=self.typed,\n                  scoped=self.scoped, key=self.key_func)\n        ret.bound_to = obj\n        return ret\n\n    def __call__(self, *args, **kwargs):\n        obj = args[0] if self.bound_to is None else self.bound_to\n        cache = self.get_cache(obj)\n        key_args = (self.bound_to, self.func) + args if self.scoped else args\n        key = self.key_func(key_args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            if self.bound_to is not None:\n                args = (self.bound_to,) + args\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        args = (cn, self.func, self.scoped, self.typed)\n        if self.bound_to is not None:\n            args += (self.bound_to,)\n            return ('<%s func=%r scoped=%r typed=%r bound_to=%r>' % args)\n        return (\"%s(func=%r, scoped=%r, typed=%r)\" % args)\n\n\ndef cached(cache, scoped=True, typed=False, key=None):\n    \"\"\"Cache any function with the cache object of your choosing. Note\n    that the function wrapped should take only `hashable`_ arguments.\n\n    Args:\n        cache (Mapping): Any :class:`dict`-like object suitable for\n            use as a cache. Instances of the :class:`LRU` and\n            :class:`LRI` are good choices, but a plain :class:`dict`\n            can work in some cases, as well. This argument can also be\n            a callable which accepts no arguments and returns a mapping.\n        scoped (bool): Whether the function itself is part of the\n            cache key.  ``True`` by default, different functions will\n            not read one another's cache entries, but can evict one\n            another's results. ``False`` can be useful for certain\n            shared cache use cases. More advanced behavior can be\n            produced through the *key* argument.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n\n    >>> my_cache = LRU()\n    >>> @cached(my_cache)\n    ... def cached_lower(x):\n    ...     return x.lower()\n    ...\n    >>> cached_lower(\"CaChInG's FuN AgAiN!\")\n    \"caching's fun again!\"\n    >>> len(my_cache)\n    1\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n\n    \"\"\"\n    def cached_func_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_func_decorator\n\n\ndef cachedmethod(cache, scoped=True, typed=False, key=None):\n    \"\"\"Similar to :func:`cached`, ``cachedmethod`` is used to cache\n    methods based on their arguments, using any :class:`dict`-like\n    *cache* object.\n\n    Args:\n        cache (str/Mapping/callable): Can be the name of an attribute\n            on the instance, any Mapping/:class:`dict`-like object, or\n            a callable which returns a Mapping.\n        scoped (bool): Whether the method itself and the object it is\n            bound to are part of the cache keys. ``True`` by default,\n            different methods will not read one another's cache\n            results. ``False`` can be useful for certain shared cache\n            use cases. More advanced behavior can be produced through\n            the *key* arguments.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n        key (callable): A callable with a signature that matches\n            :func:`make_cache_key` that returns a tuple of hashable\n            values to be used as the key in the cache.\n\n    >>> class Lowerer(object):\n    ...     def __init__(self):\n    ...         self.cache = LRI()\n    ...\n    ...     @cachedmethod('cache')\n    ...     def lower(self, text):\n    ...         return text.lower()\n    ...\n    >>> lowerer = Lowerer()\n    >>> lowerer.lower('WOW WHO COULD GUESS CACHING COULD BE SO NEAT')\n    'wow who could guess caching could be so neat'\n    >>> len(lowerer.cache)\n    1\n\n    \"\"\"\n    def cached_method_decorator(func):\n        return CachedMethod(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_method_decorator\n\n\nclass cachedproperty(object):\n    \"\"\"The ``cachedproperty`` is used similar to :class:`property`, except\n    that the wrapped method is only called once. This is commonly used\n    to implement lazy attributes.\n\n    After the property has been accessed, the value is stored on the\n    instance itself, using the same name as the cachedproperty. This\n    allows the cache to be cleared with :func:`delattr`, or through\n    manipulating the object's ``__dict__``.\n    \"\"\"\n    def __init__(self, func):\n        self.__doc__ = getattr(func, '__doc__')\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        self.func = func\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        return '<%s func=%s>' % (cn, self.func)\n\n\nclass ThresholdCounter(object):\n    \"\"\"A **bounded** dict-like Mapping from keys to counts. The\n    ThresholdCounter automatically compacts after every (1 /\n    *threshold*) additions, maintaining exact counts for any keys\n    whose count represents at least a *threshold* ratio of the total\n    data. In other words, if a particular key is not present in the\n    ThresholdCounter, its count represents less than *threshold* of\n    the total data.\n\n    >>> tc = ThresholdCounter(threshold=0.1)\n    >>> tc.add(1)\n    >>> tc.items()\n    [(1, 1)]\n    >>> tc.update([2] * 10)\n    >>> tc.get(1)\n    0\n    >>> tc.add(5)\n    >>> 5 in tc\n    True\n    >>> len(list(tc.elements()))\n    11\n\n    As you can see above, the API is kept similar to\n    :class:`collections.Counter`. The most notable feature omissions\n    being that counted items cannot be set directly, uncounted, or\n    removed, as this would disrupt the math.\n\n    Use the ThresholdCounter when you need best-effort long-lived\n    counts for dynamically-keyed data. Without a bounded datastructure\n    such as this one, the dynamic keys often represent a memory leak\n    and can impact application reliability. The ThresholdCounter's\n    item replacement strategy is fully deterministic and can be\n    thought of as *Amortized Least Relevant*. The absolute upper bound\n    of keys it will store is *(2/threshold)*, but realistically\n    *(1/threshold)* is expected for uniformly random datastreams, and\n    one or two orders of magnitude better for real-world data.\n\n    This algorithm is an implementation of the Lossy Counting\n    algorithm described in \"Approximate Frequency Counts over Data\n    Streams\" by Manku & Motwani. Hat tip to Kurt Rose for discovery\n    and initial implementation.\n\n    \"\"\"\n    # TODO: hit_count/miss_count?\n    def __init__(self, threshold=0.001):\n        if not 0 < threshold < 1:\n            raise ValueError('expected threshold between 0 and 1, not: %r'\n                             % threshold)\n\n        self.total = 0\n        self._count_map = {}\n        self._threshold = threshold\n        self._thresh_count = int(1 / threshold)\n        self._cur_bucket = 1\n\n    @property\n    def threshold(self):\n        return self._threshold\n\n    def add(self, key):\n        \"\"\"Increment the count of *key* by 1, automatically adding it if it\n        does not exist.\n\n        Cache compaction is triggered every *1/threshold* additions.\n        \"\"\"\n        self.total += 1\n        try:\n            self._count_map[key][0] += 1\n        except KeyError:\n            self._count_map[key] = [1, self._cur_bucket - 1]\n\n        if self.total % self._thresh_count == 0:\n            self._count_map = dict([(k, v) for k, v in self._count_map.items()\n                                    if sum(v) > self._cur_bucket])\n            self._cur_bucket += 1\n        return\n\n    def elements(self):\n        \"\"\"Return an iterator of all the common elements tracked by the\n        counter. Yields each key as many times as it has been seen.\n        \"\"\"\n        repeaters = itertools.starmap(itertools.repeat, self.iteritems())\n        return itertools.chain.from_iterable(repeaters)\n\n    def most_common(self, n=None):\n        \"\"\"Get the top *n* keys and counts as tuples. If *n* is omitted,\n        returns all the pairs.\n        \"\"\"\n        if n <= 0:\n            return []\n        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        if n is None or n >= len(ret):\n            return ret\n        return ret[:n]\n\n    def get_common_count(self):\n        \"\"\"Get the sum of counts for keys exceeding the configured data\n        threshold.\n        \"\"\"\n        return sum([count for count, _ in self._count_map.values()])\n\n    def get_uncommon_count(self):\n        \"\"\"Get the sum of counts for keys that were culled because the\n        associated counts represented less than the configured\n        threshold. The long-tail counts.\n        \"\"\"\n        return self.total - self.get_common_count()\n\n    def get_commonality(self):\n        \"\"\"Get a float representation of the effective count accuracy. The\n        higher the number, the less uniform the keys being added, and\n        the higher accuracy and efficiency of the ThresholdCounter.\n\n        If a stronger measure of data cardinality is required,\n        consider using hyperloglog.\n        \"\"\"\n        return float(self.get_common_count()) / self.total\n\n    def __getitem__(self, key):\n        return self._count_map[key][0]\n\n    def __len__(self):\n        return len(self._count_map)\n\n    def __contains__(self, key):\n        return key in self._count_map\n\n    def iterkeys(self):\n        return iter(self._count_map)\n\n    def keys(self):\n        return list(self.iterkeys())\n\n    def itervalues(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield count_map[k][0]\n\n    def values(self):\n        return list(self.itervalues())\n\n    def iteritems(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield (k, count_map[k][0])\n\n    def items(self):\n        return list(self.iteritems())\n\n    def get(self, key, default=0):\n        \"Get count for *key*, defaulting to 0.\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def update(self, iterable, **kwargs):\n        \"\"\"Like dict.update() but add counts instead of replacing them, used\n        to add multiple items in one call.\n\n        Source can be an iterable of keys to add, or a mapping of keys\n        to integer counts.\n        \"\"\"\n        if iterable is not None:\n            if callable(getattr(iterable, 'iteritems', None)):\n                for key, count in iterable.iteritems():\n                    for i in xrange(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if kwargs:\n            self.update(kwargs)\n\n\nclass MinIDMap(object):\n    \"\"\"\n    Assigns arbitrary weakref-able objects the smallest possible unique\n    integer IDs, such that no two objects have the same ID at the same\n    time.\n\n    Maps arbitrary hashable objects to IDs.\n\n    Based on https://gist.github.com/kurtbrose/25b48114de216a5e55df\n    \"\"\"\n    def __init__(self):\n        self.mapping = weakref.WeakKeyDictionary()\n        self.ref_map = {}\n        self.free = []\n\n    def get(self, a):\n        try:\n            return self.mapping[a][0]  # if object is mapped, return ID\n        except KeyError:\n            pass\n\n        if self.free:  # if there are any free IDs, use the smallest\n            nxt = heapq.heappop(self.free)\n        else:  # if there are no free numbers, use the next highest ID\n            nxt = len(self.mapping)\n        ref = weakref.ref(a, self._clean)\n        self.mapping[a] = (nxt, ref)\n        self.ref_map[ref] = nxt\n        return nxt\n\n    def drop(self, a):\n        freed, ref = self.mapping[a]\n        del self.mapping[a]\n        del self.ref_map[ref]\n        heapq.heappush(self.free, freed)\n\n    def _clean(self, ref):\n        print(self.ref_map[ref])\n        heapq.heappush(self.free, self.ref_map[ref])\n        del self.ref_map[ref]\n\n    def __contains__(self, a):\n        return a in self.mapping\n\n    def __iter__(self):\n        return iter(self.mapping)\n\n    def __len__(self):\n        return self.mapping.__len__()\n\n    def iteritems(self):\n        return iter((k, self.mapping[k][0]) for k in iter(self.mapping))\n\n\n# end cacheutils.py\n", "prompt": "Please write a python function called 'pop' base the context. Pop the key in the LRI instance and return the corresponding value. If the key is not found and the default value is not passed, the exception is re-raised. This function bypasses the hit count and miss count.\n:param self: LRI, an instance of the LRI class.\n:param key: The key to remove in the instance.\n:param default: The value to return if the key is not found in the instance. Defaults to _UNSET.\n:return: The value corresponding to the key.\n.\n        The context you need to refer to is as follows: # -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, Mahmoud Hashemi\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n#      notice, this list of conditions and the following disclaimer.\n#\n#    * Redistributions in binary form must reproduce the above\n#      copyright notice, this list of conditions and the following\n#      disclaimer in the documentation and/or other materials provided\n#      with the distribution.\n#\n#    * The names of the contributors may not be used to endorse or\n#      promote products derived from this software without specific\n#      prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"``cacheutils`` contains consistent implementations of fundamental\ncache types. Currently there are two to choose from:\n\n  * :class:`LRI` - Least-recently inserted\n  * :class:`LRU` - Least-recently used\n\nBoth caches are :class:`dict` subtypes, designed to be as\ninterchangeable as possible, to facilitate experimentation. A key\npractice with performance enhancement with caching is ensuring that\nthe caching strategy is working. If the cache is constantly missing,\nit is just adding more overhead and code complexity. The standard\nstatistics are:\n\n  * ``hit_count`` - the number of times the queried key has been in\n    the cache\n  * ``miss_count`` - the number of times a key has been absent and/or\n    fetched by the cache\n  * ``soft_miss_count`` - the number of times a key has been absent,\n    but a default has been provided by the caller, as with\n    :meth:`dict.get` and :meth:`dict.setdefault`. Soft misses are a\n    subset of misses, so this number is always less than or equal to\n    ``miss_count``.\n\nAdditionally, ``cacheutils`` provides :class:`ThresholdCounter`, a\ncache-like bounded counter useful for online statistics collection.\n\nLearn more about `caching algorithms on Wikipedia\n<https://en.wikipedia.org/wiki/Cache_algorithms#Examples>`_.\n\n\"\"\"\n\n# TODO: TimedLRI\n# TODO: support 0 max_size?\n\n\nimport heapq\nimport weakref\nimport itertools\nfrom operator import attrgetter\n\ntry:\n    from threading import RLock\nexcept Exception:\n    class RLock(object):\n        'Dummy reentrant lock for builds without threads'\n        def __enter__(self):\n            pass\n\n        def __exit__(self, exctype, excinst, exctb):\n            pass\n\ntry:\n    from .typeutils import make_sentinel\n    _MISSING = make_sentinel(var_name='_MISSING')\n    _KWARG_MARK = make_sentinel(var_name='_KWARG_MARK')\nexcept ImportError:\n    _MISSING = object()\n    _KWARG_MARK = object()\n\ntry:\n    xrange\nexcept NameError:\n    # py3\n    xrange = range\n    unicode, str, bytes, basestring = str, bytes, bytes, (str, bytes)\n\nPREV, NEXT, KEY, VALUE = range(4)   # names for the link fields\nDEFAULT_MAX_SIZE = 128\n\n\nclass LRI(dict):\n    \"\"\"The ``LRI`` implements the basic *Least Recently Inserted* strategy to\n    caching. One could also think of this as a ``SizeLimitedDefaultDict``.\n\n    *on_miss* is a callable that accepts the missing key (as opposed\n    to :class:`collections.defaultdict`'s \"default_factory\", which\n    accepts no arguments.) Also note that, like the :class:`LRI`,\n    the ``LRI`` is instrumented with statistics tracking.\n\n    >>> cap_cache = LRI(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n    \"\"\"\n    def __init__(self, max_size=DEFAULT_MAX_SIZE, values=None,\n                 on_miss=None):\n        if max_size <= 0:\n            raise ValueError('expected max_size > 0, not %r' % max_size)\n        self.hit_count = self.miss_count = self.soft_miss_count = 0\n        self.max_size = max_size\n        self._lock = RLock()\n        self._init_ll()\n\n        if on_miss is not None and not callable(on_miss):\n            raise TypeError('expected on_miss to be a callable'\n                            ' (or None), not %r' % on_miss)\n        self.on_miss = on_miss\n\n        if values:\n            self.update(values)\n\n    # TODO: fromkeys()?\n\n    # linked list manipulation methods.\n    #\n    # invariants:\n    # 1) 'anchor' is the sentinel node in the doubly linked list.  there is\n    #    always only one, and its KEY and VALUE are both _MISSING.\n    # 2) the most recently accessed node comes immediately before 'anchor'.\n    # 3) the least recently accessed node comes immediately after 'anchor'.\n    def _init_ll(self):\n        anchor = []\n        anchor[:] = [anchor, anchor, _MISSING, _MISSING]\n        # a link lookup table for finding linked list links in O(1)\n        # time.\n        self._link_lookup = {}\n        self._anchor = anchor\n\n    def _print_ll(self):\n        print('***')\n        for (key, val) in self._get_flattened_ll():\n            print(key, val)\n        print('***')\n        return\n\n    def _get_flattened_ll(self):\n        flattened_list = []\n        link = self._anchor\n        while True:\n            flattened_list.append((link[KEY], link[VALUE]))\n            link = link[NEXT]\n            if link is self._anchor:\n                break\n        return flattened_list\n\n    def _get_link_and_move_to_front_of_ll(self, key):\n        # find what will become the newest link. this may raise a\n        # KeyError, which is useful to __getitem__ and __setitem__\n        newest = self._link_lookup[key]\n\n        # splice out what will become the newest link.\n        newest[PREV][NEXT] = newest[NEXT]\n        newest[NEXT][PREV] = newest[PREV]\n\n        # move what will become the newest link immediately before\n        # anchor (invariant 2)\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        second_newest[NEXT] = anchor[PREV] = newest\n        newest[PREV] = second_newest\n        newest[NEXT] = anchor\n        return newest\n\n    def _set_key_and_add_to_front_of_ll(self, key, value):\n        # create a new link and place it immediately before anchor\n        # (invariant 2).\n        anchor = self._anchor\n        second_newest = anchor[PREV]\n        newest = [second_newest, anchor, key, value]\n        second_newest[NEXT] = anchor[PREV] = newest\n        self._link_lookup[key] = newest\n\n    def _set_key_and_evict_last_in_ll(self, key, value):\n        # the link after anchor is the oldest in the linked list\n        # (invariant 3).  the current anchor becomes a link that holds\n        # the newest key, and the oldest link becomes the new anchor\n        # (invariant 1).  now the newest link comes before anchor\n        # (invariant 2).  no links are moved; only their keys\n        # and values are changed.\n        oldanchor = self._anchor\n        oldanchor[KEY] = key\n        oldanchor[VALUE] = value\n\n        self._anchor = anchor = oldanchor[NEXT]\n        evicted = anchor[KEY]\n        anchor[KEY] = anchor[VALUE] = _MISSING\n        del self._link_lookup[evicted]\n        self._link_lookup[key] = oldanchor\n        return evicted\n\n    def _remove_from_ll(self, key):\n        # splice a link out of the list and drop it from our lookup\n        # table.\n        link = self._link_lookup.pop(key)\n        link[PREV][NEXT] = link[NEXT]\n        link[NEXT][PREV] = link[PREV]\n\n    def __setitem__(self, key, value):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                if len(self) < self.max_size:\n                    self._set_key_and_add_to_front_of_ll(key, value)\n                else:\n                    evicted = self._set_key_and_evict_last_in_ll(key, value)\n                    super(LRI, self).__delitem__(evicted)\n                super(LRI, self).__setitem__(key, value)\n            else:\n                link[VALUE] = value\n\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self.soft_miss_count += 1\n            return default\n\n    def __delitem__(self, key):\n        with self._lock:\n            super(LRI, self).__delitem__(key)\n            self._remove_from_ll(key)\n\n###The function: pop###\n    def popitem(self):\n        with self._lock:\n            item = super(LRI, self).popitem()\n            self._remove_from_ll(item[0])\n            return item\n\n    def clear(self):\n        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n\n    def copy(self):\n        return self.__class__(max_size=self.max_size, values=self)\n\n    def setdefault(self, key, default=None):\n        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n\n    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__\n        with self._lock:\n            if E is self:\n                return\n            setitem = self.__setitem__\n            if callable(getattr(E, 'keys', None)):\n                for k in E.keys():\n                    setitem(k, E[k])\n            else:\n                for k, v in E:\n                    setitem(k, v)\n            for k in F:\n                setitem(k, F[k])\n            return\n\n    def __eq__(self, other):\n        with self._lock:\n            if self is other:\n                return True\n            if len(other) != len(self):\n                return False\n            if not isinstance(other, LRI):\n                return other == self\n            return super(LRI, self).__eq__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        val_map = super(LRI, self).__repr__()\n        return ('%s(max_size=%r, on_miss=%r, values=%s)'\n                % (cn, self.max_size, self.on_miss, val_map))\n\n\nclass LRU(LRI):\n    \"\"\"The ``LRU`` is :class:`dict` subtype implementation of the\n    *Least-Recently Used* caching strategy.\n\n    Args:\n        max_size (int): Max number of items to cache. Defaults to ``128``.\n        values (iterable): Initial values for the cache. Defaults to ``None``.\n        on_miss (callable): a callable which accepts a single argument, the\n            key not present in the cache, and returns the value to be cached.\n\n    >>> cap_cache = LRU(max_size=2)\n    >>> cap_cache['a'], cap_cache['b'] = 'A', 'B'\n    >>> from pprint import pprint as pp\n    >>> pp(dict(cap_cache))\n    {'a': 'A', 'b': 'B'}\n    >>> [cap_cache['b'] for i in range(3)][0]\n    'B'\n    >>> cap_cache['c'] = 'C'\n    >>> print(cap_cache.get('a'))\n    None\n\n    This cache is also instrumented with statistics\n    collection. ``hit_count``, ``miss_count``, and ``soft_miss_count``\n    are all integer members that can be used to introspect the\n    performance of the cache. (\"Soft\" misses are misses that did not\n    raise :exc:`KeyError`, e.g., ``LRU.get()`` or ``on_miss`` was used to\n    cache a default.\n\n    >>> cap_cache.hit_count, cap_cache.miss_count, cap_cache.soft_miss_count\n    (3, 1, 1)\n\n    Other than the size-limiting caching behavior and statistics,\n    ``LRU`` acts like its parent class, the built-in Python :class:`dict`.\n    \"\"\"\n    def __getitem__(self, key):\n        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n\n\n### Cached decorator\n# Key-making technique adapted from Python 3.4's functools\n\nclass _HashedKey(list):\n    \"\"\"The _HashedKey guarantees that hash() will be called no more than once\n    per cached function invocation.\n    \"\"\"\n    __slots__ = 'hash_value'\n\n    def __init__(self, key):\n        self[:] = key\n        self.hash_value = hash(tuple(key))\n\n    def __hash__(self):\n        return self.hash_value\n\n    def __repr__(self):\n        return '%s(%s)' % (self.__class__.__name__, list.__repr__(self))\n\n\ndef make_cache_key(args, kwargs, typed=False,\n                   kwarg_mark=_KWARG_MARK,\n                   fasttypes=frozenset([int, str, frozenset, type(None)])):\n    \"\"\"Make a generic key from a function's positional and keyword\n    arguments, suitable for use in caches. Arguments within *args* and\n    *kwargs* must be `hashable`_. If *typed* is ``True``, ``3`` and\n    ``3.0`` will be treated as separate keys.\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    >>> tuple(make_cache_key(('a', 'b'), {'c': ('d')}))\n    ('a', 'b', _KWARG_MARK, ('c', 'd'))\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n    \"\"\"\n\n    # key = [func_name] if func_name else []\n    # key.extend(args)\n    key = list(args)\n    if kwargs:\n        sorted_items = sorted(kwargs.items())\n        key.append(kwarg_mark)\n        key.extend(sorted_items)\n    if typed:\n        key.extend([type(v) for v in args])\n        if kwargs:\n            key.extend([type(v) for k, v in sorted_items])\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedKey(key)\n\n# for backwards compatibility in case someone was importing it\n_make_cache_key = make_cache_key\n\n\nclass CachedFunction(object):\n    \"\"\"This type is used by :func:`cached`, below. Instances of this\n    class are used to wrap functions in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        if callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be a dict-like object,'\n                            ' or callable returning a dict-like object, not %r'\n                            % cache)\n        else:\n            def _get_cache():\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n\n    def __call__(self, *args, **kwargs):\n        cache = self.get_cache()\n        key = self.key_func(args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\"\n                    % (cn, self.func, self.scoped, self.typed))\n        return \"%s(func=%r)\" % (cn, self.func)\n\n\nclass CachedMethod(object):\n    \"\"\"Similar to :class:`CachedFunction`, this type is used by\n    :func:`cachedmethod` to wrap methods in caching logic.\n    \"\"\"\n    def __init__(self, func, cache, scoped=True, typed=False, key=None):\n        self.func = func\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        if isinstance(cache, basestring):\n            self.get_cache = attrgetter(cache)\n        elif callable(cache):\n            self.get_cache = cache\n        elif not (callable(getattr(cache, '__getitem__', None))\n                  and callable(getattr(cache, '__setitem__', None))):\n            raise TypeError('expected cache to be an attribute name,'\n                            ' dict-like object, or callable returning'\n                            ' a dict-like object, not %r' % cache)\n        else:\n            def _get_cache(obj):\n                return cache\n            self.get_cache = _get_cache\n        self.scoped = scoped\n        self.typed = typed\n        self.key_func = key or make_cache_key\n        self.bound_to = None\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        cls = self.__class__\n        ret = cls(self.func, self.get_cache, typed=self.typed,\n                  scoped=self.scoped, key=self.key_func)\n        ret.bound_to = obj\n        return ret\n\n    def __call__(self, *args, **kwargs):\n        obj = args[0] if self.bound_to is None else self.bound_to\n        cache = self.get_cache(obj)\n        key_args = (self.bound_to, self.func) + args if self.scoped else args\n        key = self.key_func(key_args, kwargs, typed=self.typed)\n        try:\n            ret = cache[key]\n        except KeyError:\n            if self.bound_to is not None:\n                args = (self.bound_to,) + args\n            ret = cache[key] = self.func(*args, **kwargs)\n        return ret\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        args = (cn, self.func, self.scoped, self.typed)\n        if self.bound_to is not None:\n            args += (self.bound_to,)\n            return ('<%s func=%r scoped=%r typed=%r bound_to=%r>' % args)\n        return (\"%s(func=%r, scoped=%r, typed=%r)\" % args)\n\n\ndef cached(cache, scoped=True, typed=False, key=None):\n    \"\"\"Cache any function with the cache object of your choosing. Note\n    that the function wrapped should take only `hashable`_ arguments.\n\n    Args:\n        cache (Mapping): Any :class:`dict`-like object suitable for\n            use as a cache. Instances of the :class:`LRU` and\n            :class:`LRI` are good choices, but a plain :class:`dict`\n            can work in some cases, as well. This argument can also be\n            a callable which accepts no arguments and returns a mapping.\n        scoped (bool): Whether the function itself is part of the\n            cache key.  ``True`` by default, different functions will\n            not read one another's cache entries, but can evict one\n            another's results. ``False`` can be useful for certain\n            shared cache use cases. More advanced behavior can be\n            produced through the *key* argument.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n\n    >>> my_cache = LRU()\n    >>> @cached(my_cache)\n    ... def cached_lower(x):\n    ...     return x.lower()\n    ...\n    >>> cached_lower(\"CaChInG's FuN AgAiN!\")\n    \"caching's fun again!\"\n    >>> len(my_cache)\n    1\n\n    .. _hashable: https://docs.python.org/2/glossary.html#term-hashable\n\n    \"\"\"\n    def cached_func_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_func_decorator\n\n\ndef cachedmethod(cache, scoped=True, typed=False, key=None):\n    \"\"\"Similar to :func:`cached`, ``cachedmethod`` is used to cache\n    methods based on their arguments, using any :class:`dict`-like\n    *cache* object.\n\n    Args:\n        cache (str/Mapping/callable): Can be the name of an attribute\n            on the instance, any Mapping/:class:`dict`-like object, or\n            a callable which returns a Mapping.\n        scoped (bool): Whether the method itself and the object it is\n            bound to are part of the cache keys. ``True`` by default,\n            different methods will not read one another's cache\n            results. ``False`` can be useful for certain shared cache\n            use cases. More advanced behavior can be produced through\n            the *key* arguments.\n        typed (bool): Whether to factor argument types into the cache\n            check. Default ``False``, setting to ``True`` causes the\n            cache keys for ``3`` and ``3.0`` to be considered unequal.\n        key (callable): A callable with a signature that matches\n            :func:`make_cache_key` that returns a tuple of hashable\n            values to be used as the key in the cache.\n\n    >>> class Lowerer(object):\n    ...     def __init__(self):\n    ...         self.cache = LRI()\n    ...\n    ...     @cachedmethod('cache')\n    ...     def lower(self, text):\n    ...         return text.lower()\n    ...\n    >>> lowerer = Lowerer()\n    >>> lowerer.lower('WOW WHO COULD GUESS CACHING COULD BE SO NEAT')\n    'wow who could guess caching could be so neat'\n    >>> len(lowerer.cache)\n    1\n\n    \"\"\"\n    def cached_method_decorator(func):\n        return CachedMethod(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_method_decorator\n\n\nclass cachedproperty(object):\n    \"\"\"The ``cachedproperty`` is used similar to :class:`property`, except\n    that the wrapped method is only called once. This is commonly used\n    to implement lazy attributes.\n\n    After the property has been accessed, the value is stored on the\n    instance itself, using the same name as the cachedproperty. This\n    allows the cache to be cleared with :func:`delattr`, or through\n    manipulating the object's ``__dict__``.\n    \"\"\"\n    def __init__(self, func):\n        self.__doc__ = getattr(func, '__doc__')\n        self.__isabstractmethod__ = getattr(func, '__isabstractmethod__', False)\n        self.func = func\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n\n    def __repr__(self):\n        cn = self.__class__.__name__\n        return '<%s func=%s>' % (cn, self.func)\n\n\nclass ThresholdCounter(object):\n    \"\"\"A **bounded** dict-like Mapping from keys to counts. The\n    ThresholdCounter automatically compacts after every (1 /\n    *threshold*) additions, maintaining exact counts for any keys\n    whose count represents at least a *threshold* ratio of the total\n    data. In other words, if a particular key is not present in the\n    ThresholdCounter, its count represents less than *threshold* of\n    the total data.\n\n    >>> tc = ThresholdCounter(threshold=0.1)\n    >>> tc.add(1)\n    >>> tc.items()\n    [(1, 1)]\n    >>> tc.update([2] * 10)\n    >>> tc.get(1)\n    0\n    >>> tc.add(5)\n    >>> 5 in tc\n    True\n    >>> len(list(tc.elements()))\n    11\n\n    As you can see above, the API is kept similar to\n    :class:`collections.Counter`. The most notable feature omissions\n    being that counted items cannot be set directly, uncounted, or\n    removed, as this would disrupt the math.\n\n    Use the ThresholdCounter when you need best-effort long-lived\n    counts for dynamically-keyed data. Without a bounded datastructure\n    such as this one, the dynamic keys often represent a memory leak\n    and can impact application reliability. The ThresholdCounter's\n    item replacement strategy is fully deterministic and can be\n    thought of as *Amortized Least Relevant*. The absolute upper bound\n    of keys it will store is *(2/threshold)*, but realistically\n    *(1/threshold)* is expected for uniformly random datastreams, and\n    one or two orders of magnitude better for real-world data.\n\n    This algorithm is an implementation of the Lossy Counting\n    algorithm described in \"Approximate Frequency Counts over Data\n    Streams\" by Manku & Motwani. Hat tip to Kurt Rose for discovery\n    and initial implementation.\n\n    \"\"\"\n    # TODO: hit_count/miss_count?\n    def __init__(self, threshold=0.001):\n        if not 0 < threshold < 1:\n            raise ValueError('expected threshold between 0 and 1, not: %r'\n                             % threshold)\n\n        self.total = 0\n        self._count_map = {}\n        self._threshold = threshold\n        self._thresh_count = int(1 / threshold)\n        self._cur_bucket = 1\n\n    @property\n    def threshold(self):\n        return self._threshold\n\n    def add(self, key):\n        \"\"\"Increment the count of *key* by 1, automatically adding it if it\n        does not exist.\n\n        Cache compaction is triggered every *1/threshold* additions.\n        \"\"\"\n        self.total += 1\n        try:\n            self._count_map[key][0] += 1\n        except KeyError:\n            self._count_map[key] = [1, self._cur_bucket - 1]\n\n        if self.total % self._thresh_count == 0:\n            self._count_map = dict([(k, v) for k, v in self._count_map.items()\n                                    if sum(v) > self._cur_bucket])\n            self._cur_bucket += 1\n        return\n\n    def elements(self):\n        \"\"\"Return an iterator of all the common elements tracked by the\n        counter. Yields each key as many times as it has been seen.\n        \"\"\"\n        repeaters = itertools.starmap(itertools.repeat, self.iteritems())\n        return itertools.chain.from_iterable(repeaters)\n\n    def most_common(self, n=None):\n        \"\"\"Get the top *n* keys and counts as tuples. If *n* is omitted,\n        returns all the pairs.\n        \"\"\"\n        if n <= 0:\n            return []\n        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        if n is None or n >= len(ret):\n            return ret\n        return ret[:n]\n\n    def get_common_count(self):\n        \"\"\"Get the sum of counts for keys exceeding the configured data\n        threshold.\n        \"\"\"\n        return sum([count for count, _ in self._count_map.values()])\n\n    def get_uncommon_count(self):\n        \"\"\"Get the sum of counts for keys that were culled because the\n        associated counts represented less than the configured\n        threshold. The long-tail counts.\n        \"\"\"\n        return self.total - self.get_common_count()\n\n    def get_commonality(self):\n        \"\"\"Get a float representation of the effective count accuracy. The\n        higher the number, the less uniform the keys being added, and\n        the higher accuracy and efficiency of the ThresholdCounter.\n\n        If a stronger measure of data cardinality is required,\n        consider using hyperloglog.\n        \"\"\"\n        return float(self.get_common_count()) / self.total\n\n    def __getitem__(self, key):\n        return self._count_map[key][0]\n\n    def __len__(self):\n        return len(self._count_map)\n\n    def __contains__(self, key):\n        return key in self._count_map\n\n    def iterkeys(self):\n        return iter(self._count_map)\n\n    def keys(self):\n        return list(self.iterkeys())\n\n    def itervalues(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield count_map[k][0]\n\n    def values(self):\n        return list(self.itervalues())\n\n    def iteritems(self):\n        count_map = self._count_map\n        for k in count_map:\n            yield (k, count_map[k][0])\n\n    def items(self):\n        return list(self.iteritems())\n\n    def get(self, key, default=0):\n        \"Get count for *key*, defaulting to 0.\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def update(self, iterable, **kwargs):\n        \"\"\"Like dict.update() but add counts instead of replacing them, used\n        to add multiple items in one call.\n\n        Source can be an iterable of keys to add, or a mapping of keys\n        to integer counts.\n        \"\"\"\n        if iterable is not None:\n            if callable(getattr(iterable, 'iteritems', None)):\n                for key, count in iterable.iteritems():\n                    for i in xrange(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if kwargs:\n            self.update(kwargs)\n\n\nclass MinIDMap(object):\n    \"\"\"\n    Assigns arbitrary weakref-able objects the smallest possible unique\n    integer IDs, such that no two objects have the same ID at the same\n    time.\n\n    Maps arbitrary hashable objects to IDs.\n\n    Based on https://gist.github.com/kurtbrose/25b48114de216a5e55df\n    \"\"\"\n    def __init__(self):\n        self.mapping = weakref.WeakKeyDictionary()\n        self.ref_map = {}\n        self.free = []\n\n    def get(self, a):\n        try:\n            return self.mapping[a][0]  # if object is mapped, return ID\n        except KeyError:\n            pass\n\n        if self.free:  # if there are any free IDs, use the smallest\n            nxt = heapq.heappop(self.free)\n        else:  # if there are no free numbers, use the next highest ID\n            nxt = len(self.mapping)\n        ref = weakref.ref(a, self._clean)\n        self.mapping[a] = (nxt, ref)\n        self.ref_map[ref] = nxt\n        return nxt\n\n    def drop(self, a):\n        freed, ref = self.mapping[a]\n        del self.mapping[a]\n        del self.ref_map[ref]\n        heapq.heappush(self.free, freed)\n\n    def _clean(self, ref):\n        print(self.ref_map[ref])\n        heapq.heappush(self.free, self.ref_map[ref])\n        del self.ref_map[ref]\n\n    def __contains__(self, a):\n        return a in self.mapping\n\n    def __iter__(self):\n        return iter(self.mapping)\n\n    def __len__(self):\n        return self.mapping.__len__()\n\n    def iteritems(self):\n        return iter((k, self.mapping[k][0]) for k in iter(self.mapping))\n\n\n# end cacheutils.py\n", "test_list": ["def test_lru_basic():\n    lru = LRU(max_size=1)\n    repr(lru)\n    lru['hi'] = 0\n    lru['bye'] = 1\n    assert len(lru) == 1\n    lru['bye']\n    assert lru.get('hi') is None\n    del lru['bye']\n    assert 'bye' not in lru\n    assert len(lru) == 0\n    assert not lru\n    try:\n        lru.pop('bye')\n    except KeyError:\n        pass\n    else:\n        assert False\n    default = object()\n    assert lru.pop('bye', default) is default\n    try:\n        lru.popitem()\n    except KeyError:\n        pass\n    else:\n        assert False\n    lru['another'] = 1\n    assert lru.popitem() == ('another', 1)\n    lru['yet_another'] = 2\n    assert lru.pop('yet_another') == 2\n    lru['yet_another'] = 3\n    assert lru.pop('yet_another', default) == 3\n    lru['yet_another'] = 4\n    lru.clear()\n    assert not lru\n    lru['yet_another'] = 5\n    second_lru = LRU(max_size=1)\n    assert lru.copy() == lru\n    second_lru['yet_another'] = 5\n    assert second_lru == lru\n    assert lru == second_lru\n    lru.update(LRU(max_size=2, values=[('a', 1), ('b', 2)]))\n    assert len(lru) == 1\n    assert 'yet_another' not in lru\n    lru.setdefault('x', 2)\n    assert dict(lru) == {'x': 2}\n    lru.setdefault('x', 3)\n    assert dict(lru) == {'x': 2}\n    assert lru != second_lru\n    assert second_lru != lru"], "requirements": {"Input-Output Conditions": {"requirement": "The 'pop' function should accept a key and an optional default value, returning the value associated with the key if it exists, or the default value if the key is not found.", "unit_test": ["def test_pop_with_default():\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    assert lri.pop('key1') == 'value1'\n    default = 'default_value'\n    assert lri.pop('non_existent_key', default) == default"], "test": "tests/test_cacheutils.py::test_pop_with_default"}, "Exception Handling": {"requirement": "The 'pop' function should raise a KeyError if the key is not found and no default value is provided.", "unit_test": ["def test_pop_raises_keyerror():\n    lri = LRI(max_size=2)\n    try:\n        lri.pop('non_existent_key')\n    except KeyError:\n        pass\n    else:\n        assert False, 'Expected KeyError not raised'"], "test": "tests/test_cacheutils.py::test_pop_raises_keyerror"}, "Edge Case Handling": {"requirement": "The 'pop' function should handle the case where the cache is empty and a key is requested.", "unit_test": ["def test_pop_empty_cache():\n    lri = LRI(max_size=2)\n    default = 'default_value'\n    assert lri.pop('any_key', default) == default"], "test": "tests/test_cacheutils.py::test_pop_empty_cache"}, "Functionality Extension": {"requirement": "Extend the 'pop' function to log a message whenever a key is successfully removed from the cache.", "unit_test": ["def test_pop_logs_message():\n    import logging\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    with self.assertLogs(level='INFO') as log:\n        lri.pop('key1')\n    assert any('key1 removed' in message for message in log.output)"], "test": "tests/test_cacheutils.py::test_pop_logs_message"}, "Annotation Coverage": {"requirement": "Ensure that the 'pop' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_pop_annotations():\n    from typing import get_type_hints\n    hints = get_type_hints(LRI.pop)\n    assert hints['key'] == str\n    assert hints['default'] == object\n    assert hints['return'] == object"], "test": "tests/test_cacheutils.py::test_pop_annotations"}, "Code Complexity": {"requirement": "Ensure that the 'pop' function maintains a cyclomatic complexity of 5 or less.", "unit_test": ["def test_pop_cyclomatic_complexity():\n    import radon.complexity as rc\n    code = '''def pop(self, key, default=_UNSET): ... '''\n    complexity = rc.cc_visit_ast(rc.parse(code))\n    assert complexity[0].complexity <= 3"], "test": "tests/test_cacheutils.py::test_pop_cyclomatic_complexity"}, "Code Standard": {"requirement": "Ensure that the 'pop' function adheres to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_pop_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_pop_function.py'])\n    assert result.total_errors == 0, 'PEP 8 violations found'"], "test": "tests/test_cacheutils.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'pop' function uses the '_lock' attribute to ensure thread safety.", "unit_test": ["def test_pop_uses_lock():\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    with lri._lock:\n        assert lri.pop('key1') == 'value1'"], "test": "tests/test_cacheutils.py::test_pop_uses_lock"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'pop' function correctly uses the '_remove_from_ll' method to maintain the linked list structure.", "unit_test": ["def test_pop_correct_ll_removal():\n    lri = LRI(max_size=2)\n    lri['key1'] = 'value1'\n    lri.pop('key1')\n    assert 'key1' not in lri._link_lookup"], "test": "tests/test_cacheutils.py::test_pop_correct_ll_removal"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "type": "method", "project_path": "Communications/IMAPClient", "completion_path": "Communications/IMAPClient/imapclient/imapclient.py", "signature_position": [1488, 1488], "body_position": [1521, 1528], "dependency": {"intra_class": ["imapclient.imapclient.IMAPClient._command_and_check", "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "imapclient.imapclient.IMAPClient._imap", "imapclient.imapclient.IMAPClient.use_uid"], "intra_file": ["imapclient.imapclient.join_message_ids"], "cross_file": []}, "requirement": {"Functionality": "This function is used to expunge messages from the selected folder in an IMAP client. If no messages are specified, it removes all messages with the \"\\Deleted\" flag set. If messages are specified, it removes the specified messages with the \"\\Deleted\" flag set. The function returns the server response message followed by a list of expunge responses. The implementation takes into account whether the client is using UIDs or not.", "Arguments": ":param self: IMAPClient. An instance of the IMAPClient class.\n:param messages: List of int or str. The messages to be expunged. Defaults to None.\n:return: Tuple. The server response message followed by a list of expunge responses if no messages are specified. None if messages are specified."}, "tests": ["tests/test_imapclient.py::TestExpunge::test_expunge", "tests/test_imapclient.py::TestExpunge::test_id_expunge"], "indent": 4, "domain": "Communications", "code": "    def expunge(self, messages=None):\n        \"\"\"Use of the *messages* argument is discouraged.\n        Please see the ``uid_expunge`` method instead.\n\n        When, no *messages* are specified, remove all messages\n        from the currently selected folder that have the\n        ``\\\\Deleted`` flag set.\n\n        The return value is the server response message\n        followed by a list of expunge responses. For example::\n\n            ('Expunge completed.',\n             [(2, 'EXPUNGE'),\n              (1, 'EXPUNGE'),\n              (0, 'RECENT')])\n\n        In this case, the responses indicate that the message with\n        sequence numbers 2 and 1 where deleted, leaving no recent\n        messages in the folder.\n\n        See :rfc:`3501#section-6.4.3` section 6.4.3 and\n        :rfc:`3501#section-7.4.1` section 7.4.1 for more details.\n\n        When *messages* are specified, remove the specified messages\n        from the selected folder, provided those messages also have\n        the ``\\\\Deleted`` flag set. The return value is ``None`` in\n        this case.\n\n        Expunging messages by id(s) requires that *use_uid* is\n        ``True`` for the client.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        if messages:\n            if not self.use_uid:\n                raise ValueError(\"cannot EXPUNGE by ID when not using uids\")\n            return self._command_and_check(\n                \"EXPUNGE\", join_message_ids(messages), uid=True\n            )\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")\n", "context": "# Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport dataclasses\nimport functools\nimport imaplib\nimport itertools\nimport re\nimport select\nimport socket\nimport ssl as ssl_lib\nimport sys\nimport warnings\nfrom datetime import date, datetime\nfrom logging import getLogger, LoggerAdapter\nfrom operator import itemgetter\nfrom typing import List, Optional\n\nfrom . import exceptions, imap4, tls\nfrom .datetime_util import datetime_to_INTERNALDATE\nfrom .imap_utf7 import decode as decode_utf7\nfrom .imap_utf7 import encode as encode_utf7\nfrom .response_parser import parse_fetch_response, parse_message_list, parse_response\nfrom .util import assert_imap_protocol, to_bytes, to_unicode\n\nif hasattr(select, \"poll\"):\n    POLL_SUPPORT = True\nelse:\n    # Fallback to select() on systems that don't support poll()\n    POLL_SUPPORT = False\n\n\nlogger = getLogger(__name__)\n\n__all__ = [\n    \"IMAPClient\",\n    \"SocketTimeout\",\n    \"DELETED\",\n    \"SEEN\",\n    \"ANSWERED\",\n    \"FLAGGED\",\n    \"DRAFT\",\n    \"RECENT\",\n]\n\n\n# We also offer the gmail-specific XLIST command...\nif \"XLIST\" not in imaplib.Commands:\n    imaplib.Commands[\"XLIST\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ...and IDLE\nif \"IDLE\" not in imaplib.Commands:\n    imaplib.Commands[\"IDLE\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ..and STARTTLS\nif \"STARTTLS\" not in imaplib.Commands:\n    imaplib.Commands[\"STARTTLS\"] = (\"NONAUTH\",)\n\n# ...and ID. RFC2971 says that this command is valid in all states,\n# but not that some servers (*cough* FastMail *cough*) don't seem to\n# accept it in state NONAUTH.\nif \"ID\" not in imaplib.Commands:\n    imaplib.Commands[\"ID\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ... and UNSELECT. RFC3691 does not specify the state but there is no\n# reason to use the command without AUTH state and a mailbox selected.\nif \"UNSELECT\" not in imaplib.Commands:\n    imaplib.Commands[\"UNSELECT\"] = (\"AUTH\", \"SELECTED\")\n\n# .. and ENABLE.\nif \"ENABLE\" not in imaplib.Commands:\n    imaplib.Commands[\"ENABLE\"] = (\"AUTH\",)\n\n# .. and MOVE for RFC6851.\nif \"MOVE\" not in imaplib.Commands:\n    imaplib.Commands[\"MOVE\"] = (\"AUTH\", \"SELECTED\")\n\n# System flags\nDELETED = rb\"\\Deleted\"\nSEEN = rb\"\\Seen\"\nANSWERED = rb\"\\Answered\"\nFLAGGED = rb\"\\Flagged\"\nDRAFT = rb\"\\Draft\"\nRECENT = rb\"\\Recent\"  # This flag is read-only\n\n# Special folders, see RFC6154\n# \\Flagged is omitted because it is the same as the flag defined above\nALL = rb\"\\All\"\nARCHIVE = rb\"\\Archive\"\nDRAFTS = rb\"\\Drafts\"\nJUNK = rb\"\\Junk\"\nSENT = rb\"\\Sent\"\nTRASH = rb\"\\Trash\"\n\n# Personal namespaces that are common among providers\n# used as a fallback when the server does not support the NAMESPACE capability\n_POPULAR_PERSONAL_NAMESPACES = ((\"\", \"\"), (\"INBOX.\", \".\"))\n\n# Names of special folders that are common among providers\n_POPULAR_SPECIAL_FOLDERS = {\n    SENT: (\"Sent\", \"Sent Items\", \"Sent items\"),\n    DRAFTS: (\"Drafts\",),\n    ARCHIVE: (\"Archive\",),\n    TRASH: (\"Trash\", \"Deleted Items\", \"Deleted Messages\", \"Deleted\"),\n    JUNK: (\"Junk\", \"Spam\"),\n}\n\n_RE_SELECT_RESPONSE = re.compile(rb\"\\[(?P<key>[A-Z-]+)( \\((?P<data>.*)\\))?\\]\")\n\n\nclass Namespace(tuple):\n    def __new__(cls, personal, other, shared):\n        return tuple.__new__(cls, (personal, other, shared))\n\n    personal = property(itemgetter(0))\n    other = property(itemgetter(1))\n    shared = property(itemgetter(2))\n\n\n@dataclasses.dataclass\nclass SocketTimeout:\n    \"\"\"Represents timeout configuration for an IMAP connection.\n\n    :ivar connect: maximum time to wait for a connection attempt to remote server\n    :ivar read: maximum time to wait for performing a read/write operation\n\n    As an example, ``SocketTimeout(connect=15, read=60)`` will make the socket\n    timeout if the connection takes more than 15 seconds to establish but\n    read/write operations can take up to 60 seconds once the connection is done.\n    \"\"\"\n\n    connect: float\n    read: float\n\n\n@dataclasses.dataclass\nclass MailboxQuotaRoots:\n    \"\"\"Quota roots associated with a mailbox.\n\n    Represents the response of a GETQUOTAROOT command.\n\n    :ivar mailbox: the mailbox\n    :ivar quota_roots: list of quota roots associated with the mailbox\n    \"\"\"\n\n    mailbox: str\n    quota_roots: List[str]\n\n\n@dataclasses.dataclass\nclass Quota:\n    \"\"\"Resource quota.\n\n    Represents the response of a GETQUOTA command.\n\n    :ivar quota_roots: the quota roots for which the limit apply\n    :ivar resource: the resource being limited (STORAGE, MESSAGES...)\n    :ivar usage: the current usage of the resource\n    :ivar limit: the maximum allowed usage of the resource\n    \"\"\"\n\n    quota_root: str\n    resource: str\n    usage: bytes\n    limit: bytes\n\n\ndef require_capability(capability):\n    \"\"\"Decorator raising CapabilityError when a capability is not available.\"\"\"\n\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def wrapper(client, *args, **kwargs):\n            if not client.has_capability(capability):\n                raise exceptions.CapabilityError(\n                    \"Server does not support {} capability\".format(capability)\n                )\n            return func(client, *args, **kwargs)\n\n        return wrapper\n\n    return actual_decorator\n\n\nclass IMAPClient:\n    \"\"\"A connection to the IMAP server specified by *host* is made when\n    this class is instantiated.\n\n    *port* defaults to 993, or 143 if *ssl* is ``False``.\n\n    If *use_uid* is ``True`` unique message UIDs be used for all calls\n    that accept message ids (defaults to ``True``).\n\n    If *ssl* is ``True`` (the default) a secure connection will be made.\n    Otherwise an insecure connection over plain text will be\n    established.\n\n    If *ssl* is ``True`` the optional *ssl_context* argument can be\n    used to provide an ``ssl.SSLContext`` instance used to\n    control SSL/TLS connection parameters. If this is not provided a\n    sensible default context will be used.\n\n    If *stream* is ``True`` then *host* is used as the command to run\n    to establish a connection to the IMAP server (defaults to\n    ``False``). This is useful for exotic connection or authentication\n    setups.\n\n    Use *timeout* to specify a timeout for the socket connected to the\n    IMAP server. The timeout can be either a float number, or an instance\n    of :py:class:`imapclient.SocketTimeout`.\n\n    * If a single float number is passed, the same timeout delay applies\n      during the  initial connection to the server and for all future socket\n      reads and writes.\n\n    * In case of a ``SocketTimeout``, connection timeout and\n      read/write operations can have distinct timeouts.\n\n    * The default is ``None``, where no timeout is used.\n\n    The *normalise_times* attribute specifies whether datetimes\n    returned by ``fetch()`` are normalised to the local system time\n    and include no timezone information (native), or are datetimes\n    that include timezone information (aware). By default\n    *normalise_times* is True (times are normalised to the local\n    system time). This attribute can be changed between ``fetch()``\n    calls if required.\n\n    Can be used as a context manager to automatically close opened connections:\n\n    >>> with IMAPClient(host=\"imap.foo.org\") as client:\n    ...     client.login(\"bar@foo.org\", \"passwd\")\n\n    \"\"\"\n\n    # Those exceptions are kept for backward-compatibility, since\n    # previous versions included these attributes as references to\n    # imaplib original exceptions\n    Error = exceptions.IMAPClientError\n    AbortError = exceptions.IMAPClientAbortError\n    ReadOnlyError = exceptions.IMAPClientReadOnlyError\n\n    def __init__(\n        self,\n        host: str,\n        port: int = None,\n        use_uid: bool = True,\n        ssl: bool = True,\n        stream: bool = False,\n        ssl_context: Optional[ssl_lib.SSLContext] = None,\n        timeout: Optional[float] = None,\n    ):\n        if stream:\n            if port is not None:\n                raise ValueError(\"can't set 'port' when 'stream' True\")\n            if ssl:\n                raise ValueError(\"can't use 'ssl' when 'stream' is True\")\n        elif port is None:\n            port = ssl and 993 or 143\n\n        if ssl and port == 143:\n            logger.warning(\n                \"Attempting to establish an encrypted connection \"\n                \"to a port (143) often used for unencrypted \"\n                \"connections\"\n            )\n\n        self.host = host\n        self.port = port\n        self.ssl = ssl\n        self.ssl_context = ssl_context\n        self.stream = stream\n        self.use_uid = use_uid\n        self.folder_encode = True\n        self.normalise_times = True\n\n        # If the user gives a single timeout value, assume it is the same for\n        # connection and read/write operations\n        if not isinstance(timeout, SocketTimeout):\n            timeout = SocketTimeout(timeout, timeout)\n\n        self._timeout = timeout\n        self._starttls_done = False\n        self._cached_capabilities = None\n        self._idle_tag = None\n\n        self._imap = self._create_IMAP4()\n        logger.debug(\n            \"Connected to host %s over %s\",\n            self.host,\n            \"SSL/TLS\" if ssl else \"plain text\",\n        )\n\n        self._set_read_timeout()\n        # Small hack to make imaplib log everything to its own logger\n        imaplib_logger = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._imap.debug = 5\n        self._imap._mesg = imaplib_logger.debug\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Logout and closes the connection when exiting the context manager.\n\n        All exceptions during logout and connection shutdown are caught because\n        an error here usually means the connection was already closed.\n        \"\"\"\n        try:\n            self.logout()\n        except Exception:\n            try:\n                self.shutdown()\n            except Exception as e:\n                logger.info(\"Could not close the connection cleanly: %s\", e)\n\n    def _create_IMAP4(self):\n        if self.stream:\n            return imaplib.IMAP4_stream(self.host)\n\n        connect_timeout = getattr(self._timeout, \"connect\", None)\n\n        if self.ssl:\n            return tls.IMAP4_TLS(\n                self.host,\n                self.port,\n                self.ssl_context,\n                connect_timeout,\n            )\n\n        return imap4.IMAP4WithTimeout(self.host, self.port, connect_timeout)\n\n    def _set_read_timeout(self):\n        if self._timeout is not None:\n            self.socket().settimeout(self._timeout.read)\n\n    @property\n    def _sock(self):\n        warnings.warn(\"_sock is deprecated. Use socket().\", DeprecationWarning)\n        return self.socket()\n\n    def socket(self):\n        \"\"\"Returns socket used to connect to server.\n\n        The socket is provided for polling purposes only.\n        It can be used in,\n        for example, :py:meth:`selectors.BaseSelector.register`\n        and :py:meth:`asyncio.loop.add_reader` to wait for data.\n\n        .. WARNING::\n           All other uses of the returned socket are unsupported.\n           This includes reading from and writing to the socket,\n           as they are likely to break internal bookkeeping of messages.\n        \"\"\"\n        # In py2, imaplib has sslobj (for SSL connections), and sock for non-SSL.\n        # In the py3 version it's just sock.\n        return getattr(self._imap, \"sslobj\", self._imap.sock)\n\n    @require_capability(\"STARTTLS\")\n    def starttls(self, ssl_context=None):\n        \"\"\"Switch to an SSL encrypted connection by sending a STARTTLS command.\n\n        The *ssl_context* argument is optional and should be a\n        :py:class:`ssl.SSLContext` object. If no SSL context is given, a SSL\n        context with reasonable default settings will be used.\n\n        You can enable checking of the hostname in the certificate presented\n        by the server  against the hostname which was used for connecting, by\n        setting the *check_hostname* attribute of the SSL context to ``True``.\n        The default SSL context has this setting enabled.\n\n        Raises :py:exc:`Error` if the SSL connection could not be established.\n\n        Raises :py:exc:`AbortError` if the server does not support STARTTLS\n        or an SSL connection is already established.\n        \"\"\"\n        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n\n    def login(self, username: str, password: str):\n        \"\"\"Login using *username* and *password*, returning the\n        server response.\n        \"\"\"\n        try:\n            rv = self._command_and_check(\n                \"login\",\n                to_unicode(username),\n                to_unicode(password),\n                unpack=True,\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n        logger.debug(\"Logged in as %s\", username)\n        return rv\n\n    def oauth2_login(\n        self,\n        user: str,\n        access_token: str,\n        mech: str = \"XOAUTH2\",\n        vendor: Optional[str] = None,\n    ):\n        \"\"\"Authenticate using the OAUTH2 or XOAUTH2 methods.\n\n        Gmail and Yahoo both support the 'XOAUTH2' mechanism, but Yahoo requires\n        the 'vendor' portion in the payload.\n        \"\"\"\n        auth_string = \"user=%s\\1auth=Bearer %s\\1\" % (user, access_token)\n        if vendor:\n            auth_string += \"vendor=%s\\1\" % vendor\n        auth_string += \"\\1\"\n        try:\n            return self._command_and_check(\"authenticate\", mech, lambda x: auth_string)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def oauthbearer_login(self, identity, access_token):\n        \"\"\"Authenticate using the OAUTHBEARER method.\n\n        This is supported by Gmail and is meant to supersede the non-standard\n        'OAUTH2' and 'XOAUTH2' mechanisms.\n        \"\"\"\n        # https://tools.ietf.org/html/rfc5801#section-4\n        # Technically this is the authorization_identity, but at least for Gmail it's\n        # mandatory and practically behaves like the regular username/identity.\n        if identity:\n            gs2_header = \"n,a=%s,\" % identity.replace(\"=\", \"=3D\").replace(\",\", \"=2C\")\n        else:\n            gs2_header = \"n,,\"\n        # https://tools.ietf.org/html/rfc6750#section-2.1\n        http_authz = \"Bearer %s\" % access_token\n        # https://tools.ietf.org/html/rfc7628#section-3.1\n        auth_string = \"%s\\1auth=%s\\1\\1\" % (gs2_header, http_authz)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"OAUTHBEARER\", lambda x: auth_string\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def plain_login(self, identity, password, authorization_identity=None):\n        \"\"\"Authenticate using the PLAIN method (requires server support).\"\"\"\n        if not authorization_identity:\n            authorization_identity = \"\"\n        auth_string = \"%s\\0%s\\0%s\" % (authorization_identity, identity, password)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"PLAIN\", lambda _: auth_string, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def sasl_login(self, mech_name, mech_callable):\n        \"\"\"Authenticate using a provided SASL mechanism (requires server support).\n\n        The *mech_callable* will be called with one parameter (the server\n        challenge as bytes) and must return the corresponding client response\n        (as bytes, or as string which will be automatically encoded).\n\n        It will be called as many times as the server produces challenges,\n        which will depend on the specific SASL mechanism. (If the mechanism is\n        defined as \"client-first\", the server will nevertheless produce a\n        zero-length challenge.)\n\n        For example, PLAIN has just one step with empty challenge, so a handler\n        might look like this::\n\n            plain_mech = lambda _: \"\\\\0%s\\\\0%s\" % (username, password)\n\n            imap.sasl_login(\"PLAIN\", plain_mech)\n\n        A more complex but still stateless handler might look like this::\n\n            def example_mech(challenge):\n                if challenge == b\"Username:\"\n                    return username.encode(\"utf-8\")\n                elif challenge == b\"Password:\"\n                    return password.encode(\"utf-8\")\n                else:\n                    return b\"\"\n\n            imap.sasl_login(\"EXAMPLE\", example_mech)\n\n        A stateful handler might look like this::\n\n            class ScramSha256SaslMechanism():\n                def __init__(self, username, password):\n                    ...\n\n                def __call__(self, challenge):\n                    self.step += 1\n                    if self.step == 1:\n                        response = ...\n                    elif self.step == 2:\n                        response = ...\n                    return response\n\n            scram_mech = ScramSha256SaslMechanism(username, password)\n\n            imap.sasl_login(\"SCRAM-SHA-256\", scram_mech)\n        \"\"\"\n        try:\n            return self._command_and_check(\n                \"authenticate\", mech_name, mech_callable, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def logout(self):\n        \"\"\"Logout, returning the server response.\"\"\"\n        typ, data = self._imap.logout()\n        self._check_resp(\"BYE\", \"logout\", typ, data)\n        logger.debug(\"Logged out, connection closed\")\n        return data[0]\n\n    def shutdown(self) -> None:\n        \"\"\"Close the connection to the IMAP server (without logging out)\n\n        In most cases, :py:meth:`.logout` should be used instead of\n        this. The logout method also shutdown down the connection.\n        \"\"\"\n        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n\n    @require_capability(\"ENABLE\")\n    def enable(self, *capabilities):\n        \"\"\"Activate one or more server side capability extensions.\n\n        Most capabilities do not need to be enabled. This is only\n        required for extensions which introduce backwards incompatible\n        behaviour. Two capabilities which may require enable are\n        ``CONDSTORE`` and ``UTF8=ACCEPT``.\n\n        A list of the requested extensions that were successfully\n        enabled on the server is returned.\n\n        Once enabled each extension remains active until the IMAP\n        connection is closed.\n\n        See :rfc:`5161` for more details.\n        \"\"\"\n        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n\n    @require_capability(\"ID\")\n    def id_(self, parameters=None):\n        \"\"\"Issue the ID command, returning a dict of server implementation\n        fields.\n\n        *parameters* should be specified as a dictionary of field/value pairs,\n        for example: ``{\"name\": \"IMAPClient\", \"version\": \"0.12\"}``\n        \"\"\"\n        if parameters is None:\n            args = \"NIL\"\n        else:\n            if not isinstance(parameters, dict):\n                raise TypeError(\"'parameters' should be a dictionary\")\n            args = seq_to_parenstr(\n                _quote(v) for v in itertools.chain.from_iterable(parameters.items())\n            )\n\n        typ, data = self._imap._simple_command(\"ID\", args)\n        self._checkok(\"id\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"ID\")\n        return parse_response(data)\n\n    def capabilities(self):\n        \"\"\"Returns the server capability list.\n\n        If the session is authenticated and the server has returned an\n        untagged CAPABILITY response at authentication time, this\n        response will be returned. Otherwise, the CAPABILITY command\n        will be issued to the server, with the results cached for\n        future calls.\n\n        If the session is not yet authenticated, the capabilities\n        requested at connection time will be returned.\n        \"\"\"\n        # Ensure cached capabilities aren't used post-STARTTLS. As per\n        # https://tools.ietf.org/html/rfc2595#section-3.1\n        if self._starttls_done and self._imap.state == \"NONAUTH\":\n            self._cached_capabilities = None\n            return self._do_capabilites()\n\n        # If a capability response has been cached, use that.\n        if self._cached_capabilities:\n            return self._cached_capabilities\n\n        # If the server returned an untagged CAPABILITY response\n        # (during authentication), cache it and return that.\n        untagged = _dict_bytes_normaliser(self._imap.untagged_responses)\n        response = untagged.pop(\"CAPABILITY\", None)\n        if response:\n            self._cached_capabilities = self._normalise_capabilites(response[0])\n            return self._cached_capabilities\n\n        # If authenticated, but don't have a capability response, ask for one\n        if self._imap.state in (\"SELECTED\", \"AUTH\"):\n            self._cached_capabilities = self._do_capabilites()\n            return self._cached_capabilities\n\n        # Return capabilities that imaplib requested at connection\n        # time (pre-auth)\n        return tuple(to_bytes(c) for c in self._imap.capabilities)\n\n    def _do_capabilites(self):\n        raw_response = self._command_and_check(\"capability\", unpack=True)\n        return self._normalise_capabilites(raw_response)\n\n    def _normalise_capabilites(self, raw_response):\n        raw_response = to_bytes(raw_response)\n        return tuple(raw_response.upper().split())\n\n    def has_capability(self, capability):\n        \"\"\"Return ``True`` if the IMAP server has the given *capability*.\"\"\"\n        # FIXME: this will not detect capabilities that are backwards\n        # compatible with the current level. For instance the SORT\n        # capabilities may in the future be named SORT2 which is\n        # still compatible with the current standard and will not\n        # be detected by this method.\n        return to_bytes(capability).upper() in self.capabilities()\n\n    @require_capability(\"NAMESPACE\")\n    def namespace(self):\n        \"\"\"Return the namespace for the account as a (personal, other,\n        shared) tuple.\n\n        Each element may be None if no namespace of that type exists,\n        or a sequence of (prefix, separator) pairs.\n\n        For convenience the tuple elements may be accessed\n        positionally or using attributes named *personal*, *other* and\n        *shared*.\n\n        See :rfc:`2342` for more details.\n        \"\"\"\n        data = self._command_and_check(\"namespace\")\n        parts = []\n        for item in parse_response(data):\n            if item is None:\n                parts.append(item)\n            else:\n                converted = []\n                for prefix, separator in item:\n                    if self.folder_encode:\n                        prefix = decode_utf7(prefix)\n                    converted.append((prefix, to_unicode(separator)))\n                parts.append(tuple(converted))\n        return Namespace(*parts)\n\n    def list_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Get a listing of folders on the server as a list of\n        ``(flags, delimiter, name)`` tuples.\n\n        Specifying *directory* will limit returned folders to the\n        given base directory. The directory and any child directories\n        will returned.\n\n        Specifying *pattern* will limit returned folders to those with\n        matching names. The wildcards are supported in\n        *pattern*. ``*`` matches zero or more of any character and\n        ``%`` matches 0 or more characters except the folder\n        delimiter.\n\n        Calling list_folders with no arguments will recursively list\n        all folders available for the logged in user.\n\n        Folder names are always returned as unicode strings, and\n        decoded from modified UTF-7, except if folder_decode is not\n        set.\n        \"\"\"\n        return self._do_list(\"LIST\", directory, pattern)\n\n    @require_capability(\"XLIST\")\n    def xlist_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Execute the XLIST command, returning ``(flags, delimiter,\n        name)`` tuples.\n\n        This method returns special flags for each folder and a\n        localized name for certain folders (e.g. the name of the\n        inbox may be localized and the flags can be used to\n        determine the actual inbox, even if the name has been\n        localized.\n\n        A ``XLIST`` response could look something like::\n\n            [((b'\\\\HasNoChildren', b'\\\\Inbox'), b'/', u'Inbox'),\n             ((b'\\\\Noselect', b'\\\\HasChildren'), b'/', u'[Gmail]'),\n             ((b'\\\\HasNoChildren', b'\\\\AllMail'), b'/', u'[Gmail]/All Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Drafts'), b'/', u'[Gmail]/Drafts'),\n             ((b'\\\\HasNoChildren', b'\\\\Important'), b'/', u'[Gmail]/Important'),\n             ((b'\\\\HasNoChildren', b'\\\\Sent'), b'/', u'[Gmail]/Sent Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Spam'), b'/', u'[Gmail]/Spam'),\n             ((b'\\\\HasNoChildren', b'\\\\Starred'), b'/', u'[Gmail]/Starred'),\n             ((b'\\\\HasNoChildren', b'\\\\Trash'), b'/', u'[Gmail]/Trash')]\n\n        This is a *deprecated* Gmail-specific IMAP extension (See\n        https://developers.google.com/gmail/imap_extensions#xlist_is_deprecated\n        for more information).\n\n        The *directory* and *pattern* arguments are as per\n        list_folders().\n        \"\"\"\n        return self._do_list(\"XLIST\", directory, pattern)\n\n    def list_sub_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Return a list of subscribed folders on the server as\n        ``(flags, delimiter, name)`` tuples.\n\n        The default behaviour will list all subscribed folders. The\n        *directory* and *pattern* arguments are as per list_folders().\n        \"\"\"\n        return self._do_list(\"LSUB\", directory, pattern)\n\n    def _do_list(self, cmd, directory, pattern):\n        directory = self._normalise_folder(directory)\n        pattern = self._normalise_folder(pattern)\n        typ, dat = self._imap._simple_command(cmd, directory, pattern)\n        self._checkok(cmd, typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, cmd)\n        return self._proc_folder_list(dat)\n\n    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n\n    def find_special_folder(self, folder_flag):\n        \"\"\"Try to locate a special folder, like the Sent or Trash folder.\n\n        >>> server.find_special_folder(imapclient.SENT)\n        'INBOX.Sent'\n\n        This function tries its best to find the correct folder (if any) but\n        uses heuristics when the server is unable to precisely tell where\n        special folders are located.\n\n        Returns the name of the folder if found, or None otherwise.\n        \"\"\"\n        # Detect folder by looking for known attributes\n        # TODO: avoid listing all folders by using extended LIST (RFC6154)\n        for folder in self.list_folders():\n            if folder and len(folder[0]) > 0 and folder_flag in folder[0]:\n                return folder[2]\n\n        # Detect folder by looking for common names\n        # We only look for folders in the \"personal\" namespace of the user\n        if self.has_capability(\"NAMESPACE\"):\n            personal_namespaces = self.namespace().personal\n        else:\n            personal_namespaces = _POPULAR_PERSONAL_NAMESPACES\n\n        for personal_namespace in personal_namespaces:\n            for pattern in _POPULAR_SPECIAL_FOLDERS.get(folder_flag, tuple()):\n                pattern = personal_namespace[0] + pattern\n                sent_folders = self.list_folders(pattern=pattern)\n                if sent_folders:\n                    return sent_folders[0][2]\n\n        return None\n\n    def select_folder(self, folder, readonly=False):\n        \"\"\"Set the current folder on the server.\n\n        Future calls to methods such as search and fetch will act on\n        the selected folder.\n\n        Returns a dictionary containing the ``SELECT`` response. At least\n        the ``b'EXISTS'``, ``b'FLAGS'`` and ``b'RECENT'`` keys are guaranteed\n        to exist. An example::\n\n            {b'EXISTS': 3,\n             b'FLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'RECENT': 0,\n             b'PERMANENTFLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'READ-WRITE': True,\n             b'UIDNEXT': 11,\n             b'UIDVALIDITY': 1239278212}\n        \"\"\"\n        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n\n    @require_capability(\"UNSELECT\")\n    def unselect_folder(self):\n        r\"\"\"Unselect the current folder and release associated resources.\n\n        Unlike ``close_folder``, the ``UNSELECT`` command does not expunge\n        the mailbox, keeping messages with \\Deleted flag set for example.\n\n        Returns the UNSELECT response string returned by the server.\n        \"\"\"\n        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n\n    def _process_select_response(self, resp):\n        untagged = _dict_bytes_normaliser(resp)\n        out = {}\n\n        # imaplib doesn't parse these correctly (broken regex) so replace\n        # with the raw values out of the OK section\n        for line in untagged.get(\"OK\", []):\n            match = _RE_SELECT_RESPONSE.match(line)\n            if match:\n                key = match.group(\"key\")\n                if key == b\"PERMANENTFLAGS\":\n                    out[key] = tuple(match.group(\"data\").split())\n\n        for key, value in untagged.items():\n            key = key.upper()\n            if key in (b\"OK\", b\"PERMANENTFLAGS\"):\n                continue  # already handled above\n            if key in (\n                b\"EXISTS\",\n                b\"RECENT\",\n                b\"UIDNEXT\",\n                b\"UIDVALIDITY\",\n                b\"HIGHESTMODSEQ\",\n            ):\n                value = int(value[0])\n            elif key == b\"READ-WRITE\":\n                value = True\n            elif key == b\"FLAGS\":\n                value = tuple(value[0][1:-1].split())\n            out[key] = value\n        return out\n\n    def noop(self):\n        \"\"\"Execute the NOOP command.\n\n        This command returns immediately, returning any server side\n        status updates. It can also be used to reset any auto-logout\n        timers.\n\n        The return value is the server command response message\n        followed by a list of status responses. For example::\n\n            (b'NOOP completed.',\n             [(4, b'EXISTS'),\n              (3, b'FETCH', (b'FLAGS', (b'bar', b'sne'))),\n              (6, b'FETCH', (b'FLAGS', (b'sne',)))])\n\n        \"\"\"\n        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n\n    @require_capability(\"IDLE\")\n    def idle(self):\n        \"\"\"Put the server into IDLE mode.\n\n        In this mode the server will return unsolicited responses\n        about changes to the selected mailbox. This method returns\n        immediately. Use ``idle_check()`` to look for IDLE responses\n        and ``idle_done()`` to stop IDLE mode.\n\n        .. note::\n\n            Any other commands issued while the server is in IDLE\n            mode will fail.\n\n        See :rfc:`2177` for more information about the IDLE extension.\n        \"\"\"\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n    def _poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is more scalable because it ALLOWS your process\n        to have more than 1024 file descriptors.\n        \"\"\"\n        poller = select.poll()\n        poller.register(sock.fileno(), select.POLLIN)\n        timeout = timeout * 1000 if timeout is not None else None\n        return poller.poll(timeout)\n\n    def _select_poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is a fallback because it FAILS if your process\n        has more than 1024 file descriptors.\n        We still need this for Windows and some other niche systems.\n        \"\"\"\n        return select.select([sock], [], [], timeout)[0]\n\n    @require_capability(\"IDLE\")\n    def idle_check(self, timeout=None):\n        \"\"\"Check for any IDLE responses sent by the server.\n\n        This method should only be called if the server is in IDLE\n        mode (see ``idle()``).\n\n        By default, this method will block until an IDLE response is\n        received. If *timeout* is provided, the call will block for at\n        most this many seconds while waiting for an IDLE response.\n\n        The return value is a list of received IDLE responses. These\n        will be parsed with values converted to appropriate types. For\n        example::\n\n            [(b'OK', b'Still here'),\n             (1, b'EXISTS'),\n             (1, b'FETCH', (b'FLAGS', (b'\\\\NotJunk',)))]\n        \"\"\"\n        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n\n    @require_capability(\"IDLE\")\n    def idle_done(self):\n        \"\"\"Take the server out of IDLE mode.\n\n        This method should only be called if the server is already in\n        IDLE mode.\n\n        The return value is of the form ``(command_text,\n        idle_responses)`` where *command_text* is the text sent by the\n        server when the IDLE command finished (eg. ``b'Idle\n        terminated'``) and *idle_responses* is a list of parsed idle\n        responses received since the last call to ``idle_check()`` (if\n        any). These are returned in parsed form as per\n        ``idle_check()``.\n        \"\"\"\n        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n    def folder_status(self, folder, what=None):\n        \"\"\"Return the status of *folder*.\n\n        *what* should be a sequence of status items to query. This\n        defaults to ``('MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY',\n        'UNSEEN')``.\n\n        Returns a dictionary of the status items for the folder with\n        keys matching *what*.\n        \"\"\"\n        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n\n    def close_folder(self):\n        \"\"\"Close the currently selected folder, returning the server\n        response string.\n        \"\"\"\n        return self._command_and_check(\"close\", unpack=True)\n\n    def create_folder(self, folder):\n        \"\"\"Create *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"create\", self._normalise_folder(folder), unpack=True\n        )\n\n    def rename_folder(self, old_name, new_name):\n        \"\"\"Change the name of a folder on the server.\"\"\"\n        return self._command_and_check(\n            \"rename\",\n            self._normalise_folder(old_name),\n            self._normalise_folder(new_name),\n            unpack=True,\n        )\n\n    def delete_folder(self, folder):\n        \"\"\"Delete *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"delete\", self._normalise_folder(folder), unpack=True\n        )\n\n    def folder_exists(self, folder):\n        \"\"\"Return ``True`` if *folder* exists on the server.\"\"\"\n        return len(self.list_folders(\"\", folder)) > 0\n\n    def subscribe_folder(self, folder):\n        \"\"\"Subscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"subscribe\", self._normalise_folder(folder))\n\n    def unsubscribe_folder(self, folder):\n        \"\"\"Unsubscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"unsubscribe\", self._normalise_folder(folder))\n\n    def search(self, criteria=\"ALL\", charset=None):\n        \"\"\"Return a list of messages ids from the currently selected\n        folder matching *criteria*.\n\n        *criteria* should be a sequence of one or more criteria\n        items. Each criteria item may be either unicode or\n        bytes. Example values::\n\n            [u'UNSEEN']\n            [u'SMALLER', 500]\n            [b'NOT', b'DELETED']\n            [u'TEXT', u'foo bar', u'FLAGGED', u'SUBJECT', u'baz']\n            [u'SINCE', date(2005, 4, 3)]\n\n        IMAPClient will perform conversion and quoting as\n        required. The caller shouldn't do this.\n\n        It is also possible (but not recommended) to pass the combined\n        criteria as a single string. In this case IMAPClient won't\n        perform quoting, allowing lower-level specification of\n        criteria. Examples of this style::\n\n            u'UNSEEN'\n            u'SMALLER 500'\n            b'NOT DELETED'\n            u'TEXT \"foo bar\" FLAGGED SUBJECT \"baz\"'\n            b'SINCE 03-Apr-2005'\n\n        To support complex search expressions, criteria lists can be\n        nested. IMAPClient will insert parentheses in the right\n        places. The following will match messages that are both not\n        flagged and do not have \"foo\" in the subject::\n\n            ['NOT', ['SUBJECT', 'foo', 'FLAGGED']]\n\n        *charset* specifies the character set of the criteria. It\n        defaults to US-ASCII as this is the only charset that a server\n        is required to support by the RFC. UTF-8 is commonly supported\n        however.\n\n        Any criteria specified using unicode will be encoded as per\n        *charset*. Specifying a unicode criteria that can not be\n        encoded using *charset* will result in an error.\n\n        Any criteria specified using bytes will be sent as-is but\n        should use an encoding that matches *charset* (the character\n        set given is still passed on to the server).\n\n        See :rfc:`3501#section-6.4.4` for more details.\n\n        Note that criteria arguments that are 8-bit will be\n        transparently sent by IMAPClient as IMAP literals to ensure\n        adherence to IMAP standards.\n\n        The returned list of message ids will have a special *modseq*\n        attribute. This is set if the server included a MODSEQ value\n        to the search response (i.e. if a MODSEQ criteria was included\n        in the search).\n\n        \"\"\"\n        return self._search(criteria, charset)\n\n    @require_capability(\"X-GM-EXT-1\")\n    def gmail_search(self, query, charset=\"UTF-8\"):\n        \"\"\"Search using Gmail's X-GM-RAW attribute.\n\n        *query* should be a valid Gmail search query string. For\n        example: ``has:attachment in:unread``. The search string may\n        be unicode and will be encoded using the specified *charset*\n        (defaulting to UTF-8).\n\n        This method only works for IMAP servers that support X-GM-RAW,\n        which is only likely to be Gmail.\n\n        See https://developers.google.com/gmail/imap_extensions#extension_of_the_search_command_x-gm-raw\n        for more info.\n        \"\"\"\n        return self._search([b\"X-GM-RAW\", query], charset)\n\n    def _search(self, criteria, charset):\n        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n\n    @require_capability(\"SORT\")\n    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of message ids from the currently selected\n        folder, sorted by *sort_criteria* and optionally filtered by\n        *criteria*.\n\n        *sort_criteria* may be specified as a sequence of strings or a\n        single string. IMAPClient will take care any required\n        conversions. Valid *sort_criteria* values::\n\n            ['ARRIVAL']\n            ['SUBJECT', 'ARRIVAL']\n            'ARRIVAL'\n            'REVERSE SIZE'\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for full details.\n\n        Note that SORT is an extension to the IMAP4 standard so it may\n        not be supported by all IMAP servers.\n        \"\"\"\n        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n\n    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of messages threads from the currently\n        selected folder which match *criteria*.\n\n        Each returned thread is a list of messages ids. An example\n        return value containing three message threads::\n\n            ((1, 2), (3,), (4, 5, 6))\n\n        The optional *algorithm* argument specifies the threading\n        algorithm to use.\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for more details.\n        \"\"\"\n        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)\n\n    def get_flags(self, messages):\n        \"\"\"Return the flags set for each message in *messages* from\n        the currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (flag1, flag2, ... ), }``.\n        \"\"\"\n        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n\n    def add_flags(self, messages, flags, silent=False):\n        \"\"\"Add *flags* to *messages* in the currently selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"+FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def remove_flags(self, messages, flags, silent=False):\n        \"\"\"Remove one or more *flags* from *messages* in the currently\n        selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"-FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def set_flags(self, messages, flags, silent=False):\n        \"\"\"Set the *flags* for *messages* in the currently selected\n        folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def get_gmail_labels(self, messages):\n        \"\"\"Return the label set for each message in *messages* in the\n        currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (label1, label2, ... ), }``.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n\n    def add_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Add *labels* to *messages* in the currently selected folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"+X-GM-LABELS\", messages, labels, silent=silent)\n\n    def remove_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Remove one or more *labels* from *messages* in the\n        currently selected folder, or None if *silent* is true.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*).\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"-X-GM-LABELS\", messages, labels, silent=silent)\n\n    def set_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Set the *labels* for *messages* in the currently selected\n        folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"X-GM-LABELS\", messages, labels, silent=silent)\n\n    def delete_messages(self, messages, silent=False):\n        \"\"\"Delete one or more *messages* from the currently selected\n        folder.\n\n        Returns the flags set for each modified message (see\n        *get_flags*).\n        \"\"\"\n        return self.add_flags(messages, DELETED, silent=silent)\n\n    def fetch(self, messages, data, modifiers=None):\n        \"\"\"Retrieve selected *data* associated with one or more\n        *messages* in the currently selected folder.\n\n        *data* should be specified as a sequence of strings, one item\n        per data selector, for example ``['INTERNALDATE',\n        'RFC822']``.\n\n        *modifiers* are required for some extensions to the IMAP\n        protocol (eg. :rfc:`4551`). These should be a sequence of strings\n        if specified, for example ``['CHANGEDSINCE 123']``.\n\n        A dictionary is returned, indexed by message number. Each item\n        in this dictionary is also a dictionary, with an entry\n        corresponding to each item in *data*. Returned values will be\n        appropriately typed. For example, integer values will be returned as\n        Python integers, timestamps will be returned as datetime\n        instances and ENVELOPE responses will be returned as\n        :py:class:`Envelope <imapclient.response_types.Envelope>` instances.\n\n        String data will generally be returned as bytes (Python 3) or\n        str (Python 2).\n\n        In addition to an element for each *data* item, the dict\n        returned for each message also contains a *SEQ* key containing\n        the sequence number for the message. This allows for mapping\n        between the UID and sequence number (when the *use_uid*\n        property is ``True``).\n\n        Example::\n\n            >> c.fetch([3293, 3230], ['INTERNALDATE', 'FLAGS'])\n            {3230: {b'FLAGS': (b'\\\\Seen',),\n                    b'INTERNALDATE': datetime.datetime(2011, 1, 30, 13, 32, 9),\n                    b'SEQ': 84},\n             3293: {b'FLAGS': (),\n                    b'INTERNALDATE': datetime.datetime(2011, 2, 24, 19, 30, 36),\n                    b'SEQ': 110}}\n\n        \"\"\"\n        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            seq_to_parenstr_upper(data),\n            seq_to_parenstr_upper(modifiers) if modifiers else None,\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n\n    def append(self, folder, msg, flags=(), msg_time=None):\n        \"\"\"Append a message to *folder*.\n\n        *msg* should be a string contains the full message including\n        headers.\n\n        *flags* should be a sequence of message flags to set. If not\n        specified no flags will be set.\n\n        *msg_time* is an optional datetime instance specifying the\n        date and time to set on the message. The server will set a\n        time if it isn't specified. If *msg_time* contains timezone\n        information (tzinfo), this will be honoured. Otherwise the\n        local machine's time zone sent to the server.\n\n        Returns the APPEND response as returned by the server.\n        \"\"\"\n        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n\n    @require_capability(\"MULTIAPPEND\")\n    def multiappend(self, folder, msgs):\n        \"\"\"Append messages to *folder* using the MULTIAPPEND feature from :rfc:`3502`.\n\n        *msgs* must be an iterable. Each item must be either a string containing the\n        full message including headers, or a dict containing the keys \"msg\" with the\n        full message as before, \"flags\" with a sequence of message flags to set, and\n        \"date\" with a datetime instance specifying the internal date to set.\n        The keys \"flags\" and \"date\" are optional.\n\n        Returns the APPEND response from the server.\n        \"\"\"\n\n        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n\n    def copy(self, messages, folder):\n        \"\"\"Copy one or more messages from the current folder to\n        *folder*. Returns the COPY response string returned by the\n        server.\n        \"\"\"\n        return self._command_and_check(\n            \"copy\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    @require_capability(\"MOVE\")\n    def move(self, messages, folder):\n        \"\"\"Atomically move messages to another folder.\n\n        Requires the MOVE capability, see :rfc:`6851`.\n\n        :param messages: List of message UIDs to move.\n        :param folder: The destination folder name.\n        \"\"\"\n        return self._command_and_check(\n            \"move\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n###The function: expunge###\n    @require_capability(\"UIDPLUS\")\n    def uid_expunge(self, messages):\n        \"\"\"Expunge deleted messages with the specified message ids from the\n        folder.\n\n        This requires the UIDPLUS capability.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=True)\n\n    @require_capability(\"ACL\")\n    def getacl(self, folder):\n        \"\"\"Returns a list of ``(who, acl)`` tuples describing the\n        access controls for *folder*.\n        \"\"\"\n        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n\n    @require_capability(\"ACL\")\n    def setacl(self, folder, who, what):\n        \"\"\"Set an ACL (*what*) for user (*who*) for a folder.\n\n        Set *what* to an empty string to remove an ACL. Returns the\n        server response string.\n        \"\"\"\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n\n    @require_capability(\"QUOTA\")\n    def get_quota(self, mailbox=\"INBOX\"):\n        \"\"\"Get the quotas associated with a mailbox.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return self.get_quota_root(mailbox)[1]\n\n    @require_capability(\"QUOTA\")\n    def _get_quota(self, quota_root=\"\"):\n        \"\"\"Get the quotas associated with a quota root.\n\n        This method is not private but put behind an underscore to show that\n        it is a low-level function. Users probably want to use `get_quota`\n        instead.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return _parse_quota(self._command_and_check(\"getquota\", _quote(quota_root)))\n\n    @require_capability(\"QUOTA\")\n    def get_quota_root(self, mailbox):\n        \"\"\"Get the quota roots for a mailbox.\n\n        The IMAP server responds with the quota root and the quotas associated\n        so there is usually no need to call `get_quota` after.\n\n        See :rfc:`2087` for more details.\n\n        Return a tuple of MailboxQuotaRoots and list of Quota associated\n        \"\"\"\n        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n\n    @require_capability(\"QUOTA\")\n    def set_quota(self, quotas):\n        \"\"\"Set one or more quotas on resources.\n\n        :param quotas: list of Quota objects\n        \"\"\"\n        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n\n    def _check_resp(self, expected, command, typ, data):\n        \"\"\"Check command responses for errors.\n\n        Raises IMAPClient.Error if the command fails.\n        \"\"\"\n        if typ != expected:\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(data[0]))\n            )\n\n    def _consume_until_tagged_response(self, tag, command):\n        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n\n    def _raw_command_untagged(\n        self, command, args, response_name=None, unpack=False, uid=True\n    ):\n        # TODO: eventually this should replace _command_and_check (call it _command)\n        typ, data = self._raw_command(command, args, uid=uid)\n        if response_name is None:\n            response_name = command\n        typ, data = self._imap._untagged_response(typ, data, to_unicode(response_name))\n        self._checkok(to_unicode(command), typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _raw_command(self, command, args, uid=True):\n        \"\"\"Run the specific command with the arguments given. 8-bit arguments\n        are sent as literals. The return value is (typ, data).\n\n        This sidesteps much of imaplib's command sending\n        infrastructure because imaplib can't send more than one\n        literal.\n\n        *command* should be specified as bytes.\n        *args* should be specified as a list of bytes.\n        \"\"\"\n        command = command.upper()\n\n        if isinstance(args, tuple):\n            args = list(args)\n        if not isinstance(args, list):\n            args = [args]\n\n        tag = self._imap._new_tag()\n        prefix = [to_bytes(tag)]\n        if uid and self.use_uid:\n            prefix.append(b\"UID\")\n        prefix.append(command)\n\n        line = []\n        for item, is_last in _iter_with_last(prefix + args):\n            if not isinstance(item, bytes):\n                raise ValueError(\"command args must be passed as bytes\")\n\n            if _is8bit(item):\n                # If a line was already started send it\n                if line:\n                    out = b\" \".join(line)\n                    logger.debug(\"> %s\", out)\n                    self._imap.send(out)\n                    line = []\n\n                # Now send the (unquoted) literal\n                if isinstance(item, _quoted):\n                    item = item.original\n                self._send_literal(tag, item)\n                if not is_last:\n                    self._imap.send(b\" \")\n            else:\n                line.append(item)\n\n        if line:\n            out = b\" \".join(line)\n            logger.debug(\"> %s\", out)\n            self._imap.send(out)\n\n        self._imap.send(b\"\\r\\n\")\n\n        return self._imap._command_complete(to_unicode(command), tag)\n\n    def _send_literal(self, tag, item):\n        \"\"\"Send a single literal for the command with *tag*.\"\"\"\n        if b\"LITERAL+\" in self._cached_capabilities:\n            out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"+}\\r\\n\" + item\n            logger.debug(\"> %s\", debug_trunc(out, 64))\n            self._imap.send(out)\n            return\n\n        out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"}\\r\\n\"\n        logger.debug(\"> %s\", out)\n        self._imap.send(out)\n\n        # Wait for continuation response\n        while self._imap._get_response():\n            tagged_resp = self._imap.tagged_commands.get(tag)\n            if tagged_resp:\n                raise exceptions.IMAPClientAbortError(\n                    \"unexpected response while waiting for continuation response: \"\n                    + repr(tagged_resp)\n                )\n\n        logger.debug(\"   (literal) > %s\", debug_trunc(item, 256))\n        self._imap.send(item)\n\n    def _command_and_check(\n        self, command, *args, unpack: bool = False, uid: bool = False\n    ):\n        if uid and self.use_uid:\n            command = to_unicode(command)  # imaplib must die\n            typ, data = self._imap.uid(command, *args)\n        else:\n            meth = getattr(self._imap, to_unicode(command))\n            typ, data = meth(*args)\n        self._checkok(command, typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _checkok(self, command, typ, data):\n        self._check_resp(\"OK\", command, typ, data)\n\n    def _gm_label_store(self, cmd, messages, labels, silent):\n        response = self._store(\n            cmd, messages, self._normalise_labels(labels), b\"X-GM-LABELS\", silent=silent\n        )\n        return (\n            {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n            if response\n            else None\n        )\n\n    def _store(self, cmd, messages, flags, fetch_key, silent):\n        \"\"\"Worker function for the various flag manipulation methods.\n\n        *cmd* is the STORE command to use (eg. '+FLAGS').\n        \"\"\"\n        if not messages:\n            return {}\n        if silent:\n            cmd += b\".SILENT\"\n\n        data = self._command_and_check(\n            \"store\", join_message_ids(messages), cmd, seq_to_parenstr(flags), uid=True\n        )\n        if silent:\n            return None\n        return self._filter_fetch_dict(parse_fetch_response(data), fetch_key)\n\n    def _filter_fetch_dict(self, fetch_dict, key):\n        return dict((msgid, data[key]) for msgid, data in fetch_dict.items())\n\n    def _normalise_folder(self, folder_name):\n        if isinstance(folder_name, bytes):\n            folder_name = folder_name.decode(\"ascii\")\n        if self.folder_encode:\n            folder_name = encode_utf7(folder_name)\n        return _quote(folder_name)\n\n    def _normalise_labels(self, labels):\n        if isinstance(labels, (str, bytes)):\n            labels = (labels,)\n        return [_quote(encode_utf7(label)) for label in labels]\n\n    @property\n    def welcome(self):\n        \"\"\"access the server greeting message\"\"\"\n        try:\n            return self._imap.welcome\n        except AttributeError:\n            pass\n\n\ndef _quote(arg):\n    if isinstance(arg, str):\n        arg = arg.replace(\"\\\\\", \"\\\\\\\\\")\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b\"\\\\\", b\"\\\\\\\\\")\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q\n\n\ndef _normalise_search_criteria(criteria, charset=None):\n    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n\n\ndef _normalise_sort_criteria(criteria, charset=None):\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    return b\"(\" + b\" \".join(to_bytes(item).upper() for item in criteria) + b\")\"\n\n\nclass _literal(bytes):\n    \"\"\"Hold message data that should always be sent as a literal.\"\"\"\n\n\nclass _quoted(bytes):\n    \"\"\"\n    This class holds a quoted bytes value which provides access to the\n    unquoted value via the *original* attribute.\n\n    They should be created via the *maybe* classmethod.\n    \"\"\"\n\n    @classmethod\n    def maybe(cls, original):\n        \"\"\"Maybe quote a bytes value.\n\n        If the input requires no quoting it is returned unchanged.\n\n        If quoting is required a *_quoted* instance is returned. This\n        holds the quoted version of the input while also providing\n        access to the original unquoted source.\n        \"\"\"\n        quoted = original.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted = quoted.replace(b'\"', b'\\\\\"')\n        if quoted != original or b\" \" in quoted or not quoted:\n            out = cls(b'\"' + quoted + b'\"')\n            out.original = original\n            return out\n        return original\n\n\n# normalise_text_list, seq_to_parentstr etc have to return unicode\n# because imaplib handles flags and sort criteria assuming these are\n# passed as unicode\ndef normalise_text_list(items):\n    return list(_normalise_text_list(items))\n\n\ndef seq_to_parenstr(items):\n    return _join_and_paren(_normalise_text_list(items))\n\n\ndef seq_to_parenstr_upper(items):\n    return _join_and_paren(item.upper() for item in _normalise_text_list(items))\n\n\ndef _join_and_paren(items):\n    return \"(\" + \" \".join(items) + \")\"\n\n\ndef _normalise_text_list(items):\n    if isinstance(items, (str, bytes)):\n        items = (items,)\n    return (to_unicode(c) for c in items)\n\n\ndef join_message_ids(messages):\n    \"\"\"Convert a sequence of messages ids or a single integer message id\n    into an id byte string for use with IMAP commands\n    \"\"\"\n    if isinstance(messages, (str, bytes, int)):\n        messages = (to_bytes(messages),)\n    return b\",\".join(_maybe_int_to_bytes(m) for m in messages)\n\n\ndef _maybe_int_to_bytes(val):\n    if isinstance(val, int):\n        return str(val).encode(\"us-ascii\")\n    return to_bytes(val)\n\n\ndef _parse_untagged_response(text):\n    assert_imap_protocol(text.startswith(b\"* \"))\n    text = text[2:]\n    if text.startswith((b\"OK \", b\"NO \")):\n        return tuple(text.split(b\" \", 1))\n    return parse_response([text])\n\n\ndef as_pairs(items):\n    i = 0\n    last_item = None\n    for item in items:\n        if i % 2:\n            yield last_item, item\n        else:\n            last_item = item\n        i += 1\n\n\ndef as_triplets(items):\n    a = iter(items)\n    return zip(a, a, a)\n\n\ndef _is8bit(data):\n    return isinstance(data, _literal) or any(b > 127 for b in data)\n\n\ndef _iter_with_last(items):\n    last_i = len(items) - 1\n    for i, item in enumerate(items):\n        yield item, i == last_i\n\n\n_not_present = object()\n\n\nclass _dict_bytes_normaliser:\n    \"\"\"Wrap a dict with unicode/bytes keys and normalise the keys to\n    bytes.\n    \"\"\"\n\n    def __init__(self, d):\n        self._d = d\n\n    def iteritems(self):\n        for key, value in self._d.items():\n            yield to_bytes(key), value\n\n    # For Python 3 compatibility.\n    items = iteritems\n\n    def __contains__(self, ink):\n        for k in self._gen_keys(ink):\n            if k in self._d:\n                return True\n        return False\n\n    def get(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d[k]\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def pop(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d.pop(k)\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def _gen_keys(self, k):\n        yield k\n        if isinstance(k, bytes):\n            yield to_unicode(k)\n        else:\n            yield to_bytes(k)\n\n\ndef debug_trunc(v, maxlen):\n    if len(v) < maxlen:\n        return repr(v)\n    hl = maxlen // 2\n    return repr(v[:hl]) + \"...\" + repr(v[-hl:])\n\n\ndef utf7_decode_sequence(seq):\n    return [decode_utf7(s) for s in seq]\n\n\ndef _parse_quota(quota_rep):\n    quota_rep = parse_response(quota_rep)\n    rv = []\n    for quota_root, quota_resource_infos in as_pairs(quota_rep):\n        for quota_resource_info in as_triplets(quota_resource_infos):\n            rv.append(\n                Quota(\n                    quota_root=to_unicode(quota_root),\n                    resource=to_unicode(quota_resource_info[0]),\n                    usage=quota_resource_info[1],\n                    limit=quota_resource_info[2],\n                )\n            )\n    return rv\n\n\nclass IMAPlibLoggerAdapter(LoggerAdapter):\n    \"\"\"Adapter preventing IMAP secrets from going to the logging facility.\"\"\"\n\n    def process(self, msg, kwargs):\n        # msg is usually unicode but see #367. Convert bytes to\n        # unicode if required.\n        if isinstance(msg, bytes):\n            msg = msg.decode(\"ascii\", \"ignore\")\n\n        for command in (\"LOGIN\", \"AUTHENTICATE\"):\n            if msg.startswith(\">\") and command in msg:\n                msg_start = msg.split(command)[0]\n                msg = \"{}{} **REDACTED**\".format(msg_start, command)\n                break\n        return super().process(msg, kwargs)\n", "prompt": "Please write a python function called 'expunge' base the context. This function is used to expunge messages from the selected folder in an IMAP client. If no messages are specified, it removes all messages with the \"\\Deleted\" flag set. If messages are specified, it removes the specified messages with the \"\\Deleted\" flag set. The function returns the server response message followed by a list of expunge responses. The implementation takes into account whether the client is using UIDs or not.:param self: IMAPClient. An instance of the IMAPClient class.\n:param messages: List of int or str. The messages to be expunged. Defaults to None.\n:return: Tuple. The server response message followed by a list of expunge responses if no messages are specified. None if messages are specified..\n        The context you need to refer to is as follows: # Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport dataclasses\nimport functools\nimport imaplib\nimport itertools\nimport re\nimport select\nimport socket\nimport ssl as ssl_lib\nimport sys\nimport warnings\nfrom datetime import date, datetime\nfrom logging import getLogger, LoggerAdapter\nfrom operator import itemgetter\nfrom typing import List, Optional\n\nfrom . import exceptions, imap4, tls\nfrom .datetime_util import datetime_to_INTERNALDATE\nfrom .imap_utf7 import decode as decode_utf7\nfrom .imap_utf7 import encode as encode_utf7\nfrom .response_parser import parse_fetch_response, parse_message_list, parse_response\nfrom .util import assert_imap_protocol, to_bytes, to_unicode\n\nif hasattr(select, \"poll\"):\n    POLL_SUPPORT = True\nelse:\n    # Fallback to select() on systems that don't support poll()\n    POLL_SUPPORT = False\n\n\nlogger = getLogger(__name__)\n\n__all__ = [\n    \"IMAPClient\",\n    \"SocketTimeout\",\n    \"DELETED\",\n    \"SEEN\",\n    \"ANSWERED\",\n    \"FLAGGED\",\n    \"DRAFT\",\n    \"RECENT\",\n]\n\n\n# We also offer the gmail-specific XLIST command...\nif \"XLIST\" not in imaplib.Commands:\n    imaplib.Commands[\"XLIST\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ...and IDLE\nif \"IDLE\" not in imaplib.Commands:\n    imaplib.Commands[\"IDLE\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ..and STARTTLS\nif \"STARTTLS\" not in imaplib.Commands:\n    imaplib.Commands[\"STARTTLS\"] = (\"NONAUTH\",)\n\n# ...and ID. RFC2971 says that this command is valid in all states,\n# but not that some servers (*cough* FastMail *cough*) don't seem to\n# accept it in state NONAUTH.\nif \"ID\" not in imaplib.Commands:\n    imaplib.Commands[\"ID\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ... and UNSELECT. RFC3691 does not specify the state but there is no\n# reason to use the command without AUTH state and a mailbox selected.\nif \"UNSELECT\" not in imaplib.Commands:\n    imaplib.Commands[\"UNSELECT\"] = (\"AUTH\", \"SELECTED\")\n\n# .. and ENABLE.\nif \"ENABLE\" not in imaplib.Commands:\n    imaplib.Commands[\"ENABLE\"] = (\"AUTH\",)\n\n# .. and MOVE for RFC6851.\nif \"MOVE\" not in imaplib.Commands:\n    imaplib.Commands[\"MOVE\"] = (\"AUTH\", \"SELECTED\")\n\n# System flags\nDELETED = rb\"\\Deleted\"\nSEEN = rb\"\\Seen\"\nANSWERED = rb\"\\Answered\"\nFLAGGED = rb\"\\Flagged\"\nDRAFT = rb\"\\Draft\"\nRECENT = rb\"\\Recent\"  # This flag is read-only\n\n# Special folders, see RFC6154\n# \\Flagged is omitted because it is the same as the flag defined above\nALL = rb\"\\All\"\nARCHIVE = rb\"\\Archive\"\nDRAFTS = rb\"\\Drafts\"\nJUNK = rb\"\\Junk\"\nSENT = rb\"\\Sent\"\nTRASH = rb\"\\Trash\"\n\n# Personal namespaces that are common among providers\n# used as a fallback when the server does not support the NAMESPACE capability\n_POPULAR_PERSONAL_NAMESPACES = ((\"\", \"\"), (\"INBOX.\", \".\"))\n\n# Names of special folders that are common among providers\n_POPULAR_SPECIAL_FOLDERS = {\n    SENT: (\"Sent\", \"Sent Items\", \"Sent items\"),\n    DRAFTS: (\"Drafts\",),\n    ARCHIVE: (\"Archive\",),\n    TRASH: (\"Trash\", \"Deleted Items\", \"Deleted Messages\", \"Deleted\"),\n    JUNK: (\"Junk\", \"Spam\"),\n}\n\n_RE_SELECT_RESPONSE = re.compile(rb\"\\[(?P<key>[A-Z-]+)( \\((?P<data>.*)\\))?\\]\")\n\n\nclass Namespace(tuple):\n    def __new__(cls, personal, other, shared):\n        return tuple.__new__(cls, (personal, other, shared))\n\n    personal = property(itemgetter(0))\n    other = property(itemgetter(1))\n    shared = property(itemgetter(2))\n\n\n@dataclasses.dataclass\nclass SocketTimeout:\n    \"\"\"Represents timeout configuration for an IMAP connection.\n\n    :ivar connect: maximum time to wait for a connection attempt to remote server\n    :ivar read: maximum time to wait for performing a read/write operation\n\n    As an example, ``SocketTimeout(connect=15, read=60)`` will make the socket\n    timeout if the connection takes more than 15 seconds to establish but\n    read/write operations can take up to 60 seconds once the connection is done.\n    \"\"\"\n\n    connect: float\n    read: float\n\n\n@dataclasses.dataclass\nclass MailboxQuotaRoots:\n    \"\"\"Quota roots associated with a mailbox.\n\n    Represents the response of a GETQUOTAROOT command.\n\n    :ivar mailbox: the mailbox\n    :ivar quota_roots: list of quota roots associated with the mailbox\n    \"\"\"\n\n    mailbox: str\n    quota_roots: List[str]\n\n\n@dataclasses.dataclass\nclass Quota:\n    \"\"\"Resource quota.\n\n    Represents the response of a GETQUOTA command.\n\n    :ivar quota_roots: the quota roots for which the limit apply\n    :ivar resource: the resource being limited (STORAGE, MESSAGES...)\n    :ivar usage: the current usage of the resource\n    :ivar limit: the maximum allowed usage of the resource\n    \"\"\"\n\n    quota_root: str\n    resource: str\n    usage: bytes\n    limit: bytes\n\n\ndef require_capability(capability):\n    \"\"\"Decorator raising CapabilityError when a capability is not available.\"\"\"\n\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def wrapper(client, *args, **kwargs):\n            if not client.has_capability(capability):\n                raise exceptions.CapabilityError(\n                    \"Server does not support {} capability\".format(capability)\n                )\n            return func(client, *args, **kwargs)\n\n        return wrapper\n\n    return actual_decorator\n\n\nclass IMAPClient:\n    \"\"\"A connection to the IMAP server specified by *host* is made when\n    this class is instantiated.\n\n    *port* defaults to 993, or 143 if *ssl* is ``False``.\n\n    If *use_uid* is ``True`` unique message UIDs be used for all calls\n    that accept message ids (defaults to ``True``).\n\n    If *ssl* is ``True`` (the default) a secure connection will be made.\n    Otherwise an insecure connection over plain text will be\n    established.\n\n    If *ssl* is ``True`` the optional *ssl_context* argument can be\n    used to provide an ``ssl.SSLContext`` instance used to\n    control SSL/TLS connection parameters. If this is not provided a\n    sensible default context will be used.\n\n    If *stream* is ``True`` then *host* is used as the command to run\n    to establish a connection to the IMAP server (defaults to\n    ``False``). This is useful for exotic connection or authentication\n    setups.\n\n    Use *timeout* to specify a timeout for the socket connected to the\n    IMAP server. The timeout can be either a float number, or an instance\n    of :py:class:`imapclient.SocketTimeout`.\n\n    * If a single float number is passed, the same timeout delay applies\n      during the  initial connection to the server and for all future socket\n      reads and writes.\n\n    * In case of a ``SocketTimeout``, connection timeout and\n      read/write operations can have distinct timeouts.\n\n    * The default is ``None``, where no timeout is used.\n\n    The *normalise_times* attribute specifies whether datetimes\n    returned by ``fetch()`` are normalised to the local system time\n    and include no timezone information (native), or are datetimes\n    that include timezone information (aware). By default\n    *normalise_times* is True (times are normalised to the local\n    system time). This attribute can be changed between ``fetch()``\n    calls if required.\n\n    Can be used as a context manager to automatically close opened connections:\n\n    >>> with IMAPClient(host=\"imap.foo.org\") as client:\n    ...     client.login(\"bar@foo.org\", \"passwd\")\n\n    \"\"\"\n\n    # Those exceptions are kept for backward-compatibility, since\n    # previous versions included these attributes as references to\n    # imaplib original exceptions\n    Error = exceptions.IMAPClientError\n    AbortError = exceptions.IMAPClientAbortError\n    ReadOnlyError = exceptions.IMAPClientReadOnlyError\n\n    def __init__(\n        self,\n        host: str,\n        port: int = None,\n        use_uid: bool = True,\n        ssl: bool = True,\n        stream: bool = False,\n        ssl_context: Optional[ssl_lib.SSLContext] = None,\n        timeout: Optional[float] = None,\n    ):\n        if stream:\n            if port is not None:\n                raise ValueError(\"can't set 'port' when 'stream' True\")\n            if ssl:\n                raise ValueError(\"can't use 'ssl' when 'stream' is True\")\n        elif port is None:\n            port = ssl and 993 or 143\n\n        if ssl and port == 143:\n            logger.warning(\n                \"Attempting to establish an encrypted connection \"\n                \"to a port (143) often used for unencrypted \"\n                \"connections\"\n            )\n\n        self.host = host\n        self.port = port\n        self.ssl = ssl\n        self.ssl_context = ssl_context\n        self.stream = stream\n        self.use_uid = use_uid\n        self.folder_encode = True\n        self.normalise_times = True\n\n        # If the user gives a single timeout value, assume it is the same for\n        # connection and read/write operations\n        if not isinstance(timeout, SocketTimeout):\n            timeout = SocketTimeout(timeout, timeout)\n\n        self._timeout = timeout\n        self._starttls_done = False\n        self._cached_capabilities = None\n        self._idle_tag = None\n\n        self._imap = self._create_IMAP4()\n        logger.debug(\n            \"Connected to host %s over %s\",\n            self.host,\n            \"SSL/TLS\" if ssl else \"plain text\",\n        )\n\n        self._set_read_timeout()\n        # Small hack to make imaplib log everything to its own logger\n        imaplib_logger = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._imap.debug = 5\n        self._imap._mesg = imaplib_logger.debug\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Logout and closes the connection when exiting the context manager.\n\n        All exceptions during logout and connection shutdown are caught because\n        an error here usually means the connection was already closed.\n        \"\"\"\n        try:\n            self.logout()\n        except Exception:\n            try:\n                self.shutdown()\n            except Exception as e:\n                logger.info(\"Could not close the connection cleanly: %s\", e)\n\n    def _create_IMAP4(self):\n        if self.stream:\n            return imaplib.IMAP4_stream(self.host)\n\n        connect_timeout = getattr(self._timeout, \"connect\", None)\n\n        if self.ssl:\n            return tls.IMAP4_TLS(\n                self.host,\n                self.port,\n                self.ssl_context,\n                connect_timeout,\n            )\n\n        return imap4.IMAP4WithTimeout(self.host, self.port, connect_timeout)\n\n    def _set_read_timeout(self):\n        if self._timeout is not None:\n            self.socket().settimeout(self._timeout.read)\n\n    @property\n    def _sock(self):\n        warnings.warn(\"_sock is deprecated. Use socket().\", DeprecationWarning)\n        return self.socket()\n\n    def socket(self):\n        \"\"\"Returns socket used to connect to server.\n\n        The socket is provided for polling purposes only.\n        It can be used in,\n        for example, :py:meth:`selectors.BaseSelector.register`\n        and :py:meth:`asyncio.loop.add_reader` to wait for data.\n\n        .. WARNING::\n           All other uses of the returned socket are unsupported.\n           This includes reading from and writing to the socket,\n           as they are likely to break internal bookkeeping of messages.\n        \"\"\"\n        # In py2, imaplib has sslobj (for SSL connections), and sock for non-SSL.\n        # In the py3 version it's just sock.\n        return getattr(self._imap, \"sslobj\", self._imap.sock)\n\n    @require_capability(\"STARTTLS\")\n    def starttls(self, ssl_context=None):\n        \"\"\"Switch to an SSL encrypted connection by sending a STARTTLS command.\n\n        The *ssl_context* argument is optional and should be a\n        :py:class:`ssl.SSLContext` object. If no SSL context is given, a SSL\n        context with reasonable default settings will be used.\n\n        You can enable checking of the hostname in the certificate presented\n        by the server  against the hostname which was used for connecting, by\n        setting the *check_hostname* attribute of the SSL context to ``True``.\n        The default SSL context has this setting enabled.\n\n        Raises :py:exc:`Error` if the SSL connection could not be established.\n\n        Raises :py:exc:`AbortError` if the server does not support STARTTLS\n        or an SSL connection is already established.\n        \"\"\"\n        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n\n    def login(self, username: str, password: str):\n        \"\"\"Login using *username* and *password*, returning the\n        server response.\n        \"\"\"\n        try:\n            rv = self._command_and_check(\n                \"login\",\n                to_unicode(username),\n                to_unicode(password),\n                unpack=True,\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n        logger.debug(\"Logged in as %s\", username)\n        return rv\n\n    def oauth2_login(\n        self,\n        user: str,\n        access_token: str,\n        mech: str = \"XOAUTH2\",\n        vendor: Optional[str] = None,\n    ):\n        \"\"\"Authenticate using the OAUTH2 or XOAUTH2 methods.\n\n        Gmail and Yahoo both support the 'XOAUTH2' mechanism, but Yahoo requires\n        the 'vendor' portion in the payload.\n        \"\"\"\n        auth_string = \"user=%s\\1auth=Bearer %s\\1\" % (user, access_token)\n        if vendor:\n            auth_string += \"vendor=%s\\1\" % vendor\n        auth_string += \"\\1\"\n        try:\n            return self._command_and_check(\"authenticate\", mech, lambda x: auth_string)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def oauthbearer_login(self, identity, access_token):\n        \"\"\"Authenticate using the OAUTHBEARER method.\n\n        This is supported by Gmail and is meant to supersede the non-standard\n        'OAUTH2' and 'XOAUTH2' mechanisms.\n        \"\"\"\n        # https://tools.ietf.org/html/rfc5801#section-4\n        # Technically this is the authorization_identity, but at least for Gmail it's\n        # mandatory and practically behaves like the regular username/identity.\n        if identity:\n            gs2_header = \"n,a=%s,\" % identity.replace(\"=\", \"=3D\").replace(\",\", \"=2C\")\n        else:\n            gs2_header = \"n,,\"\n        # https://tools.ietf.org/html/rfc6750#section-2.1\n        http_authz = \"Bearer %s\" % access_token\n        # https://tools.ietf.org/html/rfc7628#section-3.1\n        auth_string = \"%s\\1auth=%s\\1\\1\" % (gs2_header, http_authz)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"OAUTHBEARER\", lambda x: auth_string\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def plain_login(self, identity, password, authorization_identity=None):\n        \"\"\"Authenticate using the PLAIN method (requires server support).\"\"\"\n        if not authorization_identity:\n            authorization_identity = \"\"\n        auth_string = \"%s\\0%s\\0%s\" % (authorization_identity, identity, password)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"PLAIN\", lambda _: auth_string, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def sasl_login(self, mech_name, mech_callable):\n        \"\"\"Authenticate using a provided SASL mechanism (requires server support).\n\n        The *mech_callable* will be called with one parameter (the server\n        challenge as bytes) and must return the corresponding client response\n        (as bytes, or as string which will be automatically encoded).\n\n        It will be called as many times as the server produces challenges,\n        which will depend on the specific SASL mechanism. (If the mechanism is\n        defined as \"client-first\", the server will nevertheless produce a\n        zero-length challenge.)\n\n        For example, PLAIN has just one step with empty challenge, so a handler\n        might look like this::\n\n            plain_mech = lambda _: \"\\\\0%s\\\\0%s\" % (username, password)\n\n            imap.sasl_login(\"PLAIN\", plain_mech)\n\n        A more complex but still stateless handler might look like this::\n\n            def example_mech(challenge):\n                if challenge == b\"Username:\"\n                    return username.encode(\"utf-8\")\n                elif challenge == b\"Password:\"\n                    return password.encode(\"utf-8\")\n                else:\n                    return b\"\"\n\n            imap.sasl_login(\"EXAMPLE\", example_mech)\n\n        A stateful handler might look like this::\n\n            class ScramSha256SaslMechanism():\n                def __init__(self, username, password):\n                    ...\n\n                def __call__(self, challenge):\n                    self.step += 1\n                    if self.step == 1:\n                        response = ...\n                    elif self.step == 2:\n                        response = ...\n                    return response\n\n            scram_mech = ScramSha256SaslMechanism(username, password)\n\n            imap.sasl_login(\"SCRAM-SHA-256\", scram_mech)\n        \"\"\"\n        try:\n            return self._command_and_check(\n                \"authenticate\", mech_name, mech_callable, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def logout(self):\n        \"\"\"Logout, returning the server response.\"\"\"\n        typ, data = self._imap.logout()\n        self._check_resp(\"BYE\", \"logout\", typ, data)\n        logger.debug(\"Logged out, connection closed\")\n        return data[0]\n\n    def shutdown(self) -> None:\n        \"\"\"Close the connection to the IMAP server (without logging out)\n\n        In most cases, :py:meth:`.logout` should be used instead of\n        this. The logout method also shutdown down the connection.\n        \"\"\"\n        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n\n    @require_capability(\"ENABLE\")\n    def enable(self, *capabilities):\n        \"\"\"Activate one or more server side capability extensions.\n\n        Most capabilities do not need to be enabled. This is only\n        required for extensions which introduce backwards incompatible\n        behaviour. Two capabilities which may require enable are\n        ``CONDSTORE`` and ``UTF8=ACCEPT``.\n\n        A list of the requested extensions that were successfully\n        enabled on the server is returned.\n\n        Once enabled each extension remains active until the IMAP\n        connection is closed.\n\n        See :rfc:`5161` for more details.\n        \"\"\"\n        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n\n    @require_capability(\"ID\")\n    def id_(self, parameters=None):\n        \"\"\"Issue the ID command, returning a dict of server implementation\n        fields.\n\n        *parameters* should be specified as a dictionary of field/value pairs,\n        for example: ``{\"name\": \"IMAPClient\", \"version\": \"0.12\"}``\n        \"\"\"\n        if parameters is None:\n            args = \"NIL\"\n        else:\n            if not isinstance(parameters, dict):\n                raise TypeError(\"'parameters' should be a dictionary\")\n            args = seq_to_parenstr(\n                _quote(v) for v in itertools.chain.from_iterable(parameters.items())\n            )\n\n        typ, data = self._imap._simple_command(\"ID\", args)\n        self._checkok(\"id\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"ID\")\n        return parse_response(data)\n\n    def capabilities(self):\n        \"\"\"Returns the server capability list.\n\n        If the session is authenticated and the server has returned an\n        untagged CAPABILITY response at authentication time, this\n        response will be returned. Otherwise, the CAPABILITY command\n        will be issued to the server, with the results cached for\n        future calls.\n\n        If the session is not yet authenticated, the capabilities\n        requested at connection time will be returned.\n        \"\"\"\n        # Ensure cached capabilities aren't used post-STARTTLS. As per\n        # https://tools.ietf.org/html/rfc2595#section-3.1\n        if self._starttls_done and self._imap.state == \"NONAUTH\":\n            self._cached_capabilities = None\n            return self._do_capabilites()\n\n        # If a capability response has been cached, use that.\n        if self._cached_capabilities:\n            return self._cached_capabilities\n\n        # If the server returned an untagged CAPABILITY response\n        # (during authentication), cache it and return that.\n        untagged = _dict_bytes_normaliser(self._imap.untagged_responses)\n        response = untagged.pop(\"CAPABILITY\", None)\n        if response:\n            self._cached_capabilities = self._normalise_capabilites(response[0])\n            return self._cached_capabilities\n\n        # If authenticated, but don't have a capability response, ask for one\n        if self._imap.state in (\"SELECTED\", \"AUTH\"):\n            self._cached_capabilities = self._do_capabilites()\n            return self._cached_capabilities\n\n        # Return capabilities that imaplib requested at connection\n        # time (pre-auth)\n        return tuple(to_bytes(c) for c in self._imap.capabilities)\n\n    def _do_capabilites(self):\n        raw_response = self._command_and_check(\"capability\", unpack=True)\n        return self._normalise_capabilites(raw_response)\n\n    def _normalise_capabilites(self, raw_response):\n        raw_response = to_bytes(raw_response)\n        return tuple(raw_response.upper().split())\n\n    def has_capability(self, capability):\n        \"\"\"Return ``True`` if the IMAP server has the given *capability*.\"\"\"\n        # FIXME: this will not detect capabilities that are backwards\n        # compatible with the current level. For instance the SORT\n        # capabilities may in the future be named SORT2 which is\n        # still compatible with the current standard and will not\n        # be detected by this method.\n        return to_bytes(capability).upper() in self.capabilities()\n\n    @require_capability(\"NAMESPACE\")\n    def namespace(self):\n        \"\"\"Return the namespace for the account as a (personal, other,\n        shared) tuple.\n\n        Each element may be None if no namespace of that type exists,\n        or a sequence of (prefix, separator) pairs.\n\n        For convenience the tuple elements may be accessed\n        positionally or using attributes named *personal*, *other* and\n        *shared*.\n\n        See :rfc:`2342` for more details.\n        \"\"\"\n        data = self._command_and_check(\"namespace\")\n        parts = []\n        for item in parse_response(data):\n            if item is None:\n                parts.append(item)\n            else:\n                converted = []\n                for prefix, separator in item:\n                    if self.folder_encode:\n                        prefix = decode_utf7(prefix)\n                    converted.append((prefix, to_unicode(separator)))\n                parts.append(tuple(converted))\n        return Namespace(*parts)\n\n    def list_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Get a listing of folders on the server as a list of\n        ``(flags, delimiter, name)`` tuples.\n\n        Specifying *directory* will limit returned folders to the\n        given base directory. The directory and any child directories\n        will returned.\n\n        Specifying *pattern* will limit returned folders to those with\n        matching names. The wildcards are supported in\n        *pattern*. ``*`` matches zero or more of any character and\n        ``%`` matches 0 or more characters except the folder\n        delimiter.\n\n        Calling list_folders with no arguments will recursively list\n        all folders available for the logged in user.\n\n        Folder names are always returned as unicode strings, and\n        decoded from modified UTF-7, except if folder_decode is not\n        set.\n        \"\"\"\n        return self._do_list(\"LIST\", directory, pattern)\n\n    @require_capability(\"XLIST\")\n    def xlist_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Execute the XLIST command, returning ``(flags, delimiter,\n        name)`` tuples.\n\n        This method returns special flags for each folder and a\n        localized name for certain folders (e.g. the name of the\n        inbox may be localized and the flags can be used to\n        determine the actual inbox, even if the name has been\n        localized.\n\n        A ``XLIST`` response could look something like::\n\n            [((b'\\\\HasNoChildren', b'\\\\Inbox'), b'/', u'Inbox'),\n             ((b'\\\\Noselect', b'\\\\HasChildren'), b'/', u'[Gmail]'),\n             ((b'\\\\HasNoChildren', b'\\\\AllMail'), b'/', u'[Gmail]/All Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Drafts'), b'/', u'[Gmail]/Drafts'),\n             ((b'\\\\HasNoChildren', b'\\\\Important'), b'/', u'[Gmail]/Important'),\n             ((b'\\\\HasNoChildren', b'\\\\Sent'), b'/', u'[Gmail]/Sent Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Spam'), b'/', u'[Gmail]/Spam'),\n             ((b'\\\\HasNoChildren', b'\\\\Starred'), b'/', u'[Gmail]/Starred'),\n             ((b'\\\\HasNoChildren', b'\\\\Trash'), b'/', u'[Gmail]/Trash')]\n\n        This is a *deprecated* Gmail-specific IMAP extension (See\n        https://developers.google.com/gmail/imap_extensions#xlist_is_deprecated\n        for more information).\n\n        The *directory* and *pattern* arguments are as per\n        list_folders().\n        \"\"\"\n        return self._do_list(\"XLIST\", directory, pattern)\n\n    def list_sub_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Return a list of subscribed folders on the server as\n        ``(flags, delimiter, name)`` tuples.\n\n        The default behaviour will list all subscribed folders. The\n        *directory* and *pattern* arguments are as per list_folders().\n        \"\"\"\n        return self._do_list(\"LSUB\", directory, pattern)\n\n    def _do_list(self, cmd, directory, pattern):\n        directory = self._normalise_folder(directory)\n        pattern = self._normalise_folder(pattern)\n        typ, dat = self._imap._simple_command(cmd, directory, pattern)\n        self._checkok(cmd, typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, cmd)\n        return self._proc_folder_list(dat)\n\n    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n\n    def find_special_folder(self, folder_flag):\n        \"\"\"Try to locate a special folder, like the Sent or Trash folder.\n\n        >>> server.find_special_folder(imapclient.SENT)\n        'INBOX.Sent'\n\n        This function tries its best to find the correct folder (if any) but\n        uses heuristics when the server is unable to precisely tell where\n        special folders are located.\n\n        Returns the name of the folder if found, or None otherwise.\n        \"\"\"\n        # Detect folder by looking for known attributes\n        # TODO: avoid listing all folders by using extended LIST (RFC6154)\n        for folder in self.list_folders():\n            if folder and len(folder[0]) > 0 and folder_flag in folder[0]:\n                return folder[2]\n\n        # Detect folder by looking for common names\n        # We only look for folders in the \"personal\" namespace of the user\n        if self.has_capability(\"NAMESPACE\"):\n            personal_namespaces = self.namespace().personal\n        else:\n            personal_namespaces = _POPULAR_PERSONAL_NAMESPACES\n\n        for personal_namespace in personal_namespaces:\n            for pattern in _POPULAR_SPECIAL_FOLDERS.get(folder_flag, tuple()):\n                pattern = personal_namespace[0] + pattern\n                sent_folders = self.list_folders(pattern=pattern)\n                if sent_folders:\n                    return sent_folders[0][2]\n\n        return None\n\n    def select_folder(self, folder, readonly=False):\n        \"\"\"Set the current folder on the server.\n\n        Future calls to methods such as search and fetch will act on\n        the selected folder.\n\n        Returns a dictionary containing the ``SELECT`` response. At least\n        the ``b'EXISTS'``, ``b'FLAGS'`` and ``b'RECENT'`` keys are guaranteed\n        to exist. An example::\n\n            {b'EXISTS': 3,\n             b'FLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'RECENT': 0,\n             b'PERMANENTFLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'READ-WRITE': True,\n             b'UIDNEXT': 11,\n             b'UIDVALIDITY': 1239278212}\n        \"\"\"\n        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n\n    @require_capability(\"UNSELECT\")\n    def unselect_folder(self):\n        r\"\"\"Unselect the current folder and release associated resources.\n\n        Unlike ``close_folder``, the ``UNSELECT`` command does not expunge\n        the mailbox, keeping messages with \\Deleted flag set for example.\n\n        Returns the UNSELECT response string returned by the server.\n        \"\"\"\n        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n\n    def _process_select_response(self, resp):\n        untagged = _dict_bytes_normaliser(resp)\n        out = {}\n\n        # imaplib doesn't parse these correctly (broken regex) so replace\n        # with the raw values out of the OK section\n        for line in untagged.get(\"OK\", []):\n            match = _RE_SELECT_RESPONSE.match(line)\n            if match:\n                key = match.group(\"key\")\n                if key == b\"PERMANENTFLAGS\":\n                    out[key] = tuple(match.group(\"data\").split())\n\n        for key, value in untagged.items():\n            key = key.upper()\n            if key in (b\"OK\", b\"PERMANENTFLAGS\"):\n                continue  # already handled above\n            if key in (\n                b\"EXISTS\",\n                b\"RECENT\",\n                b\"UIDNEXT\",\n                b\"UIDVALIDITY\",\n                b\"HIGHESTMODSEQ\",\n            ):\n                value = int(value[0])\n            elif key == b\"READ-WRITE\":\n                value = True\n            elif key == b\"FLAGS\":\n                value = tuple(value[0][1:-1].split())\n            out[key] = value\n        return out\n\n    def noop(self):\n        \"\"\"Execute the NOOP command.\n\n        This command returns immediately, returning any server side\n        status updates. It can also be used to reset any auto-logout\n        timers.\n\n        The return value is the server command response message\n        followed by a list of status responses. For example::\n\n            (b'NOOP completed.',\n             [(4, b'EXISTS'),\n              (3, b'FETCH', (b'FLAGS', (b'bar', b'sne'))),\n              (6, b'FETCH', (b'FLAGS', (b'sne',)))])\n\n        \"\"\"\n        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n\n    @require_capability(\"IDLE\")\n    def idle(self):\n        \"\"\"Put the server into IDLE mode.\n\n        In this mode the server will return unsolicited responses\n        about changes to the selected mailbox. This method returns\n        immediately. Use ``idle_check()`` to look for IDLE responses\n        and ``idle_done()`` to stop IDLE mode.\n\n        .. note::\n\n            Any other commands issued while the server is in IDLE\n            mode will fail.\n\n        See :rfc:`2177` for more information about the IDLE extension.\n        \"\"\"\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n    def _poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is more scalable because it ALLOWS your process\n        to have more than 1024 file descriptors.\n        \"\"\"\n        poller = select.poll()\n        poller.register(sock.fileno(), select.POLLIN)\n        timeout = timeout * 1000 if timeout is not None else None\n        return poller.poll(timeout)\n\n    def _select_poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is a fallback because it FAILS if your process\n        has more than 1024 file descriptors.\n        We still need this for Windows and some other niche systems.\n        \"\"\"\n        return select.select([sock], [], [], timeout)[0]\n\n    @require_capability(\"IDLE\")\n    def idle_check(self, timeout=None):\n        \"\"\"Check for any IDLE responses sent by the server.\n\n        This method should only be called if the server is in IDLE\n        mode (see ``idle()``).\n\n        By default, this method will block until an IDLE response is\n        received. If *timeout* is provided, the call will block for at\n        most this many seconds while waiting for an IDLE response.\n\n        The return value is a list of received IDLE responses. These\n        will be parsed with values converted to appropriate types. For\n        example::\n\n            [(b'OK', b'Still here'),\n             (1, b'EXISTS'),\n             (1, b'FETCH', (b'FLAGS', (b'\\\\NotJunk',)))]\n        \"\"\"\n        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n\n    @require_capability(\"IDLE\")\n    def idle_done(self):\n        \"\"\"Take the server out of IDLE mode.\n\n        This method should only be called if the server is already in\n        IDLE mode.\n\n        The return value is of the form ``(command_text,\n        idle_responses)`` where *command_text* is the text sent by the\n        server when the IDLE command finished (eg. ``b'Idle\n        terminated'``) and *idle_responses* is a list of parsed idle\n        responses received since the last call to ``idle_check()`` (if\n        any). These are returned in parsed form as per\n        ``idle_check()``.\n        \"\"\"\n        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n    def folder_status(self, folder, what=None):\n        \"\"\"Return the status of *folder*.\n\n        *what* should be a sequence of status items to query. This\n        defaults to ``('MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY',\n        'UNSEEN')``.\n\n        Returns a dictionary of the status items for the folder with\n        keys matching *what*.\n        \"\"\"\n        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n\n    def close_folder(self):\n        \"\"\"Close the currently selected folder, returning the server\n        response string.\n        \"\"\"\n        return self._command_and_check(\"close\", unpack=True)\n\n    def create_folder(self, folder):\n        \"\"\"Create *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"create\", self._normalise_folder(folder), unpack=True\n        )\n\n    def rename_folder(self, old_name, new_name):\n        \"\"\"Change the name of a folder on the server.\"\"\"\n        return self._command_and_check(\n            \"rename\",\n            self._normalise_folder(old_name),\n            self._normalise_folder(new_name),\n            unpack=True,\n        )\n\n    def delete_folder(self, folder):\n        \"\"\"Delete *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"delete\", self._normalise_folder(folder), unpack=True\n        )\n\n    def folder_exists(self, folder):\n        \"\"\"Return ``True`` if *folder* exists on the server.\"\"\"\n        return len(self.list_folders(\"\", folder)) > 0\n\n    def subscribe_folder(self, folder):\n        \"\"\"Subscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"subscribe\", self._normalise_folder(folder))\n\n    def unsubscribe_folder(self, folder):\n        \"\"\"Unsubscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"unsubscribe\", self._normalise_folder(folder))\n\n    def search(self, criteria=\"ALL\", charset=None):\n        \"\"\"Return a list of messages ids from the currently selected\n        folder matching *criteria*.\n\n        *criteria* should be a sequence of one or more criteria\n        items. Each criteria item may be either unicode or\n        bytes. Example values::\n\n            [u'UNSEEN']\n            [u'SMALLER', 500]\n            [b'NOT', b'DELETED']\n            [u'TEXT', u'foo bar', u'FLAGGED', u'SUBJECT', u'baz']\n            [u'SINCE', date(2005, 4, 3)]\n\n        IMAPClient will perform conversion and quoting as\n        required. The caller shouldn't do this.\n\n        It is also possible (but not recommended) to pass the combined\n        criteria as a single string. In this case IMAPClient won't\n        perform quoting, allowing lower-level specification of\n        criteria. Examples of this style::\n\n            u'UNSEEN'\n            u'SMALLER 500'\n            b'NOT DELETED'\n            u'TEXT \"foo bar\" FLAGGED SUBJECT \"baz\"'\n            b'SINCE 03-Apr-2005'\n\n        To support complex search expressions, criteria lists can be\n        nested. IMAPClient will insert parentheses in the right\n        places. The following will match messages that are both not\n        flagged and do not have \"foo\" in the subject::\n\n            ['NOT', ['SUBJECT', 'foo', 'FLAGGED']]\n\n        *charset* specifies the character set of the criteria. It\n        defaults to US-ASCII as this is the only charset that a server\n        is required to support by the RFC. UTF-8 is commonly supported\n        however.\n\n        Any criteria specified using unicode will be encoded as per\n        *charset*. Specifying a unicode criteria that can not be\n        encoded using *charset* will result in an error.\n\n        Any criteria specified using bytes will be sent as-is but\n        should use an encoding that matches *charset* (the character\n        set given is still passed on to the server).\n\n        See :rfc:`3501#section-6.4.4` for more details.\n\n        Note that criteria arguments that are 8-bit will be\n        transparently sent by IMAPClient as IMAP literals to ensure\n        adherence to IMAP standards.\n\n        The returned list of message ids will have a special *modseq*\n        attribute. This is set if the server included a MODSEQ value\n        to the search response (i.e. if a MODSEQ criteria was included\n        in the search).\n\n        \"\"\"\n        return self._search(criteria, charset)\n\n    @require_capability(\"X-GM-EXT-1\")\n    def gmail_search(self, query, charset=\"UTF-8\"):\n        \"\"\"Search using Gmail's X-GM-RAW attribute.\n\n        *query* should be a valid Gmail search query string. For\n        example: ``has:attachment in:unread``. The search string may\n        be unicode and will be encoded using the specified *charset*\n        (defaulting to UTF-8).\n\n        This method only works for IMAP servers that support X-GM-RAW,\n        which is only likely to be Gmail.\n\n        See https://developers.google.com/gmail/imap_extensions#extension_of_the_search_command_x-gm-raw\n        for more info.\n        \"\"\"\n        return self._search([b\"X-GM-RAW\", query], charset)\n\n    def _search(self, criteria, charset):\n        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n\n    @require_capability(\"SORT\")\n    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of message ids from the currently selected\n        folder, sorted by *sort_criteria* and optionally filtered by\n        *criteria*.\n\n        *sort_criteria* may be specified as a sequence of strings or a\n        single string. IMAPClient will take care any required\n        conversions. Valid *sort_criteria* values::\n\n            ['ARRIVAL']\n            ['SUBJECT', 'ARRIVAL']\n            'ARRIVAL'\n            'REVERSE SIZE'\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for full details.\n\n        Note that SORT is an extension to the IMAP4 standard so it may\n        not be supported by all IMAP servers.\n        \"\"\"\n        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n\n    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of messages threads from the currently\n        selected folder which match *criteria*.\n\n        Each returned thread is a list of messages ids. An example\n        return value containing three message threads::\n\n            ((1, 2), (3,), (4, 5, 6))\n\n        The optional *algorithm* argument specifies the threading\n        algorithm to use.\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for more details.\n        \"\"\"\n        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)\n\n    def get_flags(self, messages):\n        \"\"\"Return the flags set for each message in *messages* from\n        the currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (flag1, flag2, ... ), }``.\n        \"\"\"\n        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n\n    def add_flags(self, messages, flags, silent=False):\n        \"\"\"Add *flags* to *messages* in the currently selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"+FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def remove_flags(self, messages, flags, silent=False):\n        \"\"\"Remove one or more *flags* from *messages* in the currently\n        selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"-FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def set_flags(self, messages, flags, silent=False):\n        \"\"\"Set the *flags* for *messages* in the currently selected\n        folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def get_gmail_labels(self, messages):\n        \"\"\"Return the label set for each message in *messages* in the\n        currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (label1, label2, ... ), }``.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n\n    def add_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Add *labels* to *messages* in the currently selected folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"+X-GM-LABELS\", messages, labels, silent=silent)\n\n    def remove_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Remove one or more *labels* from *messages* in the\n        currently selected folder, or None if *silent* is true.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*).\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"-X-GM-LABELS\", messages, labels, silent=silent)\n\n    def set_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Set the *labels* for *messages* in the currently selected\n        folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"X-GM-LABELS\", messages, labels, silent=silent)\n\n    def delete_messages(self, messages, silent=False):\n        \"\"\"Delete one or more *messages* from the currently selected\n        folder.\n\n        Returns the flags set for each modified message (see\n        *get_flags*).\n        \"\"\"\n        return self.add_flags(messages, DELETED, silent=silent)\n\n    def fetch(self, messages, data, modifiers=None):\n        \"\"\"Retrieve selected *data* associated with one or more\n        *messages* in the currently selected folder.\n\n        *data* should be specified as a sequence of strings, one item\n        per data selector, for example ``['INTERNALDATE',\n        'RFC822']``.\n\n        *modifiers* are required for some extensions to the IMAP\n        protocol (eg. :rfc:`4551`). These should be a sequence of strings\n        if specified, for example ``['CHANGEDSINCE 123']``.\n\n        A dictionary is returned, indexed by message number. Each item\n        in this dictionary is also a dictionary, with an entry\n        corresponding to each item in *data*. Returned values will be\n        appropriately typed. For example, integer values will be returned as\n        Python integers, timestamps will be returned as datetime\n        instances and ENVELOPE responses will be returned as\n        :py:class:`Envelope <imapclient.response_types.Envelope>` instances.\n\n        String data will generally be returned as bytes (Python 3) or\n        str (Python 2).\n\n        In addition to an element for each *data* item, the dict\n        returned for each message also contains a *SEQ* key containing\n        the sequence number for the message. This allows for mapping\n        between the UID and sequence number (when the *use_uid*\n        property is ``True``).\n\n        Example::\n\n            >> c.fetch([3293, 3230], ['INTERNALDATE', 'FLAGS'])\n            {3230: {b'FLAGS': (b'\\\\Seen',),\n                    b'INTERNALDATE': datetime.datetime(2011, 1, 30, 13, 32, 9),\n                    b'SEQ': 84},\n             3293: {b'FLAGS': (),\n                    b'INTERNALDATE': datetime.datetime(2011, 2, 24, 19, 30, 36),\n                    b'SEQ': 110}}\n\n        \"\"\"\n        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            seq_to_parenstr_upper(data),\n            seq_to_parenstr_upper(modifiers) if modifiers else None,\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n\n    def append(self, folder, msg, flags=(), msg_time=None):\n        \"\"\"Append a message to *folder*.\n\n        *msg* should be a string contains the full message including\n        headers.\n\n        *flags* should be a sequence of message flags to set. If not\n        specified no flags will be set.\n\n        *msg_time* is an optional datetime instance specifying the\n        date and time to set on the message. The server will set a\n        time if it isn't specified. If *msg_time* contains timezone\n        information (tzinfo), this will be honoured. Otherwise the\n        local machine's time zone sent to the server.\n\n        Returns the APPEND response as returned by the server.\n        \"\"\"\n        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n\n    @require_capability(\"MULTIAPPEND\")\n    def multiappend(self, folder, msgs):\n        \"\"\"Append messages to *folder* using the MULTIAPPEND feature from :rfc:`3502`.\n\n        *msgs* must be an iterable. Each item must be either a string containing the\n        full message including headers, or a dict containing the keys \"msg\" with the\n        full message as before, \"flags\" with a sequence of message flags to set, and\n        \"date\" with a datetime instance specifying the internal date to set.\n        The keys \"flags\" and \"date\" are optional.\n\n        Returns the APPEND response from the server.\n        \"\"\"\n\n        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n\n    def copy(self, messages, folder):\n        \"\"\"Copy one or more messages from the current folder to\n        *folder*. Returns the COPY response string returned by the\n        server.\n        \"\"\"\n        return self._command_and_check(\n            \"copy\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    @require_capability(\"MOVE\")\n    def move(self, messages, folder):\n        \"\"\"Atomically move messages to another folder.\n\n        Requires the MOVE capability, see :rfc:`6851`.\n\n        :param messages: List of message UIDs to move.\n        :param folder: The destination folder name.\n        \"\"\"\n        return self._command_and_check(\n            \"move\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n###The function: expunge###\n    @require_capability(\"UIDPLUS\")\n    def uid_expunge(self, messages):\n        \"\"\"Expunge deleted messages with the specified message ids from the\n        folder.\n\n        This requires the UIDPLUS capability.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=True)\n\n    @require_capability(\"ACL\")\n    def getacl(self, folder):\n        \"\"\"Returns a list of ``(who, acl)`` tuples describing the\n        access controls for *folder*.\n        \"\"\"\n        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n\n    @require_capability(\"ACL\")\n    def setacl(self, folder, who, what):\n        \"\"\"Set an ACL (*what*) for user (*who*) for a folder.\n\n        Set *what* to an empty string to remove an ACL. Returns the\n        server response string.\n        \"\"\"\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n\n    @require_capability(\"QUOTA\")\n    def get_quota(self, mailbox=\"INBOX\"):\n        \"\"\"Get the quotas associated with a mailbox.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return self.get_quota_root(mailbox)[1]\n\n    @require_capability(\"QUOTA\")\n    def _get_quota(self, quota_root=\"\"):\n        \"\"\"Get the quotas associated with a quota root.\n\n        This method is not private but put behind an underscore to show that\n        it is a low-level function. Users probably want to use `get_quota`\n        instead.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return _parse_quota(self._command_and_check(\"getquota\", _quote(quota_root)))\n\n    @require_capability(\"QUOTA\")\n    def get_quota_root(self, mailbox):\n        \"\"\"Get the quota roots for a mailbox.\n\n        The IMAP server responds with the quota root and the quotas associated\n        so there is usually no need to call `get_quota` after.\n\n        See :rfc:`2087` for more details.\n\n        Return a tuple of MailboxQuotaRoots and list of Quota associated\n        \"\"\"\n        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n\n    @require_capability(\"QUOTA\")\n    def set_quota(self, quotas):\n        \"\"\"Set one or more quotas on resources.\n\n        :param quotas: list of Quota objects\n        \"\"\"\n        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n\n    def _check_resp(self, expected, command, typ, data):\n        \"\"\"Check command responses for errors.\n\n        Raises IMAPClient.Error if the command fails.\n        \"\"\"\n        if typ != expected:\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(data[0]))\n            )\n\n    def _consume_until_tagged_response(self, tag, command):\n        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n\n    def _raw_command_untagged(\n        self, command, args, response_name=None, unpack=False, uid=True\n    ):\n        # TODO: eventually this should replace _command_and_check (call it _command)\n        typ, data = self._raw_command(command, args, uid=uid)\n        if response_name is None:\n            response_name = command\n        typ, data = self._imap._untagged_response(typ, data, to_unicode(response_name))\n        self._checkok(to_unicode(command), typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _raw_command(self, command, args, uid=True):\n        \"\"\"Run the specific command with the arguments given. 8-bit arguments\n        are sent as literals. The return value is (typ, data).\n\n        This sidesteps much of imaplib's command sending\n        infrastructure because imaplib can't send more than one\n        literal.\n\n        *command* should be specified as bytes.\n        *args* should be specified as a list of bytes.\n        \"\"\"\n        command = command.upper()\n\n        if isinstance(args, tuple):\n            args = list(args)\n        if not isinstance(args, list):\n            args = [args]\n\n        tag = self._imap._new_tag()\n        prefix = [to_bytes(tag)]\n        if uid and self.use_uid:\n            prefix.append(b\"UID\")\n        prefix.append(command)\n\n        line = []\n        for item, is_last in _iter_with_last(prefix + args):\n            if not isinstance(item, bytes):\n                raise ValueError(\"command args must be passed as bytes\")\n\n            if _is8bit(item):\n                # If a line was already started send it\n                if line:\n                    out = b\" \".join(line)\n                    logger.debug(\"> %s\", out)\n                    self._imap.send(out)\n                    line = []\n\n                # Now send the (unquoted) literal\n                if isinstance(item, _quoted):\n                    item = item.original\n                self._send_literal(tag, item)\n                if not is_last:\n                    self._imap.send(b\" \")\n            else:\n                line.append(item)\n\n        if line:\n            out = b\" \".join(line)\n            logger.debug(\"> %s\", out)\n            self._imap.send(out)\n\n        self._imap.send(b\"\\r\\n\")\n\n        return self._imap._command_complete(to_unicode(command), tag)\n\n    def _send_literal(self, tag, item):\n        \"\"\"Send a single literal for the command with *tag*.\"\"\"\n        if b\"LITERAL+\" in self._cached_capabilities:\n            out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"+}\\r\\n\" + item\n            logger.debug(\"> %s\", debug_trunc(out, 64))\n            self._imap.send(out)\n            return\n\n        out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"}\\r\\n\"\n        logger.debug(\"> %s\", out)\n        self._imap.send(out)\n\n        # Wait for continuation response\n        while self._imap._get_response():\n            tagged_resp = self._imap.tagged_commands.get(tag)\n            if tagged_resp:\n                raise exceptions.IMAPClientAbortError(\n                    \"unexpected response while waiting for continuation response: \"\n                    + repr(tagged_resp)\n                )\n\n        logger.debug(\"   (literal) > %s\", debug_trunc(item, 256))\n        self._imap.send(item)\n\n    def _command_and_check(\n        self, command, *args, unpack: bool = False, uid: bool = False\n    ):\n        if uid and self.use_uid:\n            command = to_unicode(command)  # imaplib must die\n            typ, data = self._imap.uid(command, *args)\n        else:\n            meth = getattr(self._imap, to_unicode(command))\n            typ, data = meth(*args)\n        self._checkok(command, typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _checkok(self, command, typ, data):\n        self._check_resp(\"OK\", command, typ, data)\n\n    def _gm_label_store(self, cmd, messages, labels, silent):\n        response = self._store(\n            cmd, messages, self._normalise_labels(labels), b\"X-GM-LABELS\", silent=silent\n        )\n        return (\n            {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n            if response\n            else None\n        )\n\n    def _store(self, cmd, messages, flags, fetch_key, silent):\n        \"\"\"Worker function for the various flag manipulation methods.\n\n        *cmd* is the STORE command to use (eg. '+FLAGS').\n        \"\"\"\n        if not messages:\n            return {}\n        if silent:\n            cmd += b\".SILENT\"\n\n        data = self._command_and_check(\n            \"store\", join_message_ids(messages), cmd, seq_to_parenstr(flags), uid=True\n        )\n        if silent:\n            return None\n        return self._filter_fetch_dict(parse_fetch_response(data), fetch_key)\n\n    def _filter_fetch_dict(self, fetch_dict, key):\n        return dict((msgid, data[key]) for msgid, data in fetch_dict.items())\n\n    def _normalise_folder(self, folder_name):\n        if isinstance(folder_name, bytes):\n            folder_name = folder_name.decode(\"ascii\")\n        if self.folder_encode:\n            folder_name = encode_utf7(folder_name)\n        return _quote(folder_name)\n\n    def _normalise_labels(self, labels):\n        if isinstance(labels, (str, bytes)):\n            labels = (labels,)\n        return [_quote(encode_utf7(label)) for label in labels]\n\n    @property\n    def welcome(self):\n        \"\"\"access the server greeting message\"\"\"\n        try:\n            return self._imap.welcome\n        except AttributeError:\n            pass\n\n\ndef _quote(arg):\n    if isinstance(arg, str):\n        arg = arg.replace(\"\\\\\", \"\\\\\\\\\")\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b\"\\\\\", b\"\\\\\\\\\")\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q\n\n\ndef _normalise_search_criteria(criteria, charset=None):\n    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n\n\ndef _normalise_sort_criteria(criteria, charset=None):\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    return b\"(\" + b\" \".join(to_bytes(item).upper() for item in criteria) + b\")\"\n\n\nclass _literal(bytes):\n    \"\"\"Hold message data that should always be sent as a literal.\"\"\"\n\n\nclass _quoted(bytes):\n    \"\"\"\n    This class holds a quoted bytes value which provides access to the\n    unquoted value via the *original* attribute.\n\n    They should be created via the *maybe* classmethod.\n    \"\"\"\n\n    @classmethod\n    def maybe(cls, original):\n        \"\"\"Maybe quote a bytes value.\n\n        If the input requires no quoting it is returned unchanged.\n\n        If quoting is required a *_quoted* instance is returned. This\n        holds the quoted version of the input while also providing\n        access to the original unquoted source.\n        \"\"\"\n        quoted = original.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted = quoted.replace(b'\"', b'\\\\\"')\n        if quoted != original or b\" \" in quoted or not quoted:\n            out = cls(b'\"' + quoted + b'\"')\n            out.original = original\n            return out\n        return original\n\n\n# normalise_text_list, seq_to_parentstr etc have to return unicode\n# because imaplib handles flags and sort criteria assuming these are\n# passed as unicode\ndef normalise_text_list(items):\n    return list(_normalise_text_list(items))\n\n\ndef seq_to_parenstr(items):\n    return _join_and_paren(_normalise_text_list(items))\n\n\ndef seq_to_parenstr_upper(items):\n    return _join_and_paren(item.upper() for item in _normalise_text_list(items))\n\n\ndef _join_and_paren(items):\n    return \"(\" + \" \".join(items) + \")\"\n\n\ndef _normalise_text_list(items):\n    if isinstance(items, (str, bytes)):\n        items = (items,)\n    return (to_unicode(c) for c in items)\n\n\ndef join_message_ids(messages):\n    \"\"\"Convert a sequence of messages ids or a single integer message id\n    into an id byte string for use with IMAP commands\n    \"\"\"\n    if isinstance(messages, (str, bytes, int)):\n        messages = (to_bytes(messages),)\n    return b\",\".join(_maybe_int_to_bytes(m) for m in messages)\n\n\ndef _maybe_int_to_bytes(val):\n    if isinstance(val, int):\n        return str(val).encode(\"us-ascii\")\n    return to_bytes(val)\n\n\ndef _parse_untagged_response(text):\n    assert_imap_protocol(text.startswith(b\"* \"))\n    text = text[2:]\n    if text.startswith((b\"OK \", b\"NO \")):\n        return tuple(text.split(b\" \", 1))\n    return parse_response([text])\n\n\ndef as_pairs(items):\n    i = 0\n    last_item = None\n    for item in items:\n        if i % 2:\n            yield last_item, item\n        else:\n            last_item = item\n        i += 1\n\n\ndef as_triplets(items):\n    a = iter(items)\n    return zip(a, a, a)\n\n\ndef _is8bit(data):\n    return isinstance(data, _literal) or any(b > 127 for b in data)\n\n\ndef _iter_with_last(items):\n    last_i = len(items) - 1\n    for i, item in enumerate(items):\n        yield item, i == last_i\n\n\n_not_present = object()\n\n\nclass _dict_bytes_normaliser:\n    \"\"\"Wrap a dict with unicode/bytes keys and normalise the keys to\n    bytes.\n    \"\"\"\n\n    def __init__(self, d):\n        self._d = d\n\n    def iteritems(self):\n        for key, value in self._d.items():\n            yield to_bytes(key), value\n\n    # For Python 3 compatibility.\n    items = iteritems\n\n    def __contains__(self, ink):\n        for k in self._gen_keys(ink):\n            if k in self._d:\n                return True\n        return False\n\n    def get(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d[k]\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def pop(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d.pop(k)\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def _gen_keys(self, k):\n        yield k\n        if isinstance(k, bytes):\n            yield to_unicode(k)\n        else:\n            yield to_bytes(k)\n\n\ndef debug_trunc(v, maxlen):\n    if len(v) < maxlen:\n        return repr(v)\n    hl = maxlen // 2\n    return repr(v[:hl]) + \"...\" + repr(v[-hl:])\n\n\ndef utf7_decode_sequence(seq):\n    return [decode_utf7(s) for s in seq]\n\n\ndef _parse_quota(quota_rep):\n    quota_rep = parse_response(quota_rep)\n    rv = []\n    for quota_root, quota_resource_infos in as_pairs(quota_rep):\n        for quota_resource_info in as_triplets(quota_resource_infos):\n            rv.append(\n                Quota(\n                    quota_root=to_unicode(quota_root),\n                    resource=to_unicode(quota_resource_info[0]),\n                    usage=quota_resource_info[1],\n                    limit=quota_resource_info[2],\n                )\n            )\n    return rv\n\n\nclass IMAPlibLoggerAdapter(LoggerAdapter):\n    \"\"\"Adapter preventing IMAP secrets from going to the logging facility.\"\"\"\n\n    def process(self, msg, kwargs):\n        # msg is usually unicode but see #367. Convert bytes to\n        # unicode if required.\n        if isinstance(msg, bytes):\n            msg = msg.decode(\"ascii\", \"ignore\")\n\n        for command in (\"LOGIN\", \"AUTHENTICATE\"):\n            if msg.startswith(\">\") and command in msg:\n                msg_start = msg.split(command)[0]\n                msg = \"{}{} **REDACTED**\".format(msg_start, command)\n                break\n        return super().process(msg, kwargs)\n", "test_list": ["def test_expunge(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=sentinel.out)\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge()\n    mockCommand.assert_called_with('EXPUNGE')\n    mockConsume.assert_called_with(sentinel.tag, 'EXPUNGE')\n    self.assertEqual(sentinel.out, result)", "def test_id_expunge(self):\n    self.client._imap.uid.return_value = ('OK', [None])\n    self.assertEqual([None], self.client.expunge(['4', '5', '6']))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'expunge' function should accept a list of message IDs as input and return a tuple containing the server response message and a list of expunge responses. If no message IDs are provided, it should expunge all messages with the '\\Deleted' flag set.", "unit_test": ["def test_expunge_with_messages(self):\n    self.client._imap.uid.return_value = ('OK', ['Expunged'])\n    result = self.client.expunge(['1', '2', '3'])\n    self.assertEqual(('OK', ['Expunged']), result)\n\ndef test_expunge_without_messages(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=('OK', ['Expunged']))\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge()\n    self.assertEqual(('OK', ['Expunged']), result)"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_with_messages"}, "Exception Handling": {"requirement": "The 'expunge' function should raise a ValueError if the input message IDs are not of type list, int, or str.", "unit_test": ["def test_expunge_invalid_input(self):\n    with self.assertRaises(ValueError):\n        self.client.expunge(123.45)"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_invalid_input"}, "Edge Case Handling": {"requirement": "The 'expunge' function should handle the edge case where an empty list of message IDs is provided by expunging all messages with the '\\Deleted' flag set.", "unit_test": ["def test_expunge_with_empty_list(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=('OK', ['Expunged']))\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge([])\n    self.assertEqual(('OK', ['Expunged']), result)"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_with_empty_list"}, "Functionality Extension": {"requirement": "Extend the 'expunge' function to optionally log the number of messages expunged when a logger is provided.", "unit_test": ["def test_expunge_with_logging(self):\n    logger = Mock()\n    self.client._imap.uid.return_value = ('OK', ['Expunged'])\n    self.client.expunge(['1', '2', '3'], logger=logger)\n    logger.info.assert_called_with('3 messages expunged.')"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_with_logging"}, "Annotation Coverage": {"requirement": "Ensure that the 'expunge' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_expunge_annotations(self):\n    from typing import Optional, List, Union, Tuple\n    from typing import get_type_hints\n    hints = get_type_hints(self.client.expunge)\n    self.assertEqual(hints['messages'], Optional[List[Union[int, str]]])\n    self.assertEqual(hints['return'], Optional[Tuple[str, List[str]]])"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_annotations"}, "Code Complexity": {"requirement": "The 'expunge' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": ["def test_expunge_complexity(self):\n    from radon.complexity import cc_visit\n    with open('imapclient/imapclient.py', 'r') as f:\n        code = f.read()\n    complexity = [c for c in cc_visit(code) if c.name == 'expunge']\n    self.assertTrue(complexity and complexity[0].complexity <= 5)"], "test": "tests/test_imapclient.py::TestExpunge::test_code_complexity"}, "Code Standard": {"requirement": "The 'expunge' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_expunge_pep8(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['imapclient/imapclient.py'])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/test_imapclient.py::TestExpunge::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'expunge' function should utilize the '_command_and_check' and '_consume_until_tagged_response' methods from the IMAPClient class.", "unit_test": ["def test_expunge_context_usage(self):\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=sentinel.out)\n    self.client._command_and_check = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    self.client.expunge()\n    mockCommand.assert_called()\n    mockConsume.assert_called()"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'expunge' function should correctly use the '_command_and_check' and '_consume_until_tagged_response' from the IMAPClient class.", "unit_test": ["def test_expunge_uid_usage(self):\n    self.client.use_uid = True\n    self.client._imap.uid.return_value = ('OK', ['Expunged'])\n    result = self.client.expunge(['1', '2', '3'])\n    self.assertEqual(('OK', ['Expunged']), result)\n    self.client.use_uid = False\n    mockCommand = Mock(return_value=sentinel.tag)\n    mockConsume = Mock(return_value=('OK', ['Expunged']))\n    self.client._imap._command = mockCommand\n    self.client._consume_until_tagged_response = mockConsume\n    result = self.client.expunge(['1', '2', '3'])\n    self.assertEqual(('OK', ['Expunged']), result)"], "test": "tests/test_imapclient.py::TestExpunge::test_expunge_context_correct_usage"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "mopidy.ext.load_extensions", "type": "function", "project_path": "Multimedia/Mopidy", "completion_path": "Multimedia/Mopidy/mopidy/ext.py", "signature_position": [210, 210], "body_position": [216, 268], "dependency": {"intra_class": ["mopidy.ext.Extension.dist_name", "mopidy.ext.Extension.ext_name", "mopidy.ext.Extension.get_command", "mopidy.ext.Extension.get_config_schema", "mopidy.ext.Extension.get_default_config", "mopidy.ext.Extension.version"], "intra_file": ["mopidy.ext.Extension", "mopidy.ext.ExtensionData", "mopidy.ext.logger"], "cross_file": []}, "requirement": {"Functionality": "This function finds all installed extensions by iterating through the entry points of the \"mopidy.ext\" package. It loads each entry point, checks if it is a valid extension class, and creates an ExtensionData object with the necessary attributes. The function then appends the ExtensionData object to a list of installed extensions and returns the list.", "Arguments": ":param: No input parameters.\n:return: List[ExtensionData]. A list of installed extensions, where each extension is represented by an ExtensionData object."}, "tests": ["tests/test_ext.py::TestLoadExtensions::test_gets_instance", "tests/test_ext.py::TestLoadExtensions::test_no_extensions", "tests/test_ext.py::TestLoadExtensions::test_gets_wrong_class", "tests/test_ext.py::TestLoadExtensions::test_creating_instance_fails", "tests/test_ext.py::TestLoadExtensions::test_get_default_config_fails"], "indent": 0, "domain": "Multimedia", "code": "def load_extensions() -> List[ExtensionData]:\n    \"\"\"Find all installed extensions.\n\n    :returns: list of installed extensions\n    \"\"\"\n\n    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        logger.debug(\"Loading entry point: %s\", entry_point)\n        try:\n            extension_class = entry_point.resolve()\n        except Exception as e:\n            logger.exception(\n                f\"Failed to load extension {entry_point.name}: {e}\"\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                raise TypeError  # issubclass raises TypeError on non-class\n        except TypeError:\n            logger.error(\n                \"Entry point %s did not contain a valid extension\" \"class: %r\",\n                entry_point.name,\n                extension_class,\n            )\n            continue\n\n        try:\n            extension = extension_class()\n            # Ensure required extension attributes are present after try block\n            _ = extension.dist_name\n            _ = extension.ext_name\n            _ = extension.version\n            extension_data = ExtensionData(\n                entry_point=entry_point,\n                extension=extension,\n                config_schema=extension.get_config_schema(),\n                config_defaults=extension.get_default_config(),\n                command=extension.get_command(),\n            )\n        except Exception:\n            logger.exception(\n                \"Setup of extension from entry point %s failed, \"\n                \"ignoring extension.\",\n                entry_point.name,\n            )\n            continue\n\n        installed_extensions.append(extension_data)\n\n        logger.debug(\n            \"Loaded extension: %s %s\", extension.dist_name, extension.version\n        )\n\n    names = (ed.extension.ext_name for ed in installed_extensions)\n    logger.debug(\"Discovered extensions: %s\", \", \".join(names))\n    return installed_extensions\n", "context": "from __future__ import annotations\n\nimport logging\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, NamedTuple\n\nimport pkg_resources\n\nfrom mopidy import config as config_lib\n\nfrom mopidy.internal import path\n\nif TYPE_CHECKING:\n    from pathlib import Path\n    from typing import Any, Dict, Iterator, List, Optional, Type\n\n    from mopidy.commands import Command\n    from mopidy.config import ConfigSchema\n\n    Config = Dict[str, Dict[str, Any]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtensionData(NamedTuple):\n    extension: \"Extension\"\n    entry_point: Any\n    config_schema: ConfigSchema\n    config_defaults: Any\n    command: Optional[Command]\n\n\nclass Extension:\n\n    \"\"\"Base class for Mopidy extensions\"\"\"\n\n    dist_name: str\n    \"\"\"The extension's distribution name, as registered on PyPI\n\n    Example: ``Mopidy-Soundspot``\n    \"\"\"\n\n    ext_name: str\n    \"\"\"The extension's short name, as used in setup.py and as config section\n    name\n\n    Example: ``soundspot``\n    \"\"\"\n\n    version: str\n    \"\"\"The extension's version\n\n    Should match the :attr:`__version__` attribute on the extension's main\n    Python module and the version registered on PyPI.\n    \"\"\"\n\n    def get_default_config(self) -> str:\n        \"\"\"The extension's default config as a text string.\n\n        :returns: str\n        \"\"\"\n        raise NotImplementedError(\n            'Add at least a config section with \"enabled = true\"'\n        )\n\n    def get_config_schema(self) -> ConfigSchema:\n        \"\"\"The extension's config validation schema\n\n        :returns: :class:`~mopidy.config.schemas.ConfigSchema`\n        \"\"\"\n        schema = config_lib.ConfigSchema(self.ext_name)\n        schema[\"enabled\"] = config_lib.Boolean()\n        return schema\n\n    @classmethod\n    def get_cache_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create cache directory for the extension.\n\n        Use this directory to cache data that can safely be thrown away.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n\n    @classmethod\n    def get_config_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create configuration directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n\n    @classmethod\n    def get_data_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create data directory for the extension.\n\n        Use this directory to store data that should be persistent.\n\n        :param config: the Mopidy config object\n        :returns: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n\n    def get_command(self) -> Optional[Command]:\n        \"\"\"Command to expose to command line users running ``mopidy``.\n\n        :returns:\n          Instance of a :class:`~mopidy.commands.Command` class.\n        \"\"\"\n        pass\n\n    def validate_environment(self) -> None:\n        \"\"\"Checks if the extension can run in the current environment.\n\n        Dependencies described by :file:`setup.py` are checked by Mopidy, so\n        you should not check their presence here.\n\n        If a problem is found, raise :exc:`~mopidy.exceptions.ExtensionError`\n        with a message explaining the issue.\n\n        :raises: :exc:`~mopidy.exceptions.ExtensionError`\n        :returns: :class:`None`\n        \"\"\"\n        pass\n\n    def setup(self, registry: \"Registry\") -> None:\n        \"\"\"\n        Register the extension's components in the extension :class:`Registry`.\n\n        For example, to register a backend::\n\n            def setup(self, registry):\n                from .backend import SoundspotBackend\n                registry.add('backend', SoundspotBackend)\n\n        See :class:`Registry` for a list of registry keys with a special\n        meaning. Mopidy will instantiate and start any classes registered under\n        the ``frontend`` and ``backend`` registry keys.\n\n        This method can also be used for other setup tasks not involving the\n        extension registry.\n\n        :param registry: the extension registry\n        :type registry: :class:`Registry`\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Registry(Mapping):\n\n    \"\"\"Registry of components provided by Mopidy extensions.\n\n    Passed to the :meth:`~Extension.setup` method of all extensions. The\n    registry can be used like a dict of string keys and lists.\n\n    Some keys have a special meaning, including, but not limited to:\n\n    - ``backend`` is used for Mopidy backend classes.\n    - ``frontend`` is used for Mopidy frontend classes.\n\n    Extensions can use the registry for allow other to extend the extension\n    itself. For example the ``Mopidy-Local`` historically used the\n    ``local:library`` key to allow other extensions to register library\n    providers for ``Mopidy-Local`` to use. Extensions should namespace\n    custom keys with the extension's :attr:`~Extension.ext_name`,\n    e.g. ``local:foo`` or ``http:bar``.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._registry: Dict[str, List[Type[Any]]] = {}\n\n    def add(self, name: str, cls: Type[Any]) -> None:\n        \"\"\"Add a component to the registry.\n\n        Multiple classes can be registered to the same name.\n        \"\"\"\n        self._registry.setdefault(name, []).append(cls)\n\n    def __getitem__(self, name: str) -> List[Type[Any]]:\n        return self._registry.setdefault(name, [])\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._registry)\n\n    def __len__(self) -> int:\n        return len(self._registry)\n\n\n###The function: load_extensions###\n\ndef validate_extension_data(data: ExtensionData) -> bool:\n    \"\"\"Verify extension's dependencies and environment.\n\n    :param extensions: an extension to check\n    :returns: if extension should be run\n    \"\"\"\n\n    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n", "prompt": "Please write a python function called 'load_extensions' base the context. This function finds all installed extensions by iterating through the entry points of the \"mopidy.ext\" package. It loads each entry point, checks if it is a valid extension class, and creates an ExtensionData object with the necessary attributes. The function then appends the ExtensionData object to a list of installed extensions and returns the list.:param: No input parameters.\n:return: List[ExtensionData]. A list of installed extensions, where each extension is represented by an ExtensionData object..\n        The context you need to refer to is as follows: from __future__ import annotations\n\nimport logging\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, NamedTuple\n\nimport pkg_resources\n\nfrom mopidy import config as config_lib\n\nfrom mopidy.internal import path\n\nif TYPE_CHECKING:\n    from pathlib import Path\n    from typing import Any, Dict, Iterator, List, Optional, Type\n\n    from mopidy.commands import Command\n    from mopidy.config import ConfigSchema\n\n    Config = Dict[str, Dict[str, Any]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtensionData(NamedTuple):\n    extension: \"Extension\"\n    entry_point: Any\n    config_schema: ConfigSchema\n    config_defaults: Any\n    command: Optional[Command]\n\n\nclass Extension:\n\n    \"\"\"Base class for Mopidy extensions\"\"\"\n\n    dist_name: str\n    \"\"\"The extension's distribution name, as registered on PyPI\n\n    Example: ``Mopidy-Soundspot``\n    \"\"\"\n\n    ext_name: str\n    \"\"\"The extension's short name, as used in setup.py and as config section\n    name\n\n    Example: ``soundspot``\n    \"\"\"\n\n    version: str\n    \"\"\"The extension's version\n\n    Should match the :attr:`__version__` attribute on the extension's main\n    Python module and the version registered on PyPI.\n    \"\"\"\n\n    def get_default_config(self) -> str:\n        \"\"\"The extension's default config as a text string.\n\n        :returns: str\n        \"\"\"\n        raise NotImplementedError(\n            'Add at least a config section with \"enabled = true\"'\n        )\n\n    def get_config_schema(self) -> ConfigSchema:\n        \"\"\"The extension's config validation schema\n\n        :returns: :class:`~mopidy.config.schemas.ConfigSchema`\n        \"\"\"\n        schema = config_lib.ConfigSchema(self.ext_name)\n        schema[\"enabled\"] = config_lib.Boolean()\n        return schema\n\n    @classmethod\n    def get_cache_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create cache directory for the extension.\n\n        Use this directory to cache data that can safely be thrown away.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n\n    @classmethod\n    def get_config_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create configuration directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n\n    @classmethod\n    def get_data_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create data directory for the extension.\n\n        Use this directory to store data that should be persistent.\n\n        :param config: the Mopidy config object\n        :returns: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n\n    def get_command(self) -> Optional[Command]:\n        \"\"\"Command to expose to command line users running ``mopidy``.\n\n        :returns:\n          Instance of a :class:`~mopidy.commands.Command` class.\n        \"\"\"\n        pass\n\n    def validate_environment(self) -> None:\n        \"\"\"Checks if the extension can run in the current environment.\n\n        Dependencies described by :file:`setup.py` are checked by Mopidy, so\n        you should not check their presence here.\n\n        If a problem is found, raise :exc:`~mopidy.exceptions.ExtensionError`\n        with a message explaining the issue.\n\n        :raises: :exc:`~mopidy.exceptions.ExtensionError`\n        :returns: :class:`None`\n        \"\"\"\n        pass\n\n    def setup(self, registry: \"Registry\") -> None:\n        \"\"\"\n        Register the extension's components in the extension :class:`Registry`.\n\n        For example, to register a backend::\n\n            def setup(self, registry):\n                from .backend import SoundspotBackend\n                registry.add('backend', SoundspotBackend)\n\n        See :class:`Registry` for a list of registry keys with a special\n        meaning. Mopidy will instantiate and start any classes registered under\n        the ``frontend`` and ``backend`` registry keys.\n\n        This method can also be used for other setup tasks not involving the\n        extension registry.\n\n        :param registry: the extension registry\n        :type registry: :class:`Registry`\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Registry(Mapping):\n\n    \"\"\"Registry of components provided by Mopidy extensions.\n\n    Passed to the :meth:`~Extension.setup` method of all extensions. The\n    registry can be used like a dict of string keys and lists.\n\n    Some keys have a special meaning, including, but not limited to:\n\n    - ``backend`` is used for Mopidy backend classes.\n    - ``frontend`` is used for Mopidy frontend classes.\n\n    Extensions can use the registry for allow other to extend the extension\n    itself. For example the ``Mopidy-Local`` historically used the\n    ``local:library`` key to allow other extensions to register library\n    providers for ``Mopidy-Local`` to use. Extensions should namespace\n    custom keys with the extension's :attr:`~Extension.ext_name`,\n    e.g. ``local:foo`` or ``http:bar``.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._registry: Dict[str, List[Type[Any]]] = {}\n\n    def add(self, name: str, cls: Type[Any]) -> None:\n        \"\"\"Add a component to the registry.\n\n        Multiple classes can be registered to the same name.\n        \"\"\"\n        self._registry.setdefault(name, []).append(cls)\n\n    def __getitem__(self, name: str) -> List[Type[Any]]:\n        return self._registry.setdefault(name, [])\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._registry)\n\n    def __len__(self) -> int:\n        return len(self._registry)\n\n\n###The function: load_extensions###\n\ndef validate_extension_data(data: ExtensionData) -> bool:\n    \"\"\"Verify extension's dependencies and environment.\n\n    :param extensions: an extension to check\n    :returns: if extension should be run\n    \"\"\"\n\n    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n", "test_list": ["def test_gets_instance(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    assert ext.load_extensions() == []", "def test_no_extensions(self, iter_entry_points_mock):\n    iter_entry_points_mock.return_value = []\n    assert ext.load_extensions() == []", "def test_gets_wrong_class(self, iter_entry_points_mock):\n\n    class WrongClass:\n        pass\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = WrongClass\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    assert ext.load_extensions() == []", "def test_creating_instance_fails(self, iter_entry_points_mock):\n    mock_extension = mock.Mock(spec=ext.Extension)\n    mock_extension.side_effect = Exception\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = mock_extension\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    assert ext.load_extensions() == []", "def test_get_default_config_fails(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with mock.patch.object(DummyExtension, 'get_default_config') as get:\n        get.side_effect = Exception\n        assert ext.load_extensions() == []\n        get.assert_called_once_with()"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'load_extensions' should return a list of ExtensionData objects, ensuring each object is correctly instantiated with valid attributes.", "unit_test": ["def test_load_extensions_returns_correct_type(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    result = ext.load_extensions()\n    assert isinstance(result, list)\n    assert all(isinstance(item, ext.ExtensionData) for item in result)"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_returns_correct_type"}, "Exception Handling": {"requirement": "The function 'load_extensions' should handle exceptions raised during the loading of extensions and log appropriate error messages.", "unit_test": ["def test_load_extensions_handles_exceptions(self, iter_entry_points_mock, caplog):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.side_effect = Exception('Test exception')\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with caplog.at_level(logging.ERROR):\n        result = ext.load_extensions()\n    assert result == []\n    assert 'Test exception' in caplog.text"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_handles_exceptions"}, "Edge Case Handling": {"requirement": "The function 'load_extensions' should correctly handle the case where no extensions are found and return an empty list.", "unit_test": ["def test_load_extensions_no_extensions_found(self, iter_entry_points_mock):\n    iter_entry_points_mock.return_value = []\n    result = ext.load_extensions()\n    assert result == []"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_no_extensions_found"}, "Functionality Extension": {"requirement": "The function 'load_extensions' should support filtering extensions based on a specific criterion, such as version compatibility.", "unit_test": ["def test_load_extensions_filters_by_version(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    DummyExtension.version = '2.0.0'\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    result = ext.load_extensions(min_version='2.0.0')\n    assert len(result) == 1\n    assert result[0].extension.version == '2.0.0'"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_filters_by_version"}, "Annotation Coverage": {"requirement": "The function 'load_extensions' should have complete type annotations for all parameters and return types.", "unit_test": ["def test_load_extensions_annotations():\n    import inspect\n    signature = inspect.signature(ext.load_extensions)\n    assert signature.return_annotation == 'List[ExtensionData]'"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_annotations"}, "Code Complexity": {"requirement": "The function 'load_extensions' should maintain a cyclomatic complexity of 10 or less.", "unit_test": ["def test_load_extensions_cyclomatic_complexity():\n    from radon.complexity import cc_visit\n    with open('path_to_file_containing_load_extensions.py', 'r') as f:\n        code = f.read()\n    complexity = cc_visit(code)\n    load_extensions_complexity = next((c for c in complexity if c.name == 'load_extensions'), None)\n    assert load_extensions_complexity is not None\n    assert load_extensions_complexity.complexity <= 10"], "test": "tests/test_ext.py::TestLoadExtensions::test_ext_code_complexity"}, "Code Standard": {"requirement": "The function 'load_extensions' should adhere to PEP 8 coding standards, including proper indentation and spacing.", "unit_test": ["def test_load_extensions_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_load_extensions.py'])\n    assert result.total_errors == 0"], "test": "tests/test_ext.py::TestLoadExtensions::test_check_ext_code_style"}, "Context Usage Verification": {"requirement": "The function 'load_extensions' should utilize the 'mopidy.ext.Extension.get_config_schema' methods.", "unit_test": ["def test_load_extensions_uses_extension_class(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with mock.patch.object(DummyExtension, 'get_config_schema') as get_config_schema_mock:\n        ext.load_extensions()\n        get_config_schema_mock.assert_called_once()"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_uses_extension_class"}, "Context Usage Correctness Verification": {"requirement": "The function 'load_extensions' should correctly use the 'mopidy.ext.Extension.get_config_schema' method to retrieve the configuration schema for each extension.", "unit_test": ["def test_load_extensions_correctly_uses_get_config_schema(self, iter_entry_points_mock):\n    mock_entry_point = mock.Mock()\n    mock_entry_point.resolve.return_value = DummyExtension()\n    iter_entry_points_mock.return_value = [mock_entry_point]\n    with mock.patch.object(DummyExtension, 'get_config_schema', return_value=config_lib.ConfigSchema('dummy')) as get_config_schema_mock:\n        ext.load_extensions()\n        get_config_schema_mock.assert_called_once()"], "test": "tests/test_ext.py::TestLoadExtensions::test_load_extensions_correctly_uses_get_config_schema"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "type": "method", "project_path": "Communications/Wikipedia-API", "completion_path": "Communications/Wikipedia-API/wikipediaapi/__init__.py", "signature_position": [934, 937], "body_position": [944, 949], "dependency": {"intra_class": ["wikipediaapi.WikipediaPage._called", "wikipediaapi.WikipediaPage._fetch", "wikipediaapi.WikipediaPage._section_mapping"], "intra_file": ["wikipediaapi.WikipediaPageSection"], "cross_file": []}, "requirement": {"Functionality": "This function returns the last section of the current Wikipedia page with the given title. It first checks if the \"extracts\" data has been fetched for the page. If not, it fetches the \"extracts\" data. Then, it retrieves the sections with the given title from the section mapping. If there are sections with the given title, it returns the last section. Otherwise, it returns None.", "Arguments": ":param self: WikipediaPage. An instance of the WikipediaPage class.\n:param title: str. The title of the section to retrieve.\n:return: Optional[WikipediaPageSection]. The last section of the current page with the given title."}, "tests": ["tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_subsubsection", "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_subsection_by_title_return_last", "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_with_erroneous_edit", "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_subsection_by_title_with_multiple_spans", "tests/extract_wiki_format_test.py::TestWikiFormatExtracts::test_subsection_by_title"], "indent": 4, "domain": "Communications", "code": "    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns last section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n", "context": "\"\"\"\nWikipedia-API is easy to use wrapper for extracting information from Wikipedia.\n\nIt supports extracting texts, sections, links, categories, translations, etc.\nfrom Wikipedia. Documentation provides code snippets for the most common use\ncases.\n\"\"\"\n\n__version__ = (0, 6, 0)\nfrom collections import defaultdict\nfrom enum import IntEnum\nimport logging\nimport re\nfrom typing import Any, Dict, List, Optional, Union\nfrom urllib import parse\n\nimport requests\n\nUSER_AGENT = (\n    \"Wikipedia-API/\"\n    + \".\".join(str(s) for s in __version__)\n    + \"; https://github.com/martin-majlis/Wikipedia-API/\"\n)\n\nlog = logging.getLogger(__name__)\n\n\n# https://www.mediawiki.org/wiki/API:Main_page\nPagesDict = Dict[str, \"WikipediaPage\"]\n\n\nclass ExtractFormat(IntEnum):\n    \"\"\"Represents extraction format.\"\"\"\n\n    WIKI = 1\n    \"\"\"\n    Allows recognizing subsections\n\n    Example: https://goo.gl/PScNVV\n    \"\"\"\n\n    HTML = 2\n    \"\"\"\n    Alows retrieval of HTML tags\n\n    Example: https://goo.gl/1Jwwpr\n    \"\"\"\n\n    # Plain: https://goo.gl/MAv2qz\n    # Doesn't allow to recognize subsections\n    # PLAIN = 3\n\n\nclass Namespace(IntEnum):\n    \"\"\"\n    Represents namespace in Wikipedia\n\n    You can gen list of possible namespaces here:\n\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace#Programming\n\n    Currently following namespaces are supported:\n    \"\"\"\n\n    MAIN = 0\n    TALK = 1\n    USER = 2\n    USER_TALK = 3\n    WIKIPEDIA = 4\n    WIKIPEDIA_TALK = 5\n    FILE = 6\n    FILE_TALK = 7\n    MEDIAWIKI = 8\n    MEDIAWIKI_TALK = 9\n    TEMPLATE = 10\n    TEMPLATE_TALK = 11\n    HELP = 12\n    HELP_TALK = 13\n    CATEGORY = 14\n    CATEGORY_TALK = 15\n    PORTAL = 100\n    PORTAL_TALK = 101\n    PROJECT = 102\n    PROJECT_TALK = 103\n    REFERENCE = 104\n    REFERENCE_TALK = 105\n    BOOK = 108\n    BOOK_TALK = 109\n    DRAFT = 118\n    DRAFT_TALK = 119\n    EDUCATION_PROGRAM = 446\n    EDUCATION_PROGRAM_TALK = 447\n    TIMED_TEXT = 710\n    TIMED_TEXT_TALK = 711\n    MODULE = 828\n    MODULE_TALK = 829\n    GADGET = 2300\n    GADGET_TALK = 2301\n    GADGET_DEFINITION = 2302\n    GADGET_DEFINITION_TALK = 2303\n\n\nWikiNamespace = Union[Namespace, int]\n\n\ndef namespace2int(namespace: WikiNamespace) -> int:\n    \"\"\"Converts namespace into integer\"\"\"\n    if isinstance(namespace, Namespace):\n        return namespace.value\n\n    return namespace\n\n\nRE_SECTION = {\n    ExtractFormat.WIKI: re.compile(r\"\\n\\n *(==+) (.*?) (==+) *\\n\"),\n    ExtractFormat.HTML: re.compile(\n        r\"\\n? *<h([1-9])[^>]*?>(<span[^>]*></span>)? *\"\n        + \"(<span[^>]*>)? *(<span[^>]*></span>)? *(.*?) *\"\n        + \"(</span>)?(<span>Edit</span>)?</h[1-9]>\\n?\"\n        #                  ^^^^\n        # Example page with 'Edit' erroneous links: https://bit.ly/2ui4FWs\n    ),\n    # ExtractFormat.PLAIN.value: re.compile(r'\\n\\n *(===*) (.*?) (===*) *\\n'),\n}\n\n\nclass Wikipedia:\n    \"\"\"Wikipedia is wrapper for Wikipedia API.\"\"\"\n\n    def __init__(\n        self,\n        user_agent: str,\n        language: str = \"en\",\n        extract_format: ExtractFormat = ExtractFormat.WIKI,\n        headers: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Constructs Wikipedia object for extracting information Wikipedia.\n\n        :param user_agent: HTTP User-Agent used in requests\n                https://meta.wikimedia.org/wiki/User-Agent_policy\n        :param language: Language mutation of Wikipedia -\n                http://meta.wikimedia.org/wiki/List_of_Wikipedias\n        :param extract_format: Format used for extractions\n                :class:`ExtractFormat` object.\n        :param headers:  Headers sent as part of HTTP request\n        :param kwargs: Optional parameters used in -\n                http://docs.python-requests.org/en/master/api/#requests.request\n\n        Examples:\n\n        * Proxy: ``Wikipedia('foo (merlin@example.com)', proxies={'http': 'http://proxy:1234'})``\n        \"\"\"\n        kwargs.setdefault(\"timeout\", 10.0)\n\n        default_headers = {} if headers is None else headers\n        if user_agent:\n            default_headers.setdefault(\n                \"User-Agent\",\n                user_agent,\n            )\n        used_user_agent = default_headers.get(\"User-Agent\")\n        if not (used_user_agent and len(used_user_agent) > 5):\n            raise AssertionError(\n                \"Please, be nice to Wikipedia and specify user agent - \"\n                + \"https://meta.wikimedia.org/wiki/User-Agent_policy. Current user_agent: '\"\n                + str(used_user_agent)\n                + \"' is not sufficient.\"\n            )\n        default_headers[\"User-Agent\"] += \" (\" + USER_AGENT + \")\"\n\n        self.language = language.strip().lower()\n        if not self.language:\n            raise AssertionError(\n                \"Specify language. Current language: '\"\n                + str(self.language)\n                + \"' is not sufficient.\"\n            )\n        self.extract_format = extract_format\n\n        log.info(\n            \"Wikipedia: language=%s, user_agent: %s, extract_format=%s\",\n            self.language,\n            default_headers[\"User-Agent\"],\n            self.extract_format,\n        )\n\n        self._session = requests.Session()\n        self._session.headers.update(default_headers)\n        self._request_kwargs = kwargs\n\n    def __del__(self) -> None:\n        \"\"\"Closes session.\"\"\"\n        if hasattr(self, \"_session\") and self._session:\n            self._session.close()\n\n    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        Creating `WikipediaPage` object is always the first step for extracting\n        any information.\n\n        Example::\n\n            wiki_wiki = wikipediaapi.Wikipedia('en')\n            page_py = wiki_wiki.page('Python_(programming_language)')\n            print(page_py.title)\n            # Python (programming language)\n\n            wiki_hi = wikipediaapi.Wikipedia('hi')\n\n            page_hi_py = wiki_hi.article(\n                title='%E0%A4%AA%E0%A4%BE%E0%A4%87%E0%A4%A5%E0%A4%A8',\n                unquote=True,\n            )\n            print(page_hi_py.title)\n            # \u092a\u093e\u0907\u0925\u0928\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n\n    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        This function is an alias for :func:`page`\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )\n\n    def extracts(self, page: \"WikipediaPage\", **kwargs) -> str:\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"extracts\",\n            \"titles\": page.title,\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params[\"explaintext\"] = 1\n            params[\"exsectionformat\"] = \"wiki\"\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return \"\"\n            return self._build_extracts(v, page)\n        return \"\"\n\n    def info(self, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"\n        https://www.mediawiki.org/w/api.php?action=help&modules=query%2Binfo\n        https://www.mediawiki.org/wiki/API:Info\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"info\",\n            \"titles\": page.title,\n            \"inprop\": \"|\".join(\n                [\n                    \"protection\",\n                    \"talkid\",\n                    \"watched\",\n                    \"watchers\",\n                    \"visitingwatchers\",\n                    \"notificationtimestamp\",\n                    \"subjectid\",\n                    \"url\",\n                    \"readable\",\n                    \"preload\",\n                    \"displaytitle\",\n                ]\n            ),\n        }\n        raw = self._query(page, params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return page\n\n            return self._build_info(v, page)\n        return page\n\n    def langlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns langlinks of the page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        - https://www.mediawiki.org/wiki/API:Langlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to pages in other languages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"langlinks\",\n            \"titles\": page.title,\n            \"lllimit\": 500,\n            \"llprop\": \"url\",\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_langlinks(v, page)\n        return {}\n\n    def links(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns links to other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        - https://www.mediawiki.org/wiki/API:Links\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to linked pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"links\",\n            \"titles\": page.title,\n            \"pllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n\n            while \"continue\" in raw:\n                params[\"plcontinue\"] = raw[\"continue\"][\"plcontinue\"]\n                raw = self._query(page, params)\n                v[\"links\"] += raw[\"query\"][\"pages\"][k][\"links\"]\n\n            return self._build_links(v, page)\n        return {}\n\n    def backlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns backlinks from other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        - https://www.mediawiki.org/wiki/API:Backlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: backlinks from other pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"backlinks\",\n            \"bltitle\": page.title,\n            \"bllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"blcontinue\"] = raw[\"continue\"][\"blcontinue\"]\n            raw = self._query(page, params)\n            v[\"backlinks\"] += raw[\"query\"][\"backlinks\"]\n        return self._build_backlinks(v, page)\n\n    def categories(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns categories for page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        - https://www.mediawiki.org/wiki/API:Categories\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: categories for page\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"categories\",\n            \"titles\": page.title,\n            \"cllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_categories(v, page)\n        return {}\n\n    def categorymembers(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns pages in given category with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        - https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: pages in given category\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"categorymembers\",\n            \"cmtitle\": page.title,\n            \"cmlimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"cmcontinue\"] = raw[\"continue\"][\"cmcontinue\"]\n            raw = self._query(page, params)\n            v[\"categorymembers\"] += raw[\"query\"][\"categorymembers\"]\n\n        return self._build_categorymembers(v, page)\n\n    def _query(self, page: \"WikipediaPage\", params: Dict[str, Any]):\n        \"\"\"Queries Wikimedia API to fetch content.\"\"\"\n        base_url = \"https://\" + page.language + \".wikipedia.org/w/api.php\"\n        log.info(\n            \"Request URL: %s\",\n            base_url + \"?\" + \"&\".join([k + \"=\" + str(v) for k, v in params.items()]),\n        )\n        params[\"format\"] = \"json\"\n        params[\"redirects\"] = 1\n        r = self._session.get(base_url, params=params, **self._request_kwargs)\n        return r.json()\n\n    def _build_extracts(self, extract, page: \"WikipediaPage\") -> str:\n        \"\"\"Constructs summary of given page.\"\"\"\n        page._summary = \"\"\n        page._section_mapping = defaultdict(list)\n\n        self._common_attributes(extract, page)\n\n        section_stack = [page]\n        section = None\n        prev_pos = 0\n\n        for match in re.finditer(RE_SECTION[self.extract_format], extract[\"extract\"]):\n            if len(page._section_mapping) == 0:\n                page._summary = extract[\"extract\"][0 : match.start()].strip()\n            elif section is not None:\n                section._text = (extract[\"extract\"][prev_pos : match.start()]).strip()\n\n            section = self._create_section(match)\n            sec_level = section.level + 1\n\n            if sec_level > len(section_stack):\n                section_stack.append(section)\n            elif sec_level == len(section_stack):\n                section_stack.pop()\n                section_stack.append(section)\n            else:\n                for _ in range(len(section_stack) - sec_level + 1):\n                    section_stack.pop()\n                section_stack.append(section)\n\n            section_stack[len(section_stack) - 2]._section.append(section)\n            # section_stack[sec_level - 1]._section.append(section)\n\n            prev_pos = match.end()\n            page._section_mapping[section.title].append(section)\n\n        # pages without sections have only summary\n        if page._summary == \"\":\n            page._summary = extract[\"extract\"].strip()\n\n        if prev_pos > 0 and section is not None:\n            section._text = extract[\"extract\"][prev_pos:]\n\n        return page._summary\n\n    def _create_section(self, match):\n        \"\"\"Creates section.\"\"\"\n        sec_title = \"\"\n        sec_level = 2\n        if self.extract_format == ExtractFormat.WIKI:\n            sec_title = match.group(2).strip()\n            sec_level = len(match.group(1))\n        elif self.extract_format == ExtractFormat.HTML:\n            sec_title = match.group(5).strip()\n            sec_level = int(match.group(1).strip())\n\n        section = WikipediaPageSection(self, sec_title, sec_level - 1)\n        return section\n\n    def _build_info(self, extract, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"Builds page from API call info.\"\"\"\n        self._common_attributes(extract, page)\n        for k, v in extract.items():\n            page._attributes[k] = v\n\n        return page\n\n    def _build_langlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call langlinks.\"\"\"\n        page._langlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for langlink in extract.get(\"langlinks\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=langlink[\"*\"],\n                ns=Namespace.MAIN,\n                language=langlink[\"lang\"],\n                url=langlink[\"url\"],\n            )\n            page._langlinks[p.language] = p\n\n        return page._langlinks\n\n    def _build_links(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call links.\"\"\"\n        page._links = {}\n\n        self._common_attributes(extract, page)\n\n        for link in extract.get(\"links\", []):\n            page._links[link[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=link[\"title\"],\n                ns=int(link[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._links\n\n    def _build_backlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call backlinks.\"\"\"\n        page._backlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for backlink in extract.get(\"backlinks\", []):\n            page._backlinks[backlink[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=backlink[\"title\"],\n                ns=int(backlink[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._backlinks\n\n    def _build_categories(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categories.\"\"\"\n        page._categories = {}\n\n        self._common_attributes(extract, page)\n\n        for category in extract.get(\"categories\", []):\n            page._categories[category[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=category[\"title\"],\n                ns=int(category[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._categories\n\n    def _build_categorymembers(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categorymembers.\"\"\"\n        page._categorymembers = {}\n\n        self._common_attributes(extract, page)\n\n        for member in extract.get(\"categorymembers\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=member[\"title\"],\n                ns=int(member[\"ns\"]),\n                language=page.language,\n            )\n            p.pageid = member[\"pageid\"]  # type: ignore\n\n            page._categorymembers[member[\"title\"]] = p\n\n        return page._categorymembers\n\n    @staticmethod\n    def _common_attributes(extract, page: \"WikipediaPage\"):\n        \"\"\"Fills in common attributes for page.\"\"\"\n        common_attributes = [\"title\", \"pageid\", \"ns\", \"redirects\"]\n\n        for attr in common_attributes:\n            if attr in extract:\n                page._attributes[attr] = extract[attr]\n\n\nclass WikipediaPageSection:\n    \"\"\"WikipediaPageSection represents section in the page.\"\"\"\n\n    def __init__(\n        self, wiki: Wikipedia, title: str, level: int = 0, text: str = \"\"\n    ) -> None:\n        \"\"\"Constructs WikipediaPageSection.\"\"\"\n        self.wiki = wiki\n        self._title = title\n        self._level = level\n        self._text = text\n        self._section = []  # type: List['WikipediaPageSection']\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current section.\n\n        :return: title of the current section\n        \"\"\"\n        return self._title\n\n    @property\n    def level(self) -> int:\n        \"\"\"\n        Returns indentation level of the current section.\n\n        :return: indentation level of the current section\n        \"\"\"\n        return self._level\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current section.\n\n        :return: text of the current section\n        \"\"\"\n        return self._text\n\n    @property\n    def sections(self) -> List[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section.\n\n        :return: subsections of the current section\n        \"\"\"\n        return self._section\n\n    def section_by_title(self, title: str) -> Optional[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section with given title.\n\n        :param title: title of the subsection\n        :return: subsection if it exists\n        \"\"\"\n        sections = [s for s in self._section if s.title == title]\n        if sections:\n            return sections[-1]\n        return None\n\n    def full_text(self, level: int = 1) -> str:\n        \"\"\"\n        Returns text of the current section as well as all its subsections.\n\n        :param level: indentation level\n        :return: text of the current section as well as all its subsections\n        \"\"\"\n        res = \"\"\n        if self.wiki.extract_format == ExtractFormat.WIKI:\n            res += self.title\n        elif self.wiki.extract_format == ExtractFormat.HTML:\n            res += f\"<h{level}>{self.title}</h{level}>\"\n        else:\n            raise NotImplementedError(\"Unknown ExtractFormat type\")\n\n        res += \"\\n\"\n        res += self._text\n        if len(self._text) > 0:\n            res += \"\\n\\n\"\n        for sec in self.sections:\n            res += sec.full_text(level + 1)\n        return res\n\n    def __repr__(self):\n        return \"Section: {} ({}):\\n{}\\nSubsections ({}):\\n{}\".format(\n            self._title,\n            self._level,\n            self._text,\n            len(self._section),\n            \"\\n\".join(map(repr, self._section)),\n        )\n\n\nclass WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n###The function: section_by_title###\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n    def __repr__(self):\n        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n", "prompt": "Please write a python function called 'section_by_title' base the context. This function returns the last section of the current Wikipedia page with the given title. It first checks if the \"extracts\" data has been fetched for the page. If not, it fetches the \"extracts\" data. Then, it retrieves the sections with the given title from the section mapping. If there are sections with the given title, it returns the last section. Otherwise, it returns None.:param self: WikipediaPage. An instance of the WikipediaPage class.\n:param title: str. The title of the section to retrieve.\n:return: Optional[WikipediaPageSection]. The last section of the current page with the given title..\n        The context you need to refer to is as follows: \"\"\"\nWikipedia-API is easy to use wrapper for extracting information from Wikipedia.\n\nIt supports extracting texts, sections, links, categories, translations, etc.\nfrom Wikipedia. Documentation provides code snippets for the most common use\ncases.\n\"\"\"\n\n__version__ = (0, 6, 0)\nfrom collections import defaultdict\nfrom enum import IntEnum\nimport logging\nimport re\nfrom typing import Any, Dict, List, Optional, Union\nfrom urllib import parse\n\nimport requests\n\nUSER_AGENT = (\n    \"Wikipedia-API/\"\n    + \".\".join(str(s) for s in __version__)\n    + \"; https://github.com/martin-majlis/Wikipedia-API/\"\n)\n\nlog = logging.getLogger(__name__)\n\n\n# https://www.mediawiki.org/wiki/API:Main_page\nPagesDict = Dict[str, \"WikipediaPage\"]\n\n\nclass ExtractFormat(IntEnum):\n    \"\"\"Represents extraction format.\"\"\"\n\n    WIKI = 1\n    \"\"\"\n    Allows recognizing subsections\n\n    Example: https://goo.gl/PScNVV\n    \"\"\"\n\n    HTML = 2\n    \"\"\"\n    Alows retrieval of HTML tags\n\n    Example: https://goo.gl/1Jwwpr\n    \"\"\"\n\n    # Plain: https://goo.gl/MAv2qz\n    # Doesn't allow to recognize subsections\n    # PLAIN = 3\n\n\nclass Namespace(IntEnum):\n    \"\"\"\n    Represents namespace in Wikipedia\n\n    You can gen list of possible namespaces here:\n\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace\n    * https://en.wikipedia.org/wiki/Wikipedia:Namespace#Programming\n\n    Currently following namespaces are supported:\n    \"\"\"\n\n    MAIN = 0\n    TALK = 1\n    USER = 2\n    USER_TALK = 3\n    WIKIPEDIA = 4\n    WIKIPEDIA_TALK = 5\n    FILE = 6\n    FILE_TALK = 7\n    MEDIAWIKI = 8\n    MEDIAWIKI_TALK = 9\n    TEMPLATE = 10\n    TEMPLATE_TALK = 11\n    HELP = 12\n    HELP_TALK = 13\n    CATEGORY = 14\n    CATEGORY_TALK = 15\n    PORTAL = 100\n    PORTAL_TALK = 101\n    PROJECT = 102\n    PROJECT_TALK = 103\n    REFERENCE = 104\n    REFERENCE_TALK = 105\n    BOOK = 108\n    BOOK_TALK = 109\n    DRAFT = 118\n    DRAFT_TALK = 119\n    EDUCATION_PROGRAM = 446\n    EDUCATION_PROGRAM_TALK = 447\n    TIMED_TEXT = 710\n    TIMED_TEXT_TALK = 711\n    MODULE = 828\n    MODULE_TALK = 829\n    GADGET = 2300\n    GADGET_TALK = 2301\n    GADGET_DEFINITION = 2302\n    GADGET_DEFINITION_TALK = 2303\n\n\nWikiNamespace = Union[Namespace, int]\n\n\ndef namespace2int(namespace: WikiNamespace) -> int:\n    \"\"\"Converts namespace into integer\"\"\"\n    if isinstance(namespace, Namespace):\n        return namespace.value\n\n    return namespace\n\n\nRE_SECTION = {\n    ExtractFormat.WIKI: re.compile(r\"\\n\\n *(==+) (.*?) (==+) *\\n\"),\n    ExtractFormat.HTML: re.compile(\n        r\"\\n? *<h([1-9])[^>]*?>(<span[^>]*></span>)? *\"\n        + \"(<span[^>]*>)? *(<span[^>]*></span>)? *(.*?) *\"\n        + \"(</span>)?(<span>Edit</span>)?</h[1-9]>\\n?\"\n        #                  ^^^^\n        # Example page with 'Edit' erroneous links: https://bit.ly/2ui4FWs\n    ),\n    # ExtractFormat.PLAIN.value: re.compile(r'\\n\\n *(===*) (.*?) (===*) *\\n'),\n}\n\n\nclass Wikipedia:\n    \"\"\"Wikipedia is wrapper for Wikipedia API.\"\"\"\n\n    def __init__(\n        self,\n        user_agent: str,\n        language: str = \"en\",\n        extract_format: ExtractFormat = ExtractFormat.WIKI,\n        headers: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Constructs Wikipedia object for extracting information Wikipedia.\n\n        :param user_agent: HTTP User-Agent used in requests\n                https://meta.wikimedia.org/wiki/User-Agent_policy\n        :param language: Language mutation of Wikipedia -\n                http://meta.wikimedia.org/wiki/List_of_Wikipedias\n        :param extract_format: Format used for extractions\n                :class:`ExtractFormat` object.\n        :param headers:  Headers sent as part of HTTP request\n        :param kwargs: Optional parameters used in -\n                http://docs.python-requests.org/en/master/api/#requests.request\n\n        Examples:\n\n        * Proxy: ``Wikipedia('foo (merlin@example.com)', proxies={'http': 'http://proxy:1234'})``\n        \"\"\"\n        kwargs.setdefault(\"timeout\", 10.0)\n\n        default_headers = {} if headers is None else headers\n        if user_agent:\n            default_headers.setdefault(\n                \"User-Agent\",\n                user_agent,\n            )\n        used_user_agent = default_headers.get(\"User-Agent\")\n        if not (used_user_agent and len(used_user_agent) > 5):\n            raise AssertionError(\n                \"Please, be nice to Wikipedia and specify user agent - \"\n                + \"https://meta.wikimedia.org/wiki/User-Agent_policy. Current user_agent: '\"\n                + str(used_user_agent)\n                + \"' is not sufficient.\"\n            )\n        default_headers[\"User-Agent\"] += \" (\" + USER_AGENT + \")\"\n\n        self.language = language.strip().lower()\n        if not self.language:\n            raise AssertionError(\n                \"Specify language. Current language: '\"\n                + str(self.language)\n                + \"' is not sufficient.\"\n            )\n        self.extract_format = extract_format\n\n        log.info(\n            \"Wikipedia: language=%s, user_agent: %s, extract_format=%s\",\n            self.language,\n            default_headers[\"User-Agent\"],\n            self.extract_format,\n        )\n\n        self._session = requests.Session()\n        self._session.headers.update(default_headers)\n        self._request_kwargs = kwargs\n\n    def __del__(self) -> None:\n        \"\"\"Closes session.\"\"\"\n        if hasattr(self, \"_session\") and self._session:\n            self._session.close()\n\n    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        Creating `WikipediaPage` object is always the first step for extracting\n        any information.\n\n        Example::\n\n            wiki_wiki = wikipediaapi.Wikipedia('en')\n            page_py = wiki_wiki.page('Python_(programming_language)')\n            print(page_py.title)\n            # Python (programming language)\n\n            wiki_hi = wikipediaapi.Wikipedia('hi')\n\n            page_hi_py = wiki_hi.article(\n                title='%E0%A4%AA%E0%A4%BE%E0%A4%87%E0%A4%A5%E0%A4%A8',\n                unquote=True,\n            )\n            print(page_hi_py.title)\n            # \u092a\u093e\u0907\u0925\u0928\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n\n    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":\n        \"\"\"\n        Constructs Wikipedia page with title `title`.\n\n        This function is an alias for :func:`page`\n\n        :param title: page title as used in Wikipedia URL\n        :param ns: :class:`WikiNamespace`\n        :param unquote: if true it will unquote title\n        :return: object representing :class:`WikipediaPage`\n        \"\"\"\n        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )\n\n    def extracts(self, page: \"WikipediaPage\", **kwargs) -> str:\n        \"\"\"\n        Returns summary of the page with respect to parameters\n\n        Parameter `exsectionformat` is taken from `Wikipedia` constructor.\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bextracts\n        - https://www.mediawiki.org/wiki/Extension:TextExtracts#API\n\n        Example::\n\n            import wikipediaapi\n            wiki = wikipediaapi.Wikipedia('en')\n\n            page = wiki.page('Python_(programming_language)')\n            print(wiki.extracts(page, exsentences=1))\n            print(wiki.extracts(page, exsentences=2))\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: summary of the page\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"extracts\",\n            \"titles\": page.title,\n        }  # type: Dict[str, Any]\n\n        if self.extract_format == ExtractFormat.HTML:\n            # we do nothing, when format is HTML\n            pass\n        elif self.extract_format == ExtractFormat.WIKI:\n            params[\"explaintext\"] = 1\n            params[\"exsectionformat\"] = \"wiki\"\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return \"\"\n            return self._build_extracts(v, page)\n        return \"\"\n\n    def info(self, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"\n        https://www.mediawiki.org/w/api.php?action=help&modules=query%2Binfo\n        https://www.mediawiki.org/wiki/API:Info\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"info\",\n            \"titles\": page.title,\n            \"inprop\": \"|\".join(\n                [\n                    \"protection\",\n                    \"talkid\",\n                    \"watched\",\n                    \"watchers\",\n                    \"visitingwatchers\",\n                    \"notificationtimestamp\",\n                    \"subjectid\",\n                    \"url\",\n                    \"readable\",\n                    \"preload\",\n                    \"displaytitle\",\n                ]\n            ),\n        }\n        raw = self._query(page, params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return page\n\n            return self._build_info(v, page)\n        return page\n\n    def langlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns langlinks of the page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        - https://www.mediawiki.org/wiki/API:Langlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to pages in other languages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"langlinks\",\n            \"titles\": page.title,\n            \"lllimit\": 500,\n            \"llprop\": \"url\",\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_langlinks(v, page)\n        return {}\n\n    def links(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns links to other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        - https://www.mediawiki.org/wiki/API:Links\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: links to linked pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"links\",\n            \"titles\": page.title,\n            \"pllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n\n            while \"continue\" in raw:\n                params[\"plcontinue\"] = raw[\"continue\"][\"plcontinue\"]\n                raw = self._query(page, params)\n                v[\"links\"] += raw[\"query\"][\"pages\"][k][\"links\"]\n\n            return self._build_links(v, page)\n        return {}\n\n    def backlinks(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns backlinks from other pages with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        - https://www.mediawiki.org/wiki/API:Backlinks\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: backlinks from other pages\n\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"backlinks\",\n            \"bltitle\": page.title,\n            \"bllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"blcontinue\"] = raw[\"continue\"][\"blcontinue\"]\n            raw = self._query(page, params)\n            v[\"backlinks\"] += raw[\"query\"][\"backlinks\"]\n        return self._build_backlinks(v, page)\n\n    def categories(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns categories for page with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        - https://www.mediawiki.org/wiki/API:Categories\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: categories for page\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"prop\": \"categories\",\n            \"titles\": page.title,\n            \"cllimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n        self._common_attributes(raw[\"query\"], page)\n        pages = raw[\"query\"][\"pages\"]\n        for k, v in pages.items():\n            if k == \"-1\":\n                page._attributes[\"pageid\"] = -1\n                return {}\n            return self._build_categories(v, page)\n        return {}\n\n    def categorymembers(self, page: \"WikipediaPage\", **kwargs) -> PagesDict:\n        \"\"\"\n        Returns pages in given category with respect to parameters\n\n        API Calls for parameters:\n\n        - https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        - https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :param page: :class:`WikipediaPage`\n        :param kwargs: parameters used in API call\n        :return: pages in given category\n        \"\"\"\n        params = {\n            \"action\": \"query\",\n            \"list\": \"categorymembers\",\n            \"cmtitle\": page.title,\n            \"cmlimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self._query(page, used_params)\n\n        self._common_attributes(raw[\"query\"], page)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"cmcontinue\"] = raw[\"continue\"][\"cmcontinue\"]\n            raw = self._query(page, params)\n            v[\"categorymembers\"] += raw[\"query\"][\"categorymembers\"]\n\n        return self._build_categorymembers(v, page)\n\n    def _query(self, page: \"WikipediaPage\", params: Dict[str, Any]):\n        \"\"\"Queries Wikimedia API to fetch content.\"\"\"\n        base_url = \"https://\" + page.language + \".wikipedia.org/w/api.php\"\n        log.info(\n            \"Request URL: %s\",\n            base_url + \"?\" + \"&\".join([k + \"=\" + str(v) for k, v in params.items()]),\n        )\n        params[\"format\"] = \"json\"\n        params[\"redirects\"] = 1\n        r = self._session.get(base_url, params=params, **self._request_kwargs)\n        return r.json()\n\n    def _build_extracts(self, extract, page: \"WikipediaPage\") -> str:\n        \"\"\"Constructs summary of given page.\"\"\"\n        page._summary = \"\"\n        page._section_mapping = defaultdict(list)\n\n        self._common_attributes(extract, page)\n\n        section_stack = [page]\n        section = None\n        prev_pos = 0\n\n        for match in re.finditer(RE_SECTION[self.extract_format], extract[\"extract\"]):\n            if len(page._section_mapping) == 0:\n                page._summary = extract[\"extract\"][0 : match.start()].strip()\n            elif section is not None:\n                section._text = (extract[\"extract\"][prev_pos : match.start()]).strip()\n\n            section = self._create_section(match)\n            sec_level = section.level + 1\n\n            if sec_level > len(section_stack):\n                section_stack.append(section)\n            elif sec_level == len(section_stack):\n                section_stack.pop()\n                section_stack.append(section)\n            else:\n                for _ in range(len(section_stack) - sec_level + 1):\n                    section_stack.pop()\n                section_stack.append(section)\n\n            section_stack[len(section_stack) - 2]._section.append(section)\n            # section_stack[sec_level - 1]._section.append(section)\n\n            prev_pos = match.end()\n            page._section_mapping[section.title].append(section)\n\n        # pages without sections have only summary\n        if page._summary == \"\":\n            page._summary = extract[\"extract\"].strip()\n\n        if prev_pos > 0 and section is not None:\n            section._text = extract[\"extract\"][prev_pos:]\n\n        return page._summary\n\n    def _create_section(self, match):\n        \"\"\"Creates section.\"\"\"\n        sec_title = \"\"\n        sec_level = 2\n        if self.extract_format == ExtractFormat.WIKI:\n            sec_title = match.group(2).strip()\n            sec_level = len(match.group(1))\n        elif self.extract_format == ExtractFormat.HTML:\n            sec_title = match.group(5).strip()\n            sec_level = int(match.group(1).strip())\n\n        section = WikipediaPageSection(self, sec_title, sec_level - 1)\n        return section\n\n    def _build_info(self, extract, page: \"WikipediaPage\") -> \"WikipediaPage\":\n        \"\"\"Builds page from API call info.\"\"\"\n        self._common_attributes(extract, page)\n        for k, v in extract.items():\n            page._attributes[k] = v\n\n        return page\n\n    def _build_langlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call langlinks.\"\"\"\n        page._langlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for langlink in extract.get(\"langlinks\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=langlink[\"*\"],\n                ns=Namespace.MAIN,\n                language=langlink[\"lang\"],\n                url=langlink[\"url\"],\n            )\n            page._langlinks[p.language] = p\n\n        return page._langlinks\n\n    def _build_links(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call links.\"\"\"\n        page._links = {}\n\n        self._common_attributes(extract, page)\n\n        for link in extract.get(\"links\", []):\n            page._links[link[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=link[\"title\"],\n                ns=int(link[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._links\n\n    def _build_backlinks(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call backlinks.\"\"\"\n        page._backlinks = {}\n\n        self._common_attributes(extract, page)\n\n        for backlink in extract.get(\"backlinks\", []):\n            page._backlinks[backlink[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=backlink[\"title\"],\n                ns=int(backlink[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._backlinks\n\n    def _build_categories(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categories.\"\"\"\n        page._categories = {}\n\n        self._common_attributes(extract, page)\n\n        for category in extract.get(\"categories\", []):\n            page._categories[category[\"title\"]] = WikipediaPage(\n                wiki=self,\n                title=category[\"title\"],\n                ns=int(category[\"ns\"]),\n                language=page.language,\n            )\n\n        return page._categories\n\n    def _build_categorymembers(self, extract, page) -> PagesDict:\n        \"\"\"Builds page from API call categorymembers.\"\"\"\n        page._categorymembers = {}\n\n        self._common_attributes(extract, page)\n\n        for member in extract.get(\"categorymembers\", []):\n            p = WikipediaPage(\n                wiki=self,\n                title=member[\"title\"],\n                ns=int(member[\"ns\"]),\n                language=page.language,\n            )\n            p.pageid = member[\"pageid\"]  # type: ignore\n\n            page._categorymembers[member[\"title\"]] = p\n\n        return page._categorymembers\n\n    @staticmethod\n    def _common_attributes(extract, page: \"WikipediaPage\"):\n        \"\"\"Fills in common attributes for page.\"\"\"\n        common_attributes = [\"title\", \"pageid\", \"ns\", \"redirects\"]\n\n        for attr in common_attributes:\n            if attr in extract:\n                page._attributes[attr] = extract[attr]\n\n\nclass WikipediaPageSection:\n    \"\"\"WikipediaPageSection represents section in the page.\"\"\"\n\n    def __init__(\n        self, wiki: Wikipedia, title: str, level: int = 0, text: str = \"\"\n    ) -> None:\n        \"\"\"Constructs WikipediaPageSection.\"\"\"\n        self.wiki = wiki\n        self._title = title\n        self._level = level\n        self._text = text\n        self._section = []  # type: List['WikipediaPageSection']\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current section.\n\n        :return: title of the current section\n        \"\"\"\n        return self._title\n\n    @property\n    def level(self) -> int:\n        \"\"\"\n        Returns indentation level of the current section.\n\n        :return: indentation level of the current section\n        \"\"\"\n        return self._level\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current section.\n\n        :return: text of the current section\n        \"\"\"\n        return self._text\n\n    @property\n    def sections(self) -> List[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section.\n\n        :return: subsections of the current section\n        \"\"\"\n        return self._section\n\n    def section_by_title(self, title: str) -> Optional[\"WikipediaPageSection\"]:\n        \"\"\"\n        Returns subsections of the current section with given title.\n\n        :param title: title of the subsection\n        :return: subsection if it exists\n        \"\"\"\n        sections = [s for s in self._section if s.title == title]\n        if sections:\n            return sections[-1]\n        return None\n\n    def full_text(self, level: int = 1) -> str:\n        \"\"\"\n        Returns text of the current section as well as all its subsections.\n\n        :param level: indentation level\n        :return: text of the current section as well as all its subsections\n        \"\"\"\n        res = \"\"\n        if self.wiki.extract_format == ExtractFormat.WIKI:\n            res += self.title\n        elif self.wiki.extract_format == ExtractFormat.HTML:\n            res += f\"<h{level}>{self.title}</h{level}>\"\n        else:\n            raise NotImplementedError(\"Unknown ExtractFormat type\")\n\n        res += \"\\n\"\n        res += self._text\n        if len(self._text) > 0:\n            res += \"\\n\\n\"\n        for sec in self.sections:\n            res += sec.full_text(level + 1)\n        return res\n\n    def __repr__(self):\n        return \"Section: {} ({}):\\n{}\\nSubsections ({}):\\n{}\".format(\n            self._title,\n            self._level,\n            self._text,\n            len(self._section),\n            \"\\n\".join(map(repr, self._section)),\n        )\n\n\nclass WikipediaPage:\n    \"\"\"\n    Represents Wikipedia page.\n\n    Except properties mentioned as part of documentation, there are also\n    these properties available:\n\n    * `fullurl` - full URL of the page\n    * `canonicalurl` - canonical URL of the page\n    * `pageid` - id of the current page\n    * `displaytitle` - title of the page to display\n    * `talkid` - id of the page with discussion\n\n    \"\"\"\n\n    ATTRIBUTES_MAPPING = {\n        \"language\": [],\n        \"pageid\": [\"info\", \"extracts\", \"langlinks\"],\n        \"ns\": [\"info\", \"extracts\", \"langlinks\"],\n        \"title\": [\"info\", \"extracts\", \"langlinks\"],\n        \"contentmodel\": [\"info\"],\n        \"pagelanguage\": [\"info\"],\n        \"pagelanguagehtmlcode\": [\"info\"],\n        \"pagelanguagedir\": [\"info\"],\n        \"touched\": [\"info\"],\n        \"lastrevid\": [\"info\"],\n        \"length\": [\"info\"],\n        \"protection\": [\"info\"],\n        \"restrictiontypes\": [\"info\"],\n        \"watchers\": [\"info\"],\n        \"visitingwatchers\": [\"info\"],\n        \"notificationtimestamp\": [\"info\"],\n        \"talkid\": [\"info\"],\n        \"fullurl\": [\"info\"],\n        \"editurl\": [\"info\"],\n        \"canonicalurl\": [\"info\"],\n        \"readable\": [\"info\"],\n        \"preload\": [\"info\"],\n        \"displaytitle\": [\"info\"],\n    }\n\n    def __init__(\n        self,\n        wiki: Wikipedia,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        language: str = \"en\",\n        url: Optional[str] = None,\n    ) -> None:\n        self.wiki = wiki\n        self._summary = \"\"  # type: str\n        self._section = []  # type: List[WikipediaPageSection]\n        self._section_mapping = {}  # type: Dict[str, List[WikipediaPageSection]]\n        self._langlinks = {}  # type: PagesDict\n        self._links = {}  # type: PagesDict\n        self._backlinks = {}  # type: PagesDict\n        self._categories = {}  # type: PagesDict\n        self._categorymembers = {}  # type: PagesDict\n\n        self._called = {\n            \"extracts\": False,\n            \"info\": False,\n            \"langlinks\": False,\n            \"links\": False,\n            \"backlinks\": False,\n            \"categories\": False,\n            \"categorymembers\": False,\n        }\n\n        self._attributes = {\n            \"title\": title,\n            \"ns\": namespace2int(ns),\n            \"language\": language,\n        }  # type: Dict[str, Any]\n\n        if url is not None:\n            self._attributes[\"fullurl\"] = url\n\n    def __getattr__(self, name):\n        if name not in self.ATTRIBUTES_MAPPING:\n            return self.__getattribute__(name)\n\n        if name in self._attributes:\n            return self._attributes[name]\n\n        for call in self.ATTRIBUTES_MAPPING[name]:\n            if not self._called[call]:\n                self._fetch(call)\n                return self._attributes[name]\n\n    @property\n    def language(self) -> str:\n        \"\"\"\n        Returns language of the current page.\n\n        :return: language\n        \"\"\"\n        return str(self._attributes[\"language\"])\n\n    @property\n    def title(self) -> str:\n        \"\"\"\n        Returns title of the current page.\n\n        :return: title\n        \"\"\"\n        return str(self._attributes[\"title\"])\n\n    @property\n    def namespace(self) -> int:\n        \"\"\"\n        Returns namespace of the current page.\n\n        :return: namespace\n        \"\"\"\n        return int(self._attributes[\"ns\"])\n\n    def exists(self) -> bool:\n        \"\"\"\n        Returns `True` if the current page exists, otherwise `False`.\n\n        :return: if current page existst or not\n        \"\"\"\n        return bool(self.pageid != -1)\n\n    @property\n    def summary(self) -> str:\n        \"\"\"\n        Returns summary of the current page.\n\n        :return: summary\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._summary\n\n    @property\n    def sections(self) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all sections of the curent page.\n\n        :return: List of :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n###The function: section_by_title###\n    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:\n        \"\"\"\n        Returns all section of the current page with given `title`.\n\n        :param title: section title\n        :return: :class:`WikipediaPageSection`\n        \"\"\"\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns text of the current page.\n\n        :return: text of the current page\n        \"\"\"\n        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()\n\n    @property\n    def langlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all language links to pages in other languages.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blanglinks\n        * https://www.mediawiki.org/wiki/API:Langlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n\n    @property\n    def links(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linked from the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Blinks\n        * https://www.mediawiki.org/wiki/API:Links\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    @property\n    def backlinks(self) -> PagesDict:\n        \"\"\"\n        Returns all pages linking to the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bbacklinks\n        * https://www.mediawiki.org/wiki/API:Backlinks\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    @property\n    def categories(self) -> PagesDict:\n        \"\"\"\n        Returns categories associated with the current page.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategories\n        * https://www.mediawiki.org/wiki/API:Categories\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categories\"]:\n            self._fetch(\"categories\")\n        return self._categories\n\n    @property\n    def categorymembers(self) -> PagesDict:\n        \"\"\"\n        Returns all pages belonging to the current category.\n\n        This is wrapper for:\n\n        * https://www.mediawiki.org/w/api.php?action=help&modules=query%2Bcategorymembers\n        * https://www.mediawiki.org/wiki/API:Categorymembers\n\n        :return: :class:`PagesDict`\n        \"\"\"\n        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n\n    def _fetch(self, call) -> \"WikipediaPage\":\n        \"\"\"Fetches some data?.\"\"\"\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n\n    def __repr__(self):\n        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"\n", "test_list": ["def test_subsubsection(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Section 4.2.2')\n    self.assertEqual(section.title, 'Section 4.2.2')\n    self.assertEqual(section.text, '<p><b>Text for section 4.2.2</b>\\n\\n\\n</p>')\n    self.assertEqual(repr(section), 'Section: Section 4.2.2 (3):\\n' + '<p><b>Text for section 4.2.2</b>\\n\\n\\n</p>\\n' + 'Subsections (0):\\n')\n    self.assertEqual(len(section.sections), 0)", "def test_subsection_by_title_return_last(self):\n    page = self.wiki.page('Test_Nested')\n    section = page.section_by_title('Subsection B')\n    self.assertEqual(section.title, 'Subsection B')\n    self.assertEqual(section.text, '<p><b>Text for section 3.B</b>\\n\\n\\n</p>')\n    self.assertEqual(len(section.sections), 0)", "def test_with_erroneous_edit(self):\n    page = self.wiki.page('Test_Edit')\n    self.maxDiff = None\n    section = page.section_by_title('Section with Edit')\n    self.assertEqual(section.title, 'Section with Edit')\n    self.assertEqual(page.text, '<p><b>Summary</b> text\\n\\n</p>\\n\\n' + '<h2>Section 1</h2>\\n' + '<p>Text for section 1</p>\\n\\n<h3>Section with Edit</h3>\\n' + '<p>Text for section with edit\\n\\n\\n</p>')", "def test_subsection_by_title_with_multiple_spans(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Section 5')\n    self.assertEqual(section.title, 'Section 5')", "def test_subsection_by_title(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Section 4')\n    self.assertEqual(section.title, 'Section 4')\n    self.assertEqual(section.level, 1)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'section_by_title' should return None if the title does not exist in the section mapping.", "unit_test": ["def test_section_by_title_nonexistent(self):\n    page = self.wiki.page('Test_1')\n    section = page.section_by_title('Nonexistent Section')\n    self.assertIsNone(section)"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_nonexistent"}, "Exception Handling": {"requirement": "The function 'section_by_title' should raise a TypeError if the title parameter is not a string.", "unit_test": ["def test_section_by_title_invalid_type(self):\n    page = self.wiki.page('Test_1')\n    with self.assertRaises(TypeError):\n        page.section_by_title(123)"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_invalid_type"}, "Edge Case Handling": {"requirement": "The function 'section_by_title' should handle pages with no sections gracefully, returning None.", "unit_test": ["def test_section_by_title_no_sections(self):\n    page = self.wiki.page('Empty_Page')\n    section = page.section_by_title('Any Section')\n    self.assertIsNone(section)"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_no_sections"}, "Functionality Extension": {"requirement": "Extend the 'section_by_title' function to return a list of all sections with the given title if a parameter 'all' is set to True.", "unit_test": ["def test_section_by_title_return_all(self):\n    page = self.wiki.page('Test_Multiple')\n    sections = page.section_by_title('Repeated Section', all=True)\n    self.assertEqual(len(sections), 3)\n    self.assertEqual(sections[0].title, 'Repeated Section')"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_return_all"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types in the 'section_by_title' function are annotated with type hints.", "unit_test": ["def test_section_by_title_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(WikipediaPage.section_by_title)\n    self.assertEqual(hints['title'], str)\n    self.assertEqual(hints['return'], Optional[WikipediaPageSection])"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_annotations"}, "Code Complexity": {"requirement": "The 'section_by_title' function should have a cyclomatic complexity of 3 or less.", "unit_test": ["def test_section_by_title_complexity(self):\n    import radon.complexity as cc\n    source = inspect.getsource(WikipediaPage.section_by_title)\n    complexity = cc.cc_visit(source)\n    self.assertLessEqual(complexity[0].complexity, 3)"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_code_complexity"}, "Code Standard": {"requirement": "The 'section_by_title' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_section_by_title_pep8(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['wikipediaapi.py'])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'section_by_title' function should utilize the '_section_mapping' attribute of the WikipediaPage class.", "unit_test": ["def test_section_by_title_context_usage(self):\n    page = self.wiki.page('Test_1')\n    page._section_mapping = {'Test Section': [WikipediaPageSection(self.wiki, 'Test Section')]}\n    section = page.section_by_title('Test Section')\n    self.assertIsNotNone(section)"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'section_by_title' function should correctly access the '_section_mapping' attribute to retrieve sections by title.", "unit_test": ["def test_section_by_title_context_correctness(self):\n    page = self.wiki.page('Test_1')\n    page._section_mapping = {'Correct Section': [WikipediaPageSection(self.wiki, 'Correct Section')]}\n    section = page.section_by_title('Correct Section')\n    self.assertEqual(section.title, 'Correct Section')"], "test": "tests/extract_html_format_test.py::TestHtmlFormatExtracts::test_section_by_title_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "csvkit.cli.CSVKitUtility.run", "type": "method", "project_path": "Scientific-Engineering/csvkit", "completion_path": "Scientific-Engineering/csvkit/csvkit/cli.py", "signature_position": [106, 106], "body_position": [111, 122], "dependency": {"intra_class": ["csvkit.cli.CSVKitUtility._open_input_file", "csvkit.cli.CSVKitUtility.args", "csvkit.cli.CSVKitUtility.input_file", "csvkit.cli.CSVKitUtility.main", "csvkit.cli.CSVKitUtility.override_flags"], "intra_file": ["csvkit.cli.LazyFile.close"], "cross_file": []}, "requirement": {"Functionality": "This function is a wrapper around the main loop of a utility. It handles opening and closing files. It first checks if the 'f' flag is not present in the override flags. If not present, it opens the input file. Then, it executes the main loop of the utility, ignoring warnings related to column names if the 'no_header_row' option is present. Finally, it closes the input file if the 'f' flag is not present in the override flags.", "Arguments": ":param self: CSVKitUtility. An instance of the CSVKitUtility class.\n:return: No return values."}, "tests": ["tests/test_utilities/test_csvsql.py::TestCSVSQL::test_before_after_insert", "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_duplicate_keys"], "indent": 4, "domain": "Scientific-Engineering", "code": "    def run(self):\n        \"\"\"\n        A wrapper around the main loop of the utility which handles opening and\n        closing files.\n        \"\"\"\n        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        try:\n            with warnings.catch_warnings():\n                if getattr(self.args, 'no_header_row', None):\n                    warnings.filterwarnings(action='ignore', message='Column names not specified', module='agate')\n\n                self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()\n", "context": "#!/usr/bin/env python\n\nimport argparse\nimport bz2\nimport datetime\nimport decimal\nimport gzip\nimport itertools\nimport lzma\nimport sys\nimport warnings\nfrom os.path import splitext\n\nimport agate\nfrom agate.data_types.base import DEFAULT_NULL_VALUES\n\nfrom csvkit.exceptions import ColumnIdentifierError, RequiredHeaderError\n\n\nclass LazyFile:\n    \"\"\"\n    A proxy for a File object that delays opening it until\n    a read method is called.\n\n    Currently this implements only the minimum methods to be useful,\n    but it could easily be expanded.\n    \"\"\"\n\n    def __init__(self, init, *args, **kwargs):\n        self.init = init\n        self.f = None\n        self._is_lazy_opened = False\n\n        self._lazy_args = args\n        self._lazy_kwargs = kwargs\n\n    def __getattr__(self, name):\n        self._open()\n        return getattr(self.f, name)\n\n    def __iter__(self):\n        return self\n\n    def close(self):\n        if self._is_lazy_opened:\n            self.f.close()\n            self.f = None\n            self._is_lazy_opened = False\n\n    def __next__(self):\n        self._open()\n        return next(self.f).replace('\\0', '')\n\n    def _open(self):\n        if not self._is_lazy_opened:\n            self.f = self.init(*self._lazy_args, **self._lazy_kwargs)\n            self._is_lazy_opened = True\n\n\nclass CSVKitUtility:\n    description = ''\n    epilog = ''\n    override_flags = ''\n\n    def __init__(self, args=None, output_file=None):\n        \"\"\"\n        Perform argument processing and other setup for a CSVKitUtility.\n        \"\"\"\n        self._init_common_parser()\n        self.add_arguments()\n        self.args = self.argparser.parse_args(args)\n        # Output file is only set during testing.\n        if output_file is None:\n            self.output_file = sys.stdout\n        else:\n            self.output_file = output_file\n\n        self.reader_kwargs = self._extract_csv_reader_kwargs()\n        self.writer_kwargs = self._extract_csv_writer_kwargs()\n\n        self._install_exception_handler()\n\n        # Ensure SIGPIPE doesn't throw an exception\n        # Prevents [Errno 32] Broken pipe errors, e.g. when piping to 'head'\n        # To test from the shell:\n        #  python -c \"for i in range(5000): print('a,b,c')\" | csvlook | head\n        # Without this fix you will see at the end:\n        #  [Errno 32] Broken pipe\n        # With this fix, there should be no error\n        # For details on Python and SIGPIPE, see https://bugs.python.org/issue1652\n        try:\n            import signal\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n        except (ImportError, AttributeError):\n            # Do nothing on platforms that don't have signals or don't have SIGPIPE\n            pass\n\n    def add_arguments(self):\n        \"\"\"\n        Called upon initialization once the parser for common arguments has been constructed.\n\n        Should be overriden by individual utilities.\n        \"\"\"\n        raise NotImplementedError('add_arguments must be provided by each subclass of CSVKitUtility.')\n\n###The function: run###\n    def main(self):\n        \"\"\"\n        Main loop of the utility.\n\n        Should be overriden by individual utilities and explicitly called by the executing script.\n        \"\"\"\n        raise NotImplementedError(' must be provided by each subclass of CSVKitUtility.')\n\n    def _init_common_parser(self):\n        \"\"\"\n        Prepare a base argparse argument parser so that flags are consistent across different shell command tools.\n        If you want to constrain which common args are present, you can pass a string for 'omitflags'. Any argument\n        whose single-letter form is contained in 'omitflags' will be left out of the configured parser. Use 'f' for\n        file.\n        \"\"\"\n        self.argparser = argparse.ArgumentParser(description=self.description, epilog=self.epilog)\n\n        # Input\n        if 'f' not in self.override_flags:\n            self.argparser.add_argument(\n                metavar='FILE', nargs='?', dest='input_path',\n                help='The CSV file to operate on. If omitted, will accept input as piped data via STDIN.')\n        if 'd' not in self.override_flags:\n            self.argparser.add_argument(\n                '-d', '--delimiter', dest='delimiter',\n                help='Delimiting character of the input CSV file.')\n        if 't' not in self.override_flags:\n            self.argparser.add_argument(\n                '-t', '--tabs', dest='tabs', action='store_true',\n                help='Specify that the input CSV file is delimited with tabs. Overrides \"-d\".')\n        if 'q' not in self.override_flags:\n            self.argparser.add_argument(\n                '-q', '--quotechar', dest='quotechar',\n                help='Character used to quote strings in the input CSV file.')\n        if 'u' not in self.override_flags:\n            self.argparser.add_argument(\n                '-u', '--quoting', dest='quoting', type=int, choices=[0, 1, 2, 3],\n                help='Quoting style used in the input CSV file. 0 = Quote Minimal, 1 = Quote All, '\n                     '2 = Quote Non-numeric, 3 = Quote None.')\n        if 'b' not in self.override_flags:\n            self.argparser.add_argument(\n                '-b', '--no-doublequote', dest='doublequote', action='store_false',\n                help='Whether or not double quotes are doubled in the input CSV file.')\n        if 'p' not in self.override_flags:\n            self.argparser.add_argument(\n                '-p', '--escapechar', dest='escapechar',\n                help='Character used to escape the delimiter if --quoting 3 (\"Quote None\") is specified and to escape '\n                     'the QUOTECHAR if --no-doublequote is specified.')\n        if 'z' not in self.override_flags:\n            self.argparser.add_argument(\n                '-z', '--maxfieldsize', dest='field_size_limit', type=int,\n                help='Maximum length of a single field in the input CSV file.')\n        if 'e' not in self.override_flags:\n            self.argparser.add_argument(\n                '-e', '--encoding', dest='encoding', default='utf-8-sig',\n                help='Specify the encoding of the input CSV file.')\n        if 'L' not in self.override_flags:\n            self.argparser.add_argument(\n                '-L', '--locale', dest='locale', default='en_US',\n                help='Specify the locale (en_US) of any formatted numbers.')\n        if 'S' not in self.override_flags:\n            self.argparser.add_argument(\n                '-S', '--skipinitialspace', dest='skipinitialspace', action='store_true',\n                help='Ignore whitespace immediately following the delimiter.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--blanks', dest='blanks', action='store_true',\n                help='Do not convert \"\", \"na\", \"n/a\", \"none\", \"null\", \".\" to NULL.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--null-value', dest='null_values', nargs='+', default=[],\n                help='Convert this value to NULL. --null-value can be specified multiple times.')\n        if 'date-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--date-format', dest='date_format',\n                help='Specify a strptime date format string like \"%%m/%%d/%%Y\".')\n        if 'datetime-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--datetime-format', dest='datetime_format',\n                help='Specify a strptime datetime format string like \"%%m/%%d/%%Y %%I:%%M %%p\".')\n        if 'H' not in self.override_flags:\n            self.argparser.add_argument(\n                '-H', '--no-header-row', dest='no_header_row', action='store_true',\n                help='Specify that the input CSV file has no header row. Will create default headers (a,b,c,...).')\n        if 'K' not in self.override_flags:\n            self.argparser.add_argument(\n                '-K', '--skip-lines', dest='skip_lines', type=int, default=0,\n                help='Specify the number of initial lines to skip before the header row (e.g. comments, copyright '\n                     'notices, empty rows).')\n        if 'v' not in self.override_flags:\n            self.argparser.add_argument(\n                '-v', '--verbose', dest='verbose', action='store_true',\n                help='Print detailed tracebacks when errors occur.')\n\n        # Output\n        if 'l' not in self.override_flags:\n            self.argparser.add_argument(\n                '-l', '--linenumbers', dest='line_numbers', action='store_true',\n                help='Insert a column of line numbers at the front of the output. Useful when piping to grep or as a '\n                     'simple primary key.')\n\n        # Input/Output\n        if 'zero' not in self.override_flags:\n            self.argparser.add_argument(\n                '--zero', dest='zero_based', action='store_true',\n                help='When interpreting or displaying column numbers, use zero-based numbering instead of the default '\n                     '1-based numbering.')\n\n        self.argparser.add_argument(\n            '-V', '--version', action='version', version='%(prog)s 1.3.0',\n            help='Display version information and exit.')\n\n    def _open_input_file(self, path, opened=False):\n        \"\"\"\n        Open the input file specified on the command line.\n        \"\"\"\n        if not path or path == '-':\n            # \"UnsupportedOperation: It is not possible to set the encoding or newline of stream after the first read\"\n            if not opened:\n                sys.stdin.reconfigure(encoding=self.args.encoding)\n            f = sys.stdin\n        else:\n            extension = splitext(path)[1]\n\n            if extension == '.gz':\n                func = gzip.open\n            elif extension == '.bz2':\n                func = bz2.open\n            elif extension == \".xz\":\n                func = lzma.open\n            else:\n                func = open\n\n            f = LazyFile(func, path, mode='rt', encoding=self.args.encoding)\n\n        return f\n\n    def _extract_csv_reader_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the input CSV reader(s).\n        \"\"\"\n        kwargs = {}\n\n        if self.args.tabs:\n            kwargs['delimiter'] = '\\t'\n        elif self.args.delimiter:\n            kwargs['delimiter'] = self.args.delimiter\n\n        for arg in ('quotechar', 'quoting', 'doublequote', 'escapechar', 'field_size_limit', 'skipinitialspace'):\n            value = getattr(self.args, arg)\n            if value is not None:\n                kwargs[arg] = value\n\n        if getattr(self.args, 'no_header_row', None):\n            kwargs['header'] = not self.args.no_header_row\n\n        return kwargs\n\n    def _extract_csv_writer_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the output CSV writer.\n        \"\"\"\n        kwargs = {}\n\n        if getattr(self.args, 'line_numbers', None):\n            kwargs['line_numbers'] = True\n\n        return kwargs\n\n    def _install_exception_handler(self):\n        \"\"\"\n        Installs a replacement for sys.excepthook, which handles pretty-printing uncaught exceptions.\n        \"\"\"\n        def handler(t, value, traceback):\n            if self.args.verbose:\n                sys.__excepthook__(t, value, traceback)\n            else:\n                # Special case handling for Unicode errors, which behave very strangely\n                # when cast with unicode()\n                if t == UnicodeDecodeError:\n                    sys.stderr.write('Your file is not \"%s\" encoded. Please specify the correct encoding with the -e '\n                                     'flag or with the PYTHONIOENCODING environment variable. Use the -v flag to see '\n                                     'the complete error.\\n' % self.args.encoding)\n                else:\n                    sys.stderr.write(f'{t.__name__}: {str(value)}\\n')\n\n        sys.excepthook = handler\n\n    def get_column_types(self):\n        if getattr(self.args, 'blanks', None):\n            type_kwargs = {'null_values': []}\n        else:\n            type_kwargs = {'null_values': list(DEFAULT_NULL_VALUES)}\n        for null_value in getattr(self.args, 'null_values', []):\n            type_kwargs['null_values'].append(null_value)\n\n        text_type = agate.Text(**type_kwargs)\n\n        if self.args.no_inference:\n            types = [text_type]\n        else:\n            number_type = agate.Number(locale=self.args.locale, **type_kwargs)\n\n            # See the order in the `agate.TypeTester` class.\n            types = [\n                agate.Boolean(**type_kwargs),\n                agate.TimeDelta(**type_kwargs),\n                agate.Date(date_format=self.args.date_format, **type_kwargs),\n                agate.DateTime(datetime_format=self.args.datetime_format, **type_kwargs),\n                text_type,\n            ]\n\n            # In order to parse dates like \"20010101\".\n            if self.args.date_format or self.args.datetime_format:\n                types.insert(-1, number_type)\n            else:\n                types.insert(1, number_type)\n\n        return agate.TypeTester(types=types)\n\n    def get_column_offset(self):\n        if self.args.zero_based:\n            return 0\n        return 1\n\n    def skip_lines(self):\n        if isinstance(self.args.skip_lines, int):\n            while self.args.skip_lines > 0:\n                self.input_file.readline()\n                self.args.skip_lines -= 1\n        else:\n            raise ValueError('skip_lines argument must be an int')\n\n        return self.input_file\n\n    def get_rows_and_column_names_and_column_ids(self, **kwargs):\n        rows = agate.csv.reader(self.skip_lines(), **kwargs)\n\n        try:\n            next_row = next(rows)\n        except StopIteration:\n            return iter([]), [], []\n\n        if self.args.no_header_row:\n            # Peek at a row to get the number of columns.\n            row = next_row\n            rows = itertools.chain([row], rows)\n            column_names = make_default_headers(len(row))\n        else:\n            column_names = next_row\n\n        column_offset = self.get_column_offset()\n        if kwargs.get('line_numbers'):\n            column_offset -= 1\n\n        column_ids = parse_column_identifiers(\n            self.args.columns,\n            column_names,\n            column_offset,\n            getattr(self.args, 'not_columns', None),\n        )\n\n        return rows, column_names, column_ids\n\n    def print_column_names(self):\n        \"\"\"\n        Pretty-prints the names and indices of all columns to a file-like object (usually sys.stdout).\n        \"\"\"\n        if getattr(self.args, 'no_header_row', None):\n            raise RequiredHeaderError('You cannot use --no-header-row with the -n or --names options.')\n\n        if getattr(self.args, 'zero_based', None):\n            start = 0\n        else:\n            start = 1\n\n        rows = agate.csv.reader(self.skip_lines(), **self.reader_kwargs)\n        column_names = next(rows)\n\n        for i, c in enumerate(column_names, start):\n            self.output_file.write('%3i: %s\\n' % (i, c))\n\n    def additional_input_expected(self):\n        return isatty(sys.stdin) and not self.args.input_path\n\n\ndef isatty(f):\n    try:\n        return f.isatty()\n    except ValueError:  # I/O operation on closed file\n        return False\n\n\ndef default_str_decimal(obj):\n    if isinstance(obj, (datetime.date, datetime.datetime)):\n        return obj.isoformat()\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    raise TypeError(f'{repr(obj)} is not JSON serializable')\n\n\ndef default_float_decimal(obj):\n    if isinstance(obj, decimal.Decimal):\n        return float(obj)\n    return default_str_decimal(obj)\n\n\ndef make_default_headers(n):\n    \"\"\"\n    Make a set of simple, default headers for files that are missing them.\n    \"\"\"\n    return tuple(agate.utils.letter_name(i) for i in range(n))\n\n\ndef match_column_identifier(column_names, c, column_offset=1):\n    \"\"\"\n    Determine what column a single column id (name or index) matches in a series of column names.\n    Note that integer values are *always* treated as positional identifiers. If you happen to have\n    column names which are also integers, you must specify them using a positional index.\n    \"\"\"\n    if isinstance(c, str) and not c.isdigit() and c in column_names:\n        return column_names.index(c)\n\n    try:\n        c = int(c) - column_offset\n    # Fail out if neither a column name nor an integer\n    except ValueError:\n        raise ColumnIdentifierError(\"Column '%s' is invalid. It is neither an integer nor a column name. \"\n                                    \"Column names are: %s\" % (c, repr(column_names)[1:-1]))\n\n    # Fail out if index is 0-based\n    if c < 0:\n        raise ColumnIdentifierError(\"Column %i is invalid. Columns are 1-based.\" % (c + column_offset))\n\n    # Fail out if index is out of range\n    if c >= len(column_names):\n        raise ColumnIdentifierError(\"Column %i is invalid. The last column is '%s' at index %i.\" % (\n            c + column_offset, column_names[-1], len(column_names) - 1 + column_offset))\n\n    return c\n\n\ndef parse_column_identifiers(ids, column_names, column_offset=1, excluded_columns=None):\n    \"\"\"\n    Parse a comma-separated list of column indices AND/OR names into a list of integer indices.\n    Ranges of integers can be specified with two integers separated by a '-' or ':' character.\n    Ranges of non-integers (e.g. column names) are not supported.\n    Note: Column indices are 1-based.\n    \"\"\"\n    if not column_names:\n        return []\n\n    if not ids and not excluded_columns:\n        return range(len(column_names))\n\n    if ids:\n        columns = []\n\n        for c in ids.split(','):\n            try:\n                columns.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names) + 1\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    columns.append(match_column_identifier(column_names, x, column_offset))\n    else:\n        columns = range(len(column_names))\n\n    excludes = []\n\n    if excluded_columns:\n        for c in excluded_columns.split(','):\n            try:\n                excludes.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names)\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    excludes.append(match_column_identifier(column_names, x, column_offset))\n\n    return [c for c in columns if c not in excludes]\n", "prompt": "Please write a python function called 'run' base the context. This function is a wrapper around the main loop of a utility. It handles opening and closing files. It first checks if the 'f' flag is not present in the override flags. If not present, it opens the input file. Then, it executes the main loop of the utility, ignoring warnings related to column names if the 'no_header_row' option is present. Finally, it closes the input file if the 'f' flag is not present in the override flags.:param self: CSVKitUtility. An instance of the CSVKitUtility class.\n:return: No return values..\n        The context you need to refer to is as follows: #!/usr/bin/env python\n\nimport argparse\nimport bz2\nimport datetime\nimport decimal\nimport gzip\nimport itertools\nimport lzma\nimport sys\nimport warnings\nfrom os.path import splitext\n\nimport agate\nfrom agate.data_types.base import DEFAULT_NULL_VALUES\n\nfrom csvkit.exceptions import ColumnIdentifierError, RequiredHeaderError\n\n\nclass LazyFile:\n    \"\"\"\n    A proxy for a File object that delays opening it until\n    a read method is called.\n\n    Currently this implements only the minimum methods to be useful,\n    but it could easily be expanded.\n    \"\"\"\n\n    def __init__(self, init, *args, **kwargs):\n        self.init = init\n        self.f = None\n        self._is_lazy_opened = False\n\n        self._lazy_args = args\n        self._lazy_kwargs = kwargs\n\n    def __getattr__(self, name):\n        self._open()\n        return getattr(self.f, name)\n\n    def __iter__(self):\n        return self\n\n    def close(self):\n        if self._is_lazy_opened:\n            self.f.close()\n            self.f = None\n            self._is_lazy_opened = False\n\n    def __next__(self):\n        self._open()\n        return next(self.f).replace('\\0', '')\n\n    def _open(self):\n        if not self._is_lazy_opened:\n            self.f = self.init(*self._lazy_args, **self._lazy_kwargs)\n            self._is_lazy_opened = True\n\n\nclass CSVKitUtility:\n    description = ''\n    epilog = ''\n    override_flags = ''\n\n    def __init__(self, args=None, output_file=None):\n        \"\"\"\n        Perform argument processing and other setup for a CSVKitUtility.\n        \"\"\"\n        self._init_common_parser()\n        self.add_arguments()\n        self.args = self.argparser.parse_args(args)\n        # Output file is only set during testing.\n        if output_file is None:\n            self.output_file = sys.stdout\n        else:\n            self.output_file = output_file\n\n        self.reader_kwargs = self._extract_csv_reader_kwargs()\n        self.writer_kwargs = self._extract_csv_writer_kwargs()\n\n        self._install_exception_handler()\n\n        # Ensure SIGPIPE doesn't throw an exception\n        # Prevents [Errno 32] Broken pipe errors, e.g. when piping to 'head'\n        # To test from the shell:\n        #  python -c \"for i in range(5000): print('a,b,c')\" | csvlook | head\n        # Without this fix you will see at the end:\n        #  [Errno 32] Broken pipe\n        # With this fix, there should be no error\n        # For details on Python and SIGPIPE, see https://bugs.python.org/issue1652\n        try:\n            import signal\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n        except (ImportError, AttributeError):\n            # Do nothing on platforms that don't have signals or don't have SIGPIPE\n            pass\n\n    def add_arguments(self):\n        \"\"\"\n        Called upon initialization once the parser for common arguments has been constructed.\n\n        Should be overriden by individual utilities.\n        \"\"\"\n        raise NotImplementedError('add_arguments must be provided by each subclass of CSVKitUtility.')\n\n###The function: run###\n    def main(self):\n        \"\"\"\n        Main loop of the utility.\n\n        Should be overriden by individual utilities and explicitly called by the executing script.\n        \"\"\"\n        raise NotImplementedError(' must be provided by each subclass of CSVKitUtility.')\n\n    def _init_common_parser(self):\n        \"\"\"\n        Prepare a base argparse argument parser so that flags are consistent across different shell command tools.\n        If you want to constrain which common args are present, you can pass a string for 'omitflags'. Any argument\n        whose single-letter form is contained in 'omitflags' will be left out of the configured parser. Use 'f' for\n        file.\n        \"\"\"\n        self.argparser = argparse.ArgumentParser(description=self.description, epilog=self.epilog)\n\n        # Input\n        if 'f' not in self.override_flags:\n            self.argparser.add_argument(\n                metavar='FILE', nargs='?', dest='input_path',\n                help='The CSV file to operate on. If omitted, will accept input as piped data via STDIN.')\n        if 'd' not in self.override_flags:\n            self.argparser.add_argument(\n                '-d', '--delimiter', dest='delimiter',\n                help='Delimiting character of the input CSV file.')\n        if 't' not in self.override_flags:\n            self.argparser.add_argument(\n                '-t', '--tabs', dest='tabs', action='store_true',\n                help='Specify that the input CSV file is delimited with tabs. Overrides \"-d\".')\n        if 'q' not in self.override_flags:\n            self.argparser.add_argument(\n                '-q', '--quotechar', dest='quotechar',\n                help='Character used to quote strings in the input CSV file.')\n        if 'u' not in self.override_flags:\n            self.argparser.add_argument(\n                '-u', '--quoting', dest='quoting', type=int, choices=[0, 1, 2, 3],\n                help='Quoting style used in the input CSV file. 0 = Quote Minimal, 1 = Quote All, '\n                     '2 = Quote Non-numeric, 3 = Quote None.')\n        if 'b' not in self.override_flags:\n            self.argparser.add_argument(\n                '-b', '--no-doublequote', dest='doublequote', action='store_false',\n                help='Whether or not double quotes are doubled in the input CSV file.')\n        if 'p' not in self.override_flags:\n            self.argparser.add_argument(\n                '-p', '--escapechar', dest='escapechar',\n                help='Character used to escape the delimiter if --quoting 3 (\"Quote None\") is specified and to escape '\n                     'the QUOTECHAR if --no-doublequote is specified.')\n        if 'z' not in self.override_flags:\n            self.argparser.add_argument(\n                '-z', '--maxfieldsize', dest='field_size_limit', type=int,\n                help='Maximum length of a single field in the input CSV file.')\n        if 'e' not in self.override_flags:\n            self.argparser.add_argument(\n                '-e', '--encoding', dest='encoding', default='utf-8-sig',\n                help='Specify the encoding of the input CSV file.')\n        if 'L' not in self.override_flags:\n            self.argparser.add_argument(\n                '-L', '--locale', dest='locale', default='en_US',\n                help='Specify the locale (en_US) of any formatted numbers.')\n        if 'S' not in self.override_flags:\n            self.argparser.add_argument(\n                '-S', '--skipinitialspace', dest='skipinitialspace', action='store_true',\n                help='Ignore whitespace immediately following the delimiter.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--blanks', dest='blanks', action='store_true',\n                help='Do not convert \"\", \"na\", \"n/a\", \"none\", \"null\", \".\" to NULL.')\n        if 'blanks' not in self.override_flags:\n            self.argparser.add_argument(\n                '--null-value', dest='null_values', nargs='+', default=[],\n                help='Convert this value to NULL. --null-value can be specified multiple times.')\n        if 'date-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--date-format', dest='date_format',\n                help='Specify a strptime date format string like \"%%m/%%d/%%Y\".')\n        if 'datetime-format' not in self.override_flags:\n            self.argparser.add_argument(\n                '--datetime-format', dest='datetime_format',\n                help='Specify a strptime datetime format string like \"%%m/%%d/%%Y %%I:%%M %%p\".')\n        if 'H' not in self.override_flags:\n            self.argparser.add_argument(\n                '-H', '--no-header-row', dest='no_header_row', action='store_true',\n                help='Specify that the input CSV file has no header row. Will create default headers (a,b,c,...).')\n        if 'K' not in self.override_flags:\n            self.argparser.add_argument(\n                '-K', '--skip-lines', dest='skip_lines', type=int, default=0,\n                help='Specify the number of initial lines to skip before the header row (e.g. comments, copyright '\n                     'notices, empty rows).')\n        if 'v' not in self.override_flags:\n            self.argparser.add_argument(\n                '-v', '--verbose', dest='verbose', action='store_true',\n                help='Print detailed tracebacks when errors occur.')\n\n        # Output\n        if 'l' not in self.override_flags:\n            self.argparser.add_argument(\n                '-l', '--linenumbers', dest='line_numbers', action='store_true',\n                help='Insert a column of line numbers at the front of the output. Useful when piping to grep or as a '\n                     'simple primary key.')\n\n        # Input/Output\n        if 'zero' not in self.override_flags:\n            self.argparser.add_argument(\n                '--zero', dest='zero_based', action='store_true',\n                help='When interpreting or displaying column numbers, use zero-based numbering instead of the default '\n                     '1-based numbering.')\n\n        self.argparser.add_argument(\n            '-V', '--version', action='version', version='%(prog)s 1.3.0',\n            help='Display version information and exit.')\n\n    def _open_input_file(self, path, opened=False):\n        \"\"\"\n        Open the input file specified on the command line.\n        \"\"\"\n        if not path or path == '-':\n            # \"UnsupportedOperation: It is not possible to set the encoding or newline of stream after the first read\"\n            if not opened:\n                sys.stdin.reconfigure(encoding=self.args.encoding)\n            f = sys.stdin\n        else:\n            extension = splitext(path)[1]\n\n            if extension == '.gz':\n                func = gzip.open\n            elif extension == '.bz2':\n                func = bz2.open\n            elif extension == \".xz\":\n                func = lzma.open\n            else:\n                func = open\n\n            f = LazyFile(func, path, mode='rt', encoding=self.args.encoding)\n\n        return f\n\n    def _extract_csv_reader_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the input CSV reader(s).\n        \"\"\"\n        kwargs = {}\n\n        if self.args.tabs:\n            kwargs['delimiter'] = '\\t'\n        elif self.args.delimiter:\n            kwargs['delimiter'] = self.args.delimiter\n\n        for arg in ('quotechar', 'quoting', 'doublequote', 'escapechar', 'field_size_limit', 'skipinitialspace'):\n            value = getattr(self.args, arg)\n            if value is not None:\n                kwargs[arg] = value\n\n        if getattr(self.args, 'no_header_row', None):\n            kwargs['header'] = not self.args.no_header_row\n\n        return kwargs\n\n    def _extract_csv_writer_kwargs(self):\n        \"\"\"\n        Extracts those from the command-line arguments those would should be passed through to the output CSV writer.\n        \"\"\"\n        kwargs = {}\n\n        if getattr(self.args, 'line_numbers', None):\n            kwargs['line_numbers'] = True\n\n        return kwargs\n\n    def _install_exception_handler(self):\n        \"\"\"\n        Installs a replacement for sys.excepthook, which handles pretty-printing uncaught exceptions.\n        \"\"\"\n        def handler(t, value, traceback):\n            if self.args.verbose:\n                sys.__excepthook__(t, value, traceback)\n            else:\n                # Special case handling for Unicode errors, which behave very strangely\n                # when cast with unicode()\n                if t == UnicodeDecodeError:\n                    sys.stderr.write('Your file is not \"%s\" encoded. Please specify the correct encoding with the -e '\n                                     'flag or with the PYTHONIOENCODING environment variable. Use the -v flag to see '\n                                     'the complete error.\\n' % self.args.encoding)\n                else:\n                    sys.stderr.write(f'{t.__name__}: {str(value)}\\n')\n\n        sys.excepthook = handler\n\n    def get_column_types(self):\n        if getattr(self.args, 'blanks', None):\n            type_kwargs = {'null_values': []}\n        else:\n            type_kwargs = {'null_values': list(DEFAULT_NULL_VALUES)}\n        for null_value in getattr(self.args, 'null_values', []):\n            type_kwargs['null_values'].append(null_value)\n\n        text_type = agate.Text(**type_kwargs)\n\n        if self.args.no_inference:\n            types = [text_type]\n        else:\n            number_type = agate.Number(locale=self.args.locale, **type_kwargs)\n\n            # See the order in the `agate.TypeTester` class.\n            types = [\n                agate.Boolean(**type_kwargs),\n                agate.TimeDelta(**type_kwargs),\n                agate.Date(date_format=self.args.date_format, **type_kwargs),\n                agate.DateTime(datetime_format=self.args.datetime_format, **type_kwargs),\n                text_type,\n            ]\n\n            # In order to parse dates like \"20010101\".\n            if self.args.date_format or self.args.datetime_format:\n                types.insert(-1, number_type)\n            else:\n                types.insert(1, number_type)\n\n        return agate.TypeTester(types=types)\n\n    def get_column_offset(self):\n        if self.args.zero_based:\n            return 0\n        return 1\n\n    def skip_lines(self):\n        if isinstance(self.args.skip_lines, int):\n            while self.args.skip_lines > 0:\n                self.input_file.readline()\n                self.args.skip_lines -= 1\n        else:\n            raise ValueError('skip_lines argument must be an int')\n\n        return self.input_file\n\n    def get_rows_and_column_names_and_column_ids(self, **kwargs):\n        rows = agate.csv.reader(self.skip_lines(), **kwargs)\n\n        try:\n            next_row = next(rows)\n        except StopIteration:\n            return iter([]), [], []\n\n        if self.args.no_header_row:\n            # Peek at a row to get the number of columns.\n            row = next_row\n            rows = itertools.chain([row], rows)\n            column_names = make_default_headers(len(row))\n        else:\n            column_names = next_row\n\n        column_offset = self.get_column_offset()\n        if kwargs.get('line_numbers'):\n            column_offset -= 1\n\n        column_ids = parse_column_identifiers(\n            self.args.columns,\n            column_names,\n            column_offset,\n            getattr(self.args, 'not_columns', None),\n        )\n\n        return rows, column_names, column_ids\n\n    def print_column_names(self):\n        \"\"\"\n        Pretty-prints the names and indices of all columns to a file-like object (usually sys.stdout).\n        \"\"\"\n        if getattr(self.args, 'no_header_row', None):\n            raise RequiredHeaderError('You cannot use --no-header-row with the -n or --names options.')\n\n        if getattr(self.args, 'zero_based', None):\n            start = 0\n        else:\n            start = 1\n\n        rows = agate.csv.reader(self.skip_lines(), **self.reader_kwargs)\n        column_names = next(rows)\n\n        for i, c in enumerate(column_names, start):\n            self.output_file.write('%3i: %s\\n' % (i, c))\n\n    def additional_input_expected(self):\n        return isatty(sys.stdin) and not self.args.input_path\n\n\ndef isatty(f):\n    try:\n        return f.isatty()\n    except ValueError:  # I/O operation on closed file\n        return False\n\n\ndef default_str_decimal(obj):\n    if isinstance(obj, (datetime.date, datetime.datetime)):\n        return obj.isoformat()\n    if isinstance(obj, decimal.Decimal):\n        return str(obj)\n    raise TypeError(f'{repr(obj)} is not JSON serializable')\n\n\ndef default_float_decimal(obj):\n    if isinstance(obj, decimal.Decimal):\n        return float(obj)\n    return default_str_decimal(obj)\n\n\ndef make_default_headers(n):\n    \"\"\"\n    Make a set of simple, default headers for files that are missing them.\n    \"\"\"\n    return tuple(agate.utils.letter_name(i) for i in range(n))\n\n\ndef match_column_identifier(column_names, c, column_offset=1):\n    \"\"\"\n    Determine what column a single column id (name or index) matches in a series of column names.\n    Note that integer values are *always* treated as positional identifiers. If you happen to have\n    column names which are also integers, you must specify them using a positional index.\n    \"\"\"\n    if isinstance(c, str) and not c.isdigit() and c in column_names:\n        return column_names.index(c)\n\n    try:\n        c = int(c) - column_offset\n    # Fail out if neither a column name nor an integer\n    except ValueError:\n        raise ColumnIdentifierError(\"Column '%s' is invalid. It is neither an integer nor a column name. \"\n                                    \"Column names are: %s\" % (c, repr(column_names)[1:-1]))\n\n    # Fail out if index is 0-based\n    if c < 0:\n        raise ColumnIdentifierError(\"Column %i is invalid. Columns are 1-based.\" % (c + column_offset))\n\n    # Fail out if index is out of range\n    if c >= len(column_names):\n        raise ColumnIdentifierError(\"Column %i is invalid. The last column is '%s' at index %i.\" % (\n            c + column_offset, column_names[-1], len(column_names) - 1 + column_offset))\n\n    return c\n\n\ndef parse_column_identifiers(ids, column_names, column_offset=1, excluded_columns=None):\n    \"\"\"\n    Parse a comma-separated list of column indices AND/OR names into a list of integer indices.\n    Ranges of integers can be specified with two integers separated by a '-' or ':' character.\n    Ranges of non-integers (e.g. column names) are not supported.\n    Note: Column indices are 1-based.\n    \"\"\"\n    if not column_names:\n        return []\n\n    if not ids and not excluded_columns:\n        return range(len(column_names))\n\n    if ids:\n        columns = []\n\n        for c in ids.split(','):\n            try:\n                columns.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names) + 1\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    columns.append(match_column_identifier(column_names, x, column_offset))\n    else:\n        columns = range(len(column_names))\n\n    excludes = []\n\n    if excluded_columns:\n        for c in excluded_columns.split(','):\n            try:\n                excludes.append(match_column_identifier(column_names, c, column_offset))\n            except ColumnIdentifierError:\n                if ':' in c:\n                    a, b = c.split(':', 1)\n                elif '-' in c:\n                    a, b = c.split('-', 1)\n                else:\n                    raise\n\n                try:\n                    a = int(a) if a else 1\n                    b = int(b) + 1 if b else len(column_names)\n                except ValueError:\n                    raise ColumnIdentifierError(\n                        \"Invalid range %s. Ranges must be two integers separated by a - or : character.\")\n\n                for x in range(a, b):\n                    excludes.append(match_column_identifier(column_names, x, column_offset))\n\n    return [c for c in columns if c not in excludes]\n", "test_list": ["def test_before_after_insert(self):\n    self.get_output(['--db', 'sqlite:///' + self.db_file, '--insert', 'examples/dummy.csv', '--before-insert', 'SELECT 1; CREATE TABLE foobar (date DATE)', '--after-insert', 'INSERT INTO dummy VALUES (0, 5, 6)'])\n    output_file = io.StringIO()\n    utility = SQL2CSV(['--db', 'sqlite:///' + self.db_file, '--query', 'SELECT * FROM foobar'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertEqual(output, 'date\\n')\n    output_file = io.StringIO()\n    utility = SQL2CSV(['--db', 'sqlite:///' + self.db_file, '--query', 'SELECT * FROM dummy'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertEqual(output, 'a,b,c\\n1,2.0,3.0\\n0,5.0,6.0\\n')", "def test_duplicate_keys(self):\n    output_file = io.StringIO()\n    utility = CSVJSON(['-k', 'a', 'examples/dummy3.csv'], output_file)\n    self.assertRaisesRegex(ValueError, 'Value True is not unique in the key column.', utility.run)\n    output_file.close()"], "requirements": {"Input-Output Conditions": {"requirement": "The 'run' function should ensure that the input file is opened correctly when the 'f' flag is not present and that the file is closed after processing. Additionally, it should verify that the input file is of a valid CSV format.", "unit_test": ["def test_input_file_handling(self):\n    utility = CSVKitUtility(['examples/valid.csv'])\n    utility.run()\n    self.assertTrue(utility.input_file.closed)\n    with self.assertRaises(ValueError):\n        utility = CSVKitUtility(['examples/invalid.csv'])\n        utility.run()"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_input_file_handling"}, "Exception Handling": {"requirement": "The 'run' function should handle exceptions gracefully, providing meaningful error messages when the input file cannot be opened or processed.", "unit_test": ["def test_exception_handling(self):\n    utility = CSVKitUtility(['non_existent_file.csv'])\n    with self.assertRaises(FileNotFoundError):\n        utility.run()"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_exception_handling"}, "Edge Case Handling": {"requirement": "The 'run' function should handle edge cases such as empty input files or files with only headers but no data rows.", "unit_test": ["def test_edge_case_handling(self):\n    utility = CSVKitUtility(['examples/empty.csv'])\n    utility.run()\n    output_file = io.StringIO()\n    utility = CSVKitUtility(['examples/only_headers.csv'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertEqual(output, 'a,b,c\\n')"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'run' function to support additional file formats such as JSON and XML, ensuring compatibility with the existing CSV processing logic.", "unit_test": ["def test_functionality_extension(self):\n    utility = CSVKitUtility(['examples/data.json'])\n    utility.run()\n    output_file = io.StringIO()\n    utility = CSVKitUtility(['examples/data.xml'], output_file)\n    utility.run()\n    output = output_file.getvalue()\n    output_file.close()\n    self.assertIn('<data>', output)"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'run' function and related methods have comprehensive docstrings and type annotations for all parameters and return types.", "unit_test": ["def test_annotation_coverage(self):\n    self.assertTrue(hasattr(CSVKitUtility.run, '__annotations__'))\n    self.assertIn('self', CSVKitUtility.run.__annotations__)\n    self.assertIn('return', CSVKitUtility.run.__annotations__)"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'run' function should maintain a cyclomatic complexity of 10 or less to ensure readability and maintainability.", "unit_test": ["def test_code_complexity(self):\n    from radon.complexity import cc_visit\n    with open('csvkit/cli.py') as f:\n        code = f.read()\n    complexity = cc_visit(code)\n    run_complexity = next((c for c in complexity if c.name == 'run'), None)\n    self.assertIsNotNone(run_complexity)\n    self.assertLessEqual(run_complexity.complexity, 10)"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_code_complexity"}, "Code Standard": {"requirement": "The 'run' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_code_standard(self):\n    import subprocess\n    result = subprocess.run(['flake8', 'csvkit/cli.py'], capture_output=True, text=True)\n    self.assertEqual(result.returncode, 0, msg=result.stdout)"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'run' function should utilize the necessary context, such as 'CSVKitUtility.args' and 'CSVKitUtility.input_file', to manage input and output operations.", "unit_test": ["def test_context_usage(self):\n    utility = CSVKitUtility(['examples/valid.csv'])\n    utility.run()\n    self.assertIsNotNone(utility.args)\n    self.assertIsNotNone(utility.input_file)"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'run' function should correctly use the context, ensuring that 'CSVKitUtility.args' is parsed correctly and 'CSVKitUtility.input_file' is opened and closed as expected.", "unit_test": ["def test_context_usage_correctness(self):\n    utility = CSVKitUtility(['examples/valid.csv'])\n    utility.run()\n    self.assertEqual(utility.args.input_path, 'examples/valid.csv')\n    self.assertTrue(utility.input_file.closed)"], "test": "tests/test_utilities/test_csvjson.py::TestCSVJSON::test_context_usage_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "type": "method", "project_path": "System/prometheus-client", "completion_path": "System/prometheus-client/prometheus_client/mmap_dict.py", "signature_position": [127, 127], "body_position": [128, 131], "dependency": {"intra_class": ["prometheus_client.mmap_dict.MmapedDict._init_value", "prometheus_client.mmap_dict.MmapedDict._m", "prometheus_client.mmap_dict.MmapedDict._positions"], "intra_file": ["prometheus_client.mmap_dict._pack_two_doubles"], "cross_file": []}, "requirement": {"Functionality": "This function writes a value to a key in the MmapedDict instance. If the key does not exist in the instance, it initializes the key and then writes the value and timestamp to the corresponding position in the memory-mapped file.", "Arguments": ":param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to write the value to.\n:param value: The value to be written.\n:param timestamp: The timestamp associated with the value.\n:return: No return values."}, "tests": ["tests/test_multiprocess.py::TestMmapedDict::test_corruption_detected", "tests/test_multiprocess.py::TestMmapedDict::test_multi_expansion", "tests/test_multiprocess.py::TestMmapedDict::test_expansion", "tests/test_multiprocess.py::TestMmapedDict::test_process_restart"], "indent": 4, "domain": "System", "code": "    def write_value(self, key, value, timestamp):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "context": "import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n\n###The function: write_value###\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "prompt": "Please write a python function called 'write_value' base the context. This function writes a value to a key in the MmapedDict instance. If the key does not exist in the instance, it initializes the key and then writes the value and timestamp to the corresponding position in the memory-mapped file.:param self: MmapedDict. An instance of the MmapedDict class.\n:param key: The key to write the value to.\n:param value: The value to be written.\n:param timestamp: The timestamp associated with the value.\n:return: No return values..\n        The context you need to refer to is as follows: import json\nimport mmap\nimport os\nimport struct\nfrom typing import List\n\n_INITIAL_MMAP_SIZE = 1 << 16\n_pack_integer_func = struct.Struct(b'i').pack\n_pack_two_doubles_func = struct.Struct(b'dd').pack\n_unpack_integer = struct.Struct(b'i').unpack_from\n_unpack_two_doubles = struct.Struct(b'dd').unpack_from\n\n\n# struct.pack_into has atomicity issues because it will temporarily write 0 into\n# the mmap, resulting in false reads to 0 when experiencing a lot of writes.\n# Using direct assignment solves this issue.\n\n\ndef _pack_two_doubles(data, pos, value, timestamp):\n    data[pos:pos + 16] = _pack_two_doubles_func(value, timestamp)\n\n\ndef _pack_integer(data, pos, value):\n    data[pos:pos + 4] = _pack_integer_func(value)\n\n\ndef _read_all_values(data, used=0):\n    \"\"\"Yield (key, value, timestamp, pos). No locking is performed.\"\"\"\n\n    if used <= 0:\n        # If not valid `used` value is passed in, read it from the file.\n        used = _unpack_integer(data, 0)[0]\n\n    pos = 8\n\n    while pos < used:\n        encoded_len = _unpack_integer(data, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = data[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(data, pos)\n        yield encoded_key.decode('utf-8'), value, timestamp, pos\n        pos += 16\n\n\nclass MmapedDict:\n    \"\"\"A dict of doubles, backed by an mmapped file.\n\n    The file starts with a 4 byte int, indicating how much of it is used.\n    Then 4 bytes of padding.\n    There's then a number of entries, consisting of a 4 byte int which is the\n    size of the next field, a utf-8 encoded string key, padding to a 8 byte\n    alignment, and then a 8 byte float which is the value and a 8 byte float\n    which is a UNIX timestamp in seconds.\n\n    Not thread safe.\n    \"\"\"\n\n    def __init__(self, filename, read_mode=False):\n        self._f = open(filename, 'rb' if read_mode else 'a+b')\n        self._fname = filename\n        capacity = os.fstat(self._f.fileno()).st_size\n        if capacity == 0:\n            self._f.truncate(_INITIAL_MMAP_SIZE)\n            capacity = _INITIAL_MMAP_SIZE\n        self._capacity = capacity\n        self._m = mmap.mmap(self._f.fileno(), self._capacity,\n                            access=mmap.ACCESS_READ if read_mode else mmap.ACCESS_WRITE)\n\n        self._positions = {}\n        self._used = _unpack_integer(self._m, 0)[0]\n        if self._used == 0:\n            self._used = 8\n            _pack_integer(self._m, 0, self._used)\n        else:\n            if not read_mode:\n                for key, _, _, pos in self._read_all_values():\n                    self._positions[key] = pos\n\n    @staticmethod\n    def read_all_values_from_file(filename):\n        with open(filename, 'rb') as infp:\n            # Read the first block of data, including the first 4 bytes which tell us\n            # how much of the file (which is preallocated to _INITIAL_MMAP_SIZE bytes) is occupied.\n            data = infp.read(mmap.PAGESIZE)\n            used = _unpack_integer(data, 0)[0]\n            if used > len(data):  # Then read in the rest, if needed.\n                data += infp.read(used - len(data))\n        return _read_all_values(data, used)\n\n    def _init_value(self, key):\n        \"\"\"Initialize a value. Lock must be held by caller.\"\"\"\n        encoded = key.encode('utf-8')\n        # Pad to be 8-byte aligned.\n        padded = encoded + (b' ' * (8 - (len(encoded) + 4) % 8))\n        value = struct.pack(f'i{len(padded)}sdd'.encode(), len(encoded), padded, 0.0, 0.0)\n        while self._used + len(value) > self._capacity:\n            self._capacity *= 2\n            self._f.truncate(self._capacity)\n            self._m = mmap.mmap(self._f.fileno(), self._capacity)\n        self._m[self._used:self._used + len(value)] = value\n\n        # Update how much space we've used.\n        self._used += len(value)\n        _pack_integer(self._m, 0, self._used)\n        self._positions[key] = self._used - 16\n\n    def _read_all_values(self):\n        \"\"\"Yield (key, value, pos). No locking is performed.\"\"\"\n        return _read_all_values(data=self._m, used=self._used)\n\n    def read_all_values(self):\n        \"\"\"Yield (key, value, timestamp). No locking is performed.\"\"\"\n        for k, v, ts, _ in self._read_all_values():\n            yield k, v, ts\n\n    def read_value(self, key):\n        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)\n\n###The function: write_value###\n    def close(self):\n        if self._f:\n            self._m.close()\n            self._m = None\n            self._f.close()\n            self._f = None\n\n\ndef mmap_key(metric_name: str, name: str, labelnames: List[str], labelvalues: List[str], help_text: str) -> str:\n    \"\"\"Format a key for use in the mmap file.\"\"\"\n    # ensure labels are in consistent order for identity\n    labels = dict(zip(labelnames, labelvalues))\n    return json.dumps([metric_name, name, labels, help_text], sort_keys=True)\n", "test_list": ["def test_corruption_detected(self):\n    self.d.write_value('abc', 42.0, 987.0)\n    self.d._m[8:16] = b'somejunk'\n    with self.assertRaises(RuntimeError):\n        list(self.d.read_all_values())", "def test_multi_expansion(self):\n    key = 'a' * mmap_dict._INITIAL_MMAP_SIZE * 4\n    self.d.write_value('abc', 42.0, 987.0)\n    self.d.write_value(key, 123.0, 876.0)\n    self.d.write_value('def', 17.0, 765.0)\n    self.assertEqual([('abc', 42.0, 987.0), (key, 123.0, 876.0), ('def', 17.0, 765.0)], list(self.d.read_all_values()))", "def test_expansion(self):\n    key = 'a' * mmap_dict._INITIAL_MMAP_SIZE\n    self.d.write_value(key, 123.0, 987.0)\n    self.assertEqual([(key, 123.0, 987.0)], list(self.d.read_all_values()))", "def test_process_restart(self):\n    self.d.write_value('abc', 123.0, 987.0)\n    self.d.close()\n    self.d = mmap_dict.MmapedDict(self.tempfile)\n    self.assertEqual((123, 987.0), self.d.read_value('abc'))\n    self.assertEqual([('abc', 123.0, 987.0)], list(self.d.read_all_values()))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'write_value' function should correctly write a float value and a timestamp to the specified key in the MmapedDict instance, and the key should be a string.", "unit_test": ["def test_write_value_correctness(self):\n    self.d.write_value('test_key', 100.0, 1609459200.0)\n    value, timestamp = self.d.read_value('test_key')\n    self.assertEqual(value, 100.0)\n    self.assertEqual(timestamp, 1609459200.0)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_correctness"}, "Exception Handling": {"requirement": "The 'write_value' function should raise a TypeError if the key is not a string or if the value or timestamp is not a float.", "unit_test": ["def test_write_value_type_error(self):\n    with self.assertRaises(TypeError):\n        self.d.write_value(123, 100.0, 1609459200.0)\n    with self.assertRaises(TypeError):\n        self.d.write_value('test_key', '100', 1609459200.0)\n    with self.assertRaises(TypeError):\n        self.d.write_value('test_key', 100.0, '1609459200')"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_type_error"}, "Edge Case Handling": {"requirement": "The 'write_value' function should handle writing to a key with an empty string and ensure it does not corrupt the data.", "unit_test": ["def test_write_value_empty_key(self):\n    self.d.write_value('', 50.0, 1609459200.0)\n    value, timestamp = self.d.read_value('')\n    self.assertEqual(value, 50.0)\n    self.assertEqual(timestamp, 1609459200.0)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_empty_key"}, "Functionality Extension": {"requirement": "Extend the 'write_value' function to return a boolean indicating whether the key was newly initialized.", "unit_test": ["def test_write_value_initialization_flag(self):\n    is_new = self.d.write_value('new_key', 200.0, 1609459200.0)\n    self.assertTrue(is_new)\n    is_new = self.d.write_value('new_key', 300.0, 1609459300.0)\n    self.assertFalse(is_new)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_initialization_flag"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'write_value' function are properly annotated with type hints.", "unit_test": ["def test_write_value_annotations(self):\n    annotations = self.d.write_value.__annotations__\n    self.assertEqual(annotations['key'], str)\n    self.assertEqual(annotations['value'], float)\n    self.assertEqual(annotations['timestamp'], float)\n    self.assertEqual(annotations['return'], bool)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_annotations"}, "Code Complexity": {"requirement": "The 'write_value' function should maintain a cyclomatic complexity of no more than 5.", "unit_test": ["def test_write_value_complexity(self):\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(self.d.write_value)\n    complexity = cc_visit(source_code)\n    self.assertTrue(all(c.cyclomatic_complexity <= 5 for c in complexity))"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_complexity"}, "Code Standard": {"requirement": "The 'write_value' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_write_value_code_style(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['mmap_dict.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_code_style"}, "Context Usage Verification": {"requirement": "The 'write_value' function should utilize the '_m', and '_positions' attributes of the MmapedDict class.", "unit_test": ["def test_write_value_context_usage(self):\n    self.d.write_value('context_key', 150.0, 1609459200.0)\n    self.assertIn('context_key', self.d._positions)\n    pos = self.d._positions['context_key']\n    value, timestamp = _unpack_two_doubles(self.d._m, pos)\n    self.assertEqual(value, 150.0)\n    self.assertEqual(timestamp, 1609459200.0)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_context_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'write_value' function correctly updates the '_positions' dictionary and writes the value and timestamp to the correct position in the '_m' memory map.", "unit_test": ["def test_write_value_correct_position_update(self):\n    self.d.write_value('correct_key', 250.0, 1609459200.0)\n    pos = self.d._positions['correct_key']\n    value, timestamp = _unpack_two_doubles(self.d._m, pos)\n    self.assertEqual(value, 250.0)\n    self.assertEqual(timestamp, 1609459200.0)"], "test": "tests/test_multiprocess.py::TestMmapedDict::test_write_value_correct_position_update"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "mopidy.config.types.LogLevel.serialize", "type": "method", "project_path": "Multimedia/Mopidy", "completion_path": "Multimedia/Mopidy/mopidy/config/types.py", "signature_position": [372, 372], "body_position": [373, 376], "dependency": {"intra_class": ["mopidy.config.types.LogLevel.levels"], "intra_file": ["mopidy.config.types.encode"], "cross_file": []}, "requirement": {"Functionality": "Serialize a value based on the LogLevel instance. It looks up the value in the levels dictionary and returns the corresponding key. If the value is not found, it returns an empty string.", "Arguments": ":param self: LogLevel. An instance of the LogLevel class.\n:param value: The value to be serialized.\n:param display: Bool. Whether to display the serialized value. Defaults to False.\n:return: String. The serialized value or an empty string if the value is not found."}, "tests": ["tests/config/test_types.py::TestLogLevel::test_serialize", "tests/config/test_types.py::TestLogLevel::test_serialize_ignores_unknown_level"], "indent": 4, "domain": "Multimedia", "code": "    def serialize(self, value, display=False):\n        lookup = {v: k for k, v in self.levels.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"\n", "context": "import logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Float(ConfigValue):\n    \"\"\"Float value.\"\"\"\n\n    def __init__(self, minimum=None, maximum=None, optional=False):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Boolean(ConfigValue):\n    \"\"\"Boolean value.\n\n    Accepts ``1``, ``yes``, ``true``, and ``on`` with any casing as\n    :class:`True`.\n\n    Accepts ``0``, ``no``, ``false``, and ``off`` with any casing as\n    :class:`False`.\n    \"\"\"\n\n    true_values = (\"1\", \"yes\", \"true\", \"on\")\n    false_values = (\"0\", \"no\", \"false\", \"off\")\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n\n    def serialize(self, value, display=False):\n        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n\n\nclass Pair(ConfigValue):\n    \"\"\"Pair value\n\n    The value is expected to be a pair of elements, separated by a specified delimiter.\n    Values can optionally not be a pair, in which case the whole input is provided for\n    both sides of the value.\n    \"\"\"\n\n    def __init__(\n        self, optional=False, optional_pair=False, separator=\"|\", subtypes=None\n    ):\n        self._required = not optional\n        self._optional_pair = optional_pair\n        self._separator = separator\n        if subtypes:\n            self._subtypes = subtypes\n        else:\n            self._subtypes = (String(), String())\n\n    def deserialize(self, value):\n        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )\n\n    def serialize(self, value, display=False):\n        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n\n\nclass List(ConfigValue):\n    \"\"\"List value.\n\n    Supports elements split by commas or newlines. Newlines take precedence and\n    empty list items will be filtered out.\n\n    Enforcing unique entries in the list will result in a set data structure\n    being used. This does not preserve ordering, which could result in the\n    serialized output being unstable.\n    \"\"\"\n\n    def __init__(self, optional=False, unique=False, subtype=None):\n        self._required = not optional\n        self._unique = unique\n        self._subtype = subtype if subtype else String()\n\n    def deserialize(self, value):\n        value = decode(value)\n        if \"\\n\" in value:\n            values = re.split(r\"\\s*\\n\\s*\", value)\n        else:\n            values = re.split(r\"\\s*,\\s*\", value)\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        values_iter = (\n            subtype.deserialize(v.strip()) for v in values if v.strip()\n        )\n        if self._unique:\n            values = frozenset(values_iter)\n        else:\n            values = tuple(values_iter)\n\n        validators.validate_required(values, self._required)\n        return values\n\n    def serialize(self, value, display=False):\n        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n\n\nclass LogColor(ConfigValue):\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n\n    def serialize(self, value, display=False):\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n\n\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n###The function: serialize###\n\nclass Hostname(ConfigValue):\n    \"\"\"Network hostname value.\"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value, display=False):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n\n\nclass Port(Integer):\n    \"\"\"Network port value.\n\n    Expects integer in the range 0-65535, zero tells the kernel to simply\n    allocate a port for us.\n    \"\"\"\n\n    def __init__(self, choices=None, optional=False):\n        super().__init__(\n            minimum=0, maximum=2**16 - 1, choices=choices, optional=optional\n        )\n\n\n# Keep this for backwards compatibility\nclass _ExpandedPath(_TransformedValue):\n    pass\n\n\nclass Path(ConfigValue):\n    \"\"\"File system path.\n\n    The following expansions of the path will be done:\n\n    - ``~`` to the current user's home directory\n    - ``$XDG_CACHE_DIR`` according to the XDG spec\n    - ``$XDG_CONFIG_DIR`` according to the XDG spec\n    - ``$XDG_DATA_DIR`` according to the XDG spec\n    - ``$XDG_MUSIC_DIR`` according to the XDG spec\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        expanded = path.expand_path(value)\n        validators.validate_required(value, self._required)\n        validators.validate_required(expanded, self._required)\n        if not value or expanded is None:\n            return None\n        return _ExpandedPath(value, expanded)\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _ExpandedPath):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return value\n", "prompt": "Please write a python function called 'serialize' base the context. Serialize a value based on the LogLevel instance. It looks up the value in the levels dictionary and returns the corresponding key. If the value is not found, it returns an empty string.:param self: LogLevel. An instance of the LogLevel class.\n:param value: The value to be serialized.\n:param display: Bool. Whether to display the serialized value. Defaults to False.\n:return: String. The serialized value or an empty string if the value is not found..\n        The context you need to refer to is as follows: import logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Float(ConfigValue):\n    \"\"\"Float value.\"\"\"\n\n    def __init__(self, minimum=None, maximum=None, optional=False):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Boolean(ConfigValue):\n    \"\"\"Boolean value.\n\n    Accepts ``1``, ``yes``, ``true``, and ``on`` with any casing as\n    :class:`True`.\n\n    Accepts ``0``, ``no``, ``false``, and ``off`` with any casing as\n    :class:`False`.\n    \"\"\"\n\n    true_values = (\"1\", \"yes\", \"true\", \"on\")\n    false_values = (\"0\", \"no\", \"false\", \"off\")\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n\n    def serialize(self, value, display=False):\n        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n\n\nclass Pair(ConfigValue):\n    \"\"\"Pair value\n\n    The value is expected to be a pair of elements, separated by a specified delimiter.\n    Values can optionally not be a pair, in which case the whole input is provided for\n    both sides of the value.\n    \"\"\"\n\n    def __init__(\n        self, optional=False, optional_pair=False, separator=\"|\", subtypes=None\n    ):\n        self._required = not optional\n        self._optional_pair = optional_pair\n        self._separator = separator\n        if subtypes:\n            self._subtypes = subtypes\n        else:\n            self._subtypes = (String(), String())\n\n    def deserialize(self, value):\n        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )\n\n    def serialize(self, value, display=False):\n        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n\n\nclass List(ConfigValue):\n    \"\"\"List value.\n\n    Supports elements split by commas or newlines. Newlines take precedence and\n    empty list items will be filtered out.\n\n    Enforcing unique entries in the list will result in a set data structure\n    being used. This does not preserve ordering, which could result in the\n    serialized output being unstable.\n    \"\"\"\n\n    def __init__(self, optional=False, unique=False, subtype=None):\n        self._required = not optional\n        self._unique = unique\n        self._subtype = subtype if subtype else String()\n\n    def deserialize(self, value):\n        value = decode(value)\n        if \"\\n\" in value:\n            values = re.split(r\"\\s*\\n\\s*\", value)\n        else:\n            values = re.split(r\"\\s*,\\s*\", value)\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        values_iter = (\n            subtype.deserialize(v.strip()) for v in values if v.strip()\n        )\n        if self._unique:\n            values = frozenset(values_iter)\n        else:\n            values = tuple(values_iter)\n\n        validators.validate_required(values, self._required)\n        return values\n\n    def serialize(self, value, display=False):\n        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n\n\nclass LogColor(ConfigValue):\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n\n    def serialize(self, value, display=False):\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n\n\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n###The function: serialize###\n\nclass Hostname(ConfigValue):\n    \"\"\"Network hostname value.\"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value, display=False):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n\n\nclass Port(Integer):\n    \"\"\"Network port value.\n\n    Expects integer in the range 0-65535, zero tells the kernel to simply\n    allocate a port for us.\n    \"\"\"\n\n    def __init__(self, choices=None, optional=False):\n        super().__init__(\n            minimum=0, maximum=2**16 - 1, choices=choices, optional=optional\n        )\n\n\n# Keep this for backwards compatibility\nclass _ExpandedPath(_TransformedValue):\n    pass\n\n\nclass Path(ConfigValue):\n    \"\"\"File system path.\n\n    The following expansions of the path will be done:\n\n    - ``~`` to the current user's home directory\n    - ``$XDG_CACHE_DIR`` according to the XDG spec\n    - ``$XDG_CONFIG_DIR`` according to the XDG spec\n    - ``$XDG_DATA_DIR`` according to the XDG spec\n    - ``$XDG_MUSIC_DIR`` according to the XDG spec\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        expanded = path.expand_path(value)\n        validators.validate_required(value, self._required)\n        validators.validate_required(expanded, self._required)\n        if not value or expanded is None:\n            return None\n        return _ExpandedPath(value, expanded)\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _ExpandedPath):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return value\n", "test_list": ["def test_serialize(self):\n    cv = types.LogLevel()\n    for name, level in self.levels.items():\n        assert cv.serialize(level) == name", "def test_serialize_ignores_unknown_level(self):\n    cv = types.LogLevel()\n    assert cv.serialize(1337) == ''"], "requirements": {"Input-Output Conditions": {"requirement": "The 'serialize' function should return the correct string representation of a log level when given a valid integer log level value.", "unit_test": ["def test_serialize_valid_input(self):\n    cv = types.LogLevel()\n    assert cv.serialize(logging.INFO) == 'info'\n    assert cv.serialize(logging.DEBUG) == 'debug'"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_valid_input"}, "Exception Handling": {"requirement": "The 'serialize' function should handle non-integer inputs gracefully by returning an empty string.", "unit_test": ["def test_serialize_non_integer_input(self):\n    cv = types.LogLevel()\n    assert cv.serialize('not_an_integer') == ''\n    assert cv.serialize(None) == ''"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_non_integer_input"}, "Edge Case Handling": {"requirement": "The 'serialize' function should return an empty string for integer values that are not defined in the levels dictionary.", "unit_test": ["def test_serialize_edge_case_unknown_level(self):\n    cv = types.LogLevel()\n    assert cv.serialize(-1) == ''\n    assert cv.serialize(9999) == ''"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_edge_case_unknown_level"}, "Functionality Extension": {"requirement": "Extend the 'serialize' function to accept log level names as input and return the corresponding integer value.", "unit_test": ["def test_serialize_with_name_input(self):\n    cv = types.LogLevel()\n    assert cv.serialize('info') == logging.INFO\n    assert cv.serialize('debug') == logging.DEBUG"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_with_name_input"}, "Annotation Coverage": {"requirement": "Ensure that the 'serialize' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_serialize_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(types.LogLevel.serialize)\n    assert hints['value'] == int\n    assert hints['display'] == bool\n    assert hints['return'] == str"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_annotations"}, "Code Complexity": {"requirement": "The 'serialize' function should maintain a cyclomatic complexity of 3, indicating a simple function structure.", "unit_test": ["def test_serialize_cyclomatic_complexity(self):\n    import radon.complexity as cc\n    complexity = cc.cc_visit_func(types.LogLevel.serialize)\n    assert complexity[0].complexity == 1"], "test": "tests/config/test_types.py::TestLogLevel::test_code_complexity"}, "Code Standard": {"requirement": "The 'serialize' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_serialize_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/your/module.py'])\n    assert result.total_errors == 0"], "test": "tests/config/test_types.py::TestLogLevel::test_code_style"}, "Context Usage Verification": {"requirement": "The 'serialize' function should utilize the 'levels' dictionary from the 'LogLevel' class context.", "unit_test": ["def test_serialize_uses_levels_dict(self):\n    cv = types.LogLevel()\n    assert 'levels' in cv.__class__.__dict__"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_log_level"}, "Context Usage Correctness Verification": {"requirement": "The 'serialize' function should correctly map integer log levels to their string representations using the 'levels' dictionary.", "unit_test": ["def test_serialize_correct_mapping(self):\n    cv = types.LogLevel()\n    for name, level in cv.levels.items():\n        assert cv.serialize(level) == name"], "test": "tests/config/test_types.py::TestLogLevel::test_serialize_maps_level_to_string"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "playhouse.kv.KeyValue.pop", "type": "method", "project_path": "Software-Development/peewee", "completion_path": "Software-Development/peewee/playhouse/kv.py", "signature_position": [163, 163], "body_position": [164, 173], "dependency": {"intra_class": ["playhouse.kv.KeyValue._database"], "intra_file": ["playhouse.kv.Sentinel"], "cross_file": []}, "requirement": {"Functionality": "This function removes the specified key from the KeyValue instance and returns the corresponding value. If the key is not found and no default value is provided, an exception is raised. The function also ensures that the operation is atomic by using a database transaction.", "Arguments": ":param self: KeyValue. An instance of the KeyValue class.\n:param key: The key to be removed from the instance.\n:param default: Optional. The value to be returned if the key is not found. Defaults to Sentinel.\n:return: The value corresponding to the key, or the default value if provided."}, "tests": ["tests/kv.py::TestKeyValue::test_basic_apis"], "indent": 4, "domain": "Software-Development", "code": "    def pop(self, key, default=Sentinel):\n        with self._database.atomic():\n            try:\n                result = self[key]\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                return default\n            del self[key]\n\n        return result\n", "context": "import operator\n\nfrom peewee import *\nfrom peewee import sqlite3\nfrom peewee import Expression\nfrom playhouse.fields import PickleField\ntry:\n    from playhouse.sqlite_ext import CSqliteExtDatabase as SqliteExtDatabase\nexcept ImportError:\n    from playhouse.sqlite_ext import SqliteExtDatabase\n\n\nSentinel = type('Sentinel', (object,), {})\n\n\nclass KeyValue(object):\n    \"\"\"\n    Persistent dictionary.\n\n    :param Field key_field: field to use for key. Defaults to CharField.\n    :param Field value_field: field to use for value. Defaults to PickleField.\n    :param bool ordered: data should be returned in key-sorted order.\n    :param Database database: database where key/value data is stored.\n    :param str table_name: table name for data.\n    \"\"\"\n    def __init__(self, key_field=None, value_field=None, ordered=False,\n                 database=None, table_name='keyvalue'):\n        if key_field is None:\n            key_field = CharField(max_length=255, primary_key=True)\n        if not key_field.primary_key:\n            raise ValueError('key_field must have primary_key=True.')\n\n        if value_field is None:\n            value_field = PickleField()\n\n        self._key_field = key_field\n        self._value_field = value_field\n        self._ordered = ordered\n        self._database = database or SqliteExtDatabase(':memory:')\n        self._table_name = table_name\n        support_on_conflict = (isinstance(self._database, PostgresqlDatabase) or\n                              (isinstance(self._database, SqliteDatabase) and\n                               self._database.server_version >= (3, 24)))\n        if support_on_conflict:\n            self.upsert = self._postgres_upsert\n            self.update = self._postgres_update\n        else:\n            self.upsert = self._upsert\n            self.update = self._update\n\n        self.model = self.create_model()\n        self.key = self.model.key\n        self.value = self.model.value\n\n        # Ensure table exists.\n        self.model.create_table()\n\n    def create_model(self):\n        class KeyValue(Model):\n            key = self._key_field\n            value = self._value_field\n            class Meta:\n                database = self._database\n                table_name = self._table_name\n        return KeyValue\n\n    def query(self, *select):\n        query = self.model.select(*select).tuples()\n        if self._ordered:\n            query = query.order_by(self.key)\n        return query\n\n    def convert_expression(self, expr):\n        if not isinstance(expr, Expression):\n            return (self.key == expr), True\n        return expr, False\n\n    def __contains__(self, key):\n        expr, _ = self.convert_expression(key)\n        return self.model.select().where(expr).exists()\n\n    def __len__(self):\n        return len(self.model)\n\n    def __getitem__(self, expr):\n        converted, is_single = self.convert_expression(expr)\n        query = self.query(self.value).where(converted)\n        item_getter = operator.itemgetter(0)\n        result = [item_getter(row) for row in query]\n        if len(result) == 0 and is_single:\n            raise KeyError(expr)\n        elif is_single:\n            return result[0]\n        return result\n\n    def _upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict('replace')\n         .execute())\n\n    def _postgres_upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict(conflict_target=[self.key],\n                      preserve=[self.value])\n         .execute())\n\n    def __setitem__(self, expr, value):\n        if isinstance(expr, Expression):\n            self.model.update(value=value).where(expr).execute()\n        else:\n            self.upsert(expr, value)\n\n    def __delitem__(self, expr):\n        converted, _ = self.convert_expression(expr)\n        self.model.delete().where(converted).execute()\n\n    def __iter__(self):\n        return iter(self.query().execute())\n\n    def keys(self):\n        return map(operator.itemgetter(0), self.query(self.key))\n\n    def values(self):\n        return map(operator.itemgetter(0), self.query(self.value))\n\n    def items(self):\n        return iter(self.query().execute())\n\n    def _update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict('replace')\n                .execute())\n\n    def _postgres_update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict(conflict_target=[self.key],\n                             preserve=[self.value])\n                .execute())\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n\n###The function: pop###\n    def clear(self):\n        self.model.delete().execute()\n", "prompt": "Please write a python function called 'pop' base the context. This function removes the specified key from the KeyValue instance and returns the corresponding value. If the key is not found and no default value is provided, an exception is raised. The function also ensures that the operation is atomic by using a database transaction.:param self: KeyValue. An instance of the KeyValue class.\n:param key: The key to be removed from the instance.\n:param default: Optional. The value to be returned if the key is not found. Defaults to Sentinel.\n:return: The value corresponding to the key, or the default value if provided..\n        The context you need to refer to is as follows: import operator\n\nfrom peewee import *\nfrom peewee import sqlite3\nfrom peewee import Expression\nfrom playhouse.fields import PickleField\ntry:\n    from playhouse.sqlite_ext import CSqliteExtDatabase as SqliteExtDatabase\nexcept ImportError:\n    from playhouse.sqlite_ext import SqliteExtDatabase\n\n\nSentinel = type('Sentinel', (object,), {})\n\n\nclass KeyValue(object):\n    \"\"\"\n    Persistent dictionary.\n\n    :param Field key_field: field to use for key. Defaults to CharField.\n    :param Field value_field: field to use for value. Defaults to PickleField.\n    :param bool ordered: data should be returned in key-sorted order.\n    :param Database database: database where key/value data is stored.\n    :param str table_name: table name for data.\n    \"\"\"\n    def __init__(self, key_field=None, value_field=None, ordered=False,\n                 database=None, table_name='keyvalue'):\n        if key_field is None:\n            key_field = CharField(max_length=255, primary_key=True)\n        if not key_field.primary_key:\n            raise ValueError('key_field must have primary_key=True.')\n\n        if value_field is None:\n            value_field = PickleField()\n\n        self._key_field = key_field\n        self._value_field = value_field\n        self._ordered = ordered\n        self._database = database or SqliteExtDatabase(':memory:')\n        self._table_name = table_name\n        support_on_conflict = (isinstance(self._database, PostgresqlDatabase) or\n                              (isinstance(self._database, SqliteDatabase) and\n                               self._database.server_version >= (3, 24)))\n        if support_on_conflict:\n            self.upsert = self._postgres_upsert\n            self.update = self._postgres_update\n        else:\n            self.upsert = self._upsert\n            self.update = self._update\n\n        self.model = self.create_model()\n        self.key = self.model.key\n        self.value = self.model.value\n\n        # Ensure table exists.\n        self.model.create_table()\n\n    def create_model(self):\n        class KeyValue(Model):\n            key = self._key_field\n            value = self._value_field\n            class Meta:\n                database = self._database\n                table_name = self._table_name\n        return KeyValue\n\n    def query(self, *select):\n        query = self.model.select(*select).tuples()\n        if self._ordered:\n            query = query.order_by(self.key)\n        return query\n\n    def convert_expression(self, expr):\n        if not isinstance(expr, Expression):\n            return (self.key == expr), True\n        return expr, False\n\n    def __contains__(self, key):\n        expr, _ = self.convert_expression(key)\n        return self.model.select().where(expr).exists()\n\n    def __len__(self):\n        return len(self.model)\n\n    def __getitem__(self, expr):\n        converted, is_single = self.convert_expression(expr)\n        query = self.query(self.value).where(converted)\n        item_getter = operator.itemgetter(0)\n        result = [item_getter(row) for row in query]\n        if len(result) == 0 and is_single:\n            raise KeyError(expr)\n        elif is_single:\n            return result[0]\n        return result\n\n    def _upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict('replace')\n         .execute())\n\n    def _postgres_upsert(self, key, value):\n        (self.model\n         .insert(key=key, value=value)\n         .on_conflict(conflict_target=[self.key],\n                      preserve=[self.value])\n         .execute())\n\n    def __setitem__(self, expr, value):\n        if isinstance(expr, Expression):\n            self.model.update(value=value).where(expr).execute()\n        else:\n            self.upsert(expr, value)\n\n    def __delitem__(self, expr):\n        converted, _ = self.convert_expression(expr)\n        self.model.delete().where(converted).execute()\n\n    def __iter__(self):\n        return iter(self.query().execute())\n\n    def keys(self):\n        return map(operator.itemgetter(0), self.query(self.key))\n\n    def values(self):\n        return map(operator.itemgetter(0), self.query(self.value))\n\n    def items(self):\n        return iter(self.query().execute())\n\n    def _update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict('replace')\n                .execute())\n\n    def _postgres_update(self, __data=None, **mapping):\n        if __data is not None:\n            mapping.update(__data)\n        return (self.model\n                .insert_many(list(mapping.items()),\n                             fields=[self.key, self.value])\n                .on_conflict(conflict_target=[self.key],\n                             preserve=[self.value])\n                .execute())\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n\n###The function: pop###\n    def clear(self):\n        self.model.delete().execute()\n", "test_list": ["def test_basic_apis(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    KV['k2'] = [0, 1, 2]\n    self.assertEqual(KV['k1'], 'v1')\n    self.assertEqual(KV['k2'], [0, 1, 2])\n    self.assertRaises(KeyError, lambda: KV['k3'])\n    self.assertTrue((KV.key < 'k2') in KV)\n    self.assertFalse((KV.key > 'k2') in KV)\n    del KV['k1']\n    KV['k3'] = 'v3'\n    self.assertFalse('k1' in KV)\n    self.assertTrue('k3' in KV)\n    self.assertEqual(sorted(KV.keys()), ['k2', 'k3'])\n    self.assertEqual(len(KV), 2)\n    data = dict(KV)\n    self.assertEqual(data, {'k2': [0, 1, 2], 'k3': 'v3'})\n    self.assertEqual(dict(KV), dict(KV.items()))\n    self.assertEqual(KV.pop('k2'), [0, 1, 2])\n    self.assertRaises(KeyError, lambda: KV['k2'])\n    self.assertRaises(KeyError, KV.pop, 'k2')\n    self.assertEqual(KV.get('k3'), 'v3')\n    self.assertTrue(KV.get('kx') is None)\n    self.assertEqual(KV.get('kx', 'vx'), 'vx')\n    self.assertTrue(KV.get('k4') is None)\n    self.assertEqual(KV.setdefault('k4', 'v4'), 'v4')\n    self.assertEqual(KV.get('k4'), 'v4')\n    self.assertEqual(KV.get('k4', 'v5'), 'v4')\n    KV.clear()\n    self.assertEqual(len(KV), 0)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'pop' function should correctly remove the specified key from the KeyValue instance and return the corresponding value. If a default value is provided and the key is not found, the default value should be returned.", "unit_test": ["def test_pop_with_default(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    self.assertEqual(KV.pop('k1'), 'v1')\n    self.assertEqual(KV.pop('k2', 'default'), 'default')\n    self.assertRaises(KeyError, KV.pop, 'k2')"], "test": "tests/kv.py::TestKeyValue::test_pop_with_default"}, "Exception Handling": {"requirement": "The 'pop' function should raise a KeyError exception with an appropriate error message: 'non_existent_key' if the specified key is not found and no default value is provided.", "unit_test": ["def test_pop_key_error(self):\n    KV = self.create_kv()\n    with self.assertRaises(KeyError) as context:\n        KV.pop('non_existent_key')\n    self.assertEqual(str(context.exception), \"'non_existent_key'\")"], "test": "tests/kv.py::TestKeyValue::test_pop_key_error"}, "Edge Case Handling": {"requirement": "The 'pop' function should handle edge cases such as popping from an empty KeyValue instance and ensure that the operation behaves as expected.", "unit_test": ["def test_pop_from_empty(self):\n    KV = self.create_kv()\n    self.assertRaises(KeyError, KV.pop, 'any_key')\n    self.assertEqual(KV.pop('any_key', 'default'), 'default')"], "test": "tests/kv.py::TestKeyValue::test_pop_from_empty"}, "Functionality Extension": {"requirement": "Extend the 'pop' function to allow popping multiple keys at once, returning a dictionary of key-value pairs for the keys that were successfully removed.", "unit_test": ["def test_pop_multiple_keys(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    KV['k2'] = 'v2'\n    KV['k3'] = 'v3'\n    result = KV.pop(['k1', 'k2'])\n    self.assertEqual(result, {'k1': 'v1', 'k2': 'v2'})\n    self.assertFalse('k1' in KV)\n    self.assertFalse('k2' in KV)\n    self.assertTrue('k3' in KV)"], "test": "tests/kv.py::TestKeyValue::test_pop_multiple_keys"}, "Annotation Coverage": {"requirement": "Ensure that the 'pop' function has complete annotation coverage, including parameter types and return type.", "unit_test": ["def test_pop_annotations(self):\n    from typing import Any, Optional\n    from typing import get_type_hints\n    hints = get_type_hints(KeyValue.pop)\n    self.assertEqual(hints['key'], str)\n    self.assertEqual(hints['default'], Optional[Any])\n    self.assertEqual(hints['return'], Any)"], "test": "tests/kv.py::TestKeyValue::test_pop_annotations"}, "Code Complexity": {"requirement": "The 'pop' function should maintain a cyclomatic complexity of 3 or less to ensure readability and maintainability.", "unit_test": ["def test_pop_complexity(self):\n    import radon.complexity as cc\n    source = inspect.getsource(KeyValue.pop)\n    complexity = cc.cc_visit(source)\n    self.assertTrue(all(block.complexity <= 3 for block in complexity))"], "test": "tests/kv.py::TestKeyValue::test_code_complexity"}, "Code Standard": {"requirement": "The 'pop' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_pop_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_keyvalue_module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/kv.py::TestKeyValue::test_code_style"}, "Context Usage Verification": {"requirement": "The 'pop' function should utilize the database transaction context to ensure atomicity of the operation.", "unit_test": ["def test_pop_transaction_usage(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    with KV._database.atomic() as txn:\n        KV.pop('k1')\n    self.assertFalse('k1' in KV)"], "test": "tests/kv.py::TestKeyValue::test_pop_transaction_usage"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'pop' function correctly uses the database context to ensure that the operation is atomic and consistent.", "unit_test": ["def test_pop_atomicity(self):\n    KV = self.create_kv()\n    KV['k1'] = 'v1'\n    with KV._database.atomic() as txn:\n        KV.pop('k1')\n        self.assertFalse('k1' in KV)\n    self.assertRaises(KeyError, lambda: KV['k1'])"], "test": "tests/kv.py::TestKeyValue::test_pop_atomicity"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "type": "method", "project_path": "Security/asyncssh", "completion_path": "Security/asyncssh/asyncssh/public_key.py", "signature_position": [624, 624], "body_position": [634, 637], "dependency": {"intra_class": ["asyncssh.public_key.SSHKey._comment", "asyncssh.public_key.SSHKey._filename", "asyncssh.public_key.SSHKey.public_data", "asyncssh.public_key.SSHKey.set_comment", "asyncssh.public_key.SSHKey.set_filename"], "intra_file": ["asyncssh.public_key.decode_ssh_public_key"], "cross_file": []}, "requirement": {"Functionality": "This method converts an SSHKey object that contains a private key into one that contains only the corresponding public key. It first decodes asymmetric encryption. Once decrypted, it proceeds to assign a relevant comment and filename to the associated key. Upon completion of these steps, the method returns the processed data as its final output.", "Arguments": ":param self: SSHKey. An instance of the SSHKey class.\n:return: SSHKey. The SSHKey object that contains only the corresponding public key."}, "tests": ["tests/test_agent.py::_TestAgent::test_lock", "tests/test_public_key.py::_TestPublicKeyTopLevel::test_rsa_encrypt_error", "tests/test_public_key.py::_TestPublicKeyTopLevel::test_generate_errors", "tests/test_agent.py::_TestAgent::test_reconnect", "tests/test_agent.py::_TestAgent::test_sign"], "indent": 4, "domain": "Security", "code": "    def convert_to_public(self) -> 'SSHKey':\n        \"\"\"Return public key corresponding to this key\n\n           This method converts an :class:`SSHKey` object which contains\n           a private key into one which contains only the corresponding\n           public key. If it is called on something which is already\n           a public key, it has no effect.\n\n        \"\"\"\n\n        result = decode_ssh_public_key(self.public_data)\n        result.set_comment(self._comment)\n        result.set_filename(self._filename)\n        return result\n", "context": "# Copyright (c) 2013-2023 by Ron Frederick <ronf@timeheart.net> and others.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v2.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-2.0/\n#\n# This program may also be made available under the following secondary\n# licenses when the conditions for such availability set forth in the\n# Eclipse Public License v2.0 are satisfied:\n#\n#    GNU General Public License, Version 2.0, or any later versions of\n#    that license\n#\n# SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"SSH asymmetric encryption handlers\"\"\"\n\nimport binascii\nimport os\nimport re\nimport time\n\nfrom datetime import datetime\nfrom hashlib import md5, sha1, sha256, sha384, sha512\nfrom pathlib import Path, PurePath\nfrom typing import Callable, Dict, List, Mapping, Optional, Sequence, Set\nfrom typing import Tuple, Type, Union, cast\nfrom typing_extensions import Protocol\n\nfrom .crypto import ed25519_available, ed448_available\nfrom .encryption import Encryption\nfrom .sk import sk_available\n\ntry:\n    # pylint: disable=unused-import\n    from .crypto import X509Certificate\n    from .crypto import generate_x509_certificate, import_x509_certificate\n    _x509_available = True\nexcept ImportError: # pragma: no cover\n    _x509_available = False\n\ntry:\n    import bcrypt\n    _bcrypt_available = hasattr(bcrypt, 'kdf')\nexcept ImportError: # pragma: no cover\n    _bcrypt_available = False\n\nfrom .asn1 import ASN1DecodeError, BitString, ObjectIdentifier\nfrom .asn1 import der_encode, der_decode, der_decode_partial\nfrom .crypto import CryptoKey, PyCAKey\nfrom .encryption import get_encryption_params, get_encryption\nfrom .misc import BytesOrStr, DefTuple, FilePath, IPNetwork\nfrom .misc import ip_network, read_file, write_file, parse_time_interval\nfrom .packet import NameList, String, UInt32, UInt64\nfrom .packet import PacketDecodeError, SSHPacket\nfrom .pbe import KeyEncryptionError, pkcs1_encrypt, pkcs8_encrypt\nfrom .pbe import pkcs1_decrypt, pkcs8_decrypt\nfrom .sk import SSH_SK_USER_PRESENCE_REQD, sk_get_resident\n\n\n_Comment = Optional[BytesOrStr]\n_CertPrincipals = Union[str, Sequence[str]]\n_Time = Union[int, float, datetime, str]\n\n_PubKeyAlgMap = Dict[bytes, Type['SSHKey']]\n_CertAlgMap = Dict[bytes, Tuple[Optional[Type['SSHKey']],\n                                Type['SSHCertificate']]]\n_CertSigAlgMap = Dict[bytes, bytes]\n_CertVersionMap = Dict[Tuple[bytes, int],\n                       Tuple[bytes, Type['SSHOpenSSHCertificate']]]\n\n_PEMMap = Dict[bytes, Type['SSHKey']]\n_PKCS8OIDMap = Dict[ObjectIdentifier, Type['SSHKey']]\n_SKAlgMap = Dict[int, Tuple[Type['SSHKey'], Tuple[object, ...]]]\n\n_OpenSSHCertOptions = Dict[str, object]\n_OpenSSHCertParams = Tuple[object, int, int, bytes, bytes,\n                           int, int, bytes, bytes]\n_OpenSSHCertEncoders = Sequence[Tuple[str, Callable[[object], bytes]]]\n_OpenSSHCertDecoders = Dict[bytes, Callable[[SSHPacket], object]]\n\nX509CertPurposes = Union[None, str, Sequence[str]]\n\n_IdentityArg = Union[bytes, FilePath, 'SSHKey', 'SSHCertificate']\nIdentityListArg = Union[_IdentityArg, Sequence[_IdentityArg]]\n_KeyArg = Union[bytes, FilePath, 'SSHKey']\nKeyListArg = Union[FilePath, Sequence[_KeyArg]]\n_CertArg = Union[bytes, FilePath, 'SSHCertificate']\nCertListArg = Union[_CertArg, Sequence[_CertArg]]\n_KeyPairArg = Union['SSHKeyPair', _KeyArg, Tuple[_KeyArg, _CertArg]]\nKeyPairListArg = Union[_KeyPairArg, Sequence[_KeyPairArg]]\n\n\n# Default file names in .ssh directory to read private keys from\n_DEFAULT_KEY_FILES = (\n    ('id_ed25519_sk', ed25519_available and sk_available),\n    ('id_ecdsa_sk', sk_available),\n    ('id_ed448', ed448_available),\n    ('id_ed25519', ed25519_available),\n    ('id_ecdsa', True),\n    ('id_rsa', True),\n    ('id_dsa', True)\n)\n\n# Default directories and file names to read host keys from\n_DEFAULT_HOST_KEY_DIRS = ('/opt/local/etc', '/opt/local/etc/ssh',\n                          '/usr/local/etc', '/usr/local/etc/ssh',\n                          '/etc', '/etc/ssh')\n_DEFAULT_HOST_KEY_FILES = ('ssh_host_ed448_key', 'ssh_host_ed25519_key',\n                           'ssh_host_ecdsa_key', 'ssh_host_rsa_key',\n                           'ssh_host_dsa_key')\n\n_hashes = {'md5': md5, 'sha1': sha1, 'sha256': sha256,\n           'sha384': sha384, 'sha512': sha512}\n\n_public_key_algs: List[bytes] = []\n_default_public_key_algs: List[bytes] = []\n\n_certificate_algs: List[bytes] = []\n_default_certificate_algs: List[bytes] = []\n\n_x509_certificate_algs: List[bytes] = []\n_default_x509_certificate_algs: List[bytes] = []\n\n_public_key_alg_map: _PubKeyAlgMap = {}\n_certificate_alg_map: _CertAlgMap = {}\n_certificate_sig_alg_map: _CertSigAlgMap = {}\n_certificate_version_map: _CertVersionMap = {}\n_pem_map: _PEMMap = {}\n_pkcs8_oid_map: _PKCS8OIDMap = {}\n_sk_alg_map: _SKAlgMap = {}\n\n_abs_date_pattern = re.compile(r'\\d{8}')\n_abs_time_pattern = re.compile(r'\\d{14}')\n\n_subject_pattern = re.compile(r'(?:Distinguished[ -_]?Name|Subject|DN)[=:]?\\s?',\n                              re.IGNORECASE)\n\n# SSH certificate types\nCERT_TYPE_USER = 1\nCERT_TYPE_HOST = 2\n\n# Flag to omit second argument in alg_params\nOMIT = object()\n\n_OPENSSH_KEY_V1 = b'openssh-key-v1\\0'\n_OPENSSH_SALT_LEN = 16\n_OPENSSH_WRAP_LEN = 70\n\n\ndef _parse_time(t: _Time) -> int:\n    \"\"\"Parse a time value\"\"\"\n\n    if isinstance(t, int):\n        return t\n    elif isinstance(t, float):\n        return int(t)\n    elif isinstance(t, datetime):\n        return int(t.timestamp())\n    elif isinstance(t, str):\n        if t == 'now':\n            return int(time.time())\n\n        match = _abs_date_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d').timestamp())\n\n        match = _abs_time_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d%H%M%S').timestamp())\n\n        try:\n            return int(time.time() + parse_time_interval(t))\n        except ValueError:\n            pass\n\n    raise ValueError('Unrecognized time value')\n\n\ndef _wrap_base64(data: bytes, wrap: int = 64) -> bytes:\n    \"\"\"Break a Base64 value into multiple lines.\"\"\"\n\n    data = binascii.b2a_base64(data)[:-1]\n    return b'\\n'.join(data[i:i+wrap]\n                      for i in range(0, len(data), wrap)) + b'\\n'\n\n\nclass KeyGenerationError(ValueError):\n    \"\"\"Key generation error\n\n       This exception is raised by :func:`generate_private_key`,\n       :meth:`generate_user_certificate() <SSHKey.generate_user_certificate>`\n       or :meth:`generate_host_certificate()\n       <SSHKey.generate_host_certificate>` when the requested parameters are\n       unsupported.\n\n    \"\"\"\n\n\nclass KeyImportError(ValueError):\n    \"\"\"Key import error\n\n       This exception is raised by key import functions when the\n       data provided cannot be imported as a valid key.\n\n    \"\"\"\n\n\nclass KeyExportError(ValueError):\n    \"\"\"Key export error\n\n       This exception is raised by key export functions when the\n       requested format is unknown or encryption is requested for a\n       format which doesn't support it.\n\n    \"\"\"\n\n\nclass SigningKey(Protocol):\n    \"\"\"Protocol for signing a block of data\"\"\"\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with a private key\"\"\"\n\n\nclass VerifyingKey(Protocol):\n    \"\"\"Protocol for verifying a signature on a block of data\"\"\"\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify a signature on a block of data with a public key\"\"\"\n\n\nclass SSHKey:\n    \"\"\"Parent class which holds an asymmetric encryption key\"\"\"\n\n    algorithm: bytes = b''\n    sig_algorithms: Sequence[bytes] = ()\n    cert_algorithms: Sequence[bytes] = ()\n    x509_algorithms: Sequence[bytes] = ()\n    all_sig_algorithms: Set[bytes] = set()\n    default_x509_hash: str = ''\n    pem_name: bytes = b''\n    pkcs8_oid: Optional[ObjectIdentifier] = None\n    use_executor: bool = False\n\n    def __init__(self, key: Optional[CryptoKey] = None):\n        self._key = key\n        self._comment: Optional[bytes] = None\n        self._filename: Optional[bytes] = None\n        self._touch_required = False\n\n    @classmethod\n    def generate(cls, algorithm: bytes, **kwargs) -> 'SSHKey':\n        \"\"\"Generate a new SSH private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_private(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_public(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a public key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def decode_pkcs1_private(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs1_public(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_private(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_public(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    @classmethod\n    def decode_ssh_private(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format private key\"\"\"\n\n    @classmethod\n    def decode_ssh_public(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format public key\"\"\"\n\n    @property\n    def private_data(self) -> bytes:\n        \"\"\"Return private key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_private()\n\n    @property\n    def public_data(self) -> bytes:\n        \"\"\"Return public key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_public()\n\n    @property\n    def pyca_key(self) -> PyCAKey:\n        \"\"\"Return PyCA key for use in X.509 module\"\"\"\n\n        assert self._key is not None\n        return self._key.pyca_key\n\n    def _generate_certificate(self, key: 'SSHKey', version: int, serial: int,\n                              cert_type: int, key_id: str,\n                              principals: _CertPrincipals,\n                              valid_after: _Time, valid_before: _Time,\n                              cert_options: _OpenSSHCertOptions,\n                              sig_alg_name: DefTuple[str],\n                              comment: DefTuple[_Comment]) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        if isinstance(principals, str):\n            principals = [p.strip() for p in principals.split(',')]\n        else:\n            principals = list(principals)\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if sig_alg_name == ():\n            sig_alg = self.sig_algorithms[0]\n        else:\n            sig_alg = cast(str, sig_alg_name).encode()\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        comment: _Comment\n\n        try:\n            algorithm, cert_handler = _certificate_version_map[key.algorithm,\n                                                               version]\n        except KeyError:\n            raise KeyGenerationError('Unknown certificate version') from None\n\n        return cert_handler.generate(self, algorithm, key, serial, cert_type,\n                                     key_id, principals, valid_after,\n                                     valid_before, cert_options,\n                                     sig_alg, comment)\n\n    def _generate_x509_certificate(self, key: 'SSHKey', subject: str,\n                                   issuer: Optional[str],\n                                   serial: Optional[int],\n                                   valid_after: _Time, valid_before: _Time,\n                                   ca: bool, ca_path_len: Optional[int],\n                                   purposes: X509CertPurposes,\n                                   user_principals: _CertPrincipals,\n                                   host_principals: _CertPrincipals,\n                                   hash_name: DefTuple[str],\n                                   comment: DefTuple[_Comment]) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation '\n                                     'requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if hash_name == ():\n            hash_name = key.default_x509_hash\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        hash_name: str\n        comment: _Comment\n\n        return SSHX509Certificate.generate(self, key, subject, issuer,\n                                           serial, valid_after, valid_before,\n                                           ca, ca_path_len, purposes,\n                                           user_principals, host_principals,\n                                           hash_name, comment)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this key\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key as a byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key as a Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def get_filename(self) -> Optional[bytes]:\n        \"\"\"Return the filename associated with this key\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._filename\n\n    def set_filename(self, filename: Union[None, bytes, FilePath]) -> None:\n        \"\"\"Set the filename associated with this key\n\n           :param filename:\n               The new filename to associate with this key\n           :type filename: `PurePath`, `str`, `bytes`, or `None`\n\n        \"\"\"\n\n        if isinstance(filename, PurePath):\n            filename = str(filename)\n\n        if isinstance(filename, str):\n            filename = filename.encode('utf-8')\n\n        self._filename = filename or None\n\n    def get_fingerprint(self, hash_name: str = 'sha256') -> str:\n        \"\"\"Get the fingerprint of this key\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           :param hash_name: (optional)\n               The hash algorithm to use to construct the fingerprint.\n           :type hash_name: `str`\n\n           :returns: `str`\n\n           :raises: :exc:`ValueError` if the hash name is invalid\n\n        \"\"\"\n\n        try:\n            hash_alg = _hashes[hash_name]\n        except KeyError:\n            raise ValueError('Unknown hash algorithm') from None\n\n        h = hash_alg(self.public_data)\n\n        if hash_name == 'md5':\n            fp = h.hexdigest()\n            fp_text = ':'.join(fp[i:i+2] for i in range(0, len(fp), 2))\n        else:\n            fpb = h.digest()\n            fp_text = binascii.b2a_base64(fpb).decode('ascii')[:-1].strip('=')\n\n        return hash_name.upper() + ':' + fp_text\n\n    def set_touch_required(self, touch_required: bool) -> None:\n        \"\"\"Set whether touch is required when using a security key\"\"\"\n\n        self._touch_required = touch_required\n\n    def sign_raw(self, data: bytes, hash_name: str) -> bytes:\n        \"\"\"Return a raw signature of the specified data\"\"\"\n\n        assert self._key is not None\n        return self._key.sign(data, hash_name)\n\n    def sign_ssh(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Abstract method to compute an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def verify_ssh(self, data: bytes, sig_algorithm: bytes,\n                   packet: SSHPacket) -> bool:\n        \"\"\"Abstract method to verify an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def sign(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Return an SSH-encoded signature of the specified data\"\"\"\n\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return b''.join((String(sig_algorithm),\n                         self.sign_ssh(data, sig_algorithm)))\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify an SSH signature of the specified data using this key\"\"\"\n\n        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, packet)\n        except PacketDecodeError:\n            return False\n\n    def encode_pkcs1_private(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 private key export not supported')\n\n    def encode_pkcs1_public(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 public key export not supported')\n\n    def encode_pkcs8_private(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 private key export not supported')\n\n    def encode_pkcs8_public(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 public key export not supported')\n\n    def encode_ssh_private(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH private key export not supported')\n\n    def encode_ssh_public(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH public key export not supported')\n\n    def encode_agent_cert_private(self) -> bytes:\n        \"\"\"Encode certificate private key data for agent\"\"\"\n\n        raise NotImplementedError\n\n###The function: convert_to_public###\n    def generate_user_certificate(\n            self, user_key: 'SSHKey', key_id: str, version: int = 1,\n            serial: int = 0, principals: _CertPrincipals = (),\n            valid_after: _Time = 0, valid_before: _Time = 0xffffffffffffffff,\n            force_command: Optional[str] = None,\n            source_address: Optional[Sequence[str]] = None,\n            permit_x11_forwarding: bool = True,\n            permit_agent_forwarding: bool = True,\n            permit_port_forwarding: bool = True, permit_pty: bool = True,\n            permit_user_rc: bool = True, touch_required: bool = True,\n            sig_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH user certificate\n\n           This method returns an SSH user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param force_command: (optional)\n               The command (if any) to force a session to run when this\n               certificate is used.\n           :param source_address: (optional)\n               A list of source addresses and networks for which the\n               certificate is valid, defaulting to all addresses.\n           :param permit_x11_forwarding: (optional)\n               Whether or not to allow this user to use X11 forwarding,\n               defaulting to `True`.\n           :param permit_agent_forwarding: (optional)\n               Whether or not to allow this user to use agent forwarding,\n               defaulting to `True`.\n           :param permit_port_forwarding: (optional)\n               Whether or not to allow this user to use port forwarding,\n               defaulting to `True`.\n           :param permit_pty: (optional)\n               Whether or not to allow this user to allocate a\n               pseudo-terminal, defaulting to `True`.\n           :param permit_user_rc: (optional)\n               Whether or not to run the user rc file when this certificate\n               is used, defaulting to `True`.\n           :param touch_required: (optional)\n               Whether or not to require the user to touch the security key\n               when authenticating with it, defaulting to `True`.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type force_command: `str` or `None`\n           :type source_address: list of ip_address and ip_network values\n           :type permit_x11_forwarding: `bool`\n           :type permit_agent_forwarding: `bool`\n           :type permit_port_forwarding: `bool`\n           :type permit_pty: `bool`\n           :type permit_user_rc: `bool`\n           :type touch_required: `bool`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        cert_options: _OpenSSHCertOptions = {}\n\n        if force_command:\n            cert_options['force-command'] = force_command\n\n        if source_address:\n            cert_options['source-address'] = [ip_network(addr)\n                                              for addr in source_address]\n\n        if permit_x11_forwarding:\n            cert_options['permit-X11-forwarding'] = True\n\n        if permit_agent_forwarding:\n            cert_options['permit-agent-forwarding'] = True\n\n        if permit_port_forwarding:\n            cert_options['permit-port-forwarding'] = True\n\n        if permit_pty:\n            cert_options['permit-pty'] = True\n\n        if permit_user_rc:\n            cert_options['permit-user-rc'] = True\n\n        if not touch_required:\n            cert_options['no-touch-required'] = True\n\n        return self._generate_certificate(user_key, version, serial,\n                                          CERT_TYPE_USER, key_id,\n                                          principals, valid_after,\n                                          valid_before, cert_options,\n                                          sig_alg, comment)\n\n    def generate_host_certificate(self, host_key: 'SSHKey', key_id: str,\n                                  version: int = 1, serial: int = 0,\n                                  principals: _CertPrincipals = (),\n                                  valid_after: _Time = 0,\n                                  valid_before: _Time = 0xffffffffffffffff,\n                                  sig_alg: DefTuple[str] = (),\n                                  comment: DefTuple[_Comment] = ()) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH host certificate\n\n           This method returns an SSH host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        if comment == ():\n            comment = host_key.get_comment_bytes()\n\n        return self._generate_certificate(host_key, version, serial,\n                                          CERT_TYPE_HOST, key_id,\n                                          principals, valid_after,\n                                          valid_before, {}, sig_alg, comment)\n\n    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 user certificate\n\n           This method returns an X.509 user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellClient'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)\n\n    def generate_x509_host_certificate(\n            self, host_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellServer',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 host certificate\n\n           This method returns an X.509 host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellServer'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(host_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, (), principals,\n                                               hash_alg, comment)\n\n    def generate_x509_ca_certificate(self, ca_key: 'SSHKey', subject: str,\n                                     issuer: Optional[str] = None,\n                                     serial: Optional[int] = None,\n                                     valid_after: _Time = 0,\n                                     valid_before: _Time = 0xffffffffffffffff,\n                                     ca_path_len: Optional[int] = None,\n                                     hash_alg: DefTuple[str] = (),\n                                     comment: DefTuple[_Comment] = ()) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 CA certificate\n\n           This method returns an X.509 CA certificate with the requested\n           attributes signed by this private key.\n\n           :param ca_key:\n               The new CA's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param ca_path_len: (optional)\n               The maximum number of levels of intermediate CAs allowed\n               below this new CA or `None` to not enforce a limit,\n               defaulting to no limit.\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               ca_key.\n           :type ca_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type ca_path_len: `int` or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(ca_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, True,\n                                               ca_path_len, None, (), (),\n                                               hash_alg, comment)\n\n    def export_private_key(self, format_name: str = 'openssh',\n                           passphrase: Optional[BytesOrStr] = None,\n                           cipher_name: str = 'aes256-cbc',\n                           hash_name: str = 'sha256',\n                           pbe_version: int = 2, rounds: int = 128,\n                           ignore_few_rounds: bool = False) -> bytes:\n        \"\"\"Export a private key in the requested format\n\n           This method returns this object's private key encoded in the\n           requested format. If a passphrase is specified, the key will\n           be exported in encrypted form.\n\n           Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh\n\n           By default, openssh format will be used.\n\n           Encryption is supported in pkcs1-pem, pkcs8-der, pkcs8-pem,\n           and openssh formats. For pkcs1-pem, only the cipher can be\n           specified. For pkcs8-der and pkcs-8, cipher,  hash and PBE\n           version can be specified. For openssh, cipher and rounds\n           can be specified.\n\n           Available ciphers for pkcs1-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, des-cbc, des3-cbc\n\n           Available ciphers for pkcs8-der and pkcs8-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, blowfish-cbc,\n               cast128-cbc, des-cbc, des2-cbc, des3-cbc, rc4-40, rc4-128\n\n           Available ciphers for openssh format include the following\n           :ref:`encryption algorithms <EncryptionAlgs>`.\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           Available PBE versions include 1 for PBES1 and 2 for PBES2.\n\n           Not all combinations of cipher, hash, and version are supported.\n\n           The default cipher is aes256. In the pkcs8 formats, the default\n           hash is sha256 and default version is PBES2.\n\n           In openssh format, the default number of rounds is 128.\n\n           .. note:: The openssh format uses bcrypt for encryption, but\n                     unlike the traditional bcrypt cost factor used in\n                     password hashing which scales logarithmically, the\n                     encryption strength here scales linearly with the\n                     rounds value. Since the cipher is rekeyed 64 times\n                     per round, the default rounds value of 128 corresponds\n                     to 8192 total iterations, which is the equivalent of\n                     a bcrypt cost factor of 13.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :param passphrase: (optional)\n               A passphrase to encrypt the private key with.\n           :param cipher_name: (optional)\n               The cipher to use for private key encryption.\n           :param hash_name: (optional)\n               The hash to use for private key encryption.\n           :param pbe_version: (optional)\n               The PBE version to use for private key encryption.\n           :param rounds: (optional)\n               The number of KDF rounds to apply to the passphrase.\n           :type format_name: `str`\n           :type passphrase: `str` or `bytes`\n           :type cipher_name: `str`\n           :type hash_name: `str`\n           :type pbe_version: `int`\n           :type rounds: `int`\n\n           :returns: `bytes` representing the exported private key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_private())\n\n            if passphrase is not None:\n                if format_name == 'pkcs1-der':\n                    raise KeyExportError('PKCS#1 DER format does not support '\n                                         'private key encryption')\n\n                alg, iv, data = pkcs1_encrypt(data, cipher_name, passphrase)\n                headers = (b'Proc-Type: 4,ENCRYPTED\\n' +\n                           b'DEK-Info: ' + alg + b',' +\n                           binascii.b2a_hex(iv).upper() + b'\\n\\n')\n            else:\n                headers = b''\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PRIVATE KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        headers + _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_private()\n\n            if alg_params is OMIT:\n                data = der_encode((0, (self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode((0, (self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if passphrase is not None:\n                data = pkcs8_encrypt(data, cipher_name, hash_name,\n                                     pbe_version, passphrase)\n\n            if format_name == 'pkcs8-pem':\n                if passphrase is not None:\n                    keytype = b'ENCRYPTED PRIVATE KEY'\n                else:\n                    keytype = b'PRIVATE KEY'\n\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            check = os.urandom(4)\n            nkeys = 1\n\n            data = b''.join((check, check, self.private_data,\n                             String(self._comment or b'')))\n\n            cipher: Optional[Encryption]\n\n            if passphrase is not None:\n                try:\n                    alg = cipher_name.encode('ascii')\n                    key_size, iv_size, block_size, _, _, _ = \\\n                        get_encryption_params(alg)\n                except (KeyError, UnicodeEncodeError):\n                    raise KeyEncryptionError('Unknown cipher: %s' %\n                                             cipher_name) from None\n\n                if not _bcrypt_available: # pragma: no cover\n                    raise KeyExportError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n                kdf = b'bcrypt'\n                salt = os.urandom(_OPENSSH_SALT_LEN)\n                kdf_data = b''.join((String(salt), UInt32(rounds)))\n\n                if isinstance(passphrase, str):\n                    passphrase = passphrase.encode('utf-8')\n\n                key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                 rounds, ignore_few_rounds)\n\n                cipher = get_encryption(alg, key[:key_size], key[key_size:])\n                block_size = max(block_size, 8)\n            else:\n                cipher = None\n                alg = b'none'\n                kdf = b'none'\n                kdf_data = b''\n                block_size = 8\n                mac = b''\n\n            pad = len(data) % block_size\n            if pad: # pragma: no branch\n                data = data + bytes(range(1, block_size + 1 - pad))\n\n            if cipher:\n                data, mac = cipher.encrypt_packet(0, b'', data)\n            else:\n                mac = b''\n\n            data = b''.join((_OPENSSH_KEY_V1, String(alg), String(kdf),\n                             String(kdf_data), UInt32(nkeys),\n                             String(self.public_data), String(data), mac))\n\n            return (b'-----BEGIN OPENSSH PRIVATE KEY-----\\n' +\n                    _wrap_base64(data, _OPENSSH_WRAP_LEN) +\n                    b'-----END OPENSSH PRIVATE KEY-----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def export_public_key(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a public key in the requested format\n\n           This method returns this object's public key encoded in the\n           requested format. Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh, rfc4716\n\n           By default, openssh format will be used.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported public key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_public())\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PUBLIC KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_public()\n            pkcs8_data = BitString(pkcs8_data)\n\n            if alg_params is OMIT:\n                data = der_encode(((self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode(((self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if format_name == 'pkcs8-pem':\n                data = (b'-----BEGIN PUBLIC KEY-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END PUBLIC KEY-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs))\n\n    def write_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs))\n\n    def append_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs), 'ab')\n\n    def append_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs), 'ab')\n\n\nclass SSHCertificate:\n    \"\"\"Parent class which holds an SSH certificate\"\"\"\n\n    is_x509 = False\n    is_x509_chain = False\n\n    def __init__(self, algorithm: bytes, sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes], key: SSHKey,\n                 public_data: bytes, comment: _Comment):\n        self.algorithm = algorithm\n        self.sig_algorithms = sig_algorithms\n        self.host_key_algorithms = host_key_algorithms\n        self.key = key\n        self.public_data = public_data\n\n        self.set_comment(comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        raise NotImplementedError\n\n    def __eq__(self, other: object) -> bool:\n        return (isinstance(other, type(self)) and\n                self.public_data == other.public_data)\n\n    def __hash__(self) -> int:\n        return hash(self.public_data)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this certificate\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this certificate\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this certificate as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this certificate as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        return self._comment.decode(encoding, errors) if self._comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this certificate\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def export_certificate(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a certificate in the requested format\n\n           This function returns this certificate encoded in the requested\n           format. Available formats include:\n\n               der, pem, openssh, rfc4716\n\n           By default, OpenSSH format will be used.\n\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported certificate\n\n        \"\"\"\n\n        if self.is_x509:\n            if format_name == 'rfc4716':\n                raise KeyExportError('RFC4716 format is not supported for '\n                                     'X.509 certificates')\n        else:\n            if format_name in ('der', 'pem'):\n                raise KeyExportError('DER and PEM formats are not supported '\n                                     'for OpenSSH certificates')\n\n        if format_name == 'der':\n            return self.public_data\n        elif format_name == 'pem':\n            return (b'-----BEGIN CERTIFICATE-----\\n' +\n                    _wrap_base64(self.public_data) +\n                    b'-----END CERTIFICATE-----\\n')\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_certificate(self, filename: FilePath,\n                          format_name: str = 'openssh') -> None:\n        \"\"\"Write a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which writes the exported certificate to a file.\n\n           :param filename:\n               The filename to write the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name))\n\n    def append_certificate(self, filename: FilePath,\n                           format_name: str = 'openssh') -> None:\n        \"\"\"Append a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which appends the exported certificate to an existing file.\n\n           :param filename:\n               The filename to append the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name), 'ab')\n\n\nclass SSHOpenSSHCertificate(SSHCertificate):\n    \"\"\"Class which holds an OpenSSH certificate\"\"\"\n\n    _user_option_encoders: _OpenSSHCertEncoders = ()\n    _user_extension_encoders: _OpenSSHCertEncoders = ()\n    _host_option_encoders: _OpenSSHCertEncoders = ()\n    _host_extension_encoders: _OpenSSHCertEncoders = ()\n\n    _user_option_decoders: _OpenSSHCertDecoders = {}\n    _user_extension_decoders: _OpenSSHCertDecoders = {}\n    _host_option_decoders: _OpenSSHCertDecoders = {}\n    _host_extension_decoders: _OpenSSHCertDecoders = {}\n\n    def __init__(self, algorithm: bytes, key: SSHKey, data: bytes,\n                 principals: Sequence[str], options: _OpenSSHCertOptions,\n                 signing_key: SSHKey, serial: int, cert_type: int,\n                 key_id: str, valid_after: int, valid_before: int,\n                 comment: _Comment):\n        super().__init__(algorithm, key.sig_algorithms,\n                         key.cert_algorithms or (algorithm,),\n                         key, data, comment)\n\n        self.principals = principals\n        self.options = options\n        self.signing_key = signing_key\n\n        self._serial = serial\n        self._cert_type = cert_type\n        self._key_id = key_id\n        self._valid_after = valid_after\n        self._valid_before = valid_before\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', algorithm: bytes, key: 'SSHKey',\n                 serial: int, cert_type: int, key_id: str,\n                 principals: Sequence[str], valid_after: int,\n                 valid_before: int, options: _OpenSSHCertOptions,\n                 sig_alg: bytes, comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        principal_bytes = b''.join(String(p) for p in principals)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._encode_options(options,\n                                               cls._user_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._user_extension_encoders)\n        else:\n            cert_options = cls._encode_options(options,\n                                               cls._host_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._host_extension_encoders)\n\n        key = key.convert_to_public()\n\n        data = b''.join((String(algorithm),\n                         cls._encode(key, serial, cert_type, key_id,\n                                     principal_bytes, valid_after,\n                                     valid_before, cert_options,\n                                     cert_extensions),\n                         String(signing_key.public_data)))\n\n        data += String(signing_key.sign(data, sig_alg))\n\n        signing_key = signing_key.convert_to_public()\n\n        return cls(algorithm, key, data, principals, options, signing_key,\n                   serial, cert_type, key_id, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        assert key_handler is not None\n\n        key_params, serial, cert_type, key_id, \\\n            principals, valid_after, valid_before, \\\n            options, extensions = cls._decode(packet, key_handler)\n\n        signing_key = decode_ssh_public_key(packet.get_string())\n        data = packet.get_consumed_payload()\n        signature = packet.get_string()\n        packet.check_end()\n\n        if not signing_key.verify(data, signature):\n            raise KeyImportError('Invalid certificate signature')\n\n        key = key_handler.make_public(key_params)\n        data = packet.get_consumed_payload()\n\n        try:\n            key_id_bytes = key_id.decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in key ID') from None\n\n        packet = SSHPacket(principals)\n        principals: List[str] = []\n\n        while packet:\n            try:\n                principal = packet.get_string().decode('utf-8')\n            except UnicodeDecodeError:\n                raise KeyImportError('Invalid characters in principal '\n                                     'name') from None\n\n            principals.append(principal)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._decode_options(\n                options, cls._user_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._user_extension_decoders, False))\n        elif cert_type == CERT_TYPE_HOST:\n            cert_options = cls._decode_options(\n                options, cls._host_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._host_extension_decoders, False))\n        else:\n            raise KeyImportError('Unknown certificate type')\n\n        return cls(algorithm, key, data, principals, cert_options, signing_key,\n                   serial, cert_type, key_id_bytes, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n\n        \"\"\"Encode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def _encode_options(options: _OpenSSHCertOptions,\n                        encoders: _OpenSSHCertEncoders) -> bytes:\n        \"\"\"Encode options found in this certificate\"\"\"\n\n        result = []\n\n        for name, encoder in encoders:\n            value = options.get(name)\n            if value:\n                result.append(String(name) + String(encoder(value)))\n\n        return b''.join(result)\n\n    @staticmethod\n    def _encode_bool(_value: object) -> bytes:\n        \"\"\"Encode a boolean option value\"\"\"\n\n        return b''\n\n    @staticmethod\n    def _encode_force_cmd(force_command: object) -> bytes:\n        \"\"\"Encode a force-command option\"\"\"\n\n        return String(cast(BytesOrStr, force_command))\n\n    @staticmethod\n    def _encode_source_addr(source_address: object) -> bytes:\n        \"\"\"Encode a source-address option\"\"\"\n\n        return NameList(str(addr).encode('ascii')\n                        for addr in cast(Sequence[IPNetwork], source_address))\n\n    @staticmethod\n    def _decode_bool(_packet: SSHPacket) -> bool:\n        \"\"\"Decode a boolean option value\"\"\"\n\n        return True\n\n    @staticmethod\n    def _decode_force_cmd(packet: SSHPacket) -> str:\n        \"\"\"Decode a force-command option\"\"\"\n\n        try:\n            return packet.get_string().decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in command') from None\n\n    @staticmethod\n    def _decode_source_addr(packet: SSHPacket) -> Sequence[IPNetwork]:\n        \"\"\"Decode a source-address option\"\"\"\n\n        try:\n            return [ip_network(addr.decode('ascii'))\n                    for addr in packet.get_namelist()]\n        except (UnicodeDecodeError, ValueError):\n            raise KeyImportError('Invalid source address') from None\n\n    @staticmethod\n    def _decode_options(options: bytes, decoders: _OpenSSHCertDecoders,\n                        critical: bool = True) -> _OpenSSHCertOptions:\n        \"\"\"Decode options found in this certificate\"\"\"\n\n        packet = SSHPacket(options)\n        result: _OpenSSHCertOptions = {}\n\n        while packet:\n            name = packet.get_string()\n\n            decoder = decoders.get(name)\n            if decoder:\n                data_packet = SSHPacket(packet.get_string())\n                result[name.decode('ascii')] = decoder(data_packet)\n                data_packet.check_end()\n            elif critical:\n                raise KeyImportError('Unrecognized critical option: %s' %\n                                     name.decode('ascii', errors='replace'))\n\n        return result\n\n    def validate(self, cert_type: int, principal: str) -> None:\n        \"\"\"Validate an OpenSSH certificate\"\"\"\n\n        if self._cert_type != cert_type:\n            raise ValueError('Invalid certificate type')\n\n        now = time.time()\n\n        if now < self._valid_after:\n            raise ValueError('Certificate not yet valid')\n\n        if now >= self._valid_before:\n            raise ValueError('Certificate expired')\n\n        if principal and self.principals and principal not in self.principals:\n            raise ValueError('Certificate principal mismatch')\n\n\nclass SSHOpenSSHCertificateV01(SSHOpenSSHCertificate):\n    \"\"\"Encoder/decoder class for version 01 OpenSSH certificates\"\"\"\n\n    _user_option_encoders = (\n        ('force-command',           SSHOpenSSHCertificate._encode_force_cmd),\n        ('source-address',          SSHOpenSSHCertificate._encode_source_addr)\n    )\n\n    _user_extension_encoders = (\n        ('permit-X11-forwarding',   SSHOpenSSHCertificate._encode_bool),\n        ('permit-agent-forwarding', SSHOpenSSHCertificate._encode_bool),\n        ('permit-port-forwarding',  SSHOpenSSHCertificate._encode_bool),\n        ('permit-pty',              SSHOpenSSHCertificate._encode_bool),\n        ('permit-user-rc',          SSHOpenSSHCertificate._encode_bool),\n        ('no-touch-required',       SSHOpenSSHCertificate._encode_bool)\n    )\n\n    _user_option_decoders = {\n        b'force-command':           SSHOpenSSHCertificate._decode_force_cmd,\n        b'source-address':          SSHOpenSSHCertificate._decode_source_addr\n    }\n\n    _user_extension_decoders = {\n        b'permit-X11-forwarding':   SSHOpenSSHCertificate._decode_bool,\n        b'permit-agent-forwarding': SSHOpenSSHCertificate._decode_bool,\n        b'permit-port-forwarding':  SSHOpenSSHCertificate._decode_bool,\n        b'permit-pty':              SSHOpenSSHCertificate._decode_bool,\n        b'permit-user-rc':          SSHOpenSSHCertificate._decode_bool,\n        b'no-touch-required':       SSHOpenSSHCertificate._decode_bool\n    }\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n        \"\"\"Encode a version 01 SSH certificate\"\"\"\n\n        return b''.join((String(os.urandom(32)), key.encode_ssh_public(),\n                         UInt64(serial), UInt32(cert_type), String(key_id),\n                         String(principals), UInt64(valid_after),\n                         UInt64(valid_before), String(options),\n                         String(extensions), String('')))\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode a version 01 SSH certificate\"\"\"\n\n        _ = packet.get_string()                             # nonce\n        key_params = key_handler.decode_ssh_public(packet)\n        serial = packet.get_uint64()\n        cert_type = packet.get_uint32()\n        key_id = packet.get_string()\n        principals = packet.get_string()\n        valid_after = packet.get_uint64()\n        valid_before = packet.get_uint64()\n        options = packet.get_string()\n        extensions = packet.get_string()\n        _ = packet.get_string()                             # reserved\n\n        return (key_params, serial, cert_type, key_id, principals,\n                valid_after, valid_before, options, extensions)\n\n\nclass SSHX509Certificate(SSHCertificate):\n    \"\"\"Encoder/decoder class for SSH X.509 certificates\"\"\"\n\n    is_x509 = True\n\n    def __init__(self, key: SSHKey, x509_cert: 'X509Certificate',\n                 comment: _Comment = None):\n        super().__init__(b'x509v3-' + key.algorithm, key.x509_algorithms,\n                         key.x509_algorithms, key, x509_cert.data,\n                         x509_cert.comment or comment)\n\n        self.subject = x509_cert.subject\n        self.issuer = x509_cert.issuer\n        self.issuer_hash = x509_cert.issuer_hash\n        self.user_principals = x509_cert.user_principals\n        self.x509_cert = x509_cert\n\n    def _expand_trust_store(self, cert: 'SSHX509Certificate',\n                            trusted_cert_paths: Sequence[FilePath],\n                            trust_store: Set['SSHX509Certificate']) -> None:\n        \"\"\"Look up certificates by issuer hash to build a trust store\"\"\"\n\n        issuer_hash = cert.issuer_hash\n\n        for path in trusted_cert_paths:\n            idx = 0\n\n            try:\n                while True:\n                    cert_path = Path(path, issuer_hash + '.' + str(idx))\n                    idx += 1\n\n                    c = cast('SSHX509Certificate', read_certificate(cert_path))\n\n                    if c.subject != cert.issuer or c in trust_store:\n                        continue\n\n                    trust_store.add(c)\n                    self._expand_trust_store(c, trusted_cert_paths, trust_store)\n            except (OSError, KeyImportError):\n                pass\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        raise RuntimeError # pragma: no cover\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', key: 'SSHKey', subject: str,\n                 issuer: Optional[str], serial: Optional[int],\n                 valid_after: int, valid_before: int, ca: bool,\n                 ca_path_len: Optional[int], purposes: X509CertPurposes,\n                 user_principals: _CertPrincipals,\n                 host_principals: _CertPrincipals, hash_name: str,\n                 comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        key = key.convert_to_public()\n\n        x509_cert = generate_x509_certificate(signing_key.pyca_key,\n                                              key.pyca_key, subject, issuer,\n                                              serial, valid_after, valid_before,\n                                              ca, ca_path_len, purposes,\n                                              user_principals, host_principals,\n                                              hash_name, comment)\n\n        return cls(key, x509_cert)\n\n    @classmethod\n    def construct_from_der(cls, data: bytes,\n                           comment: _Comment = None) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from DER data\"\"\"\n\n        try:\n            x509_cert = import_x509_certificate(data)\n            key = import_public_key(x509_cert.key_data)\n        except ValueError as exc:\n            raise KeyImportError(str(exc)) from None\n\n        return cls(key, x509_cert, comment)\n\n    def validate_chain(self, trust_chain: Sequence['SSHX509Certificate'],\n                       trusted_certs: Sequence['SSHX509Certificate'],\n                       trusted_cert_paths: Sequence[FilePath],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        trust_store = set(c for c in trust_chain if c.subject != c.issuer) | \\\n            set(c for c in trusted_certs)\n\n        if trusted_cert_paths:\n            self._expand_trust_store(self, trusted_cert_paths, trust_store)\n\n            for c in trust_chain:\n                self._expand_trust_store(c, trusted_cert_paths, trust_store)\n\n        self.x509_cert.validate([c.x509_cert for c in trust_store],\n                                purposes, user_principal, host_principal)\n\n\nclass SSHX509CertificateChain(SSHCertificate):\n    \"\"\"Encoder/decoder class for an SSH X.509 certificate chain\"\"\"\n\n    is_x509_chain = True\n\n    def __init__(self, algorithm: bytes, certs: Sequence[SSHCertificate],\n                 ocsp_responses: Sequence[bytes], comment: _Comment):\n        key = certs[0].key\n        data = self._public_data(algorithm, certs, ocsp_responses)\n\n        super().__init__(algorithm, key.x509_algorithms, key.x509_algorithms,\n                         key, data, comment)\n\n        x509_certs = cast(Sequence[SSHX509Certificate], certs)\n        first_cert = x509_certs[0]\n        last_cert = x509_certs[-1]\n\n        self.subject = first_cert.subject\n        self.issuer = last_cert.issuer\n        self.user_principals = first_cert.user_principals\n\n        self._certs = x509_certs\n        self._ocsp_responses = ocsp_responses\n\n    @staticmethod\n    def _public_data(algorithm: bytes, certs: Sequence[SSHCertificate],\n                     ocsp_responses: Sequence[bytes]) -> bytes:\n        \"\"\"Return the X509 chain public data\"\"\"\n\n        return (String(algorithm) + UInt32(len(certs)) +\n                b''.join(String(c.public_data) for c in certs) +\n                UInt32(len(ocsp_responses)) +\n                b''.join(String(resp) for resp in ocsp_responses))\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        cert_count = packet.get_uint32()\n        certs = [import_certificate(packet.get_string())\n                 for _ in range(cert_count)]\n\n        ocsp_resp_count = packet.get_uint32()\n        ocsp_responses = [packet.get_string() for _ in range(ocsp_resp_count)]\n\n        packet.check_end()\n\n        if not certs:\n            raise KeyImportError('No certificates present')\n\n        return cls(algorithm, certs, ocsp_responses, comment)\n\n    @classmethod\n    def construct_from_certs(cls, certs: Sequence['SSHCertificate']) -> \\\n            'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate chain from certificates\"\"\"\n\n        cert = certs[0]\n\n        return cls(cert.algorithm, certs, (), cert.get_comment_bytes())\n\n    def adjust_public_data(self, algorithm: bytes) -> bytes:\n        \"\"\"Adjust public data to reflect chosen signature algorithm\"\"\"\n\n        return self._public_data(algorithm, self._certs, self._ocsp_responses)\n\n    def validate_chain(self, trusted_certs: Sequence[SSHX509Certificate],\n                       trusted_cert_paths: Sequence[FilePath],\n                       revoked_certs: Set[SSHX509Certificate],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        if revoked_certs:\n            for cert in self._certs:\n                if cert in revoked_certs:\n                    raise ValueError('Revoked X.509 certificate in '\n                                     'certificate chain')\n\n        self._certs[0].validate_chain(self._certs[1:], trusted_certs,\n                                      trusted_cert_paths, purposes,\n                                      user_principal, host_principal)\n\n\nclass SSHKeyPair:\n    \"\"\"Parent class which represents an asymmetric key pair\n\n       This is an abstract class which provides a method to sign data\n       with a private key and members to access the corresponding\n       algorithm and public key or certificate information needed to\n       identify what key was used for signing.\n\n    \"\"\"\n\n    _key_type = 'unknown'\n\n    def __init__(self, algorithm: bytes, sig_algorithm: bytes,\n                 sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes],\n                 public_data: bytes, comment: _Comment,\n                 cert: Optional[SSHCertificate] = None,\n                 filename: Optional[bytes] = None,\n                 use_executor: bool = False):\n        self.key_algorithm = algorithm\n        self.key_public_data = public_data\n\n        self.set_comment(comment)\n        self._cert = cert\n        self._filename = filename\n\n        self.use_executor = use_executor\n\n        if cert:\n            if cert.key.public_data != self.key_public_data:\n                raise ValueError('Certificate key mismatch')\n\n            self.algorithm = cert.algorithm\n\n            if cert.is_x509_chain:\n                self.sig_algorithm = cert.algorithm\n            else:\n                self.sig_algorithm = sig_algorithm\n\n            self.sig_algorithms = cert.sig_algorithms\n            self.host_key_algorithms = cert.host_key_algorithms\n            self.public_data = cert.public_data\n        else:\n            self.algorithm = algorithm\n            self.sig_algorithm = algorithm\n            self.sig_algorithms = sig_algorithms\n            self.host_key_algorithms = host_key_algorithms\n            self.public_data = public_data\n\n    def get_key_type(self) -> str:\n        \"\"\"Return what type of key pair this is\n\n           This method returns 'local' for locally loaded keys, and\n           'agent' for keys managed by an SSH agent.\n\n        \"\"\"\n\n        return self._key_type\n\n    @property\n    def has_cert(self) -> bool:\n        \"\"\" Return if this key pair has an associated cert\"\"\"\n\n        return bool(self._cert)\n\n    @property\n    def has_x509_chain(self) -> bool:\n        \"\"\" Return if this key pair has an associated X.509 cert chain\"\"\"\n\n        return self._cert.is_x509_chain if self._cert else False\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key pair\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyImportError('Private key export to agent not supported')\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key pair as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key pair as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key pair\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def set_certificate(self, cert: SSHCertificate) -> None:\n        \"\"\"Set certificate to use with this key\"\"\"\n\n        if cert.key.public_data != self.key_public_data:\n            raise ValueError('Certificate key mismatch')\n\n        self._cert = cert\n        self.algorithm = cert.algorithm\n\n        if cert.is_x509_chain:\n            self.sig_algorithm = cert.algorithm\n        else:\n            self.sig_algorithm = self.key_algorithm\n\n        self.sig_algorithms = cert.sig_algorithms\n        self.host_key_algorithms = cert.host_key_algorithms\n        self.public_data = cert.public_data\n\n    def set_sig_algorithm(self, sig_algorithm: bytes) -> None:\n        \"\"\"Set the signature algorithm to use when signing data\"\"\"\n\n        try:\n            sig_algorithm = _certificate_sig_alg_map[sig_algorithm]\n        except KeyError:\n            pass\n\n        self.sig_algorithm = sig_algorithm\n\n        if not self.has_cert:\n            self.algorithm = sig_algorithm\n        elif self.has_x509_chain:\n            self.algorithm = sig_algorithm\n\n            cert = cast('SSHX509CertificateChain', self._cert)\n            self.public_data = cert.adjust_public_data(sig_algorithm)\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise RuntimeError # pragma: no cover\n\n\nclass SSHLocalKeyPair(SSHKeyPair):\n    \"\"\"Class which holds a local asymmetric key pair\n\n       This class holds a private key and associated public data\n       which can either be the matching public key or a certificate\n       which has signed that public key.\n\n    \"\"\"\n\n    _key_type = 'local'\n\n    def __init__(self, key: SSHKey, pubkey: Optional[SSHKey] = None,\n                 cert: Optional[SSHCertificate] = None):\n        if pubkey and pubkey.public_data != key.public_data:\n            raise ValueError('Public key mismatch')\n\n        if key.has_comment():\n            comment = key.get_comment_bytes()\n        elif cert and cert.has_comment():\n            comment = cert.get_comment_bytes()\n        elif pubkey and pubkey.has_comment():\n            comment = pubkey.get_comment_bytes()\n        else:\n            comment = None\n\n        super().__init__(key.algorithm, key.algorithm, key.sig_algorithms,\n                         key.sig_algorithms, key.public_data, comment, cert,\n                         key.get_filename(), key.use_executor)\n\n        self._key = key\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        if self._cert:\n            data = String(self.public_data) + \\\n                       self._key.encode_agent_cert_private()\n        else:\n            data = self._key.encode_ssh_private()\n\n        return String(self.algorithm) + data\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        return self._key.sign(data, self.sig_algorithm)\n\n\ndef _parse_openssh(data: bytes) -> Tuple[bytes, Optional[bytes], bytes]:\n    \"\"\"Parse an OpenSSH format public key or certificate\"\"\"\n\n    line = data.split(None, 2)\n\n    if len(line) < 2:\n        raise KeyImportError('Invalid OpenSSH public key or certificate')\n    elif len(line) == 2:\n        comment = None\n    else:\n        comment = line[2]\n\n    if (line[0] not in _public_key_alg_map and\n            line[0] not in _certificate_alg_map):\n        raise KeyImportError('Unknown OpenSSH public key algorithm')\n\n    try:\n        return line[0], comment, binascii.a2b_base64(line[1])\n    except binascii.Error:\n        raise KeyImportError('Invalid OpenSSH public key '\n                             'or certificate') from None\n\n\ndef _parse_pem(data: bytes) -> Tuple[Mapping[bytes, bytes], bytes]:\n    \"\"\"Parse a PEM data block\"\"\"\n\n    start = 0\n    end: Optional[int] = None\n    headers: Dict[bytes, bytes] = {}\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if b':' in line:\n            hdr, value = line.split(b':', 1)\n            headers[hdr.strip()] = value.strip()\n        else:\n            break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return headers, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid PEM data') from None\n\n\ndef _parse_rfc4716(data: bytes) -> Tuple[Optional[bytes], bytes]:\n    \"\"\"Parse an RFC 4716 data block\"\"\"\n\n    start = 0\n    end = None\n    hdr = b''\n    comment = None\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if line[-1:] == b'\\\\':\n            hdr += line[:-1]\n        else:\n            hdr += line\n            if b':' in hdr:\n                hdr, value = hdr.split(b':', 1)\n\n                if hdr.strip() == b'Comment':\n                    comment = value.strip()\n                    if comment[:1] == b'\"' and comment[-1:] == b'\"':\n                        comment = comment[1:-1]\n\n                hdr = b''\n            else:\n                break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return comment, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid RFC 4716 data') from None\n\n\ndef _match_block(data: bytes, start: int, header: bytes,\n                 fmt: str) -> Tuple[bytes, int]:\n    \"\"\"Match a block of data wrapped in a header/footer\"\"\"\n\n    match = re.compile(b'^' + header[:5] + b'END' + header[10:] +\n                       rb'[ \\t\\r\\f\\v]*$', re.M).search(data, start)\n\n    if not match:\n        raise KeyImportError('Missing %s footer' % fmt)\n\n    return data[start:match.start()], match.end()\n\n\ndef _match_next(data: bytes, keytype: bytes, public: bool = False) -> \\\n        Tuple[Optional[str], Tuple, Optional[int]]:\n    \"\"\"Find the next key/certificate and call the appropriate decode\"\"\"\n\n    end: Optional[int]\n\n    if data.startswith(b'\\x30'):\n        try:\n            key_data, end = der_decode_partial(data)\n            return 'der', (key_data,), end\n        except ASN1DecodeError:\n            pass\n\n    start = 0\n    end = None\n\n    while end != 0:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if (line.startswith(b'-----BEGIN ') and\n                line.endswith(b' ' + keytype + b'-----')):\n            pem_name = line[11:-(6+len(keytype))].strip()\n            data, end = _match_block(data, end, line, 'PEM')\n            headers, data = _parse_pem(data)\n            return 'pem', (pem_name, headers, data), end\n        elif public:\n            if line == b'---- BEGIN SSH2 PUBLIC KEY ----':\n                data, end = _match_block(data, end, line, 'RFC 4716')\n                return 'rfc4716', _parse_rfc4716(data), end\n            else:\n                try:\n                    cert = _parse_openssh(line)\n                except KeyImportError:\n                    pass\n                else:\n                    return 'openssh', cert, (end if end else len(data))\n\n        start = end\n\n    return None, (), len(data)\n\n\ndef _decode_pkcs1_private(\n        pem_name: bytes, key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_private(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s private key' %\n                             pem_name.decode('ascii'))\n\n    if pem_name == b'RSA':\n        key_params = cast(Tuple, key_params) + \\\n            (unsafe_skip_rsa_key_validation,)\n\n    return handler.make_private(key_params)\n\n\ndef _decode_pkcs1_public(pem_name: bytes, key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_public(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s public key' %\n                             pem_name.decode('ascii'))\n\n    return handler.make_public(key_params)\n\n\ndef _decode_pkcs8_private(\n        key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) >= 3 and\n            key_data[0] in (0, 1) and isinstance(key_data[1], tuple) and\n            1 <= len(key_data[1]) <= 2 and isinstance(key_data[2], bytes)):\n        if len(key_data[1]) == 2:\n            alg, alg_params = key_data[1]\n        else:\n            alg, alg_params = key_data[1][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_private(alg_params, key_data[2])\n        if key_params is None:\n            raise KeyImportError('Invalid %s private key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        if alg == ObjectIdentifier('1.2.840.113549.1.1.1'):\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        return handler.make_private(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 private key')\n\n\ndef _decode_pkcs8_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) == 2 and\n            isinstance(key_data[0], tuple) and 1 <= len(key_data[0]) <= 2 and\n            isinstance(key_data[1], BitString) and key_data[1].unused == 0):\n        if len(key_data[0]) == 2:\n            alg, alg_params = key_data[0]\n        else:\n            alg, alg_params = key_data[0][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_public(alg_params, key_data[1].value)\n        if key_params is None:\n            raise KeyImportError('Invalid %s public key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        return handler.make_public(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 public key')\n\n\ndef _decode_openssh_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode an OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        cipher_name = packet.get_string()\n        kdf = packet.get_string()\n        kdf_data = packet.get_string()\n        nkeys = packet.get_uint32()\n        _ = packet.get_string()                 # public_key\n        key_data = packet.get_string()\n        mac = packet.get_remaining_payload()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if cipher_name != b'none':\n            if passphrase is None:\n                raise KeyImportError('Passphrase must be specified to import '\n                                     'encrypted private keys')\n\n            try:\n                key_size, iv_size, block_size, _, _, _ = \\\n                    get_encryption_params(cipher_name)\n            except KeyError:\n                raise KeyEncryptionError('Unknown cipher: %s' %\n                                         cipher_name.decode('ascii')) from None\n\n            if kdf != b'bcrypt':\n                raise KeyEncryptionError('Unknown kdf: %s' %\n                                         kdf.decode('ascii'))\n\n            if not _bcrypt_available: # pragma: no cover\n                raise KeyEncryptionError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n            packet = SSHPacket(kdf_data)\n            salt = packet.get_string()\n            rounds = packet.get_uint32()\n            packet.check_end()\n\n            if isinstance(passphrase, str):\n                passphrase = passphrase.encode('utf-8')\n\n            try:\n                bcrypt_key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                        rounds, ignore_few_rounds=True)\n            except ValueError:\n                raise KeyEncryptionError('Invalid OpenSSH '\n                                         'private key') from None\n\n            cipher = get_encryption(cipher_name, bcrypt_key[:key_size],\n                                    bcrypt_key[key_size:])\n\n            decrypted_key = cipher.decrypt_packet(0, b'', key_data, 0, mac)\n\n            if decrypted_key is None:\n                raise KeyEncryptionError('Incorrect passphrase')\n\n            key_data = decrypted_key\n            block_size = max(block_size, 8)\n        else:\n            block_size = 8\n\n        packet = SSHPacket(key_data)\n\n        check1 = packet.get_uint32()\n        check2 = packet.get_uint32()\n        if check1 != check2:\n            if cipher_name != b'none':\n                raise KeyEncryptionError('Incorrect passphrase') from None\n            else:\n                raise KeyImportError('Invalid OpenSSH private key')\n\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n        if not handler:\n            raise KeyImportError('Unknown OpenSSH private key algorithm')\n\n        key_params = handler.decode_ssh_private(packet)\n        comment = packet.get_string()\n        pad = packet.get_remaining_payload()\n\n        if len(pad) >= block_size or pad != bytes(range(1, len(pad) + 1)):\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if alg == b'ssh-rsa':\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        key = handler.make_private(key_params)\n        key.set_comment(comment)\n        return key\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_openssh_public(data: bytes) -> SSHKey:\n    \"\"\"Decode public key within OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        _ = packet.get_string()                 # cipher_name\n        _ = packet.get_string()                 # kdf\n        _ = packet.get_string()                 # kdf_data\n        nkeys = packet.get_uint32()\n        pubkey = packet.get_string()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        return decode_ssh_public_key(pubkey)\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_der_private(\n        key_data: object, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a DER format private key\"\"\"\n\n    # First, if there's a passphrase, try to decrypt PKCS#8\n    if passphrase is not None:\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            # Decryption failed - try decoding it as unencrypted\n            pass\n\n    # Then, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_private(pem_name, key_data,\n                                         unsafe_skip_rsa_key_validation)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER private key')\n\n\ndef _decode_der_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a DER format public key\"\"\"\n\n    # First, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_public(key_data)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_public(pem_name, key_data)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER public key')\n\n\ndef _decode_der_certificate(data: bytes,\n                            comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a DER format X.509 certificate\"\"\"\n\n    return SSHX509Certificate.construct_from_der(data, comment)\n\n\ndef _decode_pem_private(\n        pem_name: bytes, headers: Mapping[bytes, bytes],\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PEM format private key\"\"\"\n\n    if pem_name == b'OPENSSH':\n        return _decode_openssh_private(data, passphrase,\n                                       unsafe_skip_rsa_key_validation)\n\n    if headers.get(b'Proc-Type') == b'4,ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        dek_info = headers.get(b'DEK-Info', b'').split(b',')\n        if len(dek_info) != 2:\n            raise KeyImportError('Invalid PEM encryption params')\n\n        alg, iv = dek_info\n        try:\n            iv = binascii.a2b_hex(iv)\n        except binascii.Error:\n            raise KeyImportError('Invalid PEM encryption params') from None\n\n        try:\n            data = pkcs1_decrypt(data, alg, iv, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#1 '\n                                 'private key') from None\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM private key') from None\n\n    if pem_name == b'ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        pem_name = b''\n\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#8 '\n                                 'private key') from None\n\n    if pem_name:\n        return _decode_pkcs1_private(pem_name, key_data,\n                                     unsafe_skip_rsa_key_validation)\n    else:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n\n\ndef _decode_pem_public(pem_name: bytes, data: bytes) -> SSHKey:\n    \"\"\"Decode a PEM format public key\"\"\"\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM public key') from None\n\n    if pem_name:\n        return _decode_pkcs1_public(pem_name, key_data)\n    else:\n        return _decode_pkcs8_public(key_data)\n\n\ndef _decode_pem_certificate(pem_name: bytes, data: bytes) -> SSHCertificate:\n    \"\"\"Decode a PEM format X.509 certificate\"\"\"\n\n    if pem_name == b'TRUSTED':\n        # Strip off OpenSSL trust information\n        try:\n            _, end = der_decode_partial(data)\n            data = data[:end]\n        except ASN1DecodeError:\n            raise KeyImportError('Invalid PEM trusted certificate') from None\n    elif pem_name:\n        raise KeyImportError('Invalid PEM certificate')\n\n    return SSHX509Certificate.construct_from_der(data)\n\n\ndef _decode_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> \\\n            Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a private key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_private(key_info[0], passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    elif fmt == 'pem':\n        pem_name, headers, data = key_info\n        key = _decode_pem_private(pem_name, headers, data, passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    else:\n        key = None\n\n    return key, end\n\n\ndef _decode_public(data: bytes) -> Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a public key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PUBLIC KEY', public=True)\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_public(key_info[0])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        key = _decode_pem_public(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n        key = decode_ssh_public_key(data)\n\n        if algorithm != key.algorithm:\n            raise KeyImportError('Public key algorithm mismatch')\n\n        key.set_comment(comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        key = decode_ssh_public_key(data)\n        key.set_comment(comment)\n    else:\n        fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n        if fmt == 'pem' and key_info[0] == b'OPENSSH':\n            key = _decode_openssh_public(key_info[2])\n        else:\n            key, _ = _decode_private(data, None, False)\n\n            if key:\n                key = key.convert_to_public()\n\n    return key, end\n\n\ndef _decode_certificate(data: bytes) -> \\\n        Tuple[Optional[SSHCertificate], Optional[int]]:\n    \"\"\"Decode a certificate\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'CERTIFICATE', public=True)\n\n    cert: Optional[SSHCertificate]\n\n    if fmt == 'der':\n        cert = _decode_der_certificate(data[:end])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        cert = _decode_pem_certificate(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n\n        if algorithm.startswith(b'x509v3-'):\n            cert = _decode_der_certificate(data, comment)\n        else:\n            cert = decode_ssh_certificate(data, comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        cert = decode_ssh_certificate(data, comment)\n    else:\n        cert = None\n\n    return cert, end\n\n\ndef _decode_private_list(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> Sequence[SSHKey]:\n    \"\"\"Decode a private key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_private(data, passphrase,\n                                   unsafe_skip_rsa_key_validation)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_public_list(data: bytes) -> Sequence[SSHKey]:\n    \"\"\"Decode a public key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_public(data)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_certificate_list(data: bytes) -> Sequence[SSHCertificate]:\n    \"\"\"Decode a certificate list\"\"\"\n\n    certs: List[SSHCertificate] = []\n\n    while data:\n        cert, end = _decode_certificate(data)\n\n        if cert:\n            certs.append(cert)\n\n        data = data[end:]\n\n    return certs\n\n\ndef register_sk_alg(sk_alg: int, handler: Type[SSHKey], *args: object) -> None:\n    \"\"\"Register a new security key algorithm\"\"\"\n\n    _sk_alg_map[sk_alg] = handler, args\n\n\ndef register_public_key_alg(algorithm: bytes, handler: Type[SSHKey],\n                            default: bool,\n                            sig_algorithms: Optional[Sequence[bytes]] = \\\n                                None) -> None:\n    \"\"\"Register a new public key algorithm\"\"\"\n\n    if not sig_algorithms:\n        sig_algorithms = handler.sig_algorithms\n\n    _public_key_algs.extend(sig_algorithms)\n\n    if default:\n        _default_public_key_algs.extend(sig_algorithms)\n\n    _public_key_alg_map[algorithm] = handler\n\n    if handler.pem_name:\n        _pem_map[handler.pem_name] = handler\n\n    if handler.pkcs8_oid: # pragma: no branch\n        _pkcs8_oid_map[handler.pkcs8_oid] = handler\n\n\ndef register_certificate_alg(version: int, algorithm: bytes,\n                             cert_algorithm: bytes,\n                             key_handler: Type[SSHKey],\n                             cert_handler: Type[SSHOpenSSHCertificate],\n                             default: bool) -> None:\n    \"\"\"Register a new certificate algorithm\"\"\"\n\n    _certificate_algs.append(cert_algorithm)\n\n    if default:\n        _default_certificate_algs.append(cert_algorithm)\n\n    _certificate_alg_map[cert_algorithm] = (key_handler, cert_handler)\n\n    _certificate_sig_alg_map[cert_algorithm] = algorithm\n\n    _certificate_version_map[algorithm, version] = \\\n        (cert_algorithm, cert_handler)\n\n\ndef register_x509_certificate_alg(cert_algorithm: bytes, default: bool) -> None:\n    \"\"\"Register a new X.509 certificate algorithm\"\"\"\n\n    if _x509_available: # pragma: no branch\n        _x509_certificate_algs.append(cert_algorithm)\n\n        if default:\n            _default_x509_certificate_algs.append(cert_algorithm)\n\n        _certificate_alg_map[cert_algorithm] = (None, SSHX509CertificateChain)\n\n\ndef get_public_key_algs() -> List[bytes]:\n    \"\"\"Return supported public key algorithms\"\"\"\n\n    return _public_key_algs\n\n\ndef get_default_public_key_algs() -> List[bytes]:\n    \"\"\"Return default public key algorithms\"\"\"\n\n    return _default_public_key_algs\n\n\ndef get_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported certificate-based public key algorithms\"\"\"\n\n    return _certificate_algs\n\n\ndef get_default_certificate_algs() -> List[bytes]:\n    \"\"\"Return default certificate-based public key algorithms\"\"\"\n\n    return _default_certificate_algs\n\n\ndef get_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported X.509 certificate-based public key algorithms\"\"\"\n\n    return _x509_certificate_algs\n\n\ndef get_default_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return default X.509 certificate-based public key algorithms\"\"\"\n\n    return _default_x509_certificate_algs\n\n\ndef decode_ssh_public_key(data: bytes) -> SSHKey:\n    \"\"\"Decode a packetized SSH public key\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n\n        if handler:\n            key_params = handler.decode_ssh_public(packet)\n            packet.check_end()\n\n            key = handler.make_public(key_params)\n            key.algorithm = alg\n            return key\n        else:\n            raise KeyImportError('Unknown key algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except PacketDecodeError:\n        raise KeyImportError('Invalid public key') from None\n\n\ndef decode_ssh_certificate(data: bytes,\n                           comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a packetized SSH certificate\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        key_handler, cert_handler = _certificate_alg_map.get(alg, (None, None))\n\n        if cert_handler:\n            return cert_handler.construct(packet, alg, key_handler, comment)\n        else:\n            raise KeyImportError('Unknown certificate algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except (PacketDecodeError, ValueError):\n        raise KeyImportError('Invalid OpenSSH certificate') from None\n\n\ndef generate_private_key(alg_name: str, comment: _Comment = None,\n                         **kwargs) -> SSHKey:\n    \"\"\"Generate a new private key\n\n       This function generates a new private key of a type matching\n       the requested SSH algorithm. Depending on the algorithm, additional\n       parameters can be passed which affect the generated key.\n\n       Available algorithms include:\n\n           ssh-dss, ssh-rsa, ecdsa-sha2-nistp256, ecdsa-sha2-nistp384,\n           ecdsa-sha2-nistp521, ecdsa-sha2-1.3.132.0.10, ssh-ed25519,\n           ssh-ed448, sk-ecdsa-sha2-nistp256\\\\@openssh.com,\n           sk-ssh-ed25519\\\\@openssh.com\n\n       For dss keys, no parameters are supported. The key size is fixed at\n       1024 bits due to the use of SHA1 signatures.\n\n       For rsa keys, the key size can be specified using the `key_size`\n       parameter, and the RSA public exponent can be changed using the\n       `exponent` parameter. By default, generated keys are 2048 bits\n       with a public exponent of 65537.\n\n       For ecdsa keys, the curve to use is part of the SSH algorithm name\n       and that determines the key size. No other parameters are supported.\n\n       For ed25519 and ed448 keys, no parameters are supported. The key size\n       is fixed by the algorithms at 256 bits and 448 bits, respectively.\n\n       For sk keys, the application name to associate with the generated\n       key can be specified using the `application` parameter. It defaults\n       to `'ssh:'`. The user name to associate with the generated key can\n       be specified using the `user` parameter. It defaults to `'AsyncSSH'`.\n\n       When generating an sk key, a PIN can be provided via the `pin`\n       parameter if the security key requires it.\n\n       The `resident` parameter can be set to `True` to request that a\n       resident key be created on the security key. This allows the key\n       handle and public key information to later be retrieved so that\n       the generated key can be used without having to store any\n       information on the client system. It defaults to `False`.\n\n       You can enable or disable the security key touch requirement by\n       setting the `touch_required` parameter. It defaults to `True`,\n       requiring that the user confirm their presence by touching the\n       security key each time they use it to authenticate.\n\n       :param alg_name:\n           The SSH algorithm name corresponding to the desired type of key.\n       :param comment: (optional)\n           A comment to associate with this key.\n       :param key_size: (optional)\n           The key size in bits for RSA keys.\n       :param exponent: (optional)\n           The public exponent for RSA keys.\n       :param application: (optional)\n           The application name to associate with the generated SK key,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name to associate with the generated SK key, defaulting\n           to `'AsyncSSH'`.\n       :param pin: (optional)\n           The PIN to use to access the security key, defaulting to `None`.\n       :param resident: (optional)\n           Whether or not to create a resident key on the security key,\n           defaulting to `False`.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type alg_name: `str`\n       :type comment: `str`, `bytes`, or `None`\n       :type key_size: `int`\n       :type exponent: `int`\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type resident: `bool`\n       :type touch_required: `bool`\n\n       :returns: An :class:`SSHKey` private key\n\n       :raises: :exc:`KeyGenerationError` if the requested key parameters\n                are unsupported\n    \"\"\"\n\n    algorithm = alg_name.encode('utf-8')\n    handler = _public_key_alg_map.get(algorithm)\n\n    if handler:\n        try:\n            key = handler.generate(algorithm, **kwargs)\n        except (TypeError, ValueError) as exc:\n            raise KeyGenerationError(str(exc)) from None\n    else:\n        raise KeyGenerationError('Unknown algorithm: %s' % alg_name)\n\n    key.set_comment(comment)\n    return key\n\ndef import_private_key(\n        data: BytesOrStr, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Import a private key\n\n       This function imports a private key encoded in PKCS#1 or PKCS#8 DER\n       or PEM format or OpenSSH format. Encrypted private keys can be\n       imported by specifying the passphrase needed to decrypt them.\n\n       :param data:\n           The data to import.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type data: `bytes` or ASCII `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_private(data, passphrase, unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_private_key_and_certs(\n        data: bytes, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Import a private key and optional certificate chain\"\"\"\n\n    key, end = _decode_private(data, passphrase,\n                               unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key, import_certificate_chain(data[end:])\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_public_key(data: BytesOrStr) -> SSHKey:\n    \"\"\"Import a public key\n\n       This function imports a public key encoded in OpenSSH, RFC4716, or\n       PKCS#1 or PKCS#8 DER or PEM format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_public(data)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid public key')\n\n\ndef import_certificate(data: BytesOrStr) -> SSHCertificate:\n    \"\"\"Import a certificate\n\n       This function imports an SSH certificate in DER, PEM, OpenSSH, or\n       RFC4716 format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    cert, _ = _decode_certificate(data)\n\n    if cert:\n        return cert\n    else:\n        raise KeyImportError('Invalid certificate')\n\n\ndef import_certificate_chain(data: bytes) -> Optional[SSHX509CertificateChain]:\n    \"\"\"Import an X.509 certificate chain\"\"\"\n\n    certs = _decode_certificate_list(data)\n\n    chain: Optional[SSHX509CertificateChain]\n\n    if certs:\n        chain = SSHX509CertificateChain.construct_from_certs(certs)\n    else:\n        chain = None\n\n    return chain\n\n\ndef import_certificate_subject(data: str) -> str:\n    \"\"\"Import an X.509 certificate subject name\"\"\"\n\n    try:\n        algorithm, data = data.strip().split(None, 1)\n    except ValueError:\n        raise KeyImportError('Missing certificate subject algorithm') from None\n\n    if algorithm.startswith('x509v3-'):\n        match = _subject_pattern.match(data)\n\n        if match:\n            return data[match.end():]\n\n    raise KeyImportError('Invalid certificate subject')\n\n\ndef read_private_key(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Read a private key from a file\n\n       This function reads a private key from a file. See the function\n       :func:`import_private_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    key = import_private_key(read_file(filename), passphrase,\n                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_private_key_and_certs(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Read a private key and optional certificate chain from a file\"\"\"\n\n    key, cert = import_private_key_and_certs(read_file(filename), passphrase,\n                                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key, cert\n\n\ndef read_public_key(filename: FilePath) -> SSHKey:\n    \"\"\"Read a public key from a file\n\n       This function reads a public key from a file. See the function\n       :func:`import_public_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    key = import_public_key(read_file(filename))\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_certificate(filename: FilePath) -> SSHCertificate:\n    \"\"\"Read a certificate from a file\n\n       This function reads an SSH certificate from a file. See the\n       function :func:`import_certificate` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the certificate from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    return import_certificate(read_file(filename))\n\n\ndef read_private_key_list(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKey]:\n    \"\"\"Read a list of private keys from a file\n\n       This function reads a list of private keys from a file. See the\n       function :func:`import_private_key` for information about the\n       formats supported. If any of the keys are encrypted, they must\n       all be encrypted with the same passphrase.\n\n       :param filename:\n           The file to read the keys from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKey` private keys\n\n    \"\"\"\n\n    keys = _decode_private_list(read_file(filename), passphrase,\n                                unsafe_skip_rsa_key_validation)\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_public_key_list(filename: FilePath) -> Sequence[SSHKey]:\n    \"\"\"Read a list of public keys from a file\n\n       This function reads a list of public keys from a file. See the\n       function :func:`import_public_key` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the keys from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHKey` public keys\n\n    \"\"\"\n\n    keys = _decode_public_list(read_file(filename))\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_certificate_list(filename: FilePath) -> Sequence[SSHCertificate]:\n    \"\"\"Read a list of certificates from a file\n\n       This function reads a list of SSH certificates from a file. See\n       the function :func:`import_certificate` for information about\n       the formats supported.\n\n       :param filename:\n           The file to read the certificates from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHCertificate` certificates\n\n    \"\"\"\n\n    return _decode_certificate_list(read_file(filename))\n\n\ndef load_keypairs(\n        keylist: KeyPairListArg, passphrase: Optional[BytesOrStr] = None,\n        certlist: CertListArg = (), skip_public: bool = False,\n        ignore_encrypted: bool = False,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKeyPair]:\n    \"\"\"Load SSH private keys and optional matching certificates\n\n       This function loads a list of SSH keys and optional matching\n       certificates.\n\n       When certificates are specified, the private key is added to\n       the list both with and without the certificate.\n\n       :param keylist:\n           The list of private keys and certificates to load.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt private keys.\n       :param certlist: (optional)\n           A list of certificates to attempt to pair with the provided\n           list of private keys.\n       :param skip_public: (optional)\n           An internal parameter used to skip public keys and certificates\n           when IdentitiesOnly and IdentityFile are used to specify a\n           mixture of private and public keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type keylist: *see* :ref:`SpecifyingPrivateKeys`\n       :type passphrase: `str` or `bytes`\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n       :type skip_public: `bool`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKeyPair` objects\n\n    \"\"\"\n\n    keys_to_load: Sequence[_KeyPairArg]\n    result: List[SSHKeyPair] = []\n\n    certlist = load_certificates(certlist)\n    certdict = {cert.key.public_data: cert for cert in certlist}\n\n    if isinstance(keylist, (PurePath, str)):\n        try:\n            priv_keys = read_private_key_list(keylist, passphrase,\n                                              unsafe_skip_rsa_key_validation)\n            keys_to_load = [keylist] if len(priv_keys) <= 1 else priv_keys\n        except KeyImportError:\n            keys_to_load = [keylist]\n    elif isinstance(keylist, (tuple, bytes, SSHKey, SSHKeyPair)):\n        keys_to_load = [cast(_KeyPairArg, keylist)]\n    else:\n        keys_to_load = keylist if keylist else []\n\n    for key_to_load in keys_to_load:\n        allow_certs = False\n        key_prefix = None\n        saved_exc = None\n        pubkey_or_certs = None\n        pubkey_to_load: Optional[_KeyArg] = None\n        certs_to_load: Optional[_CertArg] = None\n        key: Union['SSHKey', 'SSHKeyPair']\n\n        if isinstance(key_to_load, (PurePath, str, bytes)):\n            allow_certs = True\n        elif isinstance(key_to_load, tuple):\n            key_to_load, pubkey_or_certs = key_to_load\n\n        try:\n            if isinstance(key_to_load, (PurePath, str)):\n                key_prefix = str(key_to_load)\n\n                if allow_certs:\n                    key, certs_to_load = read_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n\n                    if not certs_to_load:\n                        certs_to_load = key_prefix + '-cert.pub'\n                else:\n                    key = read_private_key(key_to_load, passphrase,\n                                           unsafe_skip_rsa_key_validation)\n\n                pubkey_to_load = key_prefix + '.pub'\n            elif isinstance(key_to_load, bytes):\n                if allow_certs:\n                    key, certs_to_load = import_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n                else:\n                    key = import_private_key(key_to_load, passphrase,\n                                             unsafe_skip_rsa_key_validation)\n            else:\n                key = key_to_load\n        except KeyImportError as exc:\n            if skip_public or \\\n                    (ignore_encrypted and str(exc).startswith('Passphrase')):\n                continue\n\n            raise\n\n        certs: Optional[Sequence[SSHCertificate]]\n\n        if pubkey_or_certs:\n            try:\n                certs = load_certificates(pubkey_or_certs)\n            except (TypeError, OSError, KeyImportError) as exc:\n                saved_exc = exc\n                certs = None\n\n            if not certs:\n                pubkey_to_load = cast(_KeyArg, pubkey_or_certs)\n        elif certs_to_load:\n            try:\n                certs = load_certificates(certs_to_load)\n            except (OSError, KeyImportError):\n                certs = None\n        else:\n            certs = None\n\n        pubkey: Optional[SSHKey]\n\n        if pubkey_to_load:\n            try:\n                if isinstance(pubkey_to_load, (PurePath, str)):\n                    pubkey = read_public_key(pubkey_to_load)\n                elif isinstance(pubkey_to_load, bytes):\n                    pubkey = import_public_key(pubkey_to_load)\n                else:\n                    pubkey = pubkey_to_load\n            except (OSError, KeyImportError):\n                pubkey = None\n            else:\n                saved_exc = None\n        else:\n            pubkey = None\n\n        if saved_exc:\n            raise saved_exc # pylint: disable=raising-bad-type\n\n        if not certs:\n            if isinstance(key, SSHKeyPair):\n                pubdata = key.key_public_data\n            else:\n                pubdata = key.public_data\n\n            cert = certdict.get(pubdata)\n\n            if cert and cert.is_x509:\n                cert = SSHX509CertificateChain.construct_from_certs(certlist)\n        elif len(certs) == 1 and not certs[0].is_x509:\n            cert = certs[0]\n        else:\n            cert = SSHX509CertificateChain.construct_from_certs(certs)\n\n        if isinstance(key, SSHKeyPair):\n            if cert:\n                key.set_certificate(cert)\n\n            result.append(key)\n        else:\n            if cert:\n                result.append(SSHLocalKeyPair(key, pubkey, cert))\n\n            result.append(SSHLocalKeyPair(key, pubkey))\n\n    return result\n\n\ndef load_default_keypairs(passphrase: Optional[BytesOrStr] = None,\n                          certlist: CertListArg = ()) -> \\\n        Sequence[SSHKeyPair]:\n    \"\"\"Return a list of default keys from the user's home directory\"\"\"\n\n    result: List[SSHKeyPair] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                path = Path('~', '.ssh', file).expanduser()\n                result.extend(load_keypairs(path, passphrase, certlist,\n                                            ignore_encrypted=True))\n            except OSError:\n                pass\n\n    return result\n\n\ndef load_public_keys(keylist: KeyListArg) -> Sequence[SSHKey]:\n    \"\"\"Load public keys\n\n       This function loads a list of SSH public keys.\n\n       :param keylist:\n           The list of public keys to load.\n       :type keylist: *see* :ref:`SpecifyingPublicKeys`\n\n       :returns: A list of :class:`SSHKey` objects\n\n    \"\"\"\n\n    if isinstance(keylist, (PurePath, str)):\n        return read_public_key_list(keylist)\n    else:\n        result: List[SSHKey] = []\n\n        for key in keylist:\n            if isinstance(key, (PurePath, str)):\n                key = read_public_key(key)\n            elif isinstance(key, bytes):\n                key = import_public_key(key)\n\n            result.append(key)\n\n        return result\n\n\ndef load_default_host_public_keys() -> Sequence[Union[SSHKey, SSHCertificate]]:\n    \"\"\"Return a list of default host public keys or certificates\"\"\"\n\n    result: List[Union[SSHKey, SSHCertificate]] = []\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                cert = read_certificate(Path(host_key_dir, file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert)\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                key = read_public_key(Path(host_key_dir, file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key)\n\n    return result\n\n\ndef load_certificates(certlist: CertListArg) -> Sequence[SSHCertificate]:\n    \"\"\"Load certificates\n\n       This function loads a list of OpenSSH or X.509 certificates.\n\n       :param certlist:\n           The list of certificates to load.\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n\n       :returns: A list of :class:`SSHCertificate` objects\n\n    \"\"\"\n\n    if isinstance(certlist, SSHCertificate):\n        return [certlist]\n    elif isinstance(certlist, (PurePath, str, bytes)):\n        certlist = [certlist]\n\n    result: List[SSHCertificate] = []\n\n    for cert in certlist:\n        if isinstance(cert, (PurePath, str)):\n            certs = read_certificate_list(cert)\n        elif isinstance(cert, bytes):\n            certs = _decode_certificate_list(cert)\n        elif isinstance(cert, SSHCertificate):\n            certs = [cert]\n        else:\n            certs = cert\n\n        result.extend(certs)\n\n    return result\n\n\ndef load_identities(keylist: IdentityListArg,\n                    skip_private: bool = False) -> Sequence[bytes]:\n    \"\"\"Load public key and certificate identities\"\"\"\n\n    if isinstance(keylist, (bytes, str, PurePath, SSHKey, SSHCertificate)):\n        identities: Sequence[_IdentityArg] = [keylist]\n    else:\n        identities = keylist\n\n    result = []\n\n    for identity in identities:\n        if isinstance(identity, (PurePath, str)):\n            try:\n                pubdata = read_certificate(identity).public_data\n            except KeyImportError:\n                try:\n                    pubdata = read_public_key(identity).public_data\n                except KeyImportError:\n                    if skip_private:\n                        continue\n\n                    raise\n        elif isinstance(identity, (SSHKey, SSHCertificate)):\n            pubdata = identity.public_data\n        else:\n            pubdata = identity\n\n        result.append(pubdata)\n\n    return result\n\n\ndef load_default_identities() -> Sequence[bytes]:\n    \"\"\"Return a list of default public key and certificate identities\"\"\"\n\n    result: List[bytes] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                cert = read_certificate(Path('~', '.ssh', file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert.public_data)\n\n            try:\n                key = read_public_key(Path('~', '.ssh', file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key.public_data)\n\n    return result\n\n\ndef load_resident_keys(pin: str, *, application: str = 'ssh:',\n                       user: Optional[str] = None,\n                       touch_required: bool = True) -> Sequence[SSHKey]:\n    \"\"\"Load keys resident on attached FIDO2 security keys\n\n       This function loads keys resident on any FIDO2 security keys\n       currently attached to the system. The user name associated\n       with each key is returned in the key's comment field.\n\n       :param pin:\n           The PIN to use to access the security keys, defaulting to `None`.\n       :param application: (optional)\n           The application name associated with the keys to load,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name associated with the keys to load. By default,\n           keys for all users are loaded.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type touch_required: `bool`\n\n    \"\"\"\n\n    application = application.encode('utf-8')\n    flags = SSH_SK_USER_PRESENCE_REQD if touch_required else 0\n    reserved = b''\n\n    try:\n        resident_keys = sk_get_resident(application, user, pin)\n    except ValueError as exc:\n        raise KeyImportError(str(exc)) from None\n\n    result: List[SSHKey] = []\n\n    for sk_alg, name, public_value, key_handle in resident_keys:\n        handler, key_params = _sk_alg_map[sk_alg]\n        key_params += (public_value, application, flags, key_handle, reserved)\n\n        key = handler.make_private(key_params)\n        key.set_comment(name)\n\n        result.append(key)\n\n    return result\n", "prompt": "Please write a python function called 'convert_to_public' base the context. This method converts an SSHKey object that contains a private key into one that contains only the corresponding public key. It first decodes asymmetric encryption. Once decrypted, it proceeds to assign a relevant comment and filename to the associated key. Upon completion of these steps, the method returns the processed data as its final output.:param self: SSHKey. An instance of the SSHKey class.\n:return: SSHKey. The SSHKey object that contains only the corresponding public key..\n        The context you need to refer to is as follows: # Copyright (c) 2013-2023 by Ron Frederick <ronf@timeheart.net> and others.\n#\n# This program and the accompanying materials are made available under\n# the terms of the Eclipse Public License v2.0 which accompanies this\n# distribution and is available at:\n#\n#     http://www.eclipse.org/legal/epl-2.0/\n#\n# This program may also be made available under the following secondary\n# licenses when the conditions for such availability set forth in the\n# Eclipse Public License v2.0 are satisfied:\n#\n#    GNU General Public License, Version 2.0, or any later versions of\n#    that license\n#\n# SPDX-License-Identifier: EPL-2.0 OR GPL-2.0-or-later\n#\n# Contributors:\n#     Ron Frederick - initial implementation, API, and documentation\n\n\"\"\"SSH asymmetric encryption handlers\"\"\"\n\nimport binascii\nimport os\nimport re\nimport time\n\nfrom datetime import datetime\nfrom hashlib import md5, sha1, sha256, sha384, sha512\nfrom pathlib import Path, PurePath\nfrom typing import Callable, Dict, List, Mapping, Optional, Sequence, Set\nfrom typing import Tuple, Type, Union, cast\nfrom typing_extensions import Protocol\n\nfrom .crypto import ed25519_available, ed448_available\nfrom .encryption import Encryption\nfrom .sk import sk_available\n\ntry:\n    # pylint: disable=unused-import\n    from .crypto import X509Certificate\n    from .crypto import generate_x509_certificate, import_x509_certificate\n    _x509_available = True\nexcept ImportError: # pragma: no cover\n    _x509_available = False\n\ntry:\n    import bcrypt\n    _bcrypt_available = hasattr(bcrypt, 'kdf')\nexcept ImportError: # pragma: no cover\n    _bcrypt_available = False\n\nfrom .asn1 import ASN1DecodeError, BitString, ObjectIdentifier\nfrom .asn1 import der_encode, der_decode, der_decode_partial\nfrom .crypto import CryptoKey, PyCAKey\nfrom .encryption import get_encryption_params, get_encryption\nfrom .misc import BytesOrStr, DefTuple, FilePath, IPNetwork\nfrom .misc import ip_network, read_file, write_file, parse_time_interval\nfrom .packet import NameList, String, UInt32, UInt64\nfrom .packet import PacketDecodeError, SSHPacket\nfrom .pbe import KeyEncryptionError, pkcs1_encrypt, pkcs8_encrypt\nfrom .pbe import pkcs1_decrypt, pkcs8_decrypt\nfrom .sk import SSH_SK_USER_PRESENCE_REQD, sk_get_resident\n\n\n_Comment = Optional[BytesOrStr]\n_CertPrincipals = Union[str, Sequence[str]]\n_Time = Union[int, float, datetime, str]\n\n_PubKeyAlgMap = Dict[bytes, Type['SSHKey']]\n_CertAlgMap = Dict[bytes, Tuple[Optional[Type['SSHKey']],\n                                Type['SSHCertificate']]]\n_CertSigAlgMap = Dict[bytes, bytes]\n_CertVersionMap = Dict[Tuple[bytes, int],\n                       Tuple[bytes, Type['SSHOpenSSHCertificate']]]\n\n_PEMMap = Dict[bytes, Type['SSHKey']]\n_PKCS8OIDMap = Dict[ObjectIdentifier, Type['SSHKey']]\n_SKAlgMap = Dict[int, Tuple[Type['SSHKey'], Tuple[object, ...]]]\n\n_OpenSSHCertOptions = Dict[str, object]\n_OpenSSHCertParams = Tuple[object, int, int, bytes, bytes,\n                           int, int, bytes, bytes]\n_OpenSSHCertEncoders = Sequence[Tuple[str, Callable[[object], bytes]]]\n_OpenSSHCertDecoders = Dict[bytes, Callable[[SSHPacket], object]]\n\nX509CertPurposes = Union[None, str, Sequence[str]]\n\n_IdentityArg = Union[bytes, FilePath, 'SSHKey', 'SSHCertificate']\nIdentityListArg = Union[_IdentityArg, Sequence[_IdentityArg]]\n_KeyArg = Union[bytes, FilePath, 'SSHKey']\nKeyListArg = Union[FilePath, Sequence[_KeyArg]]\n_CertArg = Union[bytes, FilePath, 'SSHCertificate']\nCertListArg = Union[_CertArg, Sequence[_CertArg]]\n_KeyPairArg = Union['SSHKeyPair', _KeyArg, Tuple[_KeyArg, _CertArg]]\nKeyPairListArg = Union[_KeyPairArg, Sequence[_KeyPairArg]]\n\n\n# Default file names in .ssh directory to read private keys from\n_DEFAULT_KEY_FILES = (\n    ('id_ed25519_sk', ed25519_available and sk_available),\n    ('id_ecdsa_sk', sk_available),\n    ('id_ed448', ed448_available),\n    ('id_ed25519', ed25519_available),\n    ('id_ecdsa', True),\n    ('id_rsa', True),\n    ('id_dsa', True)\n)\n\n# Default directories and file names to read host keys from\n_DEFAULT_HOST_KEY_DIRS = ('/opt/local/etc', '/opt/local/etc/ssh',\n                          '/usr/local/etc', '/usr/local/etc/ssh',\n                          '/etc', '/etc/ssh')\n_DEFAULT_HOST_KEY_FILES = ('ssh_host_ed448_key', 'ssh_host_ed25519_key',\n                           'ssh_host_ecdsa_key', 'ssh_host_rsa_key',\n                           'ssh_host_dsa_key')\n\n_hashes = {'md5': md5, 'sha1': sha1, 'sha256': sha256,\n           'sha384': sha384, 'sha512': sha512}\n\n_public_key_algs: List[bytes] = []\n_default_public_key_algs: List[bytes] = []\n\n_certificate_algs: List[bytes] = []\n_default_certificate_algs: List[bytes] = []\n\n_x509_certificate_algs: List[bytes] = []\n_default_x509_certificate_algs: List[bytes] = []\n\n_public_key_alg_map: _PubKeyAlgMap = {}\n_certificate_alg_map: _CertAlgMap = {}\n_certificate_sig_alg_map: _CertSigAlgMap = {}\n_certificate_version_map: _CertVersionMap = {}\n_pem_map: _PEMMap = {}\n_pkcs8_oid_map: _PKCS8OIDMap = {}\n_sk_alg_map: _SKAlgMap = {}\n\n_abs_date_pattern = re.compile(r'\\d{8}')\n_abs_time_pattern = re.compile(r'\\d{14}')\n\n_subject_pattern = re.compile(r'(?:Distinguished[ -_]?Name|Subject|DN)[=:]?\\s?',\n                              re.IGNORECASE)\n\n# SSH certificate types\nCERT_TYPE_USER = 1\nCERT_TYPE_HOST = 2\n\n# Flag to omit second argument in alg_params\nOMIT = object()\n\n_OPENSSH_KEY_V1 = b'openssh-key-v1\\0'\n_OPENSSH_SALT_LEN = 16\n_OPENSSH_WRAP_LEN = 70\n\n\ndef _parse_time(t: _Time) -> int:\n    \"\"\"Parse a time value\"\"\"\n\n    if isinstance(t, int):\n        return t\n    elif isinstance(t, float):\n        return int(t)\n    elif isinstance(t, datetime):\n        return int(t.timestamp())\n    elif isinstance(t, str):\n        if t == 'now':\n            return int(time.time())\n\n        match = _abs_date_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d').timestamp())\n\n        match = _abs_time_pattern.fullmatch(t)\n        if match:\n            return int(datetime.strptime(t, '%Y%m%d%H%M%S').timestamp())\n\n        try:\n            return int(time.time() + parse_time_interval(t))\n        except ValueError:\n            pass\n\n    raise ValueError('Unrecognized time value')\n\n\ndef _wrap_base64(data: bytes, wrap: int = 64) -> bytes:\n    \"\"\"Break a Base64 value into multiple lines.\"\"\"\n\n    data = binascii.b2a_base64(data)[:-1]\n    return b'\\n'.join(data[i:i+wrap]\n                      for i in range(0, len(data), wrap)) + b'\\n'\n\n\nclass KeyGenerationError(ValueError):\n    \"\"\"Key generation error\n\n       This exception is raised by :func:`generate_private_key`,\n       :meth:`generate_user_certificate() <SSHKey.generate_user_certificate>`\n       or :meth:`generate_host_certificate()\n       <SSHKey.generate_host_certificate>` when the requested parameters are\n       unsupported.\n\n    \"\"\"\n\n\nclass KeyImportError(ValueError):\n    \"\"\"Key import error\n\n       This exception is raised by key import functions when the\n       data provided cannot be imported as a valid key.\n\n    \"\"\"\n\n\nclass KeyExportError(ValueError):\n    \"\"\"Key export error\n\n       This exception is raised by key export functions when the\n       requested format is unknown or encryption is requested for a\n       format which doesn't support it.\n\n    \"\"\"\n\n\nclass SigningKey(Protocol):\n    \"\"\"Protocol for signing a block of data\"\"\"\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with a private key\"\"\"\n\n\nclass VerifyingKey(Protocol):\n    \"\"\"Protocol for verifying a signature on a block of data\"\"\"\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify a signature on a block of data with a public key\"\"\"\n\n\nclass SSHKey:\n    \"\"\"Parent class which holds an asymmetric encryption key\"\"\"\n\n    algorithm: bytes = b''\n    sig_algorithms: Sequence[bytes] = ()\n    cert_algorithms: Sequence[bytes] = ()\n    x509_algorithms: Sequence[bytes] = ()\n    all_sig_algorithms: Set[bytes] = set()\n    default_x509_hash: str = ''\n    pem_name: bytes = b''\n    pkcs8_oid: Optional[ObjectIdentifier] = None\n    use_executor: bool = False\n\n    def __init__(self, key: Optional[CryptoKey] = None):\n        self._key = key\n        self._comment: Optional[bytes] = None\n        self._filename: Optional[bytes] = None\n        self._touch_required = False\n\n    @classmethod\n    def generate(cls, algorithm: bytes, **kwargs) -> 'SSHKey':\n        \"\"\"Generate a new SSH private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_private(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a private key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def make_public(cls, key_params: object) -> 'SSHKey':\n        \"\"\"Construct a public key\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def decode_pkcs1_private(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs1_public(cls, key_data: object) -> object:\n        \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_private(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    @classmethod\n    def decode_pkcs8_public(cls, alg_params: object, data: bytes) -> object:\n        \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    @classmethod\n    def decode_ssh_private(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format private key\"\"\"\n\n    @classmethod\n    def decode_ssh_public(cls, packet: SSHPacket) -> object:\n        \"\"\"Decode an SSH format public key\"\"\"\n\n    @property\n    def private_data(self) -> bytes:\n        \"\"\"Return private key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_private()\n\n    @property\n    def public_data(self) -> bytes:\n        \"\"\"Return public key data in OpenSSH binary format\"\"\"\n\n        return String(self.algorithm) + self.encode_ssh_public()\n\n    @property\n    def pyca_key(self) -> PyCAKey:\n        \"\"\"Return PyCA key for use in X.509 module\"\"\"\n\n        assert self._key is not None\n        return self._key.pyca_key\n\n    def _generate_certificate(self, key: 'SSHKey', version: int, serial: int,\n                              cert_type: int, key_id: str,\n                              principals: _CertPrincipals,\n                              valid_after: _Time, valid_before: _Time,\n                              cert_options: _OpenSSHCertOptions,\n                              sig_alg_name: DefTuple[str],\n                              comment: DefTuple[_Comment]) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        if isinstance(principals, str):\n            principals = [p.strip() for p in principals.split(',')]\n        else:\n            principals = list(principals)\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if sig_alg_name == ():\n            sig_alg = self.sig_algorithms[0]\n        else:\n            sig_alg = cast(str, sig_alg_name).encode()\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        comment: _Comment\n\n        try:\n            algorithm, cert_handler = _certificate_version_map[key.algorithm,\n                                                               version]\n        except KeyError:\n            raise KeyGenerationError('Unknown certificate version') from None\n\n        return cert_handler.generate(self, algorithm, key, serial, cert_type,\n                                     key_id, principals, valid_after,\n                                     valid_before, cert_options,\n                                     sig_alg, comment)\n\n    def _generate_x509_certificate(self, key: 'SSHKey', subject: str,\n                                   issuer: Optional[str],\n                                   serial: Optional[int],\n                                   valid_after: _Time, valid_before: _Time,\n                                   ca: bool, ca_path_len: Optional[int],\n                                   purposes: X509CertPurposes,\n                                   user_principals: _CertPrincipals,\n                                   host_principals: _CertPrincipals,\n                                   hash_name: DefTuple[str],\n                                   comment: DefTuple[_Comment]) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation '\n                                     'requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        valid_after = _parse_time(valid_after)\n        valid_before = _parse_time(valid_before)\n\n        if valid_before <= valid_after:\n            raise ValueError('Valid before time must be later than '\n                             'valid after time')\n\n        if hash_name == ():\n            hash_name = key.default_x509_hash\n\n        if comment == ():\n            comment = key.get_comment_bytes()\n\n        hash_name: str\n        comment: _Comment\n\n        return SSHX509Certificate.generate(self, key, subject, issuer,\n                                           serial, valid_after, valid_before,\n                                           ca, ca_path_len, purposes,\n                                           user_principals, host_principals,\n                                           hash_name, comment)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this key\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key as a byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key as a Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def get_filename(self) -> Optional[bytes]:\n        \"\"\"Return the filename associated with this key\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._filename\n\n    def set_filename(self, filename: Union[None, bytes, FilePath]) -> None:\n        \"\"\"Set the filename associated with this key\n\n           :param filename:\n               The new filename to associate with this key\n           :type filename: `PurePath`, `str`, `bytes`, or `None`\n\n        \"\"\"\n\n        if isinstance(filename, PurePath):\n            filename = str(filename)\n\n        if isinstance(filename, str):\n            filename = filename.encode('utf-8')\n\n        self._filename = filename or None\n\n    def get_fingerprint(self, hash_name: str = 'sha256') -> str:\n        \"\"\"Get the fingerprint of this key\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           :param hash_name: (optional)\n               The hash algorithm to use to construct the fingerprint.\n           :type hash_name: `str`\n\n           :returns: `str`\n\n           :raises: :exc:`ValueError` if the hash name is invalid\n\n        \"\"\"\n\n        try:\n            hash_alg = _hashes[hash_name]\n        except KeyError:\n            raise ValueError('Unknown hash algorithm') from None\n\n        h = hash_alg(self.public_data)\n\n        if hash_name == 'md5':\n            fp = h.hexdigest()\n            fp_text = ':'.join(fp[i:i+2] for i in range(0, len(fp), 2))\n        else:\n            fpb = h.digest()\n            fp_text = binascii.b2a_base64(fpb).decode('ascii')[:-1].strip('=')\n\n        return hash_name.upper() + ':' + fp_text\n\n    def set_touch_required(self, touch_required: bool) -> None:\n        \"\"\"Set whether touch is required when using a security key\"\"\"\n\n        self._touch_required = touch_required\n\n    def sign_raw(self, data: bytes, hash_name: str) -> bytes:\n        \"\"\"Return a raw signature of the specified data\"\"\"\n\n        assert self._key is not None\n        return self._key.sign(data, hash_name)\n\n    def sign_ssh(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Abstract method to compute an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def verify_ssh(self, data: bytes, sig_algorithm: bytes,\n                   packet: SSHPacket) -> bool:\n        \"\"\"Abstract method to verify an SSH-encoded signature\"\"\"\n\n        raise NotImplementedError\n\n    def sign(self, data: bytes, sig_algorithm: bytes) -> bytes:\n        \"\"\"Return an SSH-encoded signature of the specified data\"\"\"\n\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return b''.join((String(sig_algorithm),\n                         self.sign_ssh(data, sig_algorithm)))\n\n    def verify(self, data: bytes, sig: bytes) -> bool:\n        \"\"\"Verify an SSH signature of the specified data using this key\"\"\"\n\n        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, packet)\n        except PacketDecodeError:\n            return False\n\n    def encode_pkcs1_private(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 private key export not supported')\n\n    def encode_pkcs1_public(self) -> object:\n        \"\"\"Export parameters associated with a PKCS#1 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#1 public key export not supported')\n\n    def encode_pkcs8_private(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 private key export not supported')\n\n    def encode_pkcs8_public(self) -> Tuple[object, object]:\n        \"\"\"Export parameters associated with a PKCS#8 public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('PKCS#8 public key export not supported')\n\n    def encode_ssh_private(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH private key export not supported')\n\n    def encode_ssh_public(self) -> bytes:\n        \"\"\"Export parameters associated with an OpenSSH public key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyExportError('OpenSSH public key export not supported')\n\n    def encode_agent_cert_private(self) -> bytes:\n        \"\"\"Encode certificate private key data for agent\"\"\"\n\n        raise NotImplementedError\n\n###The function: convert_to_public###\n    def generate_user_certificate(\n            self, user_key: 'SSHKey', key_id: str, version: int = 1,\n            serial: int = 0, principals: _CertPrincipals = (),\n            valid_after: _Time = 0, valid_before: _Time = 0xffffffffffffffff,\n            force_command: Optional[str] = None,\n            source_address: Optional[Sequence[str]] = None,\n            permit_x11_forwarding: bool = True,\n            permit_agent_forwarding: bool = True,\n            permit_port_forwarding: bool = True, permit_pty: bool = True,\n            permit_user_rc: bool = True, touch_required: bool = True,\n            sig_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH user certificate\n\n           This method returns an SSH user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param force_command: (optional)\n               The command (if any) to force a session to run when this\n               certificate is used.\n           :param source_address: (optional)\n               A list of source addresses and networks for which the\n               certificate is valid, defaulting to all addresses.\n           :param permit_x11_forwarding: (optional)\n               Whether or not to allow this user to use X11 forwarding,\n               defaulting to `True`.\n           :param permit_agent_forwarding: (optional)\n               Whether or not to allow this user to use agent forwarding,\n               defaulting to `True`.\n           :param permit_port_forwarding: (optional)\n               Whether or not to allow this user to use port forwarding,\n               defaulting to `True`.\n           :param permit_pty: (optional)\n               Whether or not to allow this user to allocate a\n               pseudo-terminal, defaulting to `True`.\n           :param permit_user_rc: (optional)\n               Whether or not to run the user rc file when this certificate\n               is used, defaulting to `True`.\n           :param touch_required: (optional)\n               Whether or not to require the user to touch the security key\n               when authenticating with it, defaulting to `True`.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type force_command: `str` or `None`\n           :type source_address: list of ip_address and ip_network values\n           :type permit_x11_forwarding: `bool`\n           :type permit_agent_forwarding: `bool`\n           :type permit_port_forwarding: `bool`\n           :type permit_pty: `bool`\n           :type permit_user_rc: `bool`\n           :type touch_required: `bool`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        cert_options: _OpenSSHCertOptions = {}\n\n        if force_command:\n            cert_options['force-command'] = force_command\n\n        if source_address:\n            cert_options['source-address'] = [ip_network(addr)\n                                              for addr in source_address]\n\n        if permit_x11_forwarding:\n            cert_options['permit-X11-forwarding'] = True\n\n        if permit_agent_forwarding:\n            cert_options['permit-agent-forwarding'] = True\n\n        if permit_port_forwarding:\n            cert_options['permit-port-forwarding'] = True\n\n        if permit_pty:\n            cert_options['permit-pty'] = True\n\n        if permit_user_rc:\n            cert_options['permit-user-rc'] = True\n\n        if not touch_required:\n            cert_options['no-touch-required'] = True\n\n        return self._generate_certificate(user_key, version, serial,\n                                          CERT_TYPE_USER, key_id,\n                                          principals, valid_after,\n                                          valid_before, cert_options,\n                                          sig_alg, comment)\n\n    def generate_host_certificate(self, host_key: 'SSHKey', key_id: str,\n                                  version: int = 1, serial: int = 0,\n                                  principals: _CertPrincipals = (),\n                                  valid_after: _Time = 0,\n                                  valid_before: _Time = 0xffffffffffffffff,\n                                  sig_alg: DefTuple[str] = (),\n                                  comment: DefTuple[_Comment] = ()) -> \\\n            'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH host certificate\n\n           This method returns an SSH host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param key_id:\n               The key identifier associated with this certificate.\n           :param version: (optional)\n               The version of certificate to create, defaulting to 1.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to 0.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param sig_alg: (optional)\n               The algorithm to use when signing the new certificate.\n           :param comment:\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type key_id: `str`\n           :type version: `int`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type sig_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        if comment == ():\n            comment = host_key.get_comment_bytes()\n\n        return self._generate_certificate(host_key, version, serial,\n                                          CERT_TYPE_HOST, key_id,\n                                          principals, valid_after,\n                                          valid_before, {}, sig_alg, comment)\n\n    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 user certificate\n\n           This method returns an X.509 user certificate with the requested\n           attributes signed by this private key.\n\n           :param user_key:\n               The user's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The user names this certificate is valid for. By default,\n               it can be used with any user name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellClient'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               user_key.\n           :type user_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n\n        \"\"\"\n\n        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)\n\n    def generate_x509_host_certificate(\n            self, host_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellServer',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 host certificate\n\n           This method returns an X.509 host certificate with the requested\n           attributes signed by this private key.\n\n           :param host_key:\n               The host's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param principals: (optional)\n               The host names this certificate is valid for. By default,\n               it can be used with any host name.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param purposes: (optional)\n               The allowed purposes for this certificate or `None` to\n               not restrict the certificate's purpose, defaulting to\n               'secureShellServer'\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               host_key.\n           :type host_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type principals: `str` or `list` of `str`\n           :type purposes: `str`, `list` of `str`, or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(host_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, (), principals,\n                                               hash_alg, comment)\n\n    def generate_x509_ca_certificate(self, ca_key: 'SSHKey', subject: str,\n                                     issuer: Optional[str] = None,\n                                     serial: Optional[int] = None,\n                                     valid_after: _Time = 0,\n                                     valid_before: _Time = 0xffffffffffffffff,\n                                     ca_path_len: Optional[int] = None,\n                                     hash_alg: DefTuple[str] = (),\n                                     comment: DefTuple[_Comment] = ()) -> \\\n            'SSHX509Certificate':\n        \"\"\"Generate a new X.509 CA certificate\n\n           This method returns an X.509 CA certificate with the requested\n           attributes signed by this private key.\n\n           :param ca_key:\n               The new CA's public key.\n           :param subject:\n               The subject name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs.\n           :param issuer: (optional)\n               The issuer name in the certificate, expresed as a\n               comma-separated list of X.509 `name=value` pairs. If\n               not specified, the subject name will be used, creating\n               a self-signed certificate.\n           :param serial: (optional)\n               The serial number of the certificate, defaulting to a random\n               64-bit value.\n           :param valid_after: (optional)\n               The earliest time the certificate is valid for, defaulting to\n               no restriction on when the certificate starts being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param valid_before: (optional)\n               The latest time the certificate is valid for, defaulting to\n               no restriction on when the certificate stops being valid.\n               See :ref:`SpecifyingTimeValues` for allowed time specifications.\n           :param ca_path_len: (optional)\n               The maximum number of levels of intermediate CAs allowed\n               below this new CA or `None` to not enforce a limit,\n               defaulting to no limit.\n           :param hash_alg: (optional)\n               The hash algorithm to use when signing the new certificate,\n               defaulting to SHA256.\n           :param comment: (optional)\n               The comment to associate with this certificate. By default,\n               the comment will be set to the comment currently set on\n               ca_key.\n           :type ca_key: :class:`SSHKey`\n           :type subject: `str`\n           :type issuer: `str`\n           :type serial: `int`\n           :type ca_path_len: `int` or `None`\n           :type hash_alg: `str`\n           :type comment: `str`, `bytes`, or `None`\n\n           :returns: :class:`SSHCertificate`\n\n           :raises: | :exc:`ValueError` if the validity times are invalid\n                    | :exc:`KeyGenerationError` if the requested certificate\n                      parameters are unsupported\n        \"\"\"\n\n        return self._generate_x509_certificate(ca_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, True,\n                                               ca_path_len, None, (), (),\n                                               hash_alg, comment)\n\n    def export_private_key(self, format_name: str = 'openssh',\n                           passphrase: Optional[BytesOrStr] = None,\n                           cipher_name: str = 'aes256-cbc',\n                           hash_name: str = 'sha256',\n                           pbe_version: int = 2, rounds: int = 128,\n                           ignore_few_rounds: bool = False) -> bytes:\n        \"\"\"Export a private key in the requested format\n\n           This method returns this object's private key encoded in the\n           requested format. If a passphrase is specified, the key will\n           be exported in encrypted form.\n\n           Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh\n\n           By default, openssh format will be used.\n\n           Encryption is supported in pkcs1-pem, pkcs8-der, pkcs8-pem,\n           and openssh formats. For pkcs1-pem, only the cipher can be\n           specified. For pkcs8-der and pkcs-8, cipher,  hash and PBE\n           version can be specified. For openssh, cipher and rounds\n           can be specified.\n\n           Available ciphers for pkcs1-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, des-cbc, des3-cbc\n\n           Available ciphers for pkcs8-der and pkcs8-pem are:\n\n               aes128-cbc, aes192-cbc, aes256-cbc, blowfish-cbc,\n               cast128-cbc, des-cbc, des2-cbc, des3-cbc, rc4-40, rc4-128\n\n           Available ciphers for openssh format include the following\n           :ref:`encryption algorithms <EncryptionAlgs>`.\n\n           Available hashes include:\n\n               md5, sha1, sha256, sha384, sha512\n\n           Available PBE versions include 1 for PBES1 and 2 for PBES2.\n\n           Not all combinations of cipher, hash, and version are supported.\n\n           The default cipher is aes256. In the pkcs8 formats, the default\n           hash is sha256 and default version is PBES2.\n\n           In openssh format, the default number of rounds is 128.\n\n           .. note:: The openssh format uses bcrypt for encryption, but\n                     unlike the traditional bcrypt cost factor used in\n                     password hashing which scales logarithmically, the\n                     encryption strength here scales linearly with the\n                     rounds value. Since the cipher is rekeyed 64 times\n                     per round, the default rounds value of 128 corresponds\n                     to 8192 total iterations, which is the equivalent of\n                     a bcrypt cost factor of 13.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :param passphrase: (optional)\n               A passphrase to encrypt the private key with.\n           :param cipher_name: (optional)\n               The cipher to use for private key encryption.\n           :param hash_name: (optional)\n               The hash to use for private key encryption.\n           :param pbe_version: (optional)\n               The PBE version to use for private key encryption.\n           :param rounds: (optional)\n               The number of KDF rounds to apply to the passphrase.\n           :type format_name: `str`\n           :type passphrase: `str` or `bytes`\n           :type cipher_name: `str`\n           :type hash_name: `str`\n           :type pbe_version: `int`\n           :type rounds: `int`\n\n           :returns: `bytes` representing the exported private key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_private())\n\n            if passphrase is not None:\n                if format_name == 'pkcs1-der':\n                    raise KeyExportError('PKCS#1 DER format does not support '\n                                         'private key encryption')\n\n                alg, iv, data = pkcs1_encrypt(data, cipher_name, passphrase)\n                headers = (b'Proc-Type: 4,ENCRYPTED\\n' +\n                           b'DEK-Info: ' + alg + b',' +\n                           binascii.b2a_hex(iv).upper() + b'\\n\\n')\n            else:\n                headers = b''\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PRIVATE KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        headers + _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_private()\n\n            if alg_params is OMIT:\n                data = der_encode((0, (self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode((0, (self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if passphrase is not None:\n                data = pkcs8_encrypt(data, cipher_name, hash_name,\n                                     pbe_version, passphrase)\n\n            if format_name == 'pkcs8-pem':\n                if passphrase is not None:\n                    keytype = b'ENCRYPTED PRIVATE KEY'\n                else:\n                    keytype = b'PRIVATE KEY'\n\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            check = os.urandom(4)\n            nkeys = 1\n\n            data = b''.join((check, check, self.private_data,\n                             String(self._comment or b'')))\n\n            cipher: Optional[Encryption]\n\n            if passphrase is not None:\n                try:\n                    alg = cipher_name.encode('ascii')\n                    key_size, iv_size, block_size, _, _, _ = \\\n                        get_encryption_params(alg)\n                except (KeyError, UnicodeEncodeError):\n                    raise KeyEncryptionError('Unknown cipher: %s' %\n                                             cipher_name) from None\n\n                if not _bcrypt_available: # pragma: no cover\n                    raise KeyExportError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n                kdf = b'bcrypt'\n                salt = os.urandom(_OPENSSH_SALT_LEN)\n                kdf_data = b''.join((String(salt), UInt32(rounds)))\n\n                if isinstance(passphrase, str):\n                    passphrase = passphrase.encode('utf-8')\n\n                key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                 rounds, ignore_few_rounds)\n\n                cipher = get_encryption(alg, key[:key_size], key[key_size:])\n                block_size = max(block_size, 8)\n            else:\n                cipher = None\n                alg = b'none'\n                kdf = b'none'\n                kdf_data = b''\n                block_size = 8\n                mac = b''\n\n            pad = len(data) % block_size\n            if pad: # pragma: no branch\n                data = data + bytes(range(1, block_size + 1 - pad))\n\n            if cipher:\n                data, mac = cipher.encrypt_packet(0, b'', data)\n            else:\n                mac = b''\n\n            data = b''.join((_OPENSSH_KEY_V1, String(alg), String(kdf),\n                             String(kdf_data), UInt32(nkeys),\n                             String(self.public_data), String(data), mac))\n\n            return (b'-----BEGIN OPENSSH PRIVATE KEY-----\\n' +\n                    _wrap_base64(data, _OPENSSH_WRAP_LEN) +\n                    b'-----END OPENSSH PRIVATE KEY-----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def export_public_key(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a public key in the requested format\n\n           This method returns this object's public key encoded in the\n           requested format. Available formats include:\n\n               pkcs1-der, pkcs1-pem, pkcs8-der, pkcs8-pem, openssh, rfc4716\n\n           By default, openssh format will be used.\n\n           :param format_name: (optional)\n               The format to export the key in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported public key\n\n        \"\"\"\n\n        if format_name in ('pkcs1-der', 'pkcs1-pem'):\n            data = der_encode(self.encode_pkcs1_public())\n\n            if format_name == 'pkcs1-pem':\n                keytype = self.pem_name + b' PUBLIC KEY'\n                data = (b'-----BEGIN ' + keytype + b'-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END ' + keytype + b'-----\\n')\n\n            return data\n        elif format_name in ('pkcs8-der', 'pkcs8-pem'):\n            alg_params, pkcs8_data = self.encode_pkcs8_public()\n            pkcs8_data = BitString(pkcs8_data)\n\n            if alg_params is OMIT:\n                data = der_encode(((self.pkcs8_oid,), pkcs8_data))\n            else:\n                data = der_encode(((self.pkcs8_oid, alg_params), pkcs8_data))\n\n            if format_name == 'pkcs8-pem':\n                data = (b'-----BEGIN PUBLIC KEY-----\\n' +\n                        _wrap_base64(data) +\n                        b'-----END PUBLIC KEY-----\\n')\n\n            return data\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs))\n\n    def write_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Write a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which writes the exported key data to a file.\n\n           :param filename:\n               The filename to write the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs))\n\n    def append_private_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a private key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_private_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the private key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_private_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_private_key(*args, **kwargs), 'ab')\n\n    def append_public_key(self, filename: FilePath, *args, **kwargs) -> None:\n        \"\"\"Append a public key to a file in the requested format\n\n           This method is a simple wrapper around :meth:`export_public_key`\n           which appends the exported key data to an existing file.\n\n           :param filename:\n               The filename to append the public key to.\n           :param \\\\*args,\\\\ \\\\*\\\\*kwargs:\n               Additional arguments to pass through to\n               :meth:`export_public_key`.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_public_key(*args, **kwargs), 'ab')\n\n\nclass SSHCertificate:\n    \"\"\"Parent class which holds an SSH certificate\"\"\"\n\n    is_x509 = False\n    is_x509_chain = False\n\n    def __init__(self, algorithm: bytes, sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes], key: SSHKey,\n                 public_data: bytes, comment: _Comment):\n        self.algorithm = algorithm\n        self.sig_algorithms = sig_algorithms\n        self.host_key_algorithms = host_key_algorithms\n        self.key = key\n        self.public_data = public_data\n\n        self.set_comment(comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        raise NotImplementedError\n\n    def __eq__(self, other: object) -> bool:\n        return (isinstance(other, type(self)) and\n                self.public_data == other.public_data)\n\n    def __hash__(self) -> int:\n        return hash(self.public_data)\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this certificate\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def has_comment(self) -> bool:\n        \"\"\"Return whether a comment is set for this certificate\n\n           :returns: `bool`\n\n        \"\"\"\n\n        return bool(self._comment)\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this certificate as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this certificate as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        return self._comment.decode(encoding, errors) if self._comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this certificate\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def export_certificate(self, format_name: str = 'openssh') -> bytes:\n        \"\"\"Export a certificate in the requested format\n\n           This function returns this certificate encoded in the requested\n           format. Available formats include:\n\n               der, pem, openssh, rfc4716\n\n           By default, OpenSSH format will be used.\n\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type format_name: `str`\n\n           :returns: `bytes` representing the exported certificate\n\n        \"\"\"\n\n        if self.is_x509:\n            if format_name == 'rfc4716':\n                raise KeyExportError('RFC4716 format is not supported for '\n                                     'X.509 certificates')\n        else:\n            if format_name in ('der', 'pem'):\n                raise KeyExportError('DER and PEM formats are not supported '\n                                     'for OpenSSH certificates')\n\n        if format_name == 'der':\n            return self.public_data\n        elif format_name == 'pem':\n            return (b'-----BEGIN CERTIFICATE-----\\n' +\n                    _wrap_base64(self.public_data) +\n                    b'-----END CERTIFICATE-----\\n')\n        elif format_name == 'openssh':\n            if self._comment:\n                comment = b' ' + self._comment\n            else:\n                comment = b''\n\n            return (self.algorithm + b' ' +\n                    binascii.b2a_base64(self.public_data)[:-1] +\n                    comment + b'\\n')\n        elif format_name == 'rfc4716':\n            if self._comment:\n                comment = (b'Comment: \"' + self._comment + b'\"\\n')\n            else:\n                comment = b''\n\n            return (b'---- BEGIN SSH2 PUBLIC KEY ----\\n' +\n                    comment + _wrap_base64(self.public_data) +\n                    b'---- END SSH2 PUBLIC KEY ----\\n')\n        else:\n            raise KeyExportError('Unknown export format')\n\n    def write_certificate(self, filename: FilePath,\n                          format_name: str = 'openssh') -> None:\n        \"\"\"Write a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which writes the exported certificate to a file.\n\n           :param filename:\n               The filename to write the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name))\n\n    def append_certificate(self, filename: FilePath,\n                           format_name: str = 'openssh') -> None:\n        \"\"\"Append a certificate to a file in the requested format\n\n           This function is a simple wrapper around export_certificate\n           which appends the exported certificate to an existing file.\n\n           :param filename:\n               The filename to append the certificate to.\n           :param format_name: (optional)\n               The format to export the certificate in.\n           :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n           :type format_name: `str`\n\n        \"\"\"\n\n        write_file(filename, self.export_certificate(format_name), 'ab')\n\n\nclass SSHOpenSSHCertificate(SSHCertificate):\n    \"\"\"Class which holds an OpenSSH certificate\"\"\"\n\n    _user_option_encoders: _OpenSSHCertEncoders = ()\n    _user_extension_encoders: _OpenSSHCertEncoders = ()\n    _host_option_encoders: _OpenSSHCertEncoders = ()\n    _host_extension_encoders: _OpenSSHCertEncoders = ()\n\n    _user_option_decoders: _OpenSSHCertDecoders = {}\n    _user_extension_decoders: _OpenSSHCertDecoders = {}\n    _host_option_decoders: _OpenSSHCertDecoders = {}\n    _host_extension_decoders: _OpenSSHCertDecoders = {}\n\n    def __init__(self, algorithm: bytes, key: SSHKey, data: bytes,\n                 principals: Sequence[str], options: _OpenSSHCertOptions,\n                 signing_key: SSHKey, serial: int, cert_type: int,\n                 key_id: str, valid_after: int, valid_before: int,\n                 comment: _Comment):\n        super().__init__(algorithm, key.sig_algorithms,\n                         key.cert_algorithms or (algorithm,),\n                         key, data, comment)\n\n        self.principals = principals\n        self.options = options\n        self.signing_key = signing_key\n\n        self._serial = serial\n        self._cert_type = cert_type\n        self._key_id = key_id\n        self._valid_after = valid_after\n        self._valid_before = valid_before\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', algorithm: bytes, key: 'SSHKey',\n                 serial: int, cert_type: int, key_id: str,\n                 principals: Sequence[str], valid_after: int,\n                 valid_before: int, options: _OpenSSHCertOptions,\n                 sig_alg: bytes, comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Generate a new SSH certificate\"\"\"\n\n        principal_bytes = b''.join(String(p) for p in principals)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._encode_options(options,\n                                               cls._user_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._user_extension_encoders)\n        else:\n            cert_options = cls._encode_options(options,\n                                               cls._host_option_encoders)\n            cert_extensions = cls._encode_options(options,\n                                                  cls._host_extension_encoders)\n\n        key = key.convert_to_public()\n\n        data = b''.join((String(algorithm),\n                         cls._encode(key, serial, cert_type, key_id,\n                                     principal_bytes, valid_after,\n                                     valid_before, cert_options,\n                                     cert_extensions),\n                         String(signing_key.public_data)))\n\n        data += String(signing_key.sign(data, sig_alg))\n\n        signing_key = signing_key.convert_to_public()\n\n        return cls(algorithm, key, data, principals, options, signing_key,\n                   serial, cert_type, key_id, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHOpenSSHCertificate':\n        \"\"\"Construct an SSH certificate from packetized data\"\"\"\n\n        assert key_handler is not None\n\n        key_params, serial, cert_type, key_id, \\\n            principals, valid_after, valid_before, \\\n            options, extensions = cls._decode(packet, key_handler)\n\n        signing_key = decode_ssh_public_key(packet.get_string())\n        data = packet.get_consumed_payload()\n        signature = packet.get_string()\n        packet.check_end()\n\n        if not signing_key.verify(data, signature):\n            raise KeyImportError('Invalid certificate signature')\n\n        key = key_handler.make_public(key_params)\n        data = packet.get_consumed_payload()\n\n        try:\n            key_id_bytes = key_id.decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in key ID') from None\n\n        packet = SSHPacket(principals)\n        principals: List[str] = []\n\n        while packet:\n            try:\n                principal = packet.get_string().decode('utf-8')\n            except UnicodeDecodeError:\n                raise KeyImportError('Invalid characters in principal '\n                                     'name') from None\n\n            principals.append(principal)\n\n        if cert_type == CERT_TYPE_USER:\n            cert_options = cls._decode_options(\n                options, cls._user_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._user_extension_decoders, False))\n        elif cert_type == CERT_TYPE_HOST:\n            cert_options = cls._decode_options(\n                options, cls._host_option_decoders, True)\n            cert_options.update(cls._decode_options(\n                extensions, cls._host_extension_decoders, False))\n        else:\n            raise KeyImportError('Unknown certificate type')\n\n        return cls(algorithm, key, data, principals, cert_options, signing_key,\n                   serial, cert_type, key_id_bytes, valid_after, valid_before,\n                   comment)\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n\n        \"\"\"Encode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode an SSH certificate\"\"\"\n\n        raise NotImplementedError\n\n    @staticmethod\n    def _encode_options(options: _OpenSSHCertOptions,\n                        encoders: _OpenSSHCertEncoders) -> bytes:\n        \"\"\"Encode options found in this certificate\"\"\"\n\n        result = []\n\n        for name, encoder in encoders:\n            value = options.get(name)\n            if value:\n                result.append(String(name) + String(encoder(value)))\n\n        return b''.join(result)\n\n    @staticmethod\n    def _encode_bool(_value: object) -> bytes:\n        \"\"\"Encode a boolean option value\"\"\"\n\n        return b''\n\n    @staticmethod\n    def _encode_force_cmd(force_command: object) -> bytes:\n        \"\"\"Encode a force-command option\"\"\"\n\n        return String(cast(BytesOrStr, force_command))\n\n    @staticmethod\n    def _encode_source_addr(source_address: object) -> bytes:\n        \"\"\"Encode a source-address option\"\"\"\n\n        return NameList(str(addr).encode('ascii')\n                        for addr in cast(Sequence[IPNetwork], source_address))\n\n    @staticmethod\n    def _decode_bool(_packet: SSHPacket) -> bool:\n        \"\"\"Decode a boolean option value\"\"\"\n\n        return True\n\n    @staticmethod\n    def _decode_force_cmd(packet: SSHPacket) -> str:\n        \"\"\"Decode a force-command option\"\"\"\n\n        try:\n            return packet.get_string().decode('utf-8')\n        except UnicodeDecodeError:\n            raise KeyImportError('Invalid characters in command') from None\n\n    @staticmethod\n    def _decode_source_addr(packet: SSHPacket) -> Sequence[IPNetwork]:\n        \"\"\"Decode a source-address option\"\"\"\n\n        try:\n            return [ip_network(addr.decode('ascii'))\n                    for addr in packet.get_namelist()]\n        except (UnicodeDecodeError, ValueError):\n            raise KeyImportError('Invalid source address') from None\n\n    @staticmethod\n    def _decode_options(options: bytes, decoders: _OpenSSHCertDecoders,\n                        critical: bool = True) -> _OpenSSHCertOptions:\n        \"\"\"Decode options found in this certificate\"\"\"\n\n        packet = SSHPacket(options)\n        result: _OpenSSHCertOptions = {}\n\n        while packet:\n            name = packet.get_string()\n\n            decoder = decoders.get(name)\n            if decoder:\n                data_packet = SSHPacket(packet.get_string())\n                result[name.decode('ascii')] = decoder(data_packet)\n                data_packet.check_end()\n            elif critical:\n                raise KeyImportError('Unrecognized critical option: %s' %\n                                     name.decode('ascii', errors='replace'))\n\n        return result\n\n    def validate(self, cert_type: int, principal: str) -> None:\n        \"\"\"Validate an OpenSSH certificate\"\"\"\n\n        if self._cert_type != cert_type:\n            raise ValueError('Invalid certificate type')\n\n        now = time.time()\n\n        if now < self._valid_after:\n            raise ValueError('Certificate not yet valid')\n\n        if now >= self._valid_before:\n            raise ValueError('Certificate expired')\n\n        if principal and self.principals and principal not in self.principals:\n            raise ValueError('Certificate principal mismatch')\n\n\nclass SSHOpenSSHCertificateV01(SSHOpenSSHCertificate):\n    \"\"\"Encoder/decoder class for version 01 OpenSSH certificates\"\"\"\n\n    _user_option_encoders = (\n        ('force-command',           SSHOpenSSHCertificate._encode_force_cmd),\n        ('source-address',          SSHOpenSSHCertificate._encode_source_addr)\n    )\n\n    _user_extension_encoders = (\n        ('permit-X11-forwarding',   SSHOpenSSHCertificate._encode_bool),\n        ('permit-agent-forwarding', SSHOpenSSHCertificate._encode_bool),\n        ('permit-port-forwarding',  SSHOpenSSHCertificate._encode_bool),\n        ('permit-pty',              SSHOpenSSHCertificate._encode_bool),\n        ('permit-user-rc',          SSHOpenSSHCertificate._encode_bool),\n        ('no-touch-required',       SSHOpenSSHCertificate._encode_bool)\n    )\n\n    _user_option_decoders = {\n        b'force-command':           SSHOpenSSHCertificate._decode_force_cmd,\n        b'source-address':          SSHOpenSSHCertificate._decode_source_addr\n    }\n\n    _user_extension_decoders = {\n        b'permit-X11-forwarding':   SSHOpenSSHCertificate._decode_bool,\n        b'permit-agent-forwarding': SSHOpenSSHCertificate._decode_bool,\n        b'permit-port-forwarding':  SSHOpenSSHCertificate._decode_bool,\n        b'permit-pty':              SSHOpenSSHCertificate._decode_bool,\n        b'permit-user-rc':          SSHOpenSSHCertificate._decode_bool,\n        b'no-touch-required':       SSHOpenSSHCertificate._decode_bool\n    }\n\n    @classmethod\n    def _encode(cls, key: SSHKey, serial: int, cert_type: int, key_id: str,\n                principals: bytes, valid_after: int, valid_before: int,\n                options: bytes, extensions: bytes) -> bytes:\n        \"\"\"Encode a version 01 SSH certificate\"\"\"\n\n        return b''.join((String(os.urandom(32)), key.encode_ssh_public(),\n                         UInt64(serial), UInt32(cert_type), String(key_id),\n                         String(principals), UInt64(valid_after),\n                         UInt64(valid_before), String(options),\n                         String(extensions), String('')))\n\n    @classmethod\n    def _decode(cls, packet: SSHPacket,\n                key_handler: Type[SSHKey]) -> _OpenSSHCertParams:\n        \"\"\"Decode a version 01 SSH certificate\"\"\"\n\n        _ = packet.get_string()                             # nonce\n        key_params = key_handler.decode_ssh_public(packet)\n        serial = packet.get_uint64()\n        cert_type = packet.get_uint32()\n        key_id = packet.get_string()\n        principals = packet.get_string()\n        valid_after = packet.get_uint64()\n        valid_before = packet.get_uint64()\n        options = packet.get_string()\n        extensions = packet.get_string()\n        _ = packet.get_string()                             # reserved\n\n        return (key_params, serial, cert_type, key_id, principals,\n                valid_after, valid_before, options, extensions)\n\n\nclass SSHX509Certificate(SSHCertificate):\n    \"\"\"Encoder/decoder class for SSH X.509 certificates\"\"\"\n\n    is_x509 = True\n\n    def __init__(self, key: SSHKey, x509_cert: 'X509Certificate',\n                 comment: _Comment = None):\n        super().__init__(b'x509v3-' + key.algorithm, key.x509_algorithms,\n                         key.x509_algorithms, key, x509_cert.data,\n                         x509_cert.comment or comment)\n\n        self.subject = x509_cert.subject\n        self.issuer = x509_cert.issuer\n        self.issuer_hash = x509_cert.issuer_hash\n        self.user_principals = x509_cert.user_principals\n        self.x509_cert = x509_cert\n\n    def _expand_trust_store(self, cert: 'SSHX509Certificate',\n                            trusted_cert_paths: Sequence[FilePath],\n                            trust_store: Set['SSHX509Certificate']) -> None:\n        \"\"\"Look up certificates by issuer hash to build a trust store\"\"\"\n\n        issuer_hash = cert.issuer_hash\n\n        for path in trusted_cert_paths:\n            idx = 0\n\n            try:\n                while True:\n                    cert_path = Path(path, issuer_hash + '.' + str(idx))\n                    idx += 1\n\n                    c = cast('SSHX509Certificate', read_certificate(cert_path))\n\n                    if c.subject != cert.issuer or c in trust_store:\n                        continue\n\n                    trust_store.add(c)\n                    self._expand_trust_store(c, trusted_cert_paths, trust_store)\n            except (OSError, KeyImportError):\n                pass\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        raise RuntimeError # pragma: no cover\n\n    @classmethod\n    def generate(cls, signing_key: 'SSHKey', key: 'SSHKey', subject: str,\n                 issuer: Optional[str], serial: Optional[int],\n                 valid_after: int, valid_before: int, ca: bool,\n                 ca_path_len: Optional[int], purposes: X509CertPurposes,\n                 user_principals: _CertPrincipals,\n                 host_principals: _CertPrincipals, hash_name: str,\n                 comment: _Comment) -> 'SSHX509Certificate':\n        \"\"\"Generate a new X.509 certificate\"\"\"\n\n        key = key.convert_to_public()\n\n        x509_cert = generate_x509_certificate(signing_key.pyca_key,\n                                              key.pyca_key, subject, issuer,\n                                              serial, valid_after, valid_before,\n                                              ca, ca_path_len, purposes,\n                                              user_principals, host_principals,\n                                              hash_name, comment)\n\n        return cls(key, x509_cert)\n\n    @classmethod\n    def construct_from_der(cls, data: bytes,\n                           comment: _Comment = None) -> 'SSHX509Certificate':\n        \"\"\"Construct an SSH X.509 certificate from DER data\"\"\"\n\n        try:\n            x509_cert = import_x509_certificate(data)\n            key = import_public_key(x509_cert.key_data)\n        except ValueError as exc:\n            raise KeyImportError(str(exc)) from None\n\n        return cls(key, x509_cert, comment)\n\n    def validate_chain(self, trust_chain: Sequence['SSHX509Certificate'],\n                       trusted_certs: Sequence['SSHX509Certificate'],\n                       trusted_cert_paths: Sequence[FilePath],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        trust_store = set(c for c in trust_chain if c.subject != c.issuer) | \\\n            set(c for c in trusted_certs)\n\n        if trusted_cert_paths:\n            self._expand_trust_store(self, trusted_cert_paths, trust_store)\n\n            for c in trust_chain:\n                self._expand_trust_store(c, trusted_cert_paths, trust_store)\n\n        self.x509_cert.validate([c.x509_cert for c in trust_store],\n                                purposes, user_principal, host_principal)\n\n\nclass SSHX509CertificateChain(SSHCertificate):\n    \"\"\"Encoder/decoder class for an SSH X.509 certificate chain\"\"\"\n\n    is_x509_chain = True\n\n    def __init__(self, algorithm: bytes, certs: Sequence[SSHCertificate],\n                 ocsp_responses: Sequence[bytes], comment: _Comment):\n        key = certs[0].key\n        data = self._public_data(algorithm, certs, ocsp_responses)\n\n        super().__init__(algorithm, key.x509_algorithms, key.x509_algorithms,\n                         key, data, comment)\n\n        x509_certs = cast(Sequence[SSHX509Certificate], certs)\n        first_cert = x509_certs[0]\n        last_cert = x509_certs[-1]\n\n        self.subject = first_cert.subject\n        self.issuer = last_cert.issuer\n        self.user_principals = first_cert.user_principals\n\n        self._certs = x509_certs\n        self._ocsp_responses = ocsp_responses\n\n    @staticmethod\n    def _public_data(algorithm: bytes, certs: Sequence[SSHCertificate],\n                     ocsp_responses: Sequence[bytes]) -> bytes:\n        \"\"\"Return the X509 chain public data\"\"\"\n\n        return (String(algorithm) + UInt32(len(certs)) +\n                b''.join(String(c.public_data) for c in certs) +\n                UInt32(len(ocsp_responses)) +\n                b''.join(String(resp) for resp in ocsp_responses))\n\n    @classmethod\n    def construct(cls, packet: SSHPacket, algorithm: bytes,\n                  key_handler: Optional[Type[SSHKey]],\n                  comment: _Comment) -> 'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate from packetized data\"\"\"\n\n        cert_count = packet.get_uint32()\n        certs = [import_certificate(packet.get_string())\n                 for _ in range(cert_count)]\n\n        ocsp_resp_count = packet.get_uint32()\n        ocsp_responses = [packet.get_string() for _ in range(ocsp_resp_count)]\n\n        packet.check_end()\n\n        if not certs:\n            raise KeyImportError('No certificates present')\n\n        return cls(algorithm, certs, ocsp_responses, comment)\n\n    @classmethod\n    def construct_from_certs(cls, certs: Sequence['SSHCertificate']) -> \\\n            'SSHX509CertificateChain':\n        \"\"\"Construct an SSH X.509 certificate chain from certificates\"\"\"\n\n        cert = certs[0]\n\n        return cls(cert.algorithm, certs, (), cert.get_comment_bytes())\n\n    def adjust_public_data(self, algorithm: bytes) -> bytes:\n        \"\"\"Adjust public data to reflect chosen signature algorithm\"\"\"\n\n        return self._public_data(algorithm, self._certs, self._ocsp_responses)\n\n    def validate_chain(self, trusted_certs: Sequence[SSHX509Certificate],\n                       trusted_cert_paths: Sequence[FilePath],\n                       revoked_certs: Set[SSHX509Certificate],\n                       purposes: X509CertPurposes, user_principal: str = '',\n                       host_principal: str = '') -> None:\n        \"\"\"Validate an X.509 certificate chain\"\"\"\n\n        if revoked_certs:\n            for cert in self._certs:\n                if cert in revoked_certs:\n                    raise ValueError('Revoked X.509 certificate in '\n                                     'certificate chain')\n\n        self._certs[0].validate_chain(self._certs[1:], trusted_certs,\n                                      trusted_cert_paths, purposes,\n                                      user_principal, host_principal)\n\n\nclass SSHKeyPair:\n    \"\"\"Parent class which represents an asymmetric key pair\n\n       This is an abstract class which provides a method to sign data\n       with a private key and members to access the corresponding\n       algorithm and public key or certificate information needed to\n       identify what key was used for signing.\n\n    \"\"\"\n\n    _key_type = 'unknown'\n\n    def __init__(self, algorithm: bytes, sig_algorithm: bytes,\n                 sig_algorithms: Sequence[bytes],\n                 host_key_algorithms: Sequence[bytes],\n                 public_data: bytes, comment: _Comment,\n                 cert: Optional[SSHCertificate] = None,\n                 filename: Optional[bytes] = None,\n                 use_executor: bool = False):\n        self.key_algorithm = algorithm\n        self.key_public_data = public_data\n\n        self.set_comment(comment)\n        self._cert = cert\n        self._filename = filename\n\n        self.use_executor = use_executor\n\n        if cert:\n            if cert.key.public_data != self.key_public_data:\n                raise ValueError('Certificate key mismatch')\n\n            self.algorithm = cert.algorithm\n\n            if cert.is_x509_chain:\n                self.sig_algorithm = cert.algorithm\n            else:\n                self.sig_algorithm = sig_algorithm\n\n            self.sig_algorithms = cert.sig_algorithms\n            self.host_key_algorithms = cert.host_key_algorithms\n            self.public_data = cert.public_data\n        else:\n            self.algorithm = algorithm\n            self.sig_algorithm = algorithm\n            self.sig_algorithms = sig_algorithms\n            self.host_key_algorithms = host_key_algorithms\n            self.public_data = public_data\n\n    def get_key_type(self) -> str:\n        \"\"\"Return what type of key pair this is\n\n           This method returns 'local' for locally loaded keys, and\n           'agent' for keys managed by an SSH agent.\n\n        \"\"\"\n\n        return self._key_type\n\n    @property\n    def has_cert(self) -> bool:\n        \"\"\" Return if this key pair has an associated cert\"\"\"\n\n        return bool(self._cert)\n\n    @property\n    def has_x509_chain(self) -> bool:\n        \"\"\" Return if this key pair has an associated X.509 cert chain\"\"\"\n\n        return self._cert.is_x509_chain if self._cert else False\n\n    def get_algorithm(self) -> str:\n        \"\"\"Return the algorithm associated with this key pair\"\"\"\n\n        return self.algorithm.decode('ascii')\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        # pylint: disable=no-self-use\n        raise KeyImportError('Private key export to agent not supported')\n\n    def get_comment_bytes(self) -> Optional[bytes]:\n        \"\"\"Return the comment associated with this key pair as a\n           byte string\n\n           :returns: `bytes` or `None`\n\n        \"\"\"\n\n        return self._comment or self._filename\n\n    def get_comment(self, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> Optional[str]:\n        \"\"\"Return the comment associated with this key pair as a\n           Unicode string\n\n           :param encoding:\n               The encoding to use to decode the comment as a Unicode\n               string, defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode decode errors\n           :type encoding: `str`\n           :type errors: `str`\n\n           :returns: `str` or `None`\n\n           :raises: :exc:`UnicodeDecodeError` if the comment cannot be\n                    decoded using the specified encoding\n\n        \"\"\"\n\n        comment = self.get_comment_bytes()\n\n        return comment.decode(encoding, errors) if comment else None\n\n    def set_comment(self, comment: _Comment, encoding: str = 'utf-8',\n                    errors: str = 'strict') -> None:\n        \"\"\"Set the comment associated with this key pair\n\n           :param comment:\n               The new comment to associate with this key\n           :param encoding:\n               The Unicode encoding to use to encode the comment,\n               defaulting to UTF-8\n           :param errors:\n               The error handling scheme to use for Unicode encode errors\n           :type comment: `str`, `bytes`, or `None`\n           :type encoding: `str`\n           :type errors: `str`\n\n           :raises: :exc:`UnicodeEncodeError` if the comment cannot be\n                    encoded using the specified encoding\n\n        \"\"\"\n\n        if isinstance(comment, str):\n            comment = comment.encode(encoding, errors)\n\n        self._comment = comment or None\n\n    def set_certificate(self, cert: SSHCertificate) -> None:\n        \"\"\"Set certificate to use with this key\"\"\"\n\n        if cert.key.public_data != self.key_public_data:\n            raise ValueError('Certificate key mismatch')\n\n        self._cert = cert\n        self.algorithm = cert.algorithm\n\n        if cert.is_x509_chain:\n            self.sig_algorithm = cert.algorithm\n        else:\n            self.sig_algorithm = self.key_algorithm\n\n        self.sig_algorithms = cert.sig_algorithms\n        self.host_key_algorithms = cert.host_key_algorithms\n        self.public_data = cert.public_data\n\n    def set_sig_algorithm(self, sig_algorithm: bytes) -> None:\n        \"\"\"Set the signature algorithm to use when signing data\"\"\"\n\n        try:\n            sig_algorithm = _certificate_sig_alg_map[sig_algorithm]\n        except KeyError:\n            pass\n\n        self.sig_algorithm = sig_algorithm\n\n        if not self.has_cert:\n            self.algorithm = sig_algorithm\n        elif self.has_x509_chain:\n            self.algorithm = sig_algorithm\n\n            cert = cast('SSHX509CertificateChain', self._cert)\n            self.public_data = cert.adjust_public_data(sig_algorithm)\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        # pylint: disable=no-self-use\n        raise RuntimeError # pragma: no cover\n\n\nclass SSHLocalKeyPair(SSHKeyPair):\n    \"\"\"Class which holds a local asymmetric key pair\n\n       This class holds a private key and associated public data\n       which can either be the matching public key or a certificate\n       which has signed that public key.\n\n    \"\"\"\n\n    _key_type = 'local'\n\n    def __init__(self, key: SSHKey, pubkey: Optional[SSHKey] = None,\n                 cert: Optional[SSHCertificate] = None):\n        if pubkey and pubkey.public_data != key.public_data:\n            raise ValueError('Public key mismatch')\n\n        if key.has_comment():\n            comment = key.get_comment_bytes()\n        elif cert and cert.has_comment():\n            comment = cert.get_comment_bytes()\n        elif pubkey and pubkey.has_comment():\n            comment = pubkey.get_comment_bytes()\n        else:\n            comment = None\n\n        super().__init__(key.algorithm, key.algorithm, key.sig_algorithms,\n                         key.sig_algorithms, key.public_data, comment, cert,\n                         key.get_filename(), key.use_executor)\n\n        self._key = key\n\n    def get_agent_private_key(self) -> bytes:\n        \"\"\"Return binary encoding of keypair for upload to SSH agent\"\"\"\n\n        if self._cert:\n            data = String(self.public_data) + \\\n                       self._key.encode_agent_cert_private()\n        else:\n            data = self._key.encode_ssh_private()\n\n        return String(self.algorithm) + data\n\n    def sign(self, data: bytes) -> bytes:\n        \"\"\"Sign a block of data with this private key\"\"\"\n\n        return self._key.sign(data, self.sig_algorithm)\n\n\ndef _parse_openssh(data: bytes) -> Tuple[bytes, Optional[bytes], bytes]:\n    \"\"\"Parse an OpenSSH format public key or certificate\"\"\"\n\n    line = data.split(None, 2)\n\n    if len(line) < 2:\n        raise KeyImportError('Invalid OpenSSH public key or certificate')\n    elif len(line) == 2:\n        comment = None\n    else:\n        comment = line[2]\n\n    if (line[0] not in _public_key_alg_map and\n            line[0] not in _certificate_alg_map):\n        raise KeyImportError('Unknown OpenSSH public key algorithm')\n\n    try:\n        return line[0], comment, binascii.a2b_base64(line[1])\n    except binascii.Error:\n        raise KeyImportError('Invalid OpenSSH public key '\n                             'or certificate') from None\n\n\ndef _parse_pem(data: bytes) -> Tuple[Mapping[bytes, bytes], bytes]:\n    \"\"\"Parse a PEM data block\"\"\"\n\n    start = 0\n    end: Optional[int] = None\n    headers: Dict[bytes, bytes] = {}\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if b':' in line:\n            hdr, value = line.split(b':', 1)\n            headers[hdr.strip()] = value.strip()\n        else:\n            break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return headers, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid PEM data') from None\n\n\ndef _parse_rfc4716(data: bytes) -> Tuple[Optional[bytes], bytes]:\n    \"\"\"Parse an RFC 4716 data block\"\"\"\n\n    start = 0\n    end = None\n    hdr = b''\n    comment = None\n\n    while True:\n        end = data.find(b'\\n', start) + 1\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if line[-1:] == b'\\\\':\n            hdr += line[:-1]\n        else:\n            hdr += line\n            if b':' in hdr:\n                hdr, value = hdr.split(b':', 1)\n\n                if hdr.strip() == b'Comment':\n                    comment = value.strip()\n                    if comment[:1] == b'\"' and comment[-1:] == b'\"':\n                        comment = comment[1:-1]\n\n                hdr = b''\n            else:\n                break\n\n        start = end if end != 0 else len(data)\n\n    try:\n        return comment, binascii.a2b_base64(data[start:])\n    except binascii.Error:\n        raise KeyImportError('Invalid RFC 4716 data') from None\n\n\ndef _match_block(data: bytes, start: int, header: bytes,\n                 fmt: str) -> Tuple[bytes, int]:\n    \"\"\"Match a block of data wrapped in a header/footer\"\"\"\n\n    match = re.compile(b'^' + header[:5] + b'END' + header[10:] +\n                       rb'[ \\t\\r\\f\\v]*$', re.M).search(data, start)\n\n    if not match:\n        raise KeyImportError('Missing %s footer' % fmt)\n\n    return data[start:match.start()], match.end()\n\n\ndef _match_next(data: bytes, keytype: bytes, public: bool = False) -> \\\n        Tuple[Optional[str], Tuple, Optional[int]]:\n    \"\"\"Find the next key/certificate and call the appropriate decode\"\"\"\n\n    end: Optional[int]\n\n    if data.startswith(b'\\x30'):\n        try:\n            key_data, end = der_decode_partial(data)\n            return 'der', (key_data,), end\n        except ASN1DecodeError:\n            pass\n\n    start = 0\n    end = None\n\n    while end != 0:\n        end = data.find(b'\\n', start) + 1\n\n        line = data[start:end] if end else data[start:]\n        line = line.rstrip()\n\n        if (line.startswith(b'-----BEGIN ') and\n                line.endswith(b' ' + keytype + b'-----')):\n            pem_name = line[11:-(6+len(keytype))].strip()\n            data, end = _match_block(data, end, line, 'PEM')\n            headers, data = _parse_pem(data)\n            return 'pem', (pem_name, headers, data), end\n        elif public:\n            if line == b'---- BEGIN SSH2 PUBLIC KEY ----':\n                data, end = _match_block(data, end, line, 'RFC 4716')\n                return 'rfc4716', _parse_rfc4716(data), end\n            else:\n                try:\n                    cert = _parse_openssh(line)\n                except KeyImportError:\n                    pass\n                else:\n                    return 'openssh', cert, (end if end else len(data))\n\n        start = end\n\n    return None, (), len(data)\n\n\ndef _decode_pkcs1_private(\n        pem_name: bytes, key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format private key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_private(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s private key' %\n                             pem_name.decode('ascii'))\n\n    if pem_name == b'RSA':\n        key_params = cast(Tuple, key_params) + \\\n            (unsafe_skip_rsa_key_validation,)\n\n    return handler.make_private(key_params)\n\n\ndef _decode_pkcs1_public(pem_name: bytes, key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#1 format public key\"\"\"\n\n    handler = _pem_map.get(pem_name)\n    if handler is None:\n        raise KeyImportError('Unknown PEM key type: %s' %\n                             pem_name.decode('ascii'))\n\n    key_params = handler.decode_pkcs1_public(key_data)\n    if key_params is None:\n        raise KeyImportError('Invalid %s public key' %\n                             pem_name.decode('ascii'))\n\n    return handler.make_public(key_params)\n\n\ndef _decode_pkcs8_private(\n        key_data: object,\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format private key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) >= 3 and\n            key_data[0] in (0, 1) and isinstance(key_data[1], tuple) and\n            1 <= len(key_data[1]) <= 2 and isinstance(key_data[2], bytes)):\n        if len(key_data[1]) == 2:\n            alg, alg_params = key_data[1]\n        else:\n            alg, alg_params = key_data[1][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_private(alg_params, key_data[2])\n        if key_params is None:\n            raise KeyImportError('Invalid %s private key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        if alg == ObjectIdentifier('1.2.840.113549.1.1.1'):\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        return handler.make_private(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 private key')\n\n\ndef _decode_pkcs8_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a PKCS#8 format public key\"\"\"\n\n    if (isinstance(key_data, tuple) and len(key_data) == 2 and\n            isinstance(key_data[0], tuple) and 1 <= len(key_data[0]) <= 2 and\n            isinstance(key_data[1], BitString) and key_data[1].unused == 0):\n        if len(key_data[0]) == 2:\n            alg, alg_params = key_data[0]\n        else:\n            alg, alg_params = key_data[0][0], OMIT\n\n        handler = _pkcs8_oid_map.get(alg)\n        if handler is None:\n            raise KeyImportError('Unknown PKCS#8 algorithm')\n\n        key_params = handler.decode_pkcs8_public(alg_params, key_data[1].value)\n        if key_params is None:\n            raise KeyImportError('Invalid %s public key' %\n                                 handler.pem_name.decode('ascii')\n                                 if handler.pem_name else 'PKCS#8')\n\n        return handler.make_public(key_params)\n    else:\n        raise KeyImportError('Invalid PKCS#8 public key')\n\n\ndef _decode_openssh_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode an OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        cipher_name = packet.get_string()\n        kdf = packet.get_string()\n        kdf_data = packet.get_string()\n        nkeys = packet.get_uint32()\n        _ = packet.get_string()                 # public_key\n        key_data = packet.get_string()\n        mac = packet.get_remaining_payload()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if cipher_name != b'none':\n            if passphrase is None:\n                raise KeyImportError('Passphrase must be specified to import '\n                                     'encrypted private keys')\n\n            try:\n                key_size, iv_size, block_size, _, _, _ = \\\n                    get_encryption_params(cipher_name)\n            except KeyError:\n                raise KeyEncryptionError('Unknown cipher: %s' %\n                                         cipher_name.decode('ascii')) from None\n\n            if kdf != b'bcrypt':\n                raise KeyEncryptionError('Unknown kdf: %s' %\n                                         kdf.decode('ascii'))\n\n            if not _bcrypt_available: # pragma: no cover\n                raise KeyEncryptionError('OpenSSH private key encryption '\n                                         'requires bcrypt with KDF support')\n\n            packet = SSHPacket(kdf_data)\n            salt = packet.get_string()\n            rounds = packet.get_uint32()\n            packet.check_end()\n\n            if isinstance(passphrase, str):\n                passphrase = passphrase.encode('utf-8')\n\n            try:\n                bcrypt_key = bcrypt.kdf(passphrase, salt, key_size + iv_size,\n                                        rounds, ignore_few_rounds=True)\n            except ValueError:\n                raise KeyEncryptionError('Invalid OpenSSH '\n                                         'private key') from None\n\n            cipher = get_encryption(cipher_name, bcrypt_key[:key_size],\n                                    bcrypt_key[key_size:])\n\n            decrypted_key = cipher.decrypt_packet(0, b'', key_data, 0, mac)\n\n            if decrypted_key is None:\n                raise KeyEncryptionError('Incorrect passphrase')\n\n            key_data = decrypted_key\n            block_size = max(block_size, 8)\n        else:\n            block_size = 8\n\n        packet = SSHPacket(key_data)\n\n        check1 = packet.get_uint32()\n        check2 = packet.get_uint32()\n        if check1 != check2:\n            if cipher_name != b'none':\n                raise KeyEncryptionError('Incorrect passphrase') from None\n            else:\n                raise KeyImportError('Invalid OpenSSH private key')\n\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n        if not handler:\n            raise KeyImportError('Unknown OpenSSH private key algorithm')\n\n        key_params = handler.decode_ssh_private(packet)\n        comment = packet.get_string()\n        pad = packet.get_remaining_payload()\n\n        if len(pad) >= block_size or pad != bytes(range(1, len(pad) + 1)):\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        if alg == b'ssh-rsa':\n            key_params = cast(Tuple, key_params) + \\\n                (unsafe_skip_rsa_key_validation,)\n\n        key = handler.make_private(key_params)\n        key.set_comment(comment)\n        return key\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_openssh_public(data: bytes) -> SSHKey:\n    \"\"\"Decode public key within OpenSSH format private key\"\"\"\n\n    try:\n        if not data.startswith(_OPENSSH_KEY_V1):\n            raise KeyImportError('Unrecognized OpenSSH private key type')\n\n        data = data[len(_OPENSSH_KEY_V1):]\n        packet = SSHPacket(data)\n\n        _ = packet.get_string()                 # cipher_name\n        _ = packet.get_string()                 # kdf\n        _ = packet.get_string()                 # kdf_data\n        nkeys = packet.get_uint32()\n        pubkey = packet.get_string()\n\n        if nkeys != 1:\n            raise KeyImportError('Invalid OpenSSH private key')\n\n        return decode_ssh_public_key(pubkey)\n    except PacketDecodeError:\n        raise KeyImportError('Invalid OpenSSH private key') from None\n\n\ndef _decode_der_private(\n        key_data: object, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a DER format private key\"\"\"\n\n    # First, if there's a passphrase, try to decrypt PKCS#8\n    if passphrase is not None:\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            # Decryption failed - try decoding it as unencrypted\n            pass\n\n    # Then, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_private(pem_name, key_data,\n                                         unsafe_skip_rsa_key_validation)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER private key')\n\n\ndef _decode_der_public(key_data: object) -> SSHKey:\n    \"\"\"Decode a DER format public key\"\"\"\n\n    # First, try to decode PKCS#8\n    try:\n        return _decode_pkcs8_public(key_data)\n    except KeyImportError:\n        # PKCS#8 failed - try PKCS#1 instead\n        pass\n\n    # If that fails, try each of the possible PKCS#1 encodings\n    for pem_name in _pem_map:\n        try:\n            return _decode_pkcs1_public(pem_name, key_data)\n        except KeyImportError:\n            # Try the next PKCS#1 encoding\n            pass\n\n    raise KeyImportError('Invalid DER public key')\n\n\ndef _decode_der_certificate(data: bytes,\n                            comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a DER format X.509 certificate\"\"\"\n\n    return SSHX509Certificate.construct_from_der(data, comment)\n\n\ndef _decode_pem_private(\n        pem_name: bytes, headers: Mapping[bytes, bytes],\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> SSHKey:\n    \"\"\"Decode a PEM format private key\"\"\"\n\n    if pem_name == b'OPENSSH':\n        return _decode_openssh_private(data, passphrase,\n                                       unsafe_skip_rsa_key_validation)\n\n    if headers.get(b'Proc-Type') == b'4,ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        dek_info = headers.get(b'DEK-Info', b'').split(b',')\n        if len(dek_info) != 2:\n            raise KeyImportError('Invalid PEM encryption params')\n\n        alg, iv = dek_info\n        try:\n            iv = binascii.a2b_hex(iv)\n        except binascii.Error:\n            raise KeyImportError('Invalid PEM encryption params') from None\n\n        try:\n            data = pkcs1_decrypt(data, alg, iv, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#1 '\n                                 'private key') from None\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM private key') from None\n\n    if pem_name == b'ENCRYPTED':\n        if passphrase is None:\n            raise KeyImportError('Passphrase must be specified to import '\n                                 'encrypted private keys')\n\n        pem_name = b''\n\n        try:\n            key_data = pkcs8_decrypt(key_data, passphrase)\n        except KeyEncryptionError:\n            raise KeyImportError('Unable to decrypt PKCS#8 '\n                                 'private key') from None\n\n    if pem_name:\n        return _decode_pkcs1_private(pem_name, key_data,\n                                     unsafe_skip_rsa_key_validation)\n    else:\n        return _decode_pkcs8_private(key_data, unsafe_skip_rsa_key_validation)\n\n\ndef _decode_pem_public(pem_name: bytes, data: bytes) -> SSHKey:\n    \"\"\"Decode a PEM format public key\"\"\"\n\n    try:\n        key_data = der_decode(data)\n    except ASN1DecodeError:\n        raise KeyImportError('Invalid PEM public key') from None\n\n    if pem_name:\n        return _decode_pkcs1_public(pem_name, key_data)\n    else:\n        return _decode_pkcs8_public(key_data)\n\n\ndef _decode_pem_certificate(pem_name: bytes, data: bytes) -> SSHCertificate:\n    \"\"\"Decode a PEM format X.509 certificate\"\"\"\n\n    if pem_name == b'TRUSTED':\n        # Strip off OpenSSL trust information\n        try:\n            _, end = der_decode_partial(data)\n            data = data[:end]\n        except ASN1DecodeError:\n            raise KeyImportError('Invalid PEM trusted certificate') from None\n    elif pem_name:\n        raise KeyImportError('Invalid PEM certificate')\n\n    return SSHX509Certificate.construct_from_der(data)\n\n\ndef _decode_private(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> \\\n            Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a private key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_private(key_info[0], passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    elif fmt == 'pem':\n        pem_name, headers, data = key_info\n        key = _decode_pem_private(pem_name, headers, data, passphrase,\n                                  unsafe_skip_rsa_key_validation)\n    else:\n        key = None\n\n    return key, end\n\n\ndef _decode_public(data: bytes) -> Tuple[Optional[SSHKey], Optional[int]]:\n    \"\"\"Decode a public key\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'PUBLIC KEY', public=True)\n\n    key: Optional[SSHKey]\n\n    if fmt == 'der':\n        key = _decode_der_public(key_info[0])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        key = _decode_pem_public(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n        key = decode_ssh_public_key(data)\n\n        if algorithm != key.algorithm:\n            raise KeyImportError('Public key algorithm mismatch')\n\n        key.set_comment(comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        key = decode_ssh_public_key(data)\n        key.set_comment(comment)\n    else:\n        fmt, key_info, end = _match_next(data, b'PRIVATE KEY')\n\n        if fmt == 'pem' and key_info[0] == b'OPENSSH':\n            key = _decode_openssh_public(key_info[2])\n        else:\n            key, _ = _decode_private(data, None, False)\n\n            if key:\n                key = key.convert_to_public()\n\n    return key, end\n\n\ndef _decode_certificate(data: bytes) -> \\\n        Tuple[Optional[SSHCertificate], Optional[int]]:\n    \"\"\"Decode a certificate\"\"\"\n\n    fmt, key_info, end = _match_next(data, b'CERTIFICATE', public=True)\n\n    cert: Optional[SSHCertificate]\n\n    if fmt == 'der':\n        cert = _decode_der_certificate(data[:end])\n    elif fmt == 'pem':\n        pem_name, _, data = key_info\n        cert = _decode_pem_certificate(pem_name, data)\n    elif fmt == 'openssh':\n        algorithm, comment, data = key_info\n\n        if algorithm.startswith(b'x509v3-'):\n            cert = _decode_der_certificate(data, comment)\n        else:\n            cert = decode_ssh_certificate(data, comment)\n    elif fmt == 'rfc4716':\n        comment, data = key_info\n        cert = decode_ssh_certificate(data, comment)\n    else:\n        cert = None\n\n    return cert, end\n\n\ndef _decode_private_list(\n        data: bytes, passphrase: Optional[BytesOrStr],\n        unsafe_skip_rsa_key_validation: Optional[bool]) -> Sequence[SSHKey]:\n    \"\"\"Decode a private key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_private(data, passphrase,\n                                   unsafe_skip_rsa_key_validation)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_public_list(data: bytes) -> Sequence[SSHKey]:\n    \"\"\"Decode a public key list\"\"\"\n\n    keys: List[SSHKey] = []\n\n    while data:\n        key, end = _decode_public(data)\n\n        if key:\n            keys.append(key)\n\n        data = data[end:]\n\n    return keys\n\n\ndef _decode_certificate_list(data: bytes) -> Sequence[SSHCertificate]:\n    \"\"\"Decode a certificate list\"\"\"\n\n    certs: List[SSHCertificate] = []\n\n    while data:\n        cert, end = _decode_certificate(data)\n\n        if cert:\n            certs.append(cert)\n\n        data = data[end:]\n\n    return certs\n\n\ndef register_sk_alg(sk_alg: int, handler: Type[SSHKey], *args: object) -> None:\n    \"\"\"Register a new security key algorithm\"\"\"\n\n    _sk_alg_map[sk_alg] = handler, args\n\n\ndef register_public_key_alg(algorithm: bytes, handler: Type[SSHKey],\n                            default: bool,\n                            sig_algorithms: Optional[Sequence[bytes]] = \\\n                                None) -> None:\n    \"\"\"Register a new public key algorithm\"\"\"\n\n    if not sig_algorithms:\n        sig_algorithms = handler.sig_algorithms\n\n    _public_key_algs.extend(sig_algorithms)\n\n    if default:\n        _default_public_key_algs.extend(sig_algorithms)\n\n    _public_key_alg_map[algorithm] = handler\n\n    if handler.pem_name:\n        _pem_map[handler.pem_name] = handler\n\n    if handler.pkcs8_oid: # pragma: no branch\n        _pkcs8_oid_map[handler.pkcs8_oid] = handler\n\n\ndef register_certificate_alg(version: int, algorithm: bytes,\n                             cert_algorithm: bytes,\n                             key_handler: Type[SSHKey],\n                             cert_handler: Type[SSHOpenSSHCertificate],\n                             default: bool) -> None:\n    \"\"\"Register a new certificate algorithm\"\"\"\n\n    _certificate_algs.append(cert_algorithm)\n\n    if default:\n        _default_certificate_algs.append(cert_algorithm)\n\n    _certificate_alg_map[cert_algorithm] = (key_handler, cert_handler)\n\n    _certificate_sig_alg_map[cert_algorithm] = algorithm\n\n    _certificate_version_map[algorithm, version] = \\\n        (cert_algorithm, cert_handler)\n\n\ndef register_x509_certificate_alg(cert_algorithm: bytes, default: bool) -> None:\n    \"\"\"Register a new X.509 certificate algorithm\"\"\"\n\n    if _x509_available: # pragma: no branch\n        _x509_certificate_algs.append(cert_algorithm)\n\n        if default:\n            _default_x509_certificate_algs.append(cert_algorithm)\n\n        _certificate_alg_map[cert_algorithm] = (None, SSHX509CertificateChain)\n\n\ndef get_public_key_algs() -> List[bytes]:\n    \"\"\"Return supported public key algorithms\"\"\"\n\n    return _public_key_algs\n\n\ndef get_default_public_key_algs() -> List[bytes]:\n    \"\"\"Return default public key algorithms\"\"\"\n\n    return _default_public_key_algs\n\n\ndef get_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported certificate-based public key algorithms\"\"\"\n\n    return _certificate_algs\n\n\ndef get_default_certificate_algs() -> List[bytes]:\n    \"\"\"Return default certificate-based public key algorithms\"\"\"\n\n    return _default_certificate_algs\n\n\ndef get_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return supported X.509 certificate-based public key algorithms\"\"\"\n\n    return _x509_certificate_algs\n\n\ndef get_default_x509_certificate_algs() -> List[bytes]:\n    \"\"\"Return default X.509 certificate-based public key algorithms\"\"\"\n\n    return _default_x509_certificate_algs\n\n\ndef decode_ssh_public_key(data: bytes) -> SSHKey:\n    \"\"\"Decode a packetized SSH public key\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        handler = _public_key_alg_map.get(alg)\n\n        if handler:\n            key_params = handler.decode_ssh_public(packet)\n            packet.check_end()\n\n            key = handler.make_public(key_params)\n            key.algorithm = alg\n            return key\n        else:\n            raise KeyImportError('Unknown key algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except PacketDecodeError:\n        raise KeyImportError('Invalid public key') from None\n\n\ndef decode_ssh_certificate(data: bytes,\n                           comment: _Comment = None) -> SSHCertificate:\n    \"\"\"Decode a packetized SSH certificate\"\"\"\n\n    try:\n        packet = SSHPacket(data)\n        alg = packet.get_string()\n        key_handler, cert_handler = _certificate_alg_map.get(alg, (None, None))\n\n        if cert_handler:\n            return cert_handler.construct(packet, alg, key_handler, comment)\n        else:\n            raise KeyImportError('Unknown certificate algorithm: %s' %\n                                 alg.decode('ascii', errors='replace'))\n    except (PacketDecodeError, ValueError):\n        raise KeyImportError('Invalid OpenSSH certificate') from None\n\n\ndef generate_private_key(alg_name: str, comment: _Comment = None,\n                         **kwargs) -> SSHKey:\n    \"\"\"Generate a new private key\n\n       This function generates a new private key of a type matching\n       the requested SSH algorithm. Depending on the algorithm, additional\n       parameters can be passed which affect the generated key.\n\n       Available algorithms include:\n\n           ssh-dss, ssh-rsa, ecdsa-sha2-nistp256, ecdsa-sha2-nistp384,\n           ecdsa-sha2-nistp521, ecdsa-sha2-1.3.132.0.10, ssh-ed25519,\n           ssh-ed448, sk-ecdsa-sha2-nistp256\\\\@openssh.com,\n           sk-ssh-ed25519\\\\@openssh.com\n\n       For dss keys, no parameters are supported. The key size is fixed at\n       1024 bits due to the use of SHA1 signatures.\n\n       For rsa keys, the key size can be specified using the `key_size`\n       parameter, and the RSA public exponent can be changed using the\n       `exponent` parameter. By default, generated keys are 2048 bits\n       with a public exponent of 65537.\n\n       For ecdsa keys, the curve to use is part of the SSH algorithm name\n       and that determines the key size. No other parameters are supported.\n\n       For ed25519 and ed448 keys, no parameters are supported. The key size\n       is fixed by the algorithms at 256 bits and 448 bits, respectively.\n\n       For sk keys, the application name to associate with the generated\n       key can be specified using the `application` parameter. It defaults\n       to `'ssh:'`. The user name to associate with the generated key can\n       be specified using the `user` parameter. It defaults to `'AsyncSSH'`.\n\n       When generating an sk key, a PIN can be provided via the `pin`\n       parameter if the security key requires it.\n\n       The `resident` parameter can be set to `True` to request that a\n       resident key be created on the security key. This allows the key\n       handle and public key information to later be retrieved so that\n       the generated key can be used without having to store any\n       information on the client system. It defaults to `False`.\n\n       You can enable or disable the security key touch requirement by\n       setting the `touch_required` parameter. It defaults to `True`,\n       requiring that the user confirm their presence by touching the\n       security key each time they use it to authenticate.\n\n       :param alg_name:\n           The SSH algorithm name corresponding to the desired type of key.\n       :param comment: (optional)\n           A comment to associate with this key.\n       :param key_size: (optional)\n           The key size in bits for RSA keys.\n       :param exponent: (optional)\n           The public exponent for RSA keys.\n       :param application: (optional)\n           The application name to associate with the generated SK key,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name to associate with the generated SK key, defaulting\n           to `'AsyncSSH'`.\n       :param pin: (optional)\n           The PIN to use to access the security key, defaulting to `None`.\n       :param resident: (optional)\n           Whether or not to create a resident key on the security key,\n           defaulting to `False`.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type alg_name: `str`\n       :type comment: `str`, `bytes`, or `None`\n       :type key_size: `int`\n       :type exponent: `int`\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type resident: `bool`\n       :type touch_required: `bool`\n\n       :returns: An :class:`SSHKey` private key\n\n       :raises: :exc:`KeyGenerationError` if the requested key parameters\n                are unsupported\n    \"\"\"\n\n    algorithm = alg_name.encode('utf-8')\n    handler = _public_key_alg_map.get(algorithm)\n\n    if handler:\n        try:\n            key = handler.generate(algorithm, **kwargs)\n        except (TypeError, ValueError) as exc:\n            raise KeyGenerationError(str(exc)) from None\n    else:\n        raise KeyGenerationError('Unknown algorithm: %s' % alg_name)\n\n    key.set_comment(comment)\n    return key\n\ndef import_private_key(\n        data: BytesOrStr, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Import a private key\n\n       This function imports a private key encoded in PKCS#1 or PKCS#8 DER\n       or PEM format or OpenSSH format. Encrypted private keys can be\n       imported by specifying the passphrase needed to decrypt them.\n\n       :param data:\n           The data to import.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type data: `bytes` or ASCII `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_private(data, passphrase, unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_private_key_and_certs(\n        data: bytes, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Import a private key and optional certificate chain\"\"\"\n\n    key, end = _decode_private(data, passphrase,\n                               unsafe_skip_rsa_key_validation)\n\n    if key:\n        return key, import_certificate_chain(data[end:])\n    else:\n        raise KeyImportError('Invalid private key')\n\n\ndef import_public_key(data: BytesOrStr) -> SSHKey:\n    \"\"\"Import a public key\n\n       This function imports a public key encoded in OpenSSH, RFC4716, or\n       PKCS#1 or PKCS#8 DER or PEM format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    key, _ = _decode_public(data)\n\n    if key:\n        return key\n    else:\n        raise KeyImportError('Invalid public key')\n\n\ndef import_certificate(data: BytesOrStr) -> SSHCertificate:\n    \"\"\"Import a certificate\n\n       This function imports an SSH certificate in DER, PEM, OpenSSH, or\n       RFC4716 format.\n\n       :param data:\n           The data to import.\n       :type data: `bytes` or ASCII `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    if isinstance(data, str):\n        try:\n            data = data.encode('ascii')\n        except UnicodeEncodeError:\n            raise KeyImportError('Invalid encoding for key') from None\n\n    cert, _ = _decode_certificate(data)\n\n    if cert:\n        return cert\n    else:\n        raise KeyImportError('Invalid certificate')\n\n\ndef import_certificate_chain(data: bytes) -> Optional[SSHX509CertificateChain]:\n    \"\"\"Import an X.509 certificate chain\"\"\"\n\n    certs = _decode_certificate_list(data)\n\n    chain: Optional[SSHX509CertificateChain]\n\n    if certs:\n        chain = SSHX509CertificateChain.construct_from_certs(certs)\n    else:\n        chain = None\n\n    return chain\n\n\ndef import_certificate_subject(data: str) -> str:\n    \"\"\"Import an X.509 certificate subject name\"\"\"\n\n    try:\n        algorithm, data = data.strip().split(None, 1)\n    except ValueError:\n        raise KeyImportError('Missing certificate subject algorithm') from None\n\n    if algorithm.startswith('x509v3-'):\n        match = _subject_pattern.match(data)\n\n        if match:\n            return data[match.end():]\n\n    raise KeyImportError('Invalid certificate subject')\n\n\ndef read_private_key(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> SSHKey:\n    \"\"\"Read a private key from a file\n\n       This function reads a private key from a file. See the function\n       :func:`import_private_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the key.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: An :class:`SSHKey` private key\n\n    \"\"\"\n\n    key = import_private_key(read_file(filename), passphrase,\n                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_private_key_and_certs(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Tuple[SSHKey, Optional[SSHX509CertificateChain]]:\n    \"\"\"Read a private key and optional certificate chain from a file\"\"\"\n\n    key, cert = import_private_key_and_certs(read_file(filename), passphrase,\n                                             unsafe_skip_rsa_key_validation)\n\n    key.set_filename(filename)\n\n    return key, cert\n\n\ndef read_public_key(filename: FilePath) -> SSHKey:\n    \"\"\"Read a public key from a file\n\n       This function reads a public key from a file. See the function\n       :func:`import_public_key` for information about the formats\n       supported.\n\n       :param filename:\n           The file to read the key from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHKey` public key\n\n    \"\"\"\n\n    key = import_public_key(read_file(filename))\n\n    key.set_filename(filename)\n\n    return key\n\n\ndef read_certificate(filename: FilePath) -> SSHCertificate:\n    \"\"\"Read a certificate from a file\n\n       This function reads an SSH certificate from a file. See the\n       function :func:`import_certificate` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the certificate from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: An :class:`SSHCertificate` object\n\n    \"\"\"\n\n    return import_certificate(read_file(filename))\n\n\ndef read_private_key_list(\n        filename: FilePath, passphrase: Optional[BytesOrStr] = None,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKey]:\n    \"\"\"Read a list of private keys from a file\n\n       This function reads a list of private keys from a file. See the\n       function :func:`import_private_key` for information about the\n       formats supported. If any of the keys are encrypted, they must\n       all be encrypted with the same passphrase.\n\n       :param filename:\n           The file to read the keys from.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt the keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n       :type passphrase: `str` or `bytes`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKey` private keys\n\n    \"\"\"\n\n    keys = _decode_private_list(read_file(filename), passphrase,\n                                unsafe_skip_rsa_key_validation)\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_public_key_list(filename: FilePath) -> Sequence[SSHKey]:\n    \"\"\"Read a list of public keys from a file\n\n       This function reads a list of public keys from a file. See the\n       function :func:`import_public_key` for information about the\n       formats supported.\n\n       :param filename:\n           The file to read the keys from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHKey` public keys\n\n    \"\"\"\n\n    keys = _decode_public_list(read_file(filename))\n\n    for key in keys:\n        key.set_filename(filename)\n\n    return keys\n\n\ndef read_certificate_list(filename: FilePath) -> Sequence[SSHCertificate]:\n    \"\"\"Read a list of certificates from a file\n\n       This function reads a list of SSH certificates from a file. See\n       the function :func:`import_certificate` for information about\n       the formats supported.\n\n       :param filename:\n           The file to read the certificates from.\n       :type filename: :class:`PurePath <pathlib.PurePath>` or `str`\n\n       :returns: A list of :class:`SSHCertificate` certificates\n\n    \"\"\"\n\n    return _decode_certificate_list(read_file(filename))\n\n\ndef load_keypairs(\n        keylist: KeyPairListArg, passphrase: Optional[BytesOrStr] = None,\n        certlist: CertListArg = (), skip_public: bool = False,\n        ignore_encrypted: bool = False,\n        unsafe_skip_rsa_key_validation: Optional[bool] = None) -> \\\n            Sequence[SSHKeyPair]:\n    \"\"\"Load SSH private keys and optional matching certificates\n\n       This function loads a list of SSH keys and optional matching\n       certificates.\n\n       When certificates are specified, the private key is added to\n       the list both with and without the certificate.\n\n       :param keylist:\n           The list of private keys and certificates to load.\n       :param passphrase: (optional)\n           The passphrase to use to decrypt private keys.\n       :param certlist: (optional)\n           A list of certificates to attempt to pair with the provided\n           list of private keys.\n       :param skip_public: (optional)\n           An internal parameter used to skip public keys and certificates\n           when IdentitiesOnly and IdentityFile are used to specify a\n           mixture of private and public keys.\n       :param unsafe_skip_rsa_key_validation: (optional)\n           Whether or not to skip key validation when loading RSA private\n           keys, defaulting to performing these checks unless changed by\n           calling :func:`set_default_skip_rsa_key_validation`.\n       :type keylist: *see* :ref:`SpecifyingPrivateKeys`\n       :type passphrase: `str` or `bytes`\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n       :type skip_public: `bool`\n       :type unsafe_skip_rsa_key_validation: bool\n\n       :returns: A list of :class:`SSHKeyPair` objects\n\n    \"\"\"\n\n    keys_to_load: Sequence[_KeyPairArg]\n    result: List[SSHKeyPair] = []\n\n    certlist = load_certificates(certlist)\n    certdict = {cert.key.public_data: cert for cert in certlist}\n\n    if isinstance(keylist, (PurePath, str)):\n        try:\n            priv_keys = read_private_key_list(keylist, passphrase,\n                                              unsafe_skip_rsa_key_validation)\n            keys_to_load = [keylist] if len(priv_keys) <= 1 else priv_keys\n        except KeyImportError:\n            keys_to_load = [keylist]\n    elif isinstance(keylist, (tuple, bytes, SSHKey, SSHKeyPair)):\n        keys_to_load = [cast(_KeyPairArg, keylist)]\n    else:\n        keys_to_load = keylist if keylist else []\n\n    for key_to_load in keys_to_load:\n        allow_certs = False\n        key_prefix = None\n        saved_exc = None\n        pubkey_or_certs = None\n        pubkey_to_load: Optional[_KeyArg] = None\n        certs_to_load: Optional[_CertArg] = None\n        key: Union['SSHKey', 'SSHKeyPair']\n\n        if isinstance(key_to_load, (PurePath, str, bytes)):\n            allow_certs = True\n        elif isinstance(key_to_load, tuple):\n            key_to_load, pubkey_or_certs = key_to_load\n\n        try:\n            if isinstance(key_to_load, (PurePath, str)):\n                key_prefix = str(key_to_load)\n\n                if allow_certs:\n                    key, certs_to_load = read_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n\n                    if not certs_to_load:\n                        certs_to_load = key_prefix + '-cert.pub'\n                else:\n                    key = read_private_key(key_to_load, passphrase,\n                                           unsafe_skip_rsa_key_validation)\n\n                pubkey_to_load = key_prefix + '.pub'\n            elif isinstance(key_to_load, bytes):\n                if allow_certs:\n                    key, certs_to_load = import_private_key_and_certs(\n                        key_to_load, passphrase,\n                        unsafe_skip_rsa_key_validation)\n                else:\n                    key = import_private_key(key_to_load, passphrase,\n                                             unsafe_skip_rsa_key_validation)\n            else:\n                key = key_to_load\n        except KeyImportError as exc:\n            if skip_public or \\\n                    (ignore_encrypted and str(exc).startswith('Passphrase')):\n                continue\n\n            raise\n\n        certs: Optional[Sequence[SSHCertificate]]\n\n        if pubkey_or_certs:\n            try:\n                certs = load_certificates(pubkey_or_certs)\n            except (TypeError, OSError, KeyImportError) as exc:\n                saved_exc = exc\n                certs = None\n\n            if not certs:\n                pubkey_to_load = cast(_KeyArg, pubkey_or_certs)\n        elif certs_to_load:\n            try:\n                certs = load_certificates(certs_to_load)\n            except (OSError, KeyImportError):\n                certs = None\n        else:\n            certs = None\n\n        pubkey: Optional[SSHKey]\n\n        if pubkey_to_load:\n            try:\n                if isinstance(pubkey_to_load, (PurePath, str)):\n                    pubkey = read_public_key(pubkey_to_load)\n                elif isinstance(pubkey_to_load, bytes):\n                    pubkey = import_public_key(pubkey_to_load)\n                else:\n                    pubkey = pubkey_to_load\n            except (OSError, KeyImportError):\n                pubkey = None\n            else:\n                saved_exc = None\n        else:\n            pubkey = None\n\n        if saved_exc:\n            raise saved_exc # pylint: disable=raising-bad-type\n\n        if not certs:\n            if isinstance(key, SSHKeyPair):\n                pubdata = key.key_public_data\n            else:\n                pubdata = key.public_data\n\n            cert = certdict.get(pubdata)\n\n            if cert and cert.is_x509:\n                cert = SSHX509CertificateChain.construct_from_certs(certlist)\n        elif len(certs) == 1 and not certs[0].is_x509:\n            cert = certs[0]\n        else:\n            cert = SSHX509CertificateChain.construct_from_certs(certs)\n\n        if isinstance(key, SSHKeyPair):\n            if cert:\n                key.set_certificate(cert)\n\n            result.append(key)\n        else:\n            if cert:\n                result.append(SSHLocalKeyPair(key, pubkey, cert))\n\n            result.append(SSHLocalKeyPair(key, pubkey))\n\n    return result\n\n\ndef load_default_keypairs(passphrase: Optional[BytesOrStr] = None,\n                          certlist: CertListArg = ()) -> \\\n        Sequence[SSHKeyPair]:\n    \"\"\"Return a list of default keys from the user's home directory\"\"\"\n\n    result: List[SSHKeyPair] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                path = Path('~', '.ssh', file).expanduser()\n                result.extend(load_keypairs(path, passphrase, certlist,\n                                            ignore_encrypted=True))\n            except OSError:\n                pass\n\n    return result\n\n\ndef load_public_keys(keylist: KeyListArg) -> Sequence[SSHKey]:\n    \"\"\"Load public keys\n\n       This function loads a list of SSH public keys.\n\n       :param keylist:\n           The list of public keys to load.\n       :type keylist: *see* :ref:`SpecifyingPublicKeys`\n\n       :returns: A list of :class:`SSHKey` objects\n\n    \"\"\"\n\n    if isinstance(keylist, (PurePath, str)):\n        return read_public_key_list(keylist)\n    else:\n        result: List[SSHKey] = []\n\n        for key in keylist:\n            if isinstance(key, (PurePath, str)):\n                key = read_public_key(key)\n            elif isinstance(key, bytes):\n                key = import_public_key(key)\n\n            result.append(key)\n\n        return result\n\n\ndef load_default_host_public_keys() -> Sequence[Union[SSHKey, SSHCertificate]]:\n    \"\"\"Return a list of default host public keys or certificates\"\"\"\n\n    result: List[Union[SSHKey, SSHCertificate]] = []\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                cert = read_certificate(Path(host_key_dir, file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert)\n\n    for host_key_dir in _DEFAULT_HOST_KEY_DIRS:\n        for file in _DEFAULT_HOST_KEY_FILES:\n            try:\n                key = read_public_key(Path(host_key_dir, file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key)\n\n    return result\n\n\ndef load_certificates(certlist: CertListArg) -> Sequence[SSHCertificate]:\n    \"\"\"Load certificates\n\n       This function loads a list of OpenSSH or X.509 certificates.\n\n       :param certlist:\n           The list of certificates to load.\n       :type certlist: *see* :ref:`SpecifyingCertificates`\n\n       :returns: A list of :class:`SSHCertificate` objects\n\n    \"\"\"\n\n    if isinstance(certlist, SSHCertificate):\n        return [certlist]\n    elif isinstance(certlist, (PurePath, str, bytes)):\n        certlist = [certlist]\n\n    result: List[SSHCertificate] = []\n\n    for cert in certlist:\n        if isinstance(cert, (PurePath, str)):\n            certs = read_certificate_list(cert)\n        elif isinstance(cert, bytes):\n            certs = _decode_certificate_list(cert)\n        elif isinstance(cert, SSHCertificate):\n            certs = [cert]\n        else:\n            certs = cert\n\n        result.extend(certs)\n\n    return result\n\n\ndef load_identities(keylist: IdentityListArg,\n                    skip_private: bool = False) -> Sequence[bytes]:\n    \"\"\"Load public key and certificate identities\"\"\"\n\n    if isinstance(keylist, (bytes, str, PurePath, SSHKey, SSHCertificate)):\n        identities: Sequence[_IdentityArg] = [keylist]\n    else:\n        identities = keylist\n\n    result = []\n\n    for identity in identities:\n        if isinstance(identity, (PurePath, str)):\n            try:\n                pubdata = read_certificate(identity).public_data\n            except KeyImportError:\n                try:\n                    pubdata = read_public_key(identity).public_data\n                except KeyImportError:\n                    if skip_private:\n                        continue\n\n                    raise\n        elif isinstance(identity, (SSHKey, SSHCertificate)):\n            pubdata = identity.public_data\n        else:\n            pubdata = identity\n\n        result.append(pubdata)\n\n    return result\n\n\ndef load_default_identities() -> Sequence[bytes]:\n    \"\"\"Return a list of default public key and certificate identities\"\"\"\n\n    result: List[bytes] = []\n\n    for file, condition in _DEFAULT_KEY_FILES:\n        if condition: # pragma: no branch\n            try:\n                cert = read_certificate(Path('~', '.ssh', file + '-cert.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(cert.public_data)\n\n            try:\n                key = read_public_key(Path('~', '.ssh', file + '.pub'))\n            except (OSError, KeyImportError):\n                pass\n            else:\n                result.append(key.public_data)\n\n    return result\n\n\ndef load_resident_keys(pin: str, *, application: str = 'ssh:',\n                       user: Optional[str] = None,\n                       touch_required: bool = True) -> Sequence[SSHKey]:\n    \"\"\"Load keys resident on attached FIDO2 security keys\n\n       This function loads keys resident on any FIDO2 security keys\n       currently attached to the system. The user name associated\n       with each key is returned in the key's comment field.\n\n       :param pin:\n           The PIN to use to access the security keys, defaulting to `None`.\n       :param application: (optional)\n           The application name associated with the keys to load,\n           defaulting to `'ssh:'`.\n       :param user: (optional)\n           The user name associated with the keys to load. By default,\n           keys for all users are loaded.\n       :param touch_required: (optional)\n           Whether or not to require the user to touch the security key\n           when authenticating with it, defaulting to `True`.\n       :type application: `str`\n       :type user: `str`\n       :type pin: `str`\n       :type touch_required: `bool`\n\n    \"\"\"\n\n    application = application.encode('utf-8')\n    flags = SSH_SK_USER_PRESENCE_REQD if touch_required else 0\n    reserved = b''\n\n    try:\n        resident_keys = sk_get_resident(application, user, pin)\n    except ValueError as exc:\n        raise KeyImportError(str(exc)) from None\n\n    result: List[SSHKey] = []\n\n    for sk_alg, name, public_value, key_handle in resident_keys:\n        handler, key_params = _sk_alg_map[sk_alg]\n        key_params += (public_value, application, flags, key_handle, reserved)\n\n        key = handler.make_private(key_params)\n        key.set_comment(name)\n\n        result.append(key)\n\n    return result\n", "test_list": ["@agent_test\nasync def test_lock(self, agent):\n    \"\"\"Test lock and unlock\"\"\"\n    key = get_test_key('ecdsa-sha2-nistp256')\n    pubkey = key.convert_to_public()\n    await agent.add_keys([key])\n    agent_keys = await agent.get_keys()\n    await agent.lock('passphrase')\n    for agent_key in agent_keys:\n        with self.assertRaises(ValueError):\n            await agent_key.sign_async(b'test')\n    await agent.unlock('passphrase')\n    for agent_key in agent_keys:\n        sig = await agent_key.sign_async(b'test')\n        self.assertTrue(pubkey.verify(b'test', sig))", "def test_rsa_encrypt_error(self):\n    \"\"\"Test RSA encryption error\"\"\"\n    privkey = get_test_key('ssh-rsa', 2048)\n    pubkey = privkey.convert_to_public()\n    self.assertIsNone(pubkey.encrypt(os.urandom(256), pubkey.algorithm))", "def test_generate_errors(self):\n    \"\"\"Test errors in private key and certificate generation\"\"\"\n    for alg_name, kwargs in (('xxx', {}), ('ssh-dss', {'xxx': 0}), ('ssh-rsa', {'xxx': 0}), ('ecdsa-sha2-nistp256', {'xxx': 0}), ('ssh-ed25519', {'xxx': 0}), ('ssh-ed448', {'xxx': 0})):\n        with self.subTest(alg_name=alg_name, **kwargs):\n            with self.assertRaises(asyncssh.KeyGenerationError):\n                asyncssh.generate_private_key(alg_name, **kwargs)\n    privkey = get_test_key('ssh-rsa')\n    pubkey = privkey.convert_to_public()\n    privca = get_test_key('ssh-rsa', 1)\n    with self.assertRaises(asyncssh.KeyGenerationError):\n        privca.generate_user_certificate(pubkey, 'name', version=0)\n    with self.assertRaises(ValueError):\n        privca.generate_user_certificate(pubkey, 'name', valid_after=())\n    with self.assertRaises(ValueError):\n        privca.generate_user_certificate(pubkey, 'name', valid_after='xxx')\n    with self.assertRaises(ValueError):\n        privca.generate_user_certificate(pubkey, 'name', valid_after='now', valid_before='-1m')\n    with self.assertRaises(ValueError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user', valid_after=())\n    with self.assertRaises(ValueError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user', valid_after='xxx')\n    with self.assertRaises(ValueError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user', valid_after='now', valid_before='-1m')\n    privca.x509_algorithms = None\n    with self.assertRaises(asyncssh.KeyGenerationError):\n        privca.generate_x509_user_certificate(pubkey, 'OU=user')", "@agent_test\nasync def test_reconnect(self, agent):\n    \"\"\"Test reconnecting to the agent after closing it\"\"\"\n    key = get_test_key('ecdsa-sha2-nistp256')\n    pubkey = key.convert_to_public()\n    async with agent:\n        await agent.add_keys([key])\n        agent_keys = await agent.get_keys()\n    for agent_key in agent_keys:\n        sig = await agent_key.sign_async(b'test')\n        self.assertTrue(pubkey.verify(b'test', sig))", "@agent_test\nasync def test_sign(self, agent):\n    \"\"\"Test signing a block of data using the agent\"\"\"\n    algs = ['ssh-rsa', 'ecdsa-sha2-nistp256']\n    if ed25519_available:\n        algs.append('ssh-ed25519')\n    for alg_name in algs:\n        key = get_test_key(alg_name)\n        pubkey = key.convert_to_public()\n        cert = key.generate_user_certificate(key, 'name')\n        await agent.add_keys([(key, cert)])\n        agent_keys = await agent.get_keys()\n        for agent_key in agent_keys:\n            agent_key.set_sig_algorithm(agent_key.sig_algorithms[0])\n            sig = await agent_key.sign_async(b'test')\n            self.assertTrue(pubkey.verify(b'test', sig))\n        await agent.remove_keys(agent_keys)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'convert_to_public' function should accept an instance of SSHKey and return an SSHKey object containing only the public key data. The output should be of type SSHKey and should not contain any private key data.", "unit_test": ["def test_convert_to_public_output_type(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    public_key = key.convert_to_public()\n    self.assertIsInstance(public_key, SSHKey)\n    self.assertNotIn('private_data', dir(public_key))"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_output_type"}, "Exception Handling": {"requirement": "The 'convert_to_public' function should raise a ValueError if the input SSHKey object does not contain a valid private key.", "unit_test": ["def test_convert_to_public_invalid_key(self):\n    key = SSHKey()\n    with self.assertRaises(ValueError):\n        key.convert_to_public()"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_invalid_key"}, "Edge Case Handling": {"requirement": "The 'convert_to_public' function should handle cases where the SSHKey object has no comment or filename set, ensuring the output public key still functions correctly.", "unit_test": ["def test_convert_to_public_no_comment_filename(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    key.set_comment(None)\n    key.set_filename(None)\n    public_key = key.convert_to_public()\n    self.assertIsNone(public_key.get_comment_bytes())\n    self.assertIsNone(public_key.get_filename())"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_no_comment_filename"}, "Functionality Extension": {"requirement": "Extend the 'convert_to_public' function to optionally accept a new comment and filename for the public key, overriding any existing values.", "unit_test": ["def test_convert_to_public_with_new_comment_filename(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    public_key = key.convert_to_public(comment='New Comment', filename='new_filename.pub')\n    self.assertEqual(public_key.get_comment(), 'New Comment')\n    self.assertEqual(public_key.get_filename(), b'new_filename.pub')"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_with_new_comment_filename"}, "Annotation Coverage": {"requirement": "Ensure that the 'convert_to_public' function has complete type annotations for its parameters and return type.", "unit_test": ["def test_convert_to_public_annotations(self):\n    annotations = SSHKey.convert_to_public.__annotations__\n    self.assertIn('self', annotations)\n    self.assertEqual(annotations['return'], SSHKey)"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_annotations"}, "Code Complexity": {"requirement": "The 'convert_to_public' function should maintain a cyclomatic complexity of 5 or less to ensure readability and maintainability.", "unit_test": ["def test_convert_to_public_complexity(self):\n    complexity = get_cyclomatic_complexity(SSHKey.convert_to_public)\n    self.assertLessEqual(complexity, 5)"], "test": "tests/test_agent.py::_TestAgent::test_code_complexity"}, "Code Standard": {"requirement": "The 'convert_to_public' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_code_style(self):\n    import pep8\n    style = pep8.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/ssh_key_module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/test_agent.py::_TestAgent::test_code_style"}, "Context Usage Verification": {"requirement": "The 'convert_to_public' function should utilize the context attributes '_comment', '_filename', and 'public_data' of the SSHKey class.", "unit_test": ["def test_convert_to_public_context_usage(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    public_key = key.convert_to_public()\n    self.assertEqual(public_key._comment, key._comment)\n    self.assertEqual(public_key._filename, key._filename)\n    self.assertEqual(public_key.public_data, key.public_data)"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'convert_to_public' function should correctly use the '_comment' and '_filename' attributes to set the corresponding values in the public key.", "unit_test": ["def test_convert_to_public_context_correctness(self):\n    key = get_test_key('ecdsa-sha2-nistp256')\n    key.set_comment('Test Comment')\n    key.set_filename('test_filename.pub')\n    public_key = key.convert_to_public()\n    self.assertEqual(public_key.get_comment(), 'Test Comment')\n    self.assertEqual(public_key.get_filename(), b'test_filename.pub')"], "test": "tests/test_agent.py::_TestAgent::test_convert_to_public_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "rows.fields.DateField.deserialize", "type": "method", "project_path": "Text-Processing/rows", "completion_path": "Text-Processing/rows/rows/fields.py", "signature_position": [367, 367], "body_position": [368, 375], "dependency": {"intra_class": ["rows.fields.DateField.INPUT_FORMAT", "rows.fields.DateField.TYPE"], "intra_file": ["rows.fields.Field", "rows.fields.Field.deserialize", "rows.fields.as_string"], "cross_file": []}, "requirement": {"Functionality": "Deserialize a value into a date instance. It first calls the parent class's deserialize method to convert the value into a date object. Then, it checks if the value is already None or an instance of allowed type in DateField class. If so, it returns the value as is. Otherwise, it converts the value into a string, and parse the string value into a datetime object and creates a new date object using the year, month, and day attributes of the datetime object.", "Arguments": ":param cls: Class. The DateField class.\n:param value: Object. The value to be deserialized into a DateField instance.\n:param args: Object. Additional positional arguments.\n:param kwargs: Object. Additional keyword arguments.\n:return: date. The deserialized date instance."}, "tests": ["tests/tests_fields.py::FieldsTestCase::test_DateField"], "indent": 4, "domain": "Text-Processing", "code": "    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n", "context": "# coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n###The function: deserialize###\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "prompt": "Please write a python function called 'deserialize' base the context. Deserialize a value into a date instance. It first calls the parent class's deserialize method to convert the value into a date object. Then, it checks if the value is already None or an instance of allowed type in DateField class. If so, it returns the value as is. Otherwise, it converts the value into a string, and parse the string value into a datetime object and creates a new date object using the year, month, and day attributes of the datetime object.:param cls: Class. The DateField class.\n:param value: Object. The value to be deserialized into a DateField instance.\n:param args: Object. Additional positional arguments.\n:param kwargs: Object. Additional keyword arguments.\n:return: date. The deserialized date instance..\n        The context you need to refer to is as follows: # coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n###The function: deserialize###\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "test_list": ["def test_DateField(self):\n    serialized = '2015-05-27'\n    deserialized = datetime.date(2015, 5, 27)\n    self.assertEqual(fields.DateField.TYPE, (datetime.date,))\n    self.assertEqual(fields.DateField.serialize(None), '')\n    self.assertIs(type(fields.DateField.serialize(None)), six.text_type)\n    self.assertIn(type(fields.DateField.deserialize(serialized)), fields.DateField.TYPE)\n    self.assertEqual(fields.DateField.deserialize(serialized), deserialized)\n    self.assertEqual(fields.DateField.deserialize(deserialized), deserialized)\n    self.assertEqual(fields.DateField.deserialize(None), None)\n    self.assertEqual(fields.DateField.deserialize(''), None)\n    self.assertEqual(fields.DateField.serialize(deserialized), serialized)\n    self.assertIs(type(fields.DateField.serialize(deserialized)), six.text_type)\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(42)\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(serialized + 'T00:00:00')\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize('\u00c1lvaro')\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(serialized.encode('utf-8'))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'deserialize' function should correctly convert valid string representations of dates into date objects and return None for null or empty string inputs.", "unit_test": ["def test_deserialize_input_output_conditions(self):\n    self.assertEqual(fields.DateField.deserialize('2023-10-01'), datetime.date(2023, 10, 1))\n    self.assertEqual(fields.DateField.deserialize(''), None)\n    self.assertEqual(fields.DateField.deserialize(None), None)"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_input_output_conditions"}, "Exception Handling": {"requirement": "The 'deserialize' function should raise a ValueError when the input is not a valid date string or date object.", "unit_test": ["def test_deserialize_exception_handling(self):\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize('invalid-date')\n    with self.assertRaises(ValueError):\n        fields.DateField.deserialize(12345)"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_exception_handling"}, "Edge Case Handling": {"requirement": "The 'deserialize' function should handle edge cases such as leap years and the minimum and maximum representable dates.", "unit_test": ["def test_deserialize_edge_case_handling(self):\n    self.assertEqual(fields.DateField.deserialize('2020-02-29'), datetime.date(2020, 2, 29))\n    self.assertEqual(fields.DateField.deserialize('0001-01-01'), datetime.date(1, 1, 1))\n    self.assertEqual(fields.DateField.deserialize('9999-12-31'), datetime.date(9999, 12, 31))"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'deserialize' function to support additional date formats specified in the 'DateField.INPUT_FORMAT'.", "unit_test": ["def test_deserialize_functionality_extension(self):\n    fields.DateField.INPUT_FORMAT = '%d-%m-%Y'\n    self.assertEqual(fields.DateField.deserialize('01-10-2023'), datetime.date(2023, 10, 1))"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'deserialize' function has complete annotation coverage for parameters and return types.", "unit_test": ["def test_deserialize_annotation_coverage(self):\n    from datetime import date\n    annotations = fields.DateField.deserialize.__annotations__\n    self.assertNotIn('value', annotations)\n    self.assertEqual(annotations['return'], date)"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_annotation_coverage"}, "Code Complexity": {"requirement": "The 'deserialize' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": ["def test_deserialize_code_complexity(self):\n    # This is a placeholder for a complexity check tool\n    complexity = calculate_cyclomatic_complexity(fields.DateField.deserialize)\n    self.assertLessEqual(complexity, 5)"], "test": "tests/tests_fields.py::FieldsTestCase::test_code_complexity"}, "Code Standard": {"requirement": "The 'deserialize' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_deserialize_code_standard(self):\n    # This is a placeholder for a PEP 8 compliance check tool\n    pep8_violations = check_pep8_compliance(fields.DateField.deserialize)\n    self.assertEqual(pep8_violations, 0)"], "test": "tests/tests_fields.py::FieldsTestCase::test_code_style"}, "Context Usage Verification": {"requirement": "The 'deserialize' function should utilize the 'rows.fields.DateField.INPUT_FORMAT' context to parse date strings.", "unit_test": ["def test_deserialize_context_usage_verification(self):\n    self.assertIn('rows.fields.DateField.INPUT_FORMAT', fields.DateField.deserialize.__code__.co_names)"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'deserialize' function should correctly use 'rows.fields.DateField.TYPE' to verify the type of deserialized objects.", "unit_test": ["def test_deserialize_context_usage_correctness_verification(self):\n    self.assertIn('rows.fields.DateField.TYPE', fields.DateField.deserialize.__code__.co_names)"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_type_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "rows.fields.EmailField.deserialize", "type": "method", "project_path": "Text-Processing/rows", "completion_path": "Text-Processing/rows/rows/fields.py", "signature_position": [445, 445], "body_position": [446, 454], "dependency": {"intra_class": ["rows.fields.EmailField.EMAIL_REGEXP"], "intra_file": ["rows.fields.TextField", "rows.fields.TextField.deserialize", "rows.fields.value_error"], "cross_file": []}, "requirement": {"Functionality": "Deserialize the input value and validate it as an email field. It first calls the superclass's deserialize method to perform the initial deserialization. Then, it checks if the deserialized value is None or empty. If it is, it returns None. Otherwise, it uses a regular expression to validate the email format. If the email is valid, it returns the first match. If not, it raises a value error.", "Arguments": ":param cls: Class. The class object itself.\n:param value: Any. The value to be deserialized and validated as an email field.\n:param *args: Any. Additional positional arguments.\n:param **kwargs: Any. Additional keyword arguments.\n:return: Object. The deserialized and validated email value, or None if the input value is None or empty."}, "tests": ["tests/tests_fields.py::FieldsTestCase::test_EmailField"], "indent": 4, "domain": "Text-Processing", "code": "    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]\n", "context": "# coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n###The function: deserialize###\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "prompt": "Please write a python function called 'deserialize' base the context. Deserialize the input value and validate it as an email field. It first calls the superclass's deserialize method to perform the initial deserialization. Then, it checks if the deserialized value is None or empty. If it is, it returns None. Otherwise, it uses a regular expression to validate the email format. If the email is valid, it returns the first match. If not, it raises a value error.:param cls: Class. The class object itself.\n:param value: Any. The value to be deserialized and validated as an email field.\n:param *args: Any. Additional positional arguments.\n:param **kwargs: Any. Additional keyword arguments.\n:return: Object. The deserialized and validated email value, or None if the input value is None or empty..\n        The context you need to refer to is as follows: # coding: utf-8\n\n# Copyright 2014-2019 \u00c1lvaro Justen <https://github.com/turicas/rows/>\n\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Lesser General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Lesser General Public License for more details.\n\n#    You should have received a copy of the GNU Lesser General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nimport binascii\nimport datetime\nimport json\nimport locale\nimport re\nfrom base64 import b64decode, b64encode\nfrom collections import OrderedDict, defaultdict\nfrom decimal import Decimal, InvalidOperation\nfrom unicodedata import normalize\n\nimport six\n\nif six.PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\n# Order matters here\n__all__ = [\n    \"BoolField\",\n    \"IntegerField\",\n    \"FloatField\",\n    \"DatetimeField\",\n    \"DateField\",\n    \"DecimalField\",\n    \"PercentField\",\n    \"JSONField\",\n    \"EmailField\",\n    \"TextField\",\n    \"BinaryField\",\n    \"Field\",\n]\nNULL = (\"-\", \"null\", \"none\", \"nil\", \"n/a\", \"na\")\nNULL_BYTES = (b\"-\", b\"null\", b\"none\", b\"nil\", b\"n/a\", b\"na\")\nREGEXP_ONLY_NUMBERS = re.compile(\"[^0-9\\-]\")\nSHOULD_NOT_USE_LOCALE = True  # This variable is changed by rows.locale_manager\nSLUG_CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_\"\n\n\ndef value_error(value, cls):\n    value = repr(value)\n    if len(value) > 50:\n        value = value[:50] + \"...\"\n    raise ValueError(\"Value '{}' can't be {}\".format(value, cls.__name__))\n\n\nclass Field(object):\n    \"\"\"Base Field class - all fields should inherit from this\n\n    As the fallback for all other field types are the BinaryField, this Field\n    actually implements what is expected in the BinaryField\n    \"\"\"\n\n    TYPE = (type(None),)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        \"\"\"Serialize a value to be exported\n\n        `cls.serialize` should always return an unicode value, except for\n        BinaryField\n        \"\"\"\n\n        if value is None:\n            value = \"\"\n        return value\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        \"\"\"Deserialize a value just after importing it\n\n        `cls.deserialize` should always return a value of type `cls.TYPE` or\n        `None`.\n        \"\"\"\n\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return value\n\n\nclass BinaryField(Field):\n    \"\"\"Field class to represent byte arrays\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.binary_type,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                return value\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64decode(value)\n                except (TypeError, ValueError, binascii.Error):\n                    raise ValueError(\"Can't decode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return b\"\"\n\n\nclass BoolField(Field):\n    \"\"\"Base class to representing boolean\n\n    Is not locale-aware (if you need to, please customize by changing its\n    attributes like `TRUE_VALUES` and `FALSE_VALUES`)\n    \"\"\"\n\n    TYPE = (bool,)\n    SERIALIZED_VALUES = {True: \"true\", False: \"false\", None: \"\"}\n    TRUE_VALUES = (\"true\", \"yes\")\n    FALSE_VALUES = (\"false\", \"no\")\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        # TODO: should we serialize `None` as well or give it to the plugin?\n        return cls.SERIALIZED_VALUES[value]\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n\nclass IntegerField(Field):\n    \"\"\"Field class to represent integer\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (int,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%d\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(IntegerField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, float):\n            new_value = int(value)\n            if new_value != value:\n                raise ValueError(\"It's float, not integer\")\n            else:\n                value = new_value\n\n        value = as_string(value)\n        if value != \"0\" and value.startswith(\"0\"):\n            raise ValueError(\"It's string, not integer\")\n        return int(value) if SHOULD_NOT_USE_LOCALE else locale.atoi(value)\n\n\nclass FloatField(Field):\n    \"\"\"Field class to represent float\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (float,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        if SHOULD_NOT_USE_LOCALE:\n            return six.text_type(value)\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            return locale.format(\"%f\", value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(FloatField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return float(value)\n        else:\n            return locale.atof(value)\n\n\nclass DecimalField(Field):\n    \"\"\"Field class to represent decimal data (as Python's decimal.Decimal)\n\n    Is locale-aware\n    \"\"\"\n\n    TYPE = (Decimal,)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        value_as_string = six.text_type(value)\n        if SHOULD_NOT_USE_LOCALE:\n            return value_as_string\n        else:\n            grouping = kwargs.get(\"grouping\", None)\n            has_decimal_places = value_as_string.find(\".\") != -1\n            if not has_decimal_places:\n                string_format = \"%d\"\n            else:\n                decimal_places = len(value_as_string.split(\".\")[1])\n                string_format = \"%.{}f\".format(decimal_places)\n            return locale.format(string_format, value, grouping=grouping)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DecimalField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif type(value) in (int, float):\n            return Decimal(six.text_type(value))\n\n        if SHOULD_NOT_USE_LOCALE:\n            try:\n                return Decimal(value)\n            except InvalidOperation:\n                value_error(value, cls)\n        else:\n            locale_vars = locale.localeconv()\n            decimal_separator = locale_vars[\"decimal_point\"]\n            interesting_vars = (\n                \"decimal_point\",\n                \"mon_decimal_point\",\n                \"mon_thousands_sep\",\n                \"negative_sign\",\n                \"positive_sign\",\n                \"thousands_sep\",\n            )\n            chars = (\n                locale_vars[x].replace(\".\", r\"\\.\").replace(\"-\", r\"\\-\")\n                for x in interesting_vars\n            )\n            interesting_chars = \"\".join(set(chars))\n            regexp = re.compile(r\"[^0-9{} ]\".format(interesting_chars))\n            value = as_string(value)\n            if regexp.findall(value):\n                value_error(value, cls)\n\n            parts = [\n                REGEXP_ONLY_NUMBERS.subn(\"\", number)[0]\n                for number in value.split(decimal_separator)\n            ]\n            if len(parts) > 2:\n                raise ValueError(\"Can't deserialize with this locale.\")\n            try:\n                value = Decimal(parts[0])\n                if len(parts) == 2:\n                    decimal_places = len(parts[1])\n                    value = value + (Decimal(parts[1]) / (10 ** decimal_places))\n            except InvalidOperation:\n                value_error(value, cls)\n            return value\n\n\nclass PercentField(DecimalField):\n    \"\"\"Field class to represent percent values\n\n    Is locale-aware (inherit this behaviour from `rows.DecimalField`)\n    \"\"\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n        elif value == Decimal(\"0\"):\n            return \"0.00%\"\n\n        value = Decimal(six.text_type(value * 100)[:-2])\n        value = super(PercentField, cls).serialize(value, *args, **kwargs)\n        return \"{}%\".format(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n\n        value = as_string(value)\n        if \"%\" not in value:\n            value_error(value, cls)\n        value = value.replace(\"%\", \"\")\n        return super(PercentField, cls).deserialize(value) / 100\n\n\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)\n\n\nclass DatetimeField(Field):\n    \"\"\"Field class to represent date-time\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.datetime,)\n    DATETIME_REGEXP = re.compile(\n        \"^([0-9]{4})-([0-9]{2})-([0-9]{2})[ T]\" \"([0-9]{2}):([0-9]{2}):([0-9]{2})$\"\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DatetimeField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        # TODO: may use iso8601\n        groups = cls.DATETIME_REGEXP.findall(value)\n        if not groups:\n            value_error(value, cls)\n        else:\n            return datetime.datetime(*[int(x) for x in groups[0]])\n\n\nclass TextField(Field):\n    \"\"\"Field class to represent unicode strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (six.text_type,)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)\n\n\nclass EmailField(TextField):\n    \"\"\"Field class to represent e-mail addresses\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n###The function: deserialize###\n\nclass JSONField(Field):\n    \"\"\"Field class to represent JSON-encoded strings\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (list, dict)\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        return json.dumps(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n\ndef as_string(value):\n    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n\ndef is_null(value):\n    if value is None:\n        return True\n    elif type(value) is six.binary_type:\n        value = value.strip().lower()\n        return not value or value in NULL_BYTES\n    else:\n        value_str = as_string(value).strip().lower()\n        return not value_str or value_str in NULL\n\n\ndef unique_values(values):\n    result = []\n    for value in values:\n        if not is_null(value) and value not in result:\n            result.append(value)\n    return result\n\n\ndef get_items(*indexes):\n    \"\"\"Return a callable that fetches the given indexes of an object\n    Always return a tuple even when len(indexes) == 1.\n\n    Similar to `operator.itemgetter`, but will insert `None` when the object\n    does not have the desired index (instead of raising IndexError).\n    \"\"\"\n    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )\n\n\ndef slug(text, separator=\"_\", permitted_chars=SLUG_CHARS, replace_with_separator=\" -_\"):\n    \"\"\"Generate a slug for the `text`.\n\n    >>> slug(' \u00c1LVARO  justen% ')\n    'alvaro_justen'\n    >>> slug(' \u00c1LVARO  justen% ', separator='-')\n    'alvaro-justen'\n    \"\"\"\n\n    text = six.text_type(text or \"\")\n\n    # Strip non-ASCII characters\n    # Example: u' \u00c1LVARO  justen% ' -> ' ALVARO  justen% '\n    text = normalize(\"NFKD\", text.strip()).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Replace spaces and other chars with separator\n    # Example: u' ALVARO  justen% ' -> u'_ALVARO__justen%_'\n    for char in replace_with_separator:\n        text = text.replace(char, separator)\n\n    # Remove non-permitted characters and put everything to lowercase\n    # Example: u'_ALVARO__justen%_' -> u'_alvaro__justen_'\n    text = \"\".join(char for char in text if char in permitted_chars).lower()\n\n    # Remove double occurrencies of separator\n    # Example: u'_alvaro__justen_' -> u'_alvaro_justen_'\n    double_separator = separator + separator\n    while double_separator in text:\n        text = text.replace(double_separator, separator)\n\n    # Strip separators\n    # Example: u'_alvaro_justen_' -> u'alvaro_justen'\n    return text.strip(separator)\n\n\ndef make_unique_name(name, existing_names, name_format=\"{name}_{index}\", start=2):\n    \"\"\"Return a unique name based on `name_format` and `name`.\"\"\"\n    index = start\n    new_name = name\n    while new_name in existing_names:\n        new_name = name_format.format(name=name, index=index)\n        index += 1\n\n    return new_name\n\n\ndef make_header(field_names, permit_not=False):\n    \"\"\"Return unique and slugged field names.\"\"\"\n    slug_chars = SLUG_CHARS if not permit_not else SLUG_CHARS + \"^\"\n\n    header = [\n        slug(field_name, permitted_chars=slug_chars) for field_name in field_names\n    ]\n    result = []\n    for index, field_name in enumerate(header):\n        if not field_name:\n            field_name = \"field_{}\".format(index)\n        elif field_name[0].isdigit():\n            field_name = \"field_{}\".format(field_name)\n\n        if field_name in result:\n            field_name = make_unique_name(\n                name=field_name, existing_names=result, start=2\n            )\n        result.append(field_name)\n\n    return result\n\n\nDEFAULT_TYPES = (\n    BoolField,\n    IntegerField,\n    FloatField,\n    DecimalField,\n    PercentField,\n    DecimalField,\n    DatetimeField,\n    DateField,\n    JSONField,\n    TextField,\n    BinaryField,\n)\n\n\nclass TypeDetector(object):\n    \"\"\"Detect data types based on a list of Field classes\"\"\"\n\n    def __init__(\n        self,\n        field_names=None,\n        field_types=DEFAULT_TYPES,\n        fallback_type=TextField,\n        skip_indexes=None,\n    ):\n        self.field_names = field_names or []\n        self.field_types = list(field_types)\n        self.fallback_type = fallback_type\n        self._possible_types = defaultdict(lambda: list(self.field_types))\n        self._samples = []\n        self._skip = skip_indexes or tuple()\n\n    def check_type(self, index, value):\n        for type_ in self._possible_types[index][:]:\n            try:\n                type_.deserialize(value)\n            except (ValueError, TypeError):\n                self._possible_types[index].remove(type_)\n\n    def process_row(self, row):\n        for index, value in enumerate(row):\n            if index in self._skip:\n                continue\n            self.check_type(index, value)\n\n    def feed(self, data):\n        for row in data:\n            self.process_row(row)\n\n    def priority(self, *field_types):\n        \"\"\"Decide the priority between each possible type\"\"\"\n\n        return field_types[0] if field_types else self.fallback_type\n\n    @property\n    def fields(self):\n        possible, skip = self._possible_types, self._skip\n\n        if possible:\n            # Create a header with placeholder values for each detected column\n            # and then join this placeholders with original header - the\n            # original header may have less columns then the detected ones, so\n            # we end with a full header having a name for every possible\n            # column.\n            placeholders = make_header(range(max(possible.keys()) + 1))\n            header = [a or b for a, b in zip_longest(self.field_names, placeholders)]\n        else:\n            header = self.field_names\n\n        return OrderedDict(\n            [\n                (\n                    field_name,\n                    self.priority(*(possible[index] if index in possible else [])),\n                )\n                for index, field_name in enumerate(header)\n                if index not in skip\n            ]\n        )\n\n\ndef detect_types(\n    field_names,\n    field_values,\n    field_types=DEFAULT_TYPES,\n    skip_indexes=None,\n    type_detector=TypeDetector,\n    fallback_type=TextField,\n    *args,\n    **kwargs\n):\n    \"\"\"Detect column types (or \"where the magic happens\")\"\"\"\n\n    # TODO: look strategy of csv.Sniffer.has_header\n    # TODO: may receive 'type hints'\n    detector = type_detector(\n        field_names,\n        field_types=field_types,\n        fallback_type=fallback_type,\n        skip_indexes=skip_indexes,\n    )\n    detector.feed(field_values)\n    return detector.fields\n\n\ndef identify_type(value):\n    \"\"\"Identify the field type for a specific value\"\"\"\n\n    return detect_types([\"name\"], [[value]])[\"name\"]\n", "test_list": ["def test_EmailField(self):\n    serialized = 'test@domain.com'\n    self.assertEqual(fields.EmailField.TYPE, (six.text_type,))\n    deserialized = fields.EmailField.deserialize(serialized)\n    self.assertIn(type(deserialized), fields.EmailField.TYPE)\n    self.assertEqual(fields.EmailField.serialize(None), '')\n    self.assertIs(type(fields.EmailField.serialize(None)), six.text_type)\n    self.assertEqual(fields.EmailField.serialize(serialized), serialized)\n    self.assertEqual(fields.EmailField.deserialize(serialized), serialized)\n    self.assertEqual(fields.EmailField.deserialize(None), None)\n    self.assertEqual(fields.EmailField.deserialize(''), None)\n    self.assertIs(type(fields.EmailField.serialize(serialized)), six.text_type)\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize(42)\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('2015-01-01')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('\u00c1lvaro')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('test@example.com'.encode('utf-8'))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'deserialize' function should correctly return None when the input value is None or an empty string, and should return a valid email string when the input is a valid email.", "unit_test": ["def test_deserialize_input_output_conditions(self):\n    self.assertIsNone(fields.EmailField.deserialize(None))\n    self.assertIsNone(fields.EmailField.deserialize(''))\n    self.assertEqual(fields.EmailField.deserialize('test@domain.com'), 'test@domain.com')"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_input_output_conditions"}, "Exception Handling": {"requirement": "The 'deserialize' function should raise a ValueError when the input value is not a valid email format.", "unit_test": ["def test_deserialize_exception_handling(self):\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('invalid-email')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('missing@domain')\n    with self.assertRaises(ValueError):\n        fields.EmailField.deserialize('missingdomain.com')"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_exception_handling"}, "Edge Case Handling": {"requirement": "The 'deserialize' function should handle edge cases such as emails with subdomains and plus signs correctly.", "unit_test": ["def test_deserialize_edge_case_handling(self):\n    self.assertEqual(fields.EmailField.deserialize('user+tag@sub.domain.com'), 'user+tag@sub.domain.com')\n    self.assertEqual(fields.EmailField.deserialize('user@sub.domain.com'), 'user@sub.domain.com')"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the 'deserialize' function to support email validation with international domain names.", "unit_test": ["def test_deserialize_functionality_extension(self):\n    self.assertEqual(fields.EmailField.deserialize('user@xn--d1acj3b.com'), 'user@xn--d1acj3b.com')\n    self.assertEqual(fields.EmailField.deserialize('user@xn--bcher-kva.ch'), 'user@xn--bcher-kva.ch')"], "test": "tests/tests_fields.py::FieldsTestCase::test_deserialize_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'deserialize' function has complete parameter and return type annotations.", "unit_test": ["def test_annotation_coverage(self):\n    from inspect import signature\n    sig = signature(fields.EmailField.deserialize)\n    self.assertEqual(sig.parameters['value'].annotation, str)\n    self.assertEqual(sig.return_annotation, str)"], "test": "tests/tests_fields.py::FieldsTestCase::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'deserialize' function should maintain a cyclomatic complexity of 5 or lower.", "unit_test": ["def test_code_complexity(self):\n    import radon.complexity as rc\n    from radon.visitors import ComplexityVisitor\n    code = '''\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        value = as_string(value)\n        if cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            raise ValueError('Invalid email format')\n    '''\n    visitor = ComplexityVisitor.from_code(code)\n    self.assertLessEqual(visitor.functions[0].complexity, 5)"], "test": "tests/tests_fields.py::FieldsTestCase::test_EmailField_complexity"}, "Code Standard": {"requirement": "The 'deserialize' function should adhere to PEP 8 standards, including proper indentation and line length.", "unit_test": ["def test_code_standard(self):\n    import pep8\n    style_guide = pep8.StyleGuide(quiet=True)\n    result = style_guide.check_files(['fields.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/tests_fields.py::FieldsTestCase::test_EmailField_style"}, "Context Usage Verification": {"requirement": "The 'deserialize' function should utilize the EMAIL_REGEXP from the context for email validation.", "unit_test": ["def test_context_usage_verification(self):\n    self.assertTrue(hasattr(fields.EmailField, 'EMAIL_REGEXP'))\n    self.assertIsInstance(fields.EmailField.EMAIL_REGEXP, re.Pattern)"], "test": "tests/tests_fields.py::FieldsTestCase::test_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The 'deserialize' function should correctly use the EMAIL_REGEXP to match valid email formats.", "unit_test": ["def test_context_usage_correctness_verification(self):\n    valid_email = 'test@domain.com'\n    invalid_email = 'invalid-email'\n    self.assertIsNotNone(fields.EmailField.EMAIL_REGEXP.match(valid_email))\n    self.assertIsNone(fields.EmailField.EMAIL_REGEXP.match(invalid_email))"], "test": "tests/tests_fields.py::FieldsTestCase::test_context_usage_correctness_verification"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "playhouse.dataset.DataSet.freeze", "type": "method", "project_path": "Software-Development/peewee", "completion_path": "Software-Development/peewee/playhouse/dataset.py", "signature_position": [162, 163], "body_position": [164, 172], "dependency": {"intra_class": ["playhouse.dataset.DataSet._check_arguments", "playhouse.dataset.DataSet._export_formats"], "intra_file": ["playhouse.dataset.TSVExporter.export", "playhouse.dataset.open_file"], "cross_file": []}, "requirement": {"Functionality": "Freeze the dataset by exporting it to a file in the specified format. It checks the arguments, opens the file if a filename is provided, creates an exporter instance based on the format, and exports the dataset to the file. Finally, it closes the file if it was opened.", "Arguments": ":param self: DataSet. An instance of the DataSet class.\n:param query: The query to export.\n:param format: String. The format in which to export the dataset. Defaults to 'csv'.\n:param filename: String. The name of the file to export to. If provided, the file will be opened and closed automatically.\n:param file_obj: File object. The file object to export to. If provided, the file will not be opened or closed automatically.\n:param encoding: String. The encoding to use when opening the file. Defaults to 'utf8'.\n:param kwargs: Additional keyword arguments to pass to the exporter's export method.\n:return: No return values."}, "tests": ["tests/dataset.py::TestDataSet::test_export", "tests/dataset.py::TestDataSet::test_freeze_thaw"], "indent": 4, "domain": "Software-Development", "code": "    def freeze(self, query, format='csv', filename=None, file_obj=None,\n               encoding='utf8', **kwargs):\n        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n", "context": "import csv\nimport datetime\nfrom decimal import Decimal\nimport json\nimport operator\ntry:\n    from urlparse import urlparse\nexcept ImportError:\n    from urllib.parse import urlparse\nimport sys\nimport uuid\n\nfrom peewee import *\nfrom playhouse.db_url import connect\nfrom playhouse.migrate import migrate\nfrom playhouse.migrate import SchemaMigrator\nfrom playhouse.reflection import Introspector\n\nif sys.version_info[0] == 3:\n    basestring = str\n    from functools import reduce\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode, encoding=encoding)\nelse:\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode)\n\n\nclass DataSet(object):\n    def __init__(self, url, include_views=False, **kwargs):\n        if isinstance(url, Database):\n            self._url = None\n            self._database = url\n            self._database_path = self._database.database\n        else:\n            self._url = url\n            parse_result = urlparse(url)\n            self._database_path = parse_result.path[1:]\n\n            # Connect to the database.\n            self._database = connect(url)\n\n        # Open a connection if one does not already exist.\n        self._database.connect(reuse_if_open=True)\n\n        # Introspect the database and generate models.\n        self._introspector = Introspector.from_database(self._database)\n        self._include_views = include_views\n        self._models = self._introspector.generate_models(\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views,\n            **kwargs)\n        self._migrator = SchemaMigrator.from_database(self._database)\n\n        class BaseModel(Model):\n            class Meta:\n                database = self._database\n        self._base_model = BaseModel\n        self._export_formats = self.get_export_formats()\n        self._import_formats = self.get_import_formats()\n\n    def __repr__(self):\n        return '<DataSet: %s>' % self._database_path\n\n    def get_export_formats(self):\n        return {\n            'csv': CSVExporter,\n            'json': JSONExporter,\n            'tsv': TSVExporter}\n\n    def get_import_formats(self):\n        return {\n            'csv': CSVImporter,\n            'json': JSONImporter,\n            'tsv': TSVImporter}\n\n    def __getitem__(self, table):\n        if table not in self._models and table in self.tables:\n            self.update_cache(table)\n        return Table(self, table, self._models.get(table))\n\n    @property\n    def tables(self):\n        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self.views\n        return tables\n\n    @property\n    def views(self):\n        return [v.name for v in self._database.get_views()]\n\n    def __contains__(self, table):\n        return table in self.tables\n\n    def connect(self, reuse_if_open=False):\n        self._database.connect(reuse_if_open=reuse_if_open)\n\n    def close(self):\n        self._database.close()\n\n    def update_cache(self, table=None):\n        if table:\n            dependencies = [table]\n            if table in self._models:\n                model_class = self._models[table]\n                dependencies.extend([\n                    related._meta.table_name for _, related, _ in\n                    model_class._meta.model_graph()])\n            else:\n                dependencies.extend(self.get_table_dependencies(table))\n        else:\n            dependencies = None  # Update all tables.\n            self._models = {}\n        updated = self._introspector.generate_models(\n            skip_invalid=True,\n            table_names=dependencies,\n            literal_column_names=True,\n            include_views=self._include_views)\n        self._models.update(updated)\n\n    def get_table_dependencies(self, table):\n        stack = [table]\n        accum = []\n        seen = set()\n        while stack:\n            table = stack.pop()\n            for fk_meta in self._database.get_foreign_keys(table):\n                dest = fk_meta.dest_table\n                if dest not in seen:\n                    stack.append(dest)\n                    accum.append(dest)\n        return accum\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if not self._database.is_closed():\n            self.close()\n\n    def query(self, sql, params=None):\n        return self._database.execute_sql(sql, params)\n\n    def transaction(self):\n        return self._database.atomic()\n\n    def _check_arguments(self, filename, file_obj, format, format_dict):\n        if filename and file_obj:\n            raise ValueError('file is over-specified. Please use either '\n                             'filename or file_obj, but not both.')\n        if not filename and not file_obj:\n            raise ValueError('A filename or file-like object must be '\n                             'specified.')\n        if format not in format_dict:\n            valid_formats = ', '.join(sorted(format_dict.keys()))\n            raise ValueError('Unsupported format \"%s\". Use one of %s.' % (\n                format, valid_formats))\n\n###The function: freeze###\n    def thaw(self, table, format='csv', filename=None, file_obj=None,\n             strict=False, encoding='utf8', **kwargs):\n        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'r', encoding)\n\n        importer = self._import_formats[format](self[table], strict)\n        count = importer.load(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n\n        return count\n\n\nclass Table(object):\n    def __init__(self, dataset, name, model_class):\n        self.dataset = dataset\n        self.name = name\n        if model_class is None:\n            model_class = self._create_model()\n            model_class.create_table()\n            self.dataset._models[name] = model_class\n\n    @property\n    def model_class(self):\n        return self.dataset._models[self.name]\n\n    def __repr__(self):\n        return '<Table: %s>' % self.name\n\n    def __len__(self):\n        return self.find().count()\n\n    def __iter__(self):\n        return iter(self.find().iterator())\n\n    def _create_model(self):\n        class Meta:\n            table_name = self.name\n        return type(\n            str(self.name),\n            (self.dataset._base_model,),\n            {'Meta': Meta})\n\n    def create_index(self, columns, unique=False):\n        index = ModelIndex(self.model_class, columns, unique=unique)\n        self.model_class.add_index(index)\n        self.dataset._database.execute(index)\n\n    def _guess_field_type(self, value):\n        if isinstance(value, basestring):\n            return TextField\n        if isinstance(value, (datetime.date, datetime.datetime)):\n            return DateTimeField\n        elif value is True or value is False:\n            return BooleanField\n        elif isinstance(value, int):\n            return IntegerField\n        elif isinstance(value, float):\n            return FloatField\n        elif isinstance(value, Decimal):\n            return DecimalField\n        return TextField\n\n    @property\n    def columns(self):\n        return [f.name for f in self.model_class._meta.sorted_fields]\n\n    def _migrate_new_columns(self, data):\n        new_keys = set(data) - set(self.model_class._meta.fields)\n        new_keys -= set(self.model_class._meta.columns)\n        if new_keys:\n            operations = []\n            for key in new_keys:\n                field_class = self._guess_field_type(data[key])\n                field = field_class(null=True)\n                operations.append(\n                    self.dataset._migrator.add_column(self.name, key, field))\n                field.bind(self.model_class, key)\n\n            migrate(*operations)\n\n            self.dataset.update_cache(self.name)\n\n    def __getitem__(self, item):\n        try:\n            return self.model_class[item]\n        except self.model_class.DoesNotExist:\n            pass\n\n    def __setitem__(self, item, value):\n        if not isinstance(value, dict):\n            raise ValueError('Table.__setitem__() value must be a dict')\n\n        pk = self.model_class._meta.primary_key\n        value[pk.name] = item\n\n        try:\n            with self.dataset.transaction() as txn:\n                self.insert(**value)\n        except IntegrityError:\n            self.dataset.update_cache(self.name)\n            self.update(columns=[pk.name], **value)\n\n    def __delitem__(self, item):\n        del self.model_class[item]\n\n    def insert(self, **data):\n        self._migrate_new_columns(data)\n        return self.model_class.insert(**data).execute()\n\n    def _apply_where(self, query, filters, conjunction=None):\n        conjunction = conjunction or operator.and_\n        if filters:\n            expressions = [\n                (self.model_class._meta.fields[column] == value)\n                for column, value in filters.items()]\n            query = query.where(reduce(conjunction, expressions))\n        return query\n\n    def update(self, columns=None, conjunction=None, **data):\n        self._migrate_new_columns(data)\n        filters = {}\n        if columns:\n            for column in columns:\n                filters[column] = data.pop(column)\n\n        return self._apply_where(\n            self.model_class.update(**data),\n            filters,\n            conjunction).execute()\n\n    def _query(self, **query):\n        return self._apply_where(self.model_class.select(), query)\n\n    def find(self, **query):\n        return self._query(**query).dicts()\n\n    def find_one(self, **query):\n        try:\n            return self.find(**query).get()\n        except self.model_class.DoesNotExist:\n            return None\n\n    def all(self):\n        return self.find()\n\n    def delete(self, **query):\n        return self._apply_where(self.model_class.delete(), query).execute()\n\n    def freeze(self, *args, **kwargs):\n        return self.dataset.freeze(self.all(), *args, **kwargs)\n\n    def thaw(self, *args, **kwargs):\n        return self.dataset.thaw(self.name, *args, **kwargs)\n\n\nclass Exporter(object):\n    def __init__(self, query):\n        self.query = query\n\n    def export(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONExporter(Exporter):\n    def __init__(self, query, iso8601_datetimes=False):\n        super(JSONExporter, self).__init__(query)\n        self.iso8601_datetimes = iso8601_datetimes\n\n    def _make_default(self):\n        datetime_types = (datetime.datetime, datetime.date, datetime.time)\n\n        if self.iso8601_datetimes:\n            def default(o):\n                if isinstance(o, datetime_types):\n                    return o.isoformat()\n                elif isinstance(o, (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        else:\n            def default(o):\n                if isinstance(o, datetime_types + (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        return default\n\n    def export(self, file_obj, **kwargs):\n        json.dump(\n            list(self.query),\n            file_obj,\n            default=self._make_default(),\n            **kwargs)\n\n\nclass CSVExporter(Exporter):\n    def export(self, file_obj, header=True, **kwargs):\n        writer = csv.writer(file_obj, **kwargs)\n        tuples = self.query.tuples().execute()\n        tuples.initialize()\n        if header and getattr(tuples, 'columns', None):\n            writer.writerow([column for column in tuples.columns])\n        for row in tuples:\n            writer.writerow(row)\n\n\nclass TSVExporter(CSVExporter):\n    def export(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVExporter, self).export(file_obj, header, **kwargs)\n\n\nclass Importer(object):\n    def __init__(self, table, strict=False):\n        self.table = table\n        self.strict = strict\n\n        model = self.table.model_class\n        self.columns = model._meta.columns\n        self.columns.update(model._meta.fields)\n\n    def load(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONImporter(Importer):\n    def load(self, file_obj, **kwargs):\n        data = json.load(file_obj, **kwargs)\n        count = 0\n\n        for row in data:\n            if self.strict:\n                obj = {}\n                for key in row:\n                    field = self.columns.get(key)\n                    if field is not None:\n                        obj[field.name] = field.python_value(row[key])\n            else:\n                obj = row\n\n            if obj:\n                self.table.insert(**obj)\n                count += 1\n\n        return count\n\n\nclass CSVImporter(Importer):\n    def load(self, file_obj, header=True, **kwargs):\n        count = 0\n        reader = csv.reader(file_obj, **kwargs)\n        if header:\n            try:\n                header_keys = next(reader)\n            except StopIteration:\n                return count\n\n            if self.strict:\n                header_fields = []\n                for idx, key in enumerate(header_keys):\n                    if key in self.columns:\n                        header_fields.append((idx, self.columns[key]))\n            else:\n                header_fields = list(enumerate(header_keys))\n        else:\n            header_fields = list(enumerate(self.model._meta.sorted_fields))\n\n        if not header_fields:\n            return count\n\n        for row in reader:\n            obj = {}\n            for idx, field in header_fields:\n                if self.strict:\n                    obj[field.name] = field.python_value(row[idx])\n                else:\n                    obj[field] = row[idx]\n\n            self.table.insert(**obj)\n            count += 1\n\n        return count\n\n\nclass TSVImporter(CSVImporter):\n    def load(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVImporter, self).load(file_obj, header, **kwargs)\n", "prompt": "Please write a python function called 'freeze' base the context. Freeze the dataset by exporting it to a file in the specified format. It checks the arguments, opens the file if a filename is provided, creates an exporter instance based on the format, and exports the dataset to the file. Finally, it closes the file if it was opened.:param self: DataSet. An instance of the DataSet class.\n:param query: The query to export.\n:param format: String. The format in which to export the dataset. Defaults to 'csv'.\n:param filename: String. The name of the file to export to. If provided, the file will be opened and closed automatically.\n:param file_obj: File object. The file object to export to. If provided, the file will not be opened or closed automatically.\n:param encoding: String. The encoding to use when opening the file. Defaults to 'utf8'.\n:param kwargs: Additional keyword arguments to pass to the exporter's export method.\n:return: No return values..\n        The context you need to refer to is as follows: import csv\nimport datetime\nfrom decimal import Decimal\nimport json\nimport operator\ntry:\n    from urlparse import urlparse\nexcept ImportError:\n    from urllib.parse import urlparse\nimport sys\nimport uuid\n\nfrom peewee import *\nfrom playhouse.db_url import connect\nfrom playhouse.migrate import migrate\nfrom playhouse.migrate import SchemaMigrator\nfrom playhouse.reflection import Introspector\n\nif sys.version_info[0] == 3:\n    basestring = str\n    from functools import reduce\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode, encoding=encoding)\nelse:\n    def open_file(f, mode, encoding='utf8'):\n        return open(f, mode)\n\n\nclass DataSet(object):\n    def __init__(self, url, include_views=False, **kwargs):\n        if isinstance(url, Database):\n            self._url = None\n            self._database = url\n            self._database_path = self._database.database\n        else:\n            self._url = url\n            parse_result = urlparse(url)\n            self._database_path = parse_result.path[1:]\n\n            # Connect to the database.\n            self._database = connect(url)\n\n        # Open a connection if one does not already exist.\n        self._database.connect(reuse_if_open=True)\n\n        # Introspect the database and generate models.\n        self._introspector = Introspector.from_database(self._database)\n        self._include_views = include_views\n        self._models = self._introspector.generate_models(\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views,\n            **kwargs)\n        self._migrator = SchemaMigrator.from_database(self._database)\n\n        class BaseModel(Model):\n            class Meta:\n                database = self._database\n        self._base_model = BaseModel\n        self._export_formats = self.get_export_formats()\n        self._import_formats = self.get_import_formats()\n\n    def __repr__(self):\n        return '<DataSet: %s>' % self._database_path\n\n    def get_export_formats(self):\n        return {\n            'csv': CSVExporter,\n            'json': JSONExporter,\n            'tsv': TSVExporter}\n\n    def get_import_formats(self):\n        return {\n            'csv': CSVImporter,\n            'json': JSONImporter,\n            'tsv': TSVImporter}\n\n    def __getitem__(self, table):\n        if table not in self._models and table in self.tables:\n            self.update_cache(table)\n        return Table(self, table, self._models.get(table))\n\n    @property\n    def tables(self):\n        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self.views\n        return tables\n\n    @property\n    def views(self):\n        return [v.name for v in self._database.get_views()]\n\n    def __contains__(self, table):\n        return table in self.tables\n\n    def connect(self, reuse_if_open=False):\n        self._database.connect(reuse_if_open=reuse_if_open)\n\n    def close(self):\n        self._database.close()\n\n    def update_cache(self, table=None):\n        if table:\n            dependencies = [table]\n            if table in self._models:\n                model_class = self._models[table]\n                dependencies.extend([\n                    related._meta.table_name for _, related, _ in\n                    model_class._meta.model_graph()])\n            else:\n                dependencies.extend(self.get_table_dependencies(table))\n        else:\n            dependencies = None  # Update all tables.\n            self._models = {}\n        updated = self._introspector.generate_models(\n            skip_invalid=True,\n            table_names=dependencies,\n            literal_column_names=True,\n            include_views=self._include_views)\n        self._models.update(updated)\n\n    def get_table_dependencies(self, table):\n        stack = [table]\n        accum = []\n        seen = set()\n        while stack:\n            table = stack.pop()\n            for fk_meta in self._database.get_foreign_keys(table):\n                dest = fk_meta.dest_table\n                if dest not in seen:\n                    stack.append(dest)\n                    accum.append(dest)\n        return accum\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if not self._database.is_closed():\n            self.close()\n\n    def query(self, sql, params=None):\n        return self._database.execute_sql(sql, params)\n\n    def transaction(self):\n        return self._database.atomic()\n\n    def _check_arguments(self, filename, file_obj, format, format_dict):\n        if filename and file_obj:\n            raise ValueError('file is over-specified. Please use either '\n                             'filename or file_obj, but not both.')\n        if not filename and not file_obj:\n            raise ValueError('A filename or file-like object must be '\n                             'specified.')\n        if format not in format_dict:\n            valid_formats = ', '.join(sorted(format_dict.keys()))\n            raise ValueError('Unsupported format \"%s\". Use one of %s.' % (\n                format, valid_formats))\n\n###The function: freeze###\n    def thaw(self, table, format='csv', filename=None, file_obj=None,\n             strict=False, encoding='utf8', **kwargs):\n        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'r', encoding)\n\n        importer = self._import_formats[format](self[table], strict)\n        count = importer.load(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n\n        return count\n\n\nclass Table(object):\n    def __init__(self, dataset, name, model_class):\n        self.dataset = dataset\n        self.name = name\n        if model_class is None:\n            model_class = self._create_model()\n            model_class.create_table()\n            self.dataset._models[name] = model_class\n\n    @property\n    def model_class(self):\n        return self.dataset._models[self.name]\n\n    def __repr__(self):\n        return '<Table: %s>' % self.name\n\n    def __len__(self):\n        return self.find().count()\n\n    def __iter__(self):\n        return iter(self.find().iterator())\n\n    def _create_model(self):\n        class Meta:\n            table_name = self.name\n        return type(\n            str(self.name),\n            (self.dataset._base_model,),\n            {'Meta': Meta})\n\n    def create_index(self, columns, unique=False):\n        index = ModelIndex(self.model_class, columns, unique=unique)\n        self.model_class.add_index(index)\n        self.dataset._database.execute(index)\n\n    def _guess_field_type(self, value):\n        if isinstance(value, basestring):\n            return TextField\n        if isinstance(value, (datetime.date, datetime.datetime)):\n            return DateTimeField\n        elif value is True or value is False:\n            return BooleanField\n        elif isinstance(value, int):\n            return IntegerField\n        elif isinstance(value, float):\n            return FloatField\n        elif isinstance(value, Decimal):\n            return DecimalField\n        return TextField\n\n    @property\n    def columns(self):\n        return [f.name for f in self.model_class._meta.sorted_fields]\n\n    def _migrate_new_columns(self, data):\n        new_keys = set(data) - set(self.model_class._meta.fields)\n        new_keys -= set(self.model_class._meta.columns)\n        if new_keys:\n            operations = []\n            for key in new_keys:\n                field_class = self._guess_field_type(data[key])\n                field = field_class(null=True)\n                operations.append(\n                    self.dataset._migrator.add_column(self.name, key, field))\n                field.bind(self.model_class, key)\n\n            migrate(*operations)\n\n            self.dataset.update_cache(self.name)\n\n    def __getitem__(self, item):\n        try:\n            return self.model_class[item]\n        except self.model_class.DoesNotExist:\n            pass\n\n    def __setitem__(self, item, value):\n        if not isinstance(value, dict):\n            raise ValueError('Table.__setitem__() value must be a dict')\n\n        pk = self.model_class._meta.primary_key\n        value[pk.name] = item\n\n        try:\n            with self.dataset.transaction() as txn:\n                self.insert(**value)\n        except IntegrityError:\n            self.dataset.update_cache(self.name)\n            self.update(columns=[pk.name], **value)\n\n    def __delitem__(self, item):\n        del self.model_class[item]\n\n    def insert(self, **data):\n        self._migrate_new_columns(data)\n        return self.model_class.insert(**data).execute()\n\n    def _apply_where(self, query, filters, conjunction=None):\n        conjunction = conjunction or operator.and_\n        if filters:\n            expressions = [\n                (self.model_class._meta.fields[column] == value)\n                for column, value in filters.items()]\n            query = query.where(reduce(conjunction, expressions))\n        return query\n\n    def update(self, columns=None, conjunction=None, **data):\n        self._migrate_new_columns(data)\n        filters = {}\n        if columns:\n            for column in columns:\n                filters[column] = data.pop(column)\n\n        return self._apply_where(\n            self.model_class.update(**data),\n            filters,\n            conjunction).execute()\n\n    def _query(self, **query):\n        return self._apply_where(self.model_class.select(), query)\n\n    def find(self, **query):\n        return self._query(**query).dicts()\n\n    def find_one(self, **query):\n        try:\n            return self.find(**query).get()\n        except self.model_class.DoesNotExist:\n            return None\n\n    def all(self):\n        return self.find()\n\n    def delete(self, **query):\n        return self._apply_where(self.model_class.delete(), query).execute()\n\n    def freeze(self, *args, **kwargs):\n        return self.dataset.freeze(self.all(), *args, **kwargs)\n\n    def thaw(self, *args, **kwargs):\n        return self.dataset.thaw(self.name, *args, **kwargs)\n\n\nclass Exporter(object):\n    def __init__(self, query):\n        self.query = query\n\n    def export(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONExporter(Exporter):\n    def __init__(self, query, iso8601_datetimes=False):\n        super(JSONExporter, self).__init__(query)\n        self.iso8601_datetimes = iso8601_datetimes\n\n    def _make_default(self):\n        datetime_types = (datetime.datetime, datetime.date, datetime.time)\n\n        if self.iso8601_datetimes:\n            def default(o):\n                if isinstance(o, datetime_types):\n                    return o.isoformat()\n                elif isinstance(o, (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        else:\n            def default(o):\n                if isinstance(o, datetime_types + (Decimal, uuid.UUID)):\n                    return str(o)\n                raise TypeError('Unable to serialize %r as JSON' % o)\n        return default\n\n    def export(self, file_obj, **kwargs):\n        json.dump(\n            list(self.query),\n            file_obj,\n            default=self._make_default(),\n            **kwargs)\n\n\nclass CSVExporter(Exporter):\n    def export(self, file_obj, header=True, **kwargs):\n        writer = csv.writer(file_obj, **kwargs)\n        tuples = self.query.tuples().execute()\n        tuples.initialize()\n        if header and getattr(tuples, 'columns', None):\n            writer.writerow([column for column in tuples.columns])\n        for row in tuples:\n            writer.writerow(row)\n\n\nclass TSVExporter(CSVExporter):\n    def export(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVExporter, self).export(file_obj, header, **kwargs)\n\n\nclass Importer(object):\n    def __init__(self, table, strict=False):\n        self.table = table\n        self.strict = strict\n\n        model = self.table.model_class\n        self.columns = model._meta.columns\n        self.columns.update(model._meta.fields)\n\n    def load(self, file_obj):\n        raise NotImplementedError\n\n\nclass JSONImporter(Importer):\n    def load(self, file_obj, **kwargs):\n        data = json.load(file_obj, **kwargs)\n        count = 0\n\n        for row in data:\n            if self.strict:\n                obj = {}\n                for key in row:\n                    field = self.columns.get(key)\n                    if field is not None:\n                        obj[field.name] = field.python_value(row[key])\n            else:\n                obj = row\n\n            if obj:\n                self.table.insert(**obj)\n                count += 1\n\n        return count\n\n\nclass CSVImporter(Importer):\n    def load(self, file_obj, header=True, **kwargs):\n        count = 0\n        reader = csv.reader(file_obj, **kwargs)\n        if header:\n            try:\n                header_keys = next(reader)\n            except StopIteration:\n                return count\n\n            if self.strict:\n                header_fields = []\n                for idx, key in enumerate(header_keys):\n                    if key in self.columns:\n                        header_fields.append((idx, self.columns[key]))\n            else:\n                header_fields = list(enumerate(header_keys))\n        else:\n            header_fields = list(enumerate(self.model._meta.sorted_fields))\n\n        if not header_fields:\n            return count\n\n        for row in reader:\n            obj = {}\n            for idx, field in header_fields:\n                if self.strict:\n                    obj[field.name] = field.python_value(row[idx])\n                else:\n                    obj[field] = row[idx]\n\n            self.table.insert(**obj)\n            count += 1\n\n        return count\n\n\nclass TSVImporter(CSVImporter):\n    def load(self, file_obj, header=True, **kwargs):\n        kwargs.setdefault('delimiter', '\\t')\n        return super(TSVImporter, self).load(file_obj, header, **kwargs)\n", "test_list": ["def test_export(self):\n    self.create_users()\n    user = self.dataset['user']\n    buf = StringIO()\n    self.dataset.freeze(user.all(), 'json', file_obj=buf)\n    self.assertEqual(buf.getvalue(), '[{\"username\": \"charlie\"}, {\"username\": \"huey\"}]')\n    buf = StringIO()\n    self.dataset.freeze(user.all(), 'csv', file_obj=buf)\n    self.assertEqual(buf.getvalue().splitlines(), ['username', 'charlie', 'huey'])", "def test_freeze_thaw(self):\n    user = self.dataset['user']\n    user.insert(username='charlie')\n    note = self.dataset['note']\n    note_ts = datetime.datetime(2017, 1, 2, 3, 4, 5)\n    note.insert(content='foo', timestamp=note_ts, user_id='charlie', status=2)\n    buf = StringIO()\n    self.dataset.freeze(note.all(), 'json', file_obj=buf)\n    self.assertEqual(json.loads(buf.getvalue()), [{'id': 1, 'user_id': 'charlie', 'content': 'foo', 'status': 2, 'timestamp': '2017-01-02 03:04:05'}])\n    note.delete(id=1)\n    self.assertEqual(list(note.all()), [])\n    buf.seek(0)\n    note.thaw(format='json', file_obj=buf)\n    self.assertEqual(list(note.all()), [{'id': 1, 'user_id': 'charlie', 'content': 'foo', 'status': 2, 'timestamp': note_ts}])"], "requirements": {"Input-Output Conditions": {"requirement": "The 'freeze' function should validate that the 'query' parameter is iterable and contains valid data before proceeding with the export process.", "unit_test": ["def test_freeze_query_validation(self):\n    with self.assertRaises(TypeError):\n        self.dataset.freeze(None, 'csv', file_obj=StringIO())\n    with self.assertRaises(ValueError):\n        self.dataset.freeze([], 'csv', file_obj=StringIO())"], "test": "tests/dataset.py::TestDataSet::test_freeze_query_validation"}, "Exception Handling": {"requirement": "The 'freeze' function should raise a ValueError with a clear message if both 'filename' and 'file_obj' are provided.", "unit_test": ["def test_freeze_file_specification_error(self):\n    with self.assertRaises(ValueError) as cm:\n        self.dataset.freeze(self.dataset['user'].all(), 'csv', filename='test.csv', file_obj=StringIO())\n    self.assertEqual(str(cm.exception), 'file is over-specified. Please use either filename or file_obj, but not both.')"], "test": "tests/dataset.py::TestDataSet::test_freeze_file_specification_error"}, "Edge Case Handling": {"requirement": "The 'freeze' function should handle the case where the dataset is empty and export an empty file without errors.", "unit_test": ["def test_freeze_empty_dataset(self):\n    buf = StringIO()\n    self.dataset.freeze(self.dataset['user'].all(), 'csv', file_obj=buf)\n    self.assertEqual(buf.getvalue(), 'username\\n')"], "test": "tests/dataset.py::TestDataSet::test_freeze_empty_dataset"}, "Functionality Extension": {"requirement": "Extend the 'freeze' function to support exporting datasets in XML format.", "unit_test": ["def test_freeze_xml_export(self):\n    self.create_users()\n    user = self.dataset['user']\n    buf = StringIO()\n    self.dataset.freeze(user.all(), 'xml', file_obj=buf)\n    self.assertTrue('<users>' in buf.getvalue())\n    self.assertTrue('<username>charlie</username>' in buf.getvalue())"], "test": "tests/dataset.py::TestDataSet::test_freeze_xml_export"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'freeze' function are annotated with appropriate type hints.", "unit_test": ["def test_freeze_annotations(self):\n    annotations = self.dataset.freeze.__annotations__\n    self.assertEqual(annotations['format'], str)\n    self.assertEqual(annotations['filename'], str)\n    self.assertEqual(annotations['encoding'], str)"], "test": "tests/dataset.py::TestDataSet::test_freeze_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'freeze' function should not exceed 5.", "unit_test": ["def test_freeze_cyclomatic_complexity(self):\n    from radon.complexity import cc_visit\n    with open('dataset.py') as f:\n        code = f.read()\n    complexity = [c.complexity for c in cc_visit(code) if c.name == 'freeze']\n    self.assertTrue(all(c <= 10 for c in complexity))"], "test": "tests/dataset.py::TestDataSet::test_code_complexity"}, "Code Standard": {"requirement": "Ensure that the 'freeze' function adheres to PEP 8 style guidelines.", "unit_test": ["def test_code_style(self):\n    import subprocess\n    result = subprocess.run(['pycodestyle', 'dataset.py'], capture_output=True, text=True)\n    self.assertEqual(result.returncode, 0, msg=result.stdout)"], "test": "tests/dataset.py::TestDataSet::test_code_style"}, "Context Usage Verification": {"requirement": "Verify that the 'freeze' function utilizes the '_check_arguments' method to validate input parameters.", "unit_test": ["def test_freeze_uses_check_arguments(self):\n    with patch('playhouse.dataset.DataSet._check_arguments') as mock_check:\n        self.dataset.freeze(self.dataset['user'].all(), 'csv', file_obj=StringIO())\n        mock_check.assert_called_once()"], "test": "tests/dataset.py::TestDataSet::test_freeze_uses_check_arguments"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'freeze' function correctly uses the '_export_formats' dictionary to select the appropriate exporter class.", "unit_test": ["def test_freeze_uses_export_formats(self):\n    with patch.dict('playhouse.dataset.DataSet._export_formats', {'csv': MockExporter}) as mock_formats:\n        buf = StringIO()\n        self.dataset.freeze(self.dataset['user'].all(), 'csv', file_obj=buf)\n        self.assertTrue(mock_formats['csv'].called)"], "test": "tests/dataset.py::TestDataSet::test_freeze_uses_export_formats"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "type": "method", "project_path": "Security/pycoin", "completion_path": "Security/pycoin/pycoin/message/PeerAddress.py", "signature_position": [35, 35], "body_position": [36, 38], "dependency": {"intra_class": ["pycoin.message.PeerAddress.IP4_HEADER", "pycoin.message.PeerAddress.PeerAddress.ip_bin"], "intra_file": ["pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "pycoin.message.PeerAddress.ip_bin_to_ip6_addr"], "cross_file": []}, "requirement": {"Functionality": "This function determines the host address based on the IP binary string. If the IP binary string starts with the IP4 header, it converts the last 4 characters of the IP binary string to an IP4 address. Otherwise, it converts the entire IP binary string to an IP6 address.", "Arguments": ":param self: PeerAddress. An instance of the PeerAddress class.\n:return: The host address based on the IP binary string."}, "tests": ["tests/message_test.py::MessageTest::test_PeerAddress"], "indent": 4, "domain": "Security", "code": "    def host(self):\n        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        return ip_bin_to_ip6_addr(self.ip_bin)\n", "context": "import functools\nimport struct\n\n\nfrom pycoin.satoshi.satoshi_struct import parse_struct\nfrom pycoin.encoding.hexbytes import h2b\n\n\nIP4_HEADER = h2b(\"00000000000000000000FFFF\")\n\n\ndef ip_bin_to_ip6_addr(ip_bin):\n    return \":\".join(\"%x\" % v for v in struct.unpack(\">HHHHHHHH\", ip_bin))\n\n\ndef ip_bin_to_ip4_addr(ip_bin):\n    from pycoin.intbytes import iterbytes\n    return \"%d.%d.%d.%d\" % tuple(iterbytes(ip_bin[-4:]))\n\n\n@functools.total_ordering\nclass PeerAddress(object):\n    def __init__(self, services, ip_bin, port):\n        self.services = int(services)\n        assert isinstance(ip_bin, bytes)\n        if len(ip_bin) == 4:\n            ip_bin = IP4_HEADER + ip_bin\n        assert len(ip_bin) == 16\n        self.ip_bin = ip_bin\n        self.port = port\n\n    def __repr__(self):\n        return \"%s/%d\" % (self.host(), self.port)\n\n###The function: host###\n    def stream(self, f):\n        f.write(struct.pack(\"<Q\", self.services))\n        f.write(self.ip_bin)\n        f.write(struct.pack(\"!H\", self.port))\n\n    @classmethod\n    def parse(self, f):\n        services, ip_bin, port = parse_struct(\"Q@h\", f)\n        self.ip_bin = ip_bin\n        return self(services, self.ip_bin, port)\n\n    def __lt__(self, other):\n        return (self.ip_bin, self.port, self.services) < (other.ip_bin, other.port, other.services)\n\n    def __eq__(self, other):\n        return self.services == other.services and \\\n            self.ip_bin == other.ip_bin and self.port == other.port\n", "prompt": "Please write a python function called 'host' base the context. This function determines the host address based on the IP binary string. If the IP binary string starts with the IP4 header, it converts the last 4 characters of the IP binary string to an IP4 address. Otherwise, it converts the entire IP binary string to an IP6 address.:param self: PeerAddress. An instance of the PeerAddress class.\n:return: The host address based on the IP binary string..\n        The context you need to refer to is as follows: import functools\nimport struct\n\n\nfrom pycoin.satoshi.satoshi_struct import parse_struct\nfrom pycoin.encoding.hexbytes import h2b\n\n\nIP4_HEADER = h2b(\"00000000000000000000FFFF\")\n\n\ndef ip_bin_to_ip6_addr(ip_bin):\n    return \":\".join(\"%x\" % v for v in struct.unpack(\">HHHHHHHH\", ip_bin))\n\n\ndef ip_bin_to_ip4_addr(ip_bin):\n    from pycoin.intbytes import iterbytes\n    return \"%d.%d.%d.%d\" % tuple(iterbytes(ip_bin[-4:]))\n\n\n@functools.total_ordering\nclass PeerAddress(object):\n    def __init__(self, services, ip_bin, port):\n        self.services = int(services)\n        assert isinstance(ip_bin, bytes)\n        if len(ip_bin) == 4:\n            ip_bin = IP4_HEADER + ip_bin\n        assert len(ip_bin) == 16\n        self.ip_bin = ip_bin\n        self.port = port\n\n    def __repr__(self):\n        return \"%s/%d\" % (self.host(), self.port)\n\n###The function: host###\n    def stream(self, f):\n        f.write(struct.pack(\"<Q\", self.services))\n        f.write(self.ip_bin)\n        f.write(struct.pack(\"!H\", self.port))\n\n    @classmethod\n    def parse(self, f):\n        services, ip_bin, port = parse_struct(\"Q@h\", f)\n        self.ip_bin = ip_bin\n        return self(services, self.ip_bin, port)\n\n    def __lt__(self, other):\n        return (self.ip_bin, self.port, self.services) < (other.ip_bin, other.port, other.services)\n\n    def __eq__(self, other):\n        return self.services == other.services and \\\n            self.ip_bin == other.ip_bin and self.port == other.port\n", "test_list": ["def test_PeerAddress(self):\n    pa = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    pa_bytes = to_bin(pa)\n    pa1 = from_bin(PeerAddress, pa_bytes)\n    self.assertEqual(pa, pa1)\n    pa2 = PeerAddress(188, IP4_HEADER + h2b('c0a80162'), 8333)\n    self.assertTrue(pa1 > pa2)\n    self.assertTrue(pa1 >= pa2)\n    self.assertTrue(pa2 < pa1)\n    self.assertTrue(pa2 <= pa1)\n    self.assertNotEqual(pa2, pa1)\n    self.assertNotEqual(pa1, pa2)\n    self.assertEqual(pa1.host(), '192.168.1.99')\n    self.assertEqual(repr(pa1), '192.168.1.99/8333')\n    pa_v6 = PeerAddress(945, h2b('2607f8b04006080a000000000000200e'), 8333)\n    self.assertEqual(pa_v6.host(), '2607:f8b0:4006:80a:0:0:0:200e')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'host' function should correctly determine the host address based on the IP binary string, ensuring that the output is a valid IP4 or IP6 address.", "unit_test": ["def test_host_function_output(self):\n    pa_ip4 = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    self.assertEqual(pa_ip4.host(), '192.168.1.99')\n    pa_ip6 = PeerAddress(945, h2b('2607f8b04006080a000000000000200e'), 8333)\n    self.assertEqual(pa_ip6.host(), '2607:f8b0:4006:80a:0:0:0:200e')"], "test": "tests/message_test.py::MessageTest::test_host_function_output"}, "Exception Handling": {"requirement": "The 'host' function should raise a ValueError if the IP binary string is not of length 16.", "unit_test": ["def test_host_function_exception(self):\n    with self.assertRaises(ValueError):\n        PeerAddress(188, h2b('c0a801'), 8333).host()"], "test": "tests/message_test.py::MessageTest::test_host_function_exception"}, "Edge Case Handling": {"requirement": "The 'host' function should handle edge cases where the IP binary string is exactly the IP4 header followed by zeros.", "unit_test": ["def test_host_function_edge_case(self):\n    pa_edge = PeerAddress(188, IP4_HEADER + h2b('00000000'), 8333)\n    self.assertEqual(pa_edge.host(), '0.0.0.0')"], "test": "tests/message_test.py::MessageTest::test_host_function_edge_case"}, "Functionality Extension": {"requirement": "Extend the 'host' function to support conversion of IP binary strings that are exactly 4 bytes long, treating them as IP4 addresses.", "unit_test": ["def test_host_function_extension(self):\n    pa_short_ip4 = PeerAddress(188, h2b('c0a80163'), 8333)\n    self.assertEqual(pa_short_ip4.host(), '192.168.1.99')"], "test": "tests/message_test.py::MessageTest::test_host_function_extension"}, "Annotation Coverage": {"requirement": "Ensure that the 'host' function has complete parameter and return type annotations.", "unit_test": ["def test_host_function_annotations(self):\n    from inspect import signature\n    sig = signature(PeerAddress.host)\n    self.assertEqual(str(sig.parameters['self'].annotation), '<class \\'__main__.PeerAddress\\'>')\n    self.assertEqual(sig.return_annotation, str)"], "test": "tests/message_test.py::MessageTest::test_host_function_extension"}, "Code Complexity": {"requirement": "The 'host' function should maintain a cyclomatic complexity of 2, indicating a simple function with no branching.", "unit_test": ["def test_host_function_complexity(self):\n    from radon.complexity import cc_visit\n    code = '''def host(self):\n    if self.ip_bin.startswith(IP4_HEADER):\n        return ip_bin_to_ip4_addr(self.ip_bin)\n    return ip_bin_to_ip6_addr(self.ip_bin)'''\n    complexity = cc_visit(code)\n    self.assertEqual(complexity[0].complexity, 1)"], "test": "tests/message_test.py::MessageTest::test_code_complexity"}, "Code Standard": {"requirement": "The 'host' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_host_function_pep8(self):\n    import pep8\n    pep8style = pep8.StyleGuide(quiet=True)\n    result = pep8style.check_files(['peer_address.py'])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/message_test.py::MessageTest::test_code_style"}, "Context Usage Verification": {"requirement": "The 'host' function should utilize the 'IP4_HEADER' and 'ip_bin' attributes from the PeerAddress class.", "unit_test": ["def test_host_function_context_usage(self):\n    pa = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    self.assertIn('IP4_HEADER', pa.host.__code__.co_names)\n    self.assertIn('ip_bin', pa.host.__code__.co_names)"], "test": "tests/message_test.py::MessageTest::test_host_function_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'host' function should correctly use the 'IP4_HEADER' to determine if the IP binary string is an IP4 address.", "unit_test": ["def test_host_function_context_correctness(self):\n    pa = PeerAddress(188, IP4_HEADER + h2b('c0a80163'), 8333)\n    self.assertTrue(pa.ip_bin.startswith(IP4_HEADER))\n    self.assertEqual(pa.host(), '192.168.1.99')"], "test": "tests/message_test.py::MessageTest::test_host_function_context_correctness"}}, "multi-turn": ["Input-Output Conditions", "Exception Handling", "Edge Case Handling", "Annotation Coverage", "Code Complexity", "Code Standard", "Context Usage Verification", "Context Usage Correctness Verification", "Functionality Extension"]}
