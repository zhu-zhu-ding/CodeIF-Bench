{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "\ndef _stream_history_log_dirs(self, output_dir=None):\n    \"\"\"Yield lists of directories to search for the history log in.\n\n    :param self: HadoopJobRunner. An instance of the HadoopJobRunner class.\n    :param output_dir: str. The output directory to search for the history log. Defaults to None.\n    :return: Generator. Yields lists of directories to search for the history log in.\n    \"\"\"\n    if not self._read_logs():\n        return\n\n    # get unique log directories from hadoop log dirs\n    for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n        log.info('Looking for history log in %s...' % log_dir)\n        yield [log_dir]\n", "Result": "Error"}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "\ndef to_json(self, handler=None):\n    \"\"\"Return a JSON representation of the error.\n\n    Args:\n        handler (Handler object): An optional handler object that will be used to\n            serialize the representation of this error to JSON. Defaults to None.\n\n    Returns:\n        bytes: A JSON document representing the error.\n    \"\"\"\n    if handler is None:\n        handler = _DEFAULT_JSON_HANDLER\n\n    obj = self.to_dict()\n    return handler.serialize(obj)\n", "Result": "Error"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "\ndef mkdir(self, path):\n    \"\"\"Create a directory in the Hadoop filesystem.\n\n    :param path: The path of the directory to be created\n    :type path: str\n    :raises IOError: If the directory creation fails (except when directory already exists)\n    \"\"\"\n    version = self.get_hadoop_version()\n    args = ['fs', '-mkdir']\n    \n    # Add -p option for Hadoop 2.0+ (YARN)\n    if uses_yarn(version):\n        args.append('-p')\n    \n    args.append(path)\n\n    try:\n        self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n    except CalledProcessError:\n        raise IOError('Could not mkdir %s' % path)\n", "Result": "Error"}
{"namespace": "twtxt.config.Config.following", "completion": "\n@property\ndef following(self):\n    \"\"\"Returns a list of Source objects from the following section.\n\n    :return: list of Source objects that represent followed feeds\n    :rtype: list\n    \"\"\"\n    following_list = []\n    try:\n        for (nick, url) in self.cfg.items(\"following\"):\n            following_list.append(Source(nick, url))\n        return following_list\n    except configparser.NoSectionError as e:\n        logger.debug(e)\n        return []\n", "Result": "Error"}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "\n@require_capability(\"THREAD\")\ndef thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n    \"\"\"Return a list of message threads from the currently selected folder.\n    \n    Each thread is represented as a tuple of message IDs.\n    \n    Args:\n        algorithm (str): The threading algorithm to use. Defaults to \"REFERENCES\".\n                        Common values are \"REFERENCES\" or \"ORDEREDSUBJECT\"\n        criteria (str): The search criteria to match messages. Defaults to \"ALL\".\n                       Same format as used in the search() method.\n        charset (str): Character set for the search criteria. Defaults to \"UTF-8\".\n                      \n    Returns:\n        List[Tuple]: A list of tuples where each tuple contains the message IDs \n                     that make up a thread. For example: ((1, 2), (3,), (4, 5, 6))\n                     \n    Raises:\n        CapabilityError: If the IMAP server does not support the THREAD capability\n        IMAPClientError: For other IMAP-related errors\n    \"\"\"\n    args = [\n        to_bytes(algorithm),\n        to_bytes(charset),\n    ]\n    args.extend(_normalise_search_criteria(criteria, charset))\n    \n    try:\n        data = self._raw_command_untagged(b\"THREAD\", args)\n    except self.Error as e:\n        # If the response was BAD, the server might not support the algorithm\n        if \"THREAD\" in str(e) and algorithm != \"ORDEREDSUBJECT\":\n            # Fallback to ORDEREDSUBJECT which servers must support\n            return self.thread(\"ORDEREDSUBJECT\", criteria, charset)\n        raise\n\n    return parse_response(data)\n\n", "Result": "Pass"}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "\ndef get_node(self, page: int) -> Node:\n    \"\"\"Retrieve a node from storage.\n    \n    First checks if the node is in cache. If not, retrieves from storage\n    and adds to cache.\n    \n    Args:\n        self: FileMemory instance\n        page: Page number of node to retrieve\n        \n    Returns:\n        Node: The retrieved node\n        \n    Raises:\n        ValueError: If page number is invalid\n    \"\"\"\n    # Check cache first\n    node = self._cache.get(page)\n    if node is not None:\n        return node\n        \n    # Not in cache, load from storage\n    data = self.get_page(page)\n    node = Node.from_page_data(self._tree_conf, data, page)\n    \n    # Add to cache and return\n    self._cache[page] = node\n    return node\n", "Result": "Error"}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "\ndef generate_url_sigv4(self, expires_in, method, bucket='', key='', headers=None,\n                      force_http=False, response_headers=None, version_id=None,\n                      iso_date=None):\n    \"\"\"\n    Generate a presigned URL with Signature Version 4 for accessing an S3 object.\n    \n    :type expires_in: int \n    :param expires_in: The number of seconds the presigned URL is valid for\n    \n    :type method: str\n    :param method: The HTTP method to use (GET, PUT, etc)\n    \n    :type bucket: str\n    :param bucket: The name of the bucket\n    \n    :type key: str\n    :param key: The key (object path) to sign\n    \n    :type headers: dict\n    :param headers: Additional headers to include in the signed request\n    \n    :type force_http: bool\n    :param force_http: If True, forces URL to use HTTP protocol\n    \n    :type response_headers: dict\n    :param response_headers: Additional response headers to include in URL\n    \n    :type version_id: str\n    :param version_id: Version ID of the object to get\n    \n    :type iso_date: str\n    :param iso_date: ISO format date to use for signing. If not provided, \n                    current date will be used\n                    \n    :rtype: str\n    :return: The presigned URL\n    \"\"\"\n    if headers is None:\n        headers = {}\n    \n    # Create a request object to sign\n    http_request = self.build_base_http_request(\n        method, \n        self.calling_format.build_path_base(bucket, key),\n        self.calling_format.build_auth_path(bucket, key),\n        {},  # Params will be added by presign\n        headers,\n        '',   # Body not needed for presigned URL\n        self.calling_format.build_host(self.server_name(), bucket)\n    )\n    \n    # For presigned URLs we should ignore the port if it's HTTPS\n    if http_request.protocol == 'https':\n        http_request.port = None\n        \n    # Add response headers if specified\n    if response_headers:\n        for key, value in response_headers.items():\n            http_request.params[key] = value\n            \n    # Add version ID if specified  \n    if version_id:\n        http_request.params['versionId'] = version_id\n        \n    # Force protocol if specified\n    if force_http:\n        http_request.protocol = 'http'\n        \n    # Get auth handler and generate presigned URL\n    auth = self._auth_handler\n    return auth.presign(http_request, expires_in, iso_date)\n", "Result": "Error"}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "\n@property\ndef logger_class(self):\n    \"\"\"Get the logger class based on configuration settings.\n    \n    :param self: Config instance\n    :return: The configured logger class\n    \"\"\"\n    # Get the logger_class setting\n    uri = self.settings['logger_class'].get()\n    \n    # If using simple logger and statsd is enabled, switch to statsd logger\n    if uri == \"simple\" and self.statsd_host:\n        uri = \"gunicorn.instrument.statsd.Statsd\"\n    \n    # Load and install the logger class\n    logger_class = util.load_class(\n        uri,\n        default=\"gunicorn.glogging.Logger\",\n        section=\"gunicorn.loggers\"\n    )\n    \n    # Install the logger if it has an install method\n    if hasattr(logger_class, \"install\"):\n        logger_class.install()\n        \n    return logger_class\n", "Result": "Error"}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "\ndef to_constraint(\n    self, migration_context: Optional[MigrationContext] = None\n) -> Constraint:\n    \"\"\"Convert this DropConstraintOp to a Constraint instance.\n    \n    :param migration_context: Optional MigrationContext object\n    :return: The converted Constraint instance\n    :raises ValueError: If the reverse operation is not present\n    \"\"\"\n    if self._reverse is None:\n        raise ValueError(\n            \"Can't reverse DROP CONSTRAINT operation - reverse \"\n            \"information not available\"\n        )\n    \n    constraint = self._reverse.to_constraint(migration_context)\n    \n    # Set the constraint name, table name and schema from this operation\n    # in case they differ from the reverse operation\n    constraint.name = self.constraint_name\n    table = constraint.table\n    table.name = self.table_name\n    table.schema = self.schema\n    \n    return constraint\n", "Result": "Error"}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "\n@classmethod\ndef get_bootstrap_from_recipes(cls, recipes, ctx):\n    \"\"\"Selects a recommended default bootstrap from a list of recipes.\n    \n    Args:\n        cls: The Bootstrap class\n        recipes: List of strings - recipes to consider when selecting bootstrap\n        ctx: The context in which the function is being called\n        \n    Returns:\n        Bootstrap: The selected default bootstrap\n        \n    The function follows these rules:\n    - SDL2 bootstrap if there's an sdl2 dependency\n    - Webview bootstrap if depending on common web recipes\n    - Otherwise selects based on default recipe priorities\n    \"\"\"\n    # Get all usable bootstraps that don't conflict with recipes\n    available_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n    \n    # Return None if no compatible bootstraps found\n    if not available_bootstraps:\n        return None\n        \n    # Check for SDL2 dependency\n    if 'sdl2' in recipes:\n        for bs in available_bootstraps:\n            if bs.name == 'sdl2':\n                return bs\n                \n    # Check for webview dependency\n    if 'webview' in recipes:\n        for bs in available_bootstraps:\n            if bs.name == 'webview':\n                return bs\n                \n    # If no special cases matched, sort by priority and return highest\n    sorted_bootstraps = sorted(\n        list(available_bootstraps),\n        key=functools.cmp_to_key(_cmp_bootstraps_by_priority)\n    )\n    return sorted_bootstraps[-1] if sorted_bootstraps else None\n", "Result": "Error"}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "\ndef deserialize(self, value):\n    \"\"\"Deserialize a value and return a pair of deserialized values.\n    \n    :param value: The value to be deserialized\n    :return: A tuple containing the pair of deserialized values\n    \"\"\"\n    value = decode(value).strip()\n    validators.validate_required(value, self._required)\n    if not value:\n        return None\n\n    if self._separator in value:\n        first_value, second_value = value.split(self._separator, 1)\n    elif self._optional_pair:\n        first_value = second_value = value\n    else:\n        raise ValueError(\n            f\"config value must include separator {self._separator!r}\"\n        )\n\n    return (\n        self._subtypes[0].deserialize(first_value),\n        self._subtypes[1].deserialize(second_value),\n    )\n", "Result": "Error"}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "\n@classmethod\ndef get_data_dir(cls, config: Config) -> Path:\n    \"\"\"Get or create data directory for the extension.\n\n    :param cls: Class. The Extension class\n    :param config: Config. The Mopidy config object\n    :return: Path. The path to the data directory for the extension\n    \"\"\"\n    if cls.ext_name is None:\n        raise AssertionError\n    data_dir_path = (\n        path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n    )\n    path.get_or_create_dir(data_dir_path)\n    return data_dir_path\n", "Result": "Error"}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "\ndef fit(self, X, y, sample_weight=None):\n    \"\"\"Fit linear regression model with differential privacy.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Training data.\n\n    y : array_like, shape (n_samples, n_targets)\n        Target values.\n\n    sample_weight : ignored\n        Ignored by diffprivlib. Present for consistency with sklearn API.\n\n    Returns\n    -------\n    self : LinearRegression\n        Returns self.\n    \"\"\"\n    # Validate parameters\n    self._validate_params()\n    \n    if sample_weight is not None:\n        self._warn_unused_args(\"sample_weight\")\n\n    # Check random state\n    random_state = check_random_state(self.random_state)\n\n    # Check bounds\n    if self.bounds_X is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                     \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                     \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        self.bounds_X = (np.min(X, axis=0), np.max(X, axis=0))\n\n    if self.bounds_y is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                     \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                     \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        self.bounds_y = (np.min(y, axis=0), np.max(y, axis=0))\n\n    self.bounds_X = self._check_bounds(self.bounds_X, X.shape[1])\n    self.bounds_y = self._check_bounds(self.bounds_y, 1 if y.ndim == 1 else y.shape[1])\n\n    # Preprocess data\n    X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n        X, y, self.fit_intercept, epsilon=self.epsilon, bounds_X=self.bounds_X, bounds_y=self.bounds_y,\n        copy=self.copy_X, check_input=True, random_state=random_state)\n\n    # Check accountant\n    self.accountant = BudgetAccountant.load_default(self.accountant)\n    self.accountant.check(self.epsilon, 0)\n\n    # Construct regression objects and optimize coefficients\n    regression_objs, noisy_coefs = _construct_regression_obj(X, y, self.bounds_X, self.bounds_y, self.epsilon, 0,\n                                                           random_state)\n\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    n_targets = y.shape[1]\n    coef = np.zeros((n_targets, X.shape[1]))\n\n    for i in range(n_targets):\n        res = minimize(regression_objs[i], np.zeros(X.shape[1]), jac=True, method='L-BFGS-B')\n        coef[i, :] = res.x\n\n    self.coef_ = coef\n    self.accountant.spend(self.epsilon, 0)\n\n    # Set intercept\n    if self.fit_intercept:\n        self._set_intercept(X_offset, y_offset, X_scale)\n    else:\n        self.intercept_ = np.zeros(1 if n_targets == 1 else n_targets, dtype=X.dtype)\n\n    return self\n", "Result": "Error"}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "\ndef compute_rarest_windows(\n    self,\n    window_len: int,\n    use_start_end_tokens: bool,\n    use_geo_mean: bool = False\n) -> None:\n    \"\"\"\n    Compute the rarest windows and corresponding likelihood for each session.\n\n    Parameters\n    ----------\n    window_len : int\n        The length of the sliding window for likelihood calculations.\n    use_start_end_tokens : bool\n        If True, start and end tokens will be added to each session before calculations.\n    use_geo_mean : bool, optional\n        If True, the likelihoods of the sliding windows will be raised to \n        the power of (1/window_len), by default False.\n\n    Notes\n    -----\n    This function updates the following instance attributes:\n    - rare_windows[window_len] : List of rarest windows for each session\n    - rare_window_likelihoods[window_len] : List of likelihoods for the rarest windows\n    - rare_windows_geo[window_len] : List of rarest windows using geometric mean\n    - rare_window_likelihoods_geo[window_len] : List of geometric mean likelihoods\n    \"\"\"\n    if self.prior_probs is None:\n        raise MsticpyException(\n            \"please train the model first before using this method\"\n        )\n\n    rare_windows = []\n    rare_window_likelihoods = []\n\n    # Process each session\n    for session in self.sessions:\n        # Get all possible windows of length window_len from the session\n        windows = []\n        for i in range(len(session) - window_len + 1):\n            windows.append(session[i:i + window_len])\n\n        # If no valid windows found, append None values\n        if not windows:\n            rare_windows.append(None)\n            rare_window_likelihoods.append(None)\n            continue\n\n        # Calculate likelihood for each window based on session type\n        window_likelihoods = []\n        for window in windows:\n            if self.session_type == SessionType.cmds_only:\n                likelihood = cmds_only.compute_likelihood_window(\n                    window=window,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            elif self.session_type == SessionType.cmds_params_only:\n                likelihood = cmds_params_only.compute_likelihood_window(\n                    window=window,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:  # SessionType.cmds_params_values\n                likelihood = cmds_params_values.compute_likelihood_window(\n                    window=window,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n\n            if use_geo_mean:\n                likelihood = likelihood ** (1 / window_len)\n            window_likelihoods.append(likelihood)\n\n        # Find the window with minimum likelihood\n        min_likelihood_idx = window_likelihoods.index(min(window_likelihoods))\n        rare_windows.append(windows[min_likelihood_idx])\n        rare_window_likelihoods.append(window_likelihoods[min_likelihood_idx])\n\n    # Store results in appropriate instance attributes\n    if use_geo_mean:\n        self.rare_windows_geo[window_len] = rare_windows\n        self.rare_window_likelihoods_geo[window_len] = rare_window_likelihoods\n    else:\n        self.rare_windows[window_len] = rare_windows\n        self.rare_window_likelihoods[window_len] = rare_window_likelihoods\n", "Result": "Error"}
