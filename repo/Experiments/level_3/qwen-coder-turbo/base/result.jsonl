[
    {
        "namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs",
        "completion": "```python\n    def _stream_history_log_dirs(self, output_dir=None):\n        \"\"\"Yield lists of directories to look for the history log in.\"\"\"\n        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            path = self.fs.join(log_dir, 'history')\n            if _logs_exist(self.fs, path):\n                log.info(f'Looking for history log in {path}...')\n                yield [path]\n```"
    },
    {
        "namespace": "falcon.http_error.HTTPError.to_json",
        "completion": "Here's the `to_json` function based on the reference context:\n\n```python\ndef to_json(self, handler=None):\n    \"\"\"\n    Convert the HTTPError instance into a JSON representation.\n\n    Parameters:\n        handler (Handler object, optional): An optional handler object that will be used \n                                          to serialize the representation of this \n                                          error to JSON. Defaults to None.\n\n    Returns:\n        bytes: A JSON document representing the error.\n    \"\"\"\n    if handler is None:\n        handler = _DEFAULT_JSON_HANDLER\n\n    if not callable(handler):\n        raise TypeError(\"handler must be a callable function\")\n\n    error_data = self.to_dict()\n    return handler.serialize(error_data, content_type=falcon.constants.MEDIA_JSON)\n```\n\nThis function uses the provided `handler` to serialize the error data into JSON. If no handler is provided, it falls back to the `_DEFAULT_JSON_HANDLER`. It checks if the provided handler is callable and then calls the `serialize` method of the handler to get the JSON representation of the error.\n\nNote that I've made some assumptions about the `Handler` object's interface based on the `_DEFAULT_JSON_HANDLER` class defined in the cross-file context. Specifically, I've assumed that the `Handler` object has a `serialize` method that takes the error data and a content type as arguments, and returns a byte string containing the serialized JSON. If your actual `Handler` class has a different interface, you'll need to adjust the `to_json` function accordingly."
    },
    {
        "namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir",
        "completion": "Sure! Here's the implementation of the `mkdir` function:\n\n```python\ndef mkdir(self, path):\n    \"\"\"Create a directory in the Hadoop filesystem using the 'fs -mkdir' command.\n    \n    Parameters:\n    :param self: HadoopFilesystem. An instance of the HadoopFilesystem class.\n    :param path: str. The path of the directory to be created.\n    :return: No return values.\n    \"\"\"\n    # Use the '-p' option to create parent directories as needed\n    args = self.get_hadoop_bin() + ['fs', '-mkdir', '-p', path]\n\n    try:\n        self.invoke_hadoop(args)\n    except CalledProcessError:\n        raise IOError(f'Could not mkdir {path}')\n```\n\nThis function takes two parameters: `self`, which is an instance of the `HadoopFilesystem` class, and `path`, which is the path of the directory to be created. It constructs the appropriate command using `self.get_hadoop_bin()` to get the Hadoop binary and appends the necessary arguments to create the directory. \n\nIf the command fails, it catches the `CalledProcessError` and raises an `IOError` with a message indicating that the directory could not be created. The `-p` option ensures that parent directories are created as needed."
    },
    {
        "namespace": "twtxt.config.Config.following",
        "completion": "Here's the complete implementation of the `following` function within the `Config` class:\n\n```python\n@property\ndef following(self):\n    \"\"\"Retrieves a list of all Source objects that are stored in the \"following\" section of the Config instance.\"\"\"\n    following_list = []\n    if self.cfg.has_section(\"following\"):\n        for item in self.cfg.items(\"following\"):\n            source = Source(item[0], item[1])\n            following_list.append(source)\n    else:\n        logger.debug('No \"following\" section found in the config.')\n    return following_list\n```\n\nThis function checks if the \"following\" section exists in the configuration. If it does, it iterates through each item in the section, creates a `Source` object using the nickname and URL from the item, and appends it to the `following_list`. If the \"following\" section does not exist, it logs a debug message and returns an empty list."
    },
    {
        "namespace": "imapclient.imapclient.IMAPClient.thread",
        "completion": "```python\ndef thread(self, algorithm='REFERENCES', criteria='ALL', charset='UTF-8'):\n    \"\"\"Return a list of message threads from the currently selected folder that match the specified criteria.\n    \n    Args:\n        algorithm (str, optional): The threading algorithm to use. Defaults to 'REFERENCES'.\n        criteria (str, optional): The search criteria to match the messages. Defaults to 'ALL'.\n        charset (str, optional): The character set to be used. Defaults to 'UTF-8'.\n\n    Returns:\n        list[tuple]: Each tuple represents a message thread, where each element of the tuple is a message ID.\n    \"\"\"\n    # Fetch the list of message IDs based on the provided criteria\n    message_ids = self.search(criteria=criteria, charset=charset)\n\n    # Group the message IDs into threads based on the specified algorithm\n    if algorithm == 'REFERENCES':\n        thread_groups = self._group_by_references(message_ids)\n    elif algorithm == 'FROM':\n        thread_groups = self._group_by_from(message_ids)\n    elif algorithm == 'DATE':\n        thread_groups = self._group_by_date(message_ids)\n    else:\n        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n\n    return thread_groups\n\n\ndef _group_by_references(self, message_ids):\n    \"\"\"Group messages into threads based on their References header.\"\"\"\n    # Initialize a dictionary to hold the thread groups\n    thread_groups = defaultdict(list)\n\n    # Iterate through each message ID and fetch its References header\n    for msg_id in message_ids:\n        ref_headers = self.get_headers(msg_id, ['References'])\n\n        # Extract the message IDs referenced by this message\n        refs = ref_headers.get(b'References', b'').decode('utf-8', errors='ignore').split()\n\n        # Add the current message ID to the corresponding thread group\n        for ref in refs:\n            thread_groups.setdefault(ref.strip('<> '), []).append(msg_id)\n\n    # Create a list of tuples representing the final thread groups\n    thread_groups_list = [tuple(group) for group in thread_groups.values()]\n\n    return thread_groups_list\n\n\ndef _group_by_from(self, message_ids):\n    \"\"\"Group messages into threads based on their From header.\"\"\"\n    # Initialize a dictionary to hold the thread groups\n    thread_groups = defaultdict(list)\n\n    # Iterate through each message ID and fetch its From header\n    for msg_id in message_ids:\n        from_header = self.get_headers(msg_id, ['From'])\n\n        # Extract the sender's email address from the From header\n        sender_email = from_header.get(b'From', b'').decode('utf-8', errors='ignore')\n\n        # Add the current message ID to the corresponding thread group\n        thread_groups[sender_email].append(msg_id)\n\n    # Create a list of tuples representing the final thread groups\n    thread_groups_list = [tuple(group) for group in thread_groups.values()]\n\n    return thread_groups_list\n\n\ndef _group_by_date(self, message_ids):\n    \"\"\"Group messages into threads based on their date.\"\"\"\n    # Initialize a dictionary to hold the thread groups\n    thread_groups = defaultdict(list)\n\n    # Iterate through each message ID and fetch its date\n    for msg_id in message_ids:\n        date_header = self.get_headers(msg_id, ['Date'])\n\n        # Extract the message date from the Date header\n        message_date = date_header.get(b'Date', b'').decode('utf-8', errors='ignore')\n\n        # Add the current message ID to the corresponding thread group\n        thread_groups[message_date].append(msg_id)\n\n    # Create a list of tuples representing the final thread groups\n    thread_groups_list = [tuple(group) for group in thread_groups.values()]\n\n    return thread_groups_list\n```\n\nThe `thread` function groups messages into threads based on the specified algorithm. It first retrieves the list of message IDs that match the given criteria. Then, depending on the algorithm (`REFERENCES`, `FROM`, or `DATE`), it groups these messages accordingly.\n\nFor the `REFERENCES` algorithm, it checks the `References` header of each message to identify which messages it refers to, thereby forming threads.\n\nFor the `FROM` algorithm, it categorizes messages by the sender's email address.\n\nFor the `DATE` algorithm, it groups messages based on the date they were received.\n\nEach thread group is represented as a tuple containing message IDs, and the function returns a list of these tuples."
    },
    {
        "namespace": "bplustree.memory.FileMemory.get_node",
        "completion": "```python\ndef get_node(self, page: int) -> Node:\n    # Check if the node is already in the cache\n    if page in self._cache:\n        logger.debug(f\"Retrieving node {page} from cache\")\n        return self._cache[page]\n\n    # Retrieve the node data from storage\n    logger.debug(f\"Reading node {page} from disk\")\n    data = self.get_page(page)\n\n    # Create a new Node object using the data\n    logger.debug(f\"Creating Node object for page {page}\")\n    node = Node.from_page_data(self._tree_conf, data, page=page)\n\n    # Add the node to the cache\n    logger.debug(f\"Caching node {page}\")\n    self._cache[node.page] = node\n\n    return node\n```\n\nThis function aims to retrieve a node from storage and add it to the cache for future use. Here's how it works:\n\n1. **Check Cache**: First, it checks if the requested node (`page`) is already present in the cache using `_cache`. If the node is found in the cache, it directly returns the node from the cache.\n\n2. **Read from Storage**: If the node is not in the cache, it reads the node data from the storage using the `get_page()` method. This method fetches the node data from the file based on the given page number.\n\n3. **Create Node Object**: The retrieved data is then used to create a new `Node` object. The `from_page_data()` class method of the `Node` class is responsible for creating a `Node` object from the byte data. This method is designed to handle different types of nodes (like `LonelyRootNode`, `LeafNode`, etc.) based on the node type specified in the byte data.\n\n4. **Add to Cache**: After creating the node, it adds the newly created node to the cache for future use. This helps in reducing the number of disk I/O operations by providing quick access to frequently accessed nodes.\n\n5. **Return Node**: Finally, the function returns the newly created or cached node.\n\nThis implementation ensures efficient retrieval of nodes from both memory (cache) and persistent storage, leveraging caching to improve performance."
    },
    {
        "namespace": "boto.s3.connection.S3Connection.generate_url_sigv4",
        "completion": "Based on the provided context, here is the `generate_url_sigv4` function implemented in the `S3Connection` class:\n\n```python\ndef generate_url_sigv4(self, expires_in, method, bucket='', key='', headers=None,\n                       force_http=False, response_headers=None, version_id=None, iso_date=None):\n    if not iso_date:\n        iso_date = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n\n    # Construct the canonical request\n    canonical_request = self._canonical_request(method, bucket, key, headers, iso_date, version_id)\n\n    # Derive the string to sign\n    string_to_sign = self._string_to_sign(canonical_request, self.service_name, self.region_name, iso_date)\n\n    # Calculate the signature\n    signature = self._sign(string_to_sign)\n\n    # Build the presigned URL\n    query_string = self.QueryString % (signature, expires_in, self.aws_access_key_id)\n    if version_id:\n        query_string += '&versionId=' + version_id\n    if response_headers:\n        for k, v in response_headers.items():\n            query_string += '&response-' + k.replace('_', '-') + '=' + urllib.parse.quote(v)\n\n    url = self.build_base_http_request(method, bucket, key, headers=headers, data=query_string)\n    return url.get_url()\n```\n\nNote: The `_canonical_request`, `_string_to_sign`, and `_sign` methods are placeholders and should be implemented based on the actual requirements and signature calculation logic according to AWS's Signature Version 4 specification. These methods are responsible for constructing the appropriate canonical request, generating the string to sign, and computing the signature respectively."
    },
    {
        "namespace": "gunicorn.config.Config.logger_class",
        "completion": "```python\ndef logger_class(self):\n    logger_class = self.logger_class\n    if logger_class == \"simple\":\n        logger_class = \"gunicorn.glogging.Logger\"\n    \n    if logger_class == \"gunicorn.glogging.Logger\" and self.statsd_host:\n        logger_class = \"gunicorn.instrument.statsd.Statsd\"\n        \n    logger_class = util.load_class(\n        logger_class,\n        default=\"gunicorn.glogging.Logger\",\n        section=\"gunicorn.loggers\"\n    )\n\n    if hasattr(logger_class, \"install\"):\n        logger_class.install()\n    \n    return logger_class\n```"
    },
    {
        "namespace": "alembic.operations.ops.DropConstraintOp.to_constraint",
        "completion": "Here's the implementation of the `to_constraint` function based on the provided code:\n\n```python\ndef to_constraint(self, migration_context: Optional[MigrationContext] = None) -> Constraint:\n    if self._reverse:\n        constraint = self._reverse.to_constraint(migration_context=migration_context)\n        constraint.name = self.constraint_name\n        constraint.table.compile()\n        constraint.schema = self.schema\n        return constraint\n    else:\n        raise ValueError(\"Cannot convert to Constraint because there is no reverse operation present.\")\n```\n\nThis function checks if the `_reverse` attribute is present. If it is, it calls the `to_constraint` method on the reverse operation, copies the name and schema from the current operation to the resulting constraint, and returns it. If the `_reverse` attribute is not present (meaning there is no reverse operation), it raises a `ValueError` stating that conversion to a Constraint is not possible without a reverse operation."
    },
    {
        "namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes",
        "completion": "```python\ndef get_bootstrap_from_recipes(cls, recipes, ctx):\n    \"\"\"Selects a recommended default bootstrap from a list of recipes and returns it.\n    \n    Parameters:\n    :param cls: Class. The Bootstrap class.\n    :param recipes: List of strings. The list of recipes to consider when selecting the bootstrap.\n    :param ctx: Context. The context in which the function is being called.\n    \n    Returns:\n    Bootstrap. The selected default bootstrap.\n    \"\"\"\n    usable_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n    \n    if not usable_bootstraps:\n        raise ValueError(\"No suitable bootstrap found for the given recipes.\")\n        \n    sdl2_needed = any(\"sdl2\" in rec.name.lower() for rec in usable_bootstraps)\n    webview_needed = any(\"webview\" in rec.name.lower() for rec in usable_bootstraps)\n    \n    if sdl2_needed:\n        return cls.get_bootstrap(\"sdl2\", ctx)\n    elif webview_needed:\n        return cls.get_bootstrap(\"webview\", ctx)\n    else:\n        # Default to the first bootstrap in the list if none specifically requested\n        return next(iter(usable_bootstraps))\n```\n\nThis function first filters out all bootstraps that can potentially satisfy the given recipes using `get_usable_bootstraps_for_recipes`. It then checks if either \"sdl2\" or \"webview\" is needed among the selected bootstraps. If \"sdl2\" is needed, it selects the \"sdl2\" bootstrap; otherwise, if \"webview\" is needed, it selects the \"webview\" bootstrap. If neither is needed, it falls back to the first bootstrap in the list of usable bootstraps. If no suitable bootstrap is found, it raises a `ValueError`."
    },
    {
        "namespace": "mopidy.config.types.Pair.deserialize",
        "completion": "Here is a Python function named `deserialize` that meets your requirements:\n\n```python\ndef deserialize(self, value):\n    \"\"\"\n    Deserialize a value and return a pair of deserialized values.\n    \n    It first decodes the input value, removes any leading or trailing whitespace.\n    Then, it validates the raw value based on whether it is required or not.\n    If the raw value is empty, it returns None.\n    If the separator is present in the raw value, it splits the value into two parts.\n    If the optional pair flag is set, it assigns the same value to both parts.\n    Otherwise, it raises a ValueError indicating that the config value must include the separator.\n    Finally, it encodes and deserializes each part of the pair using the corresponding subtypes.\n    \n    :param self: Pair. An instance of the Pair class.\n    :param value: The value to be deserialized.\n    :return: Tuple. A pair of deserialized values.\n    \"\"\"\n    # Decode the input value\n    value = decode(value).strip()\n    \n    # Validate the raw value based on whether it is required or not\n    validators.validate_required(value, self._required)\n    if not value:\n        return None\n    \n    # Split the value into two parts using the separator\n    if self._separator in value:\n        parts = value.split(self._separator, 1)\n    else:\n        if self._optional_pair:\n            parts = [value, value]\n        else:\n            raise ValueError(\"config value must include the separator\")\n    \n    # Deserialize each part of the pair using the corresponding subtypes\n    first_value = self._subtypes[0].deserialize(parts[0])\n    second_value = self._subtypes[1].deserialize(parts[1])\n    \n    return first_value, second_value\n```\n\nThis function performs the following steps:\n1. Decodes the input value and strips any leading or trailing whitespace.\n2. Validates the raw value to ensure it is set if required.\n3. Splits the value into two parts using the specified separator, unless the optional pair flag is set.\n4. Deserializes each part of the pair using the corresponding subtypes.\n\nMake sure to include the necessary imports and other classes from the provided reference context in your actual implementation."
    },
    {
        "namespace": "mopidy.ext.Extension.get_data_dir",
        "completion": "Here is the `get_data_dir` function based on your requirements:\n\n```python\n@classmethod\ndef get_data_dir(cls, config: Config) -> Path:\n    \"\"\"\n    Get or create data directory for the extension.\n\n    Use this directory to store data that persists between Mopidy sessions.\n\n    :param config: the Mopidy config object\n    :return: pathlib.Path\n    \"\"\"\n    if cls.ext_name is None:\n        raise AssertionError\n    \n    data_dir_path = (\n        path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n    )\n    path.get_or_create_dir(data_dir_path)\n    return data_dir_path\n```\n\nThis function takes a `cls` parameter representing the Extension class and a `config` parameter representing the Mopidy config object. It returns the path to the data directory for the extension."
    },
    {
        "namespace": "diffprivlib.models.linear_regression.LinearRegression.fit",
        "completion": "```python\ndef fit(self, X, y, sample_weight=None):\n    \"\"\"\n    Fits a linear regression model to the given training data. It preprocesses the data, determines the bounds,\n    constructs regression objects, and optimizes the coefficients using the minimize function. It also sets the intercept \n    and updates the accountant's spending.\n\n    Parameters:\n    :param self: LinearRegression. An instance of the LinearRegression class.\n    :param X: array-like or sparse matrix. The training data with shape (n_samples, n_features).\n    :param y: array_like. The target values with shape (n_samples, n_targets).\n    :param sample_weight: ignored. Ignored by diffprivlib. Present for consistency with sklearn API.\n    :return: self. An instance of the LinearRegression class.\n    \"\"\"\n    epsilon = self.epsilon\n    bounds_X = self.bounds_X\n    bounds_y = self.bounds_y\n    fit_intercept = self.fit_intercept\n    copy_X = self.copy_X\n    random_state = self.random_state\n    accountant = self.accountant\n\n    X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept, epsilon, bounds_X, bounds_y, copy_X, True,\n                                                         random_state, accountant=accountant)\n\n    n_targets = y.shape[1] if y.ndim > 1 else 1\n    alpha = self.alpha\n\n    objectives, noisy_coefs = _construct_regression_obj(X, y, bounds_X, bounds_y, epsilon, alpha, random_state)\n\n    def optimize_coeffs(objectives, budget_accountant=None):\n        coeffs_optimized = []\n        for target_idx, obj in enumerate(objectives):\n            result = minimize(fun=obj, x0=np.zeros(X.shape[1]), method='L-BFGS-B')\n            if budget_accountant:\n                budget_accountant.spend(result.fun, 0)\n            coeffs_optimized.append(result.x)\n\n        return np.array(coeffs_optimized).reshape((-1, n_targets))\n\n    if fit_intercept:\n        intercepts_optimized = np.zeros(n_targets)\n        for target_idx, obj in enumerate(objectives):\n            result = minimize(fun=lambda z: obj(z)[0] + alpha * z**2, x0=0, method='BFGS')\n            if accountant:\n                accountant.spend(result.fun, 0)\n            intercepts_optimized[target_idx] = result.x\n\n        self.coef_ = optimize_coeffs(objectives, accountant) + intercepts_optimized[:, np.newaxis] * X_offset\n        self.intercept_ = intercepts_optimized * y_offset\n    else:\n        self.coef_ = optimize_coeffs(objectives, accountant)\n        self.intercept_ = np.zeros(n_targets)\n\n    return self\n```"
    },
    {
        "namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows",
        "completion": "```python\ndef compute_rarest_windows(self, window_len: int, use_geo_mean: bool, use_start_end_tokens: bool):\n    \"\"\"\n    Compute the rarest windows and their likelihood for each session.\n\n    :param self: Model. An instance of the Model class.\n    :param window_len: int. The length of the sliding window for likelihood calculations.\n    :param use_geo_mean: bool. If True, the likelihoods of the sliding windows will be raised to the power of (1/window_len).\n    :param use_start_end_tokens: bool. If True, start and end tokens will be added to each session before calculations.\n    :return: None. The function updates the rarest windows and corresponding likelihoods in the Model instance.\n    \"\"\"\n    \n    if use_start_end_tokens:\n        sessions_extended = [[self.start_token] + sess + [self.end_token] for sess in self.sessions]\n    else:\n        sessions_extended = self.sessions\n    \n    for idx, session in enumerate(sessions_extended):\n        session_length = len(session)\n        if session_length <= window_len:\n            continue\n        \n        window_likelihoods = []\n        for i in range(session_length - window_len + 1):\n            window = session[i:i+window_len]\n            if use_geo_mean:\n                likelihood = self.compute_geomean_window_likelihood(window, window_len)\n            else:\n                likelihood = self.compute_window_likelihood(window, window_len)\n            \n            window_likelihoods.append((window, likelihood))\n        \n        window_likelihoods.sort(key=lambda x: x[1])\n        num_sessions_processed = len(window_likelihoods) // 100  # Assuming we want the top 1% rarest windows\n        self.rare_windows[idx] = [wnd for wnd, _ in window_likelihoods[:num_sessions_processed]]\n        self.rare_window_likelihoods[idx] = [likelihood for _, likelihood in window_likelihoods[:num_sessions_processed]]\n\n        if use_geo_mean:\n            self.rare_windows_geo[idx] = [wnd for wnd, _ in window_likelihoods[:num_sessions_processed]]\n            self.rare_window_likelihoods_geo[idx] = [likelihood for _, likelihood in window_likelihoods[:num_sessions_processed]]\n\n    if use_geo_mean:\n        self.rare_windows = self.rare_windows_geo\n        self.rare_window_likelihoods = self.rare_window_likelihoods_geo\n\n    self.update_session_rarity()\n```\n\nThis function calculates the rarest windows in each session using a sliding window approach. It considers the specified window size (`window_len`) and whether to apply the geometric mean to the likelihoods (`use_geo_mean`). If start and end tokens are enabled, they are added to each session. The function sorts the windows by likelihood and keeps the top 1% as the rarest windows, updating the respective dictionaries in the model instance. Finally, if `use_geo_mean` is True, it updates the main rare window dictionaries with the geometry mean results."
    }
]