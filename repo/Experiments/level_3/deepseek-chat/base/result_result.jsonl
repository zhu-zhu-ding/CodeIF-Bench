{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "\ndef _stream_history_log_dirs(self, output_dir=None):\n    \"\"\"Yield lists of directories to search for the history log in.\n    \n    This function first checks if logs should be read, and then iterates over\n    unique log directories obtained from the Hadoop log directories. If the\n    directory exists, it logs an info message: 'Looking for history log in {directory}...'.\n    It then yields a list containing the directory.\n\n    :param self: HadoopJobRunner. An instance of the HadoopJobRunner class.\n    :param output_dir: str. The output directory to search for the history log. Defaults to None.\n    :return: Generator. Yields lists of directories to search for the history log in.\n    \"\"\"\n    if not self._read_logs():\n        return\n\n    for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n        if _logs_exist(self.fs, log_dir):\n            log.info('Looking for history log in %s...' % log_dir)\n            yield [log_dir]\n", "Result": "Error"}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "\ndef to_json(self, handler=None):\n    \"\"\"Convert the HTTPError instance into a JSON representation.\n\n    This function uses the provided handler to serialize the error to JSON.\n    If no handler is provided, a default handler using the built-in JSON\n    library is used.\n\n    Args:\n        handler (Optional[object]): A handler object that will be used to\n            serialize the representation of this error to JSON. If not\n            provided, a default handler is used.\n\n    Returns:\n        bytes: A JSON document representing the error.\n    \"\"\"\n    if handler is None:\n        handler = _DEFAULT_JSON_HANDLER\n\n    # Convert the error to a dictionary representation\n    error_dict = self.to_dict()\n\n    # Serialize the dictionary to JSON using the handler\n    json_bytes = handler.serialize(error_dict)\n\n    return json_bytes\n", "Result": "Error"}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "\ndef mkdir(self, path):\n    \"\"\"\n    Create a directory in the Hadoop filesystem.\n\n    :param path: str. The path of the directory to be created.\n    :return: No return values.\n    :raises IOError: If the directory creation fails and the directory does not already exist.\n    \"\"\"\n    # Determine if the Hadoop version uses YARN (Hadoop 2+)\n    version = self.get_hadoop_version()\n    use_p_option = uses_yarn(version)\n\n    # Construct the command with the appropriate options\n    mkdir_args = ['fs', '-mkdir']\n    if use_p_option:\n        mkdir_args.append('-p')\n    mkdir_args.append(path)\n\n    try:\n        # Execute the Hadoop command\n        self.invoke_hadoop(mkdir_args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n    except CalledProcessError as e:\n        # If the directory already exists, do nothing\n        if _HADOOP_FILE_EXISTS_RE.search(e.stderr):\n            log.info(\"Directory already exists: %s\" % path)\n        else:\n            # Otherwise, raise an IOError\n            raise IOError(\"Could not mkdir %s\" % path)\n", "Result": "Error"}
{"namespace": "twtxt.config.Config.following", "completion": "\n@property\ndef following(self):\n    \"\"\"Retrieve a list of all Source objects stored in the \"following\" section of the Config instance.\n\n    :return: list. A list of Source objects.\n    \"\"\"\n    following_list = []\n    if not self.cfg.has_section(\"following\"):\n        logger.debug(\"No 'following' section found in the config.\")\n        return following_list\n\n    for nick, url in self.cfg.items(\"following\"):\n        source = Source(nick, url)\n        following_list.append(source)\n\n    return following_list\n", "Result": "Error"}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "\ndef thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n    \"\"\"\n    Return a list of message threads from the currently selected folder that match the specified criteria.\n    Each returned thread is a list of message IDs.\n\n    :param algorithm: String, the threading algorithm to use. It defaults to \"REFERENCES\" if not specified.\n    :param criteria: String, the search criteria to match the messages. It defaults to \"ALL\" if not specified.\n    :param charset: String, the character set to be used. It defaults to \"UTF-8\" if not specified.\n    :return: List[Tuple], each tuple represents a message thread, where each element of the tuple is a message ID.\n             For example, \"((1, 2), (3,), (4, 5, 6))\".\n    \"\"\"\n    # Ensure the server supports the THREAD capability\n    if not self.has_capability(\"THREAD\"):\n        raise exceptions.CapabilityError(\"Server does not support THREAD capability\")\n\n    # Prepare the arguments for the THREAD command\n    args = [algorithm.upper(), charset.upper()]\n    if criteria != \"ALL\":\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n    # Execute the THREAD command\n    try:\n        data = self._raw_command_untagged(b\"THREAD\", args, response_name=\"THREAD\")\n    except imaplib.IMAP4.error as e:\n        raise exceptions.IMAPClientError(f\"THREAD command failed: {e}\")\n\n    # Parse the response into a list of tuples representing threads\n    threads = []\n    for thread_data in parse_response(data):\n        if isinstance(thread_data, tuple):\n            threads.append(tuple(int(msg_id) for msg_id in thread_data))\n        else:\n            threads.append((int(thread_data),))\n\n    return threads\n", "Result": "Pass"}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "\ndef get_node(self, page: int) -> Node:\n    \"\"\"\n    Retrieve a node from storage.\n\n    This function first checks if the node is present in the cache. If not,\n    it retrieves the data from storage and creates a Node object using the data.\n    The created node is then added to the cache for future use.\n\n    :param self: FileMemory. An instance of the FileMemory class.\n    :param page: int. The page number of the node to retrieve.\n    :return: Node. The retrieved node.\n    \"\"\"\n    # Check if the node is in the cache\n    node = self._cache.get(page)\n    if node is not None:\n        return node\n\n    # If not in cache, retrieve the page data from storage\n    page_data = self.get_page(page)\n    \n    # Create a Node object from the page data\n    node = Node.from_page_data(self._tree_conf, page_data, page)\n    \n    # Add the node to the cache for future use\n    self._cache[page] = node\n    \n    return node\n", "Result": "Error"}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "\ndef generate_url_sigv4(self, expires_in, method, bucket='', key='', headers=None,\n                       force_http=False, response_headers=None, version_id=None,\n                       iso_date=None):\n    \"\"\"\n    Generate a presigned URL with Signature Version 4 for accessing an S3 object.\n\n    :param self: S3Connection. An instance of S3Connection class.\n    :param expires_in: Integer. The number of seconds until the presigned URL expires.\n    :param method: String. The HTTP method to be used for the request.\n    :param bucket: String. The name of the S3 bucket.\n    :param key: String. The key of the S3 object.\n    :param headers: Dictionary. Additional headers to include in the request.\n    :param force_http: Bool. Whether to force the use of HTTP instead of HTTPS.\n    :param response_headers: Dictionary. Additional response headers to include in the presigned URL.\n    :param version_id: String. The version ID of the S3 object.\n    :param iso_date: String. The ISO-formatted date to be used for signing the request.\n    :return: String. The generated presigned URL.\n    \"\"\"\n    headers = headers or {}\n    response_headers = response_headers or {}\n\n    # Construct the base HTTP request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n    auth_path = self.get_path(auth_path)\n    extra_qp = []\n\n    if version_id is not None:\n        extra_qp.append(\"versionId=%s\" % version_id)\n\n    if response_headers:\n        for k, v in response_headers.items():\n            extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n\n    if self.provider.security_token:\n        headers['x-amz-security-token'] = self.provider.security_token\n\n    if extra_qp:\n        delimiter = '?' if '?' not in auth_path else '&'\n        auth_path += delimiter + '&'.join(extra_qp)\n\n    if force_http:\n        protocol = 'http'\n        port = 80\n    else:\n        protocol = self.protocol\n        port = self.port\n\n    # Build the base URL\n    url_base = self.calling_format.build_url_base(self, protocol,\n                                                  self.server_name(port),\n                                                  bucket, key)\n\n    # Create the HTTP request object\n    http_request = HTTPRequest(method, protocol, self.host, port,\n                               auth_path, auth_path, {}, headers, '')\n\n    # Generate the presigned URL using the S3HmacAuthV4Handler\n    presigned_url = self._auth_handler.presign(http_request, expires_in, iso_date)\n\n    return presigned_url\n", "Result": "Error"}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "\ndef logger_class(self):\n    \"\"\"\n    Retrieves the logger class based on the configuration settings.\n    \n    This function first checks the 'logger_class' setting and if it is \"simple\", \n    it uses the default logger class. If the default logger class is being used \n    and statsd is on, it automatically switches to the gunicorn.instrument.statsd.Statsd class. \n    Then, it loads the logger class (with default: \"gunicorn.glogging.Logger\" and section: \"gunicorn.loggers\") \n    and install it if possible, finally returns it.\n    \n    :param self: Config. An instance of the Config class.\n    :return: The logger class based on the configuration settings.\n    \"\"\"\n    # Get the logger class setting from the configuration\n    logger_class_uri = self.settings['logger_class'].get()\n    \n    # If the logger class is set to \"simple\", use the default logger class\n    if logger_class_uri == \"simple\":\n        logger_class_uri = \"gunicorn.glogging.Logger\"\n        \n        # If statsd is enabled, switch to the Statsd logger class\n        if self.settings['statsd_host'].get() is not None:\n            logger_class_uri = \"gunicorn.instrument.statsd.Statsd\"\n    \n    # Load the logger class using the load_class utility function\n    logger_class = util.load_class(\n        logger_class_uri,\n        default=\"gunicorn.glogging.Logger\",\n        section=\"gunicorn.loggers\"\n    )\n    \n    # If the logger class has an 'install' method, call it\n    if hasattr(logger_class, \"install\"):\n        logger_class.install()\n    \n    return logger_class\n", "Result": "Error"}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "\ndef to_constraint(self) -> Constraint:\n    \"\"\"Convert this DropConstraintOp to a Constraint instance.\n\n    This method checks if the reverse operation (an AddConstraintOp) is present.\n    If it is, it converts the reverse operation to a Constraint instance and\n    sets the name, table name, and schema of the constraint. If the reverse\n    operation is not present, it raises a ValueError.\n\n    :return: The converted Constraint instance.\n    :raises ValueError: If the reverse operation is not present.\n    \"\"\"\n    if self._reverse is None:\n        raise ValueError(\"Cannot convert DropConstraintOp to Constraint: no reverse operation present\")\n\n    # Convert the reverse operation to a Constraint instance\n    constraint = self._reverse.to_constraint()\n\n    # Set the name, table name, and schema of the constraint\n    constraint.name = self.constraint_name\n    constraint.table.name = self.table_name\n    constraint.table.schema = self.schema\n\n    return constraint\n", "Result": "Error"}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "\n@classmethod\ndef get_bootstrap_from_recipes(cls, recipes, ctx):\n    \"\"\"\n    Selects a recommended default bootstrap from a list of recipes and returns it.\n    The function follows a set of rules to determine the appropriate bootstrap:\n    - SDL2 bootstrap if there's an SDL2 dependency.\n    - \"webview\" if we depend on the common web recipe.\n    - Otherwise, fall back to the highest priority bootstrap from the default list.\n\n    :param cls: Class. The Bootstrap class.\n    :param recipes: List of strings. The list of recipes to consider when selecting the bootstrap.\n    :param ctx: Context. The context in which the function is being called.\n    :return: Bootstrap. The selected default bootstrap.\n    \"\"\"\n    # Get all acceptable bootstraps for the given recipes\n    acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n\n    # Rule 1: Prefer SDL2 bootstrap if there's an SDL2 dependency\n    if 'sdl2' in recipes:\n        sdl2_bootstrap = cls.get_bootstrap('sdl2', ctx)\n        if sdl2_bootstrap in acceptable_bootstraps:\n            return sdl2_bootstrap\n\n    # Rule 2: Prefer \"webview\" bootstrap if there's a dependency on the common web recipe\n    if 'webview' in recipes:\n        webview_bootstrap = cls.get_bootstrap('webview', ctx)\n        if webview_bootstrap in acceptable_bootstraps:\n            return webview_bootstrap\n\n    # Rule 3: Fall back to the highest priority bootstrap from the default list\n    # Sort acceptable bootstraps by priority\n    sorted_bootstraps = sorted(acceptable_bootstraps, key=functools.cmp_to_key(_cmp_bootstraps_by_priority), reverse=True)\n\n    if sorted_bootstraps:\n        return sorted_bootstraps[0]\n\n    # If no acceptable bootstrap is found, raise an exception\n    raise BuildInterruptingException(\n        \"No suitable bootstrap found for the given recipes: {}\".format(recipes)\n    )\n", "Result": "Error"}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "\ndef deserialize(self, value):\n    \"\"\"Deserialize a value and return a pair of deserialized values.\n\n    :param value: The value to be deserialized.\n    :return: Tuple. A pair of deserialized values.\n    \"\"\"\n    # Decode the input value and remove any leading or trailing whitespace\n    raw_value = decode(value).strip()\n\n    # Validate the raw value based on whether it is required or not\n    validators.validate_required(raw_value, self._required)\n\n    # If the raw value is empty, return None\n    if not raw_value:\n        return None\n\n    # If the separator is present in the raw value, split the value into two parts\n    if self._separator in raw_value:\n        first_part, second_part = raw_value.split(self._separator, 1)\n    else:\n        # If the optional pair flag is set, assign the same value to both parts\n        if self._optional_pair:\n            first_part = second_part = raw_value\n        else:\n            # Otherwise, raise a ValueError indicating that the config value must include the separator\n            raise ValueError(f\"Config value must include the separator '{self._separator}'\")\n\n    # Encode and deserialize each part of the pair using the corresponding subtypes\n    deserialized_first_value = self._subtypes[0].deserialize(first_part.strip())\n    deserialized_second_value = self._subtypes[1].deserialize(second_part.strip())\n\n    return deserialized_first_value, deserialized_second_value\n", "Result": "Error"}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "\n@classmethod\ndef get_data_dir(cls, config: Config) -> Path:\n    \"\"\"Get or create data directory for the extension.\n\n    Use this directory to store data that should persist across sessions.\n\n    :param config: the Mopidy config object\n    :return: pathlib.Path\n    \"\"\"\n    if cls.ext_name is None:\n        raise AssertionError(\"Extension name (ext_name) is not set.\")\n    \n    # Get the base data directory from the config\n    base_data_dir = path.expand_path(config[\"core\"][\"data_dir\"])\n    \n    # Create the extension-specific data directory path\n    data_dir_path = base_data_dir / cls.ext_name\n    \n    # Ensure the directory exists\n    path.get_or_create_dir(data_dir_path)\n    \n    return data_dir_path\n", "Result": "Error"}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "\ndef fit(self, X, y, sample_weight=None):\n    \"\"\"\n    Fit the linear regression model to the given training data.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        The training data.\n    y : array-like, shape (n_samples, n_targets)\n        The target values.\n    sample_weight : ignored\n        Ignored by diffprivlib. Present for consistency with sklearn API.\n\n    Returns\n    -------\n    self : LinearRegression\n        Returns the instance itself.\n    \"\"\"\n    # Preprocess the data\n    X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n        X, y, self.fit_intercept, self.epsilon, self.bounds_X, self.bounds_y, self.copy_X, True, self.random_state\n    )\n\n    # Determine the bounds for X and y\n    bounds_X = self._check_bounds(self.bounds_X, X.shape[1])\n    bounds_y = self._check_bounds(self.bounds_y, y.shape[1] if y.ndim > 1 else 1)\n\n    # Construct the regression objects\n    regression_objs, noisy_coefs = _construct_regression_obj(X, y, bounds_X, bounds_y, self.epsilon, 0, self.random_state)\n\n    # Optimize the coefficients using the minimize function\n    coefs = []\n    for obj in regression_objs:\n        res = minimize(obj, np.zeros(X.shape[1]), jac=True, method='L-BFGS-B')\n        coefs.append(res.x)\n\n    # Set the coefficients and intercept\n    self.coef_ = np.array(coefs).T\n    if self.fit_intercept:\n        self.intercept_ = y_offset - np.dot(X_offset, self.coef_)\n    else:\n        self.intercept_ = 0.0\n\n    # Update the accountant's spending\n    self.accountant.spend(self.epsilon, 0)\n\n    return self\n", "Result": "Error"}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "\ndef compute_rarest_windows(\n    self, window_len: int, use_start_end_tokens: bool, use_geo_mean: bool\n) -> None:\n    \"\"\"\n    Compute the rarest windows and corresponding likelihood for each session.\n\n    This function uses a sliding window approach to identify the rarest window\n    and its likelihood in each session. The function takes into account the\n    length of the sliding window, whether to use start and end tokens, and\n    whether to use the geometric mean for likelihood calculations.\n\n    Parameters\n    ----------\n    window_len : int\n        The length of the sliding window for likelihood calculations.\n    use_start_end_tokens : bool\n        If True, start and end tokens will be added to each session before calculations.\n    use_geo_mean : bool\n        If True, the likelihoods of the sliding windows will be raised to the power of (1/window_len).\n\n    Returns\n    -------\n    None\n        The function updates the rarest windows and corresponding likelihoods in the Model instance.\n    \"\"\"\n    if self.prior_probs is None:\n        raise MsticpyException(\"Please train the model first before using this method.\")\n\n    # Initialize dictionaries to store results\n    rare_windows = {}\n    rare_window_likelihoods = {}\n\n    for session_idx, session in enumerate(self.sessions):\n        # Prepare the session by adding start and end tokens if required\n        if use_start_end_tokens:\n            session = [self.start_token] + session + [self.end_token]\n\n        # Skip sessions that are shorter than the window length\n        if len(session) < window_len:\n            rare_windows[session_idx] = []\n            rare_window_likelihoods[session_idx] = []\n            continue\n\n        # Initialize variables to track the rarest window and its likelihood\n        min_likelihood = float(\"inf\")\n        rarest_window = []\n\n        # Slide the window over the session\n        for i in range(len(session) - window_len + 1):\n            window = session[i : i + window_len]\n\n            # Compute the likelihood of the current window\n            if self.session_type == SessionType.cmds_only:\n                likelihood = cmds_only.compute_likelihood_window(\n                    window=window,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    use_start_token=False,  # Tokens are already added if required\n                    use_end_token=False,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            elif self.session_type == SessionType.cmds_params_only:\n                likelihood = cmds_params_only.compute_likelihood_window(\n                    window=window,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    use_start_token=False,\n                    use_end_token=False,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            elif self.session_type == SessionType.cmds_params_values:\n                likelihood = cmds_params_values.compute_likelihood_window(\n                    window=window,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    use_start_token=False,\n                    use_end_token=False,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:\n                raise MsticpyException(\"Invalid session type.\")\n\n            # Apply geometric mean if required\n            if use_geo_mean:\n                likelihood = likelihood ** (1 / window_len)\n\n            # Update the rarest window if the current window is rarer\n            if likelihood < min_likelihood:\n                min_likelihood = likelihood\n                rarest_window = window\n\n        # Store the rarest window and its likelihood for the current session\n        rare_windows[session_idx] = rarest_window\n        rare_window_likelihoods[session_idx] = min_likelihood\n\n    # Update the Model instance with the results\n    if use_geo_mean:\n        self.rare_windows_geo = rare_windows\n        self.rare_window_likelihoods_geo = rare_window_likelihoods\n    else:\n        self.rare_windows = rare_windows\n        self.rare_window_likelihoods = rare_window_likelihoods\n", "Result": "Error"}
