{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "\ndef read_value(self, key: str):\n    \"\"\"\n    Read the value corresponding to the given key from the MmapedDict instance.\n    If the key is not found in the instance, it initializes the value and then returns it.\n    \n    Parameters:\n    :param self: MmapedDict. An instance of the MmapedDict class.\n    :param key: The key to read the value from the instance.\n    :return: The value corresponding to the key.\n    \"\"\"\n    if key not in self._positions:\n        self._init_value(key)\n    pos = self._positions[key]\n    value, timestamp = _unpack_two_doubles(self._m, pos)\n    return value\n", "Result": "Error"}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "\ndef add_rule(self, ip_protocol, from_port, to_port,\n            src_group_name=None, src_group_owner_id=None, cidr_ip=None,\n            src_group_group_id=None, dry_run=False):\n    \"\"\"\n    Add a new rule to this security group.\n    \n    :type ip_protocol: str\n    :param ip_protocol: The IP protocol for the rule (e.g., 'tcp', 'udp', 'icmp').\n    \n    :type from_port: int\n    :param from_port: The starting port range for the rule.\n    \n    :type to_port: int\n    :param to_port: The ending port range for the rule.\n    \n    :type src_group_name: str\n    :param src_group_name: The name of the source security group.\n    \n    :type src_group_owner_id: str\n    :param src_group_owner_id: The ID of the owner of the source security group.\n    \n    :type cidr_ip: str\n    :param cidr_ip: The CIDR IP range for the rule.\n    \n    :type src_group_group_id: str\n    :param src_group_group_id: The ID of the source security group.\n    \n    :type dry_run: bool\n    :param dry_run: Whether to perform a dry run. Defaults to False.\n    \n    :rtype: None\n    :return: None\n    \"\"\"\n    new_rule = IPPermissions(self)\n    new_rule.ip_protocol = ip_protocol\n    new_rule.from_port = from_port\n    new_rule.to_port = to_port\n    \n    if src_group_name:\n        src_grant = GroupOrCIDR(new_rule)\n        src_grant.name = src_group_name\n        src_grant.owner_id = src_group_owner_id\n        new_rule.grants.append(src_grant)\n    elif cidr_ip:\n        cidr_grant = GroupOrCIDR(new_rule)\n        cidr_grant.cidr_ip = cidr_ip\n        new_rule.grants.append(cidr_grant)\n    elif src_group_group_id:\n        src_group_grant = GroupOrCIDR(new_rule)\n        src_group_grant.group_id = src_group_group_id\n        src_group_grant.owner_id = src_group_owner_id\n        new_rule.grants.append(src_group_grant)\n    \n    self.rules.append(new_rule)\n", "Result": "Pass"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "\nclass DefaultContainer(DataContainer[t.Any, t.List[t.Any]]):\n    # ... (previous code remains unchanged)\n\n    @classmethod\n    def from_batch_payloads(\n        cls, payloads: t.Sequence[Payload], batch_dim: int = 0\n    ) -> tuple[t.Any, list[int]]:\n        # Initialize an empty list to store the batches\n        batches = []\n\n        # Iterate through each payload in the sequence\n        for payload in payloads:\n            # Convert each payload back to its original batch using the from_payload method\n            batch = cls.from_payload(payload)\n            \n            # Append the converted batch to our list of batches\n            batches.append(batch)\n\n        # Combine all individual batches into a single batch along the specified batch dimension\n        combined_batch, batch_sizes = cls.batches_to_batch(batches, batch_dim)\n\n        # Return the combined batch and the corresponding batch sizes\n        return combined_batch, batch_sizes\n", "Result": "Error"}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "\ndef commit(self, blocking=True):\n    if blocking:\n        # by default, we await completion of commit() unless\n        # blocking=False.  This ensures any available exceptions for any\n        # previous statement are thrown before returning, and that the\n        # data has actually persisted to disk!\n        self.select_one(_REQUEST_COMMIT)\n    else:\n        # otherwise, we fire and forget as usual.\n        self.execute(_REQUEST_COMMIT)\n", "Result": "Error"}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "\ndef from_column_and_tablename(cls, schema: Optional[str], tname: str, col: Column[Any]) -> AddColumnOp:\n    return cls(tname, col, schema=schema)\n", "Result": "Error"}
{"namespace": "pyramid.i18n.Translations.add", "completion": "\ndef add(self, translations, merge=True):\n    \"\"\"Adds the given translations to the catalog. If the domain of the\n       translations is different from the current catalog, they are added as a\n       separate catalog. Optionally merges translations for message domains\n       that have already been added.\n\n    Args:\n        translations (Translations): The Translations instance with the\n                                    messages to add.\n        merge (bool, optional): Whether translations for message domains\n                                that have already been added should be merged\n                                with the existing translations. Defaults to True.\n\n    Returns:\n        Translations: The Translations instance (self) so that merge calls\n                     can be easily chained.\n    \"\"\"\n    if translations.domain not in self._domains:\n        self._domains[translations.domain] = translations\n    else:\n        if merge:\n            self._domains[translations.domain].merge(translations)\n        else:\n            raise ValueError(\"Domain '{}' already exists and merge is False\".format(translations.domain))\n\n    return self\n", "Result": "Error"}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "\nclass OpenSSLPrerequisite(Prerequisite):\n    # ... other methods ...\n\n    def darwin_checker(self):\n        brew_prefix = self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name)\n        if brew_prefix is not None:\n            openssl_bin = os.path.join(brew_prefix, \"bin\", \"openssl\")\n            if os.path.isfile(openssl_bin):\n                return True\n        return False\n", "Result": "Error"}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "\ndef pop(self, key, default=_MISSING):\n    \"\"\"Pop the key in the LRI instance and return the corresponding value.\n    \n    If the key is not found and the default value is not passed, \n    the exception is re-raised. This function bypasses the hit count \n    and miss count.\n    \n    Parameters:\n    - key: The key to remove in the instance.\n    - default: The value to return if the key is not found in the instance. \n               Defaults to _UNSET.\n    \n    Returns:\n    - The value corresponding to the key.\n    \"\"\"\n    with self._lock:\n        try:\n            value = super(LRI, self).__getitem__(key)\n            super(LRI, self).__delitem__(key)\n            self._remove_from_ll(key)\n            return value\n        except KeyError:\n            if default is _MISSING:\n                raise\n            return default\n", "Result": "Pass"}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "\n@require_capability(\"UIDPLUS\")\ndef expunge(self, messages=None):\n    \"\"\"\n    Expunge messages from the selected folder in an IMAP client.\n\n    If no messages are specified, it removes all messages with the \"\\Deleted\" flag set.\n    \n    If messages are specified, it removes the specified messages with the \"\\Deleted\" flag set.\n    \n    Parameters:\n    - self: IMAPClient. An instance of the IMAPClient class.\n    - messages: List[int] or List[str]. The messages to be expunged. Defaults to None.\n    \n    Returns:\n    - Tuple. The server response message followed by a list of expunge responses if no messages are specified. \n    - None if messages are specified.\n    \"\"\"\n    if messages is None:\n        # No messages specified, expunge all messages with the \"\\Deleted\" flag\n        expunge_response = self._command_and_check(\"EXPUNGE\", uid=True)\n        return (expunge_response, self._imap.untagged_responses.get(\"EXPUNGE\", []))\n    else:\n        # Messages specified, expunge the specified messages with the \"\\Deleted\" flag\n        expunge_response = self.uid_expunge(messages)\n        return None\n", "Result": "Error"}
{"namespace": "mopidy.ext.load_extensions", "completion": "\ndef load_extensions() -> List[ExtensionData]:\n    installed_extensions = []\n    \n    # Iterate over all entry points with group 'mopidy.ext'\n    for entry_point in pkg_resources.iter_entry_points(group='mopidy.ext'):\n        try:\n            # Load the extension using its load() method\n            extension_class = entry_point.load()\n            \n            # Check if the loaded object is a subclass of the base Extension class\n            if not issubclass(extension_class, Extension):\n                continue\n            \n            # Create an instance of the extension class\n            extension_instance = extension_class()\n            \n            # Get the default configuration schema and defaults\n            config_schema = extension_instance.get_config_schema()\n            config_defaults = extension_instance.get_default_config()\n            \n            # Create an extension command (if available)\n            command = extension_instance.get_command()\n            \n            # Validate the extension data\n            if validate_extension_data(ExtensionData(extension=extension_instance, \n                                                    entry_point=entry_point, \n                                                    config_schema=config_schema, \n                                                    config_defaults=config_defaults, \n                                                    command=command)):\n                # If validation passes, append the ExtensionData object to the list\n                installed_extensions.append(ExtensionData(extension=extension_instance, \n                                                        entry_point=entry_point, \n                                                        config_schema=config_schema, \n                                                        config_defaults=config_defaults, \n                                                        command=command))\n        \n        except Exception as e:\n            logger.error(f\"Failed to load extension {entry_point.name}: {e}\")\n    \n    return installed_extensions\n", "Result": "Error"}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "\nclass WikipediaPage:\n    # ... (rest of the class remains unchanged)\n\n    def section_by_title(self, title: str) -> Optional[WikipediaPageSection]:\n        \"\"\"\n        Returns the last section of the current Wikipedia page with the given title.\n        If no such section exists, returns None.\n        \n        :param title: str. The title of the section to retrieve.\n        :return: Optional[WikipediaPageSection]. Last section with the given title.\n        \"\"\"\n        sections = self.section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None\n", "Result": "Error"}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "\ndef run(self):\n    \"\"\"\n    Wrapper around the main loop of a utility. Handles opening and closing files.\n\n    Parameters:\n    :param self: CSVKitUtility. An instance of the CSVKitUtility class.\n\n    Returns:\n    :return: None. No return value.\n    \"\"\"\n    if 'f' not in self.override_flags:\n        # Open the input file\n        self.input_file = self._open_input_file(self.args.input_path)\n\n    try:\n        # Execute the main loop of the utility\n        if 'no_header_row' in self.override_flags:\n            warnings.filterwarnings('ignore', category=UserWarning, module='agate')\n        self.main()\n    finally:\n        # Close the input file if it was opened\n        if 'f' not in self.override_flags:\n            self.input_file.close()\n", "Result": "Pass"}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "\ndef write_value(self, key, value, timestamp):\n    \"\"\"\n    Writes a value to a key in the MmapedDict instance. If the key does not exist in the instance, \n    it initializes the key and then writes the value and timestamp to the corresponding position in \n    the memory-mapped file.\n\n    Parameters:\n    :param self: MmapedDict. An instance of the MmapedDict class.\n    :param key: The key to write the value to.\n    :param value: The value to be written.\n    :param timestamp: The timestamp associated with the value.\n    :return: No return values.\n    \"\"\"\n    if key not in self._positions:\n        self._init_value(key)\n    \n    pos = self._positions[key]\n    _pack_two_doubles(self._m, pos, value, timestamp)\n", "Result": "Pass"}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        \n        # Loop through the levels dictionary to find the key that corresponds to the given value\n        for key, val in self.levels.items():\n            if val == value:\n                return key\n        \n        # Return an empty string if the value is not found\n        return \"\"\n", "Result": "Error"}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "\ndef pop(self, key, default=Sentinel):\n    with self._database.atomic():\n        try:\n            value = self[key]  # Retrieve the value associated with the key\n            del self[key]     # Remove the key-value pair from the database\n            return value      # Return the retrieved value\n        except KeyError:\n            if default is Sentinel:\n                raise KeyError(f\"Key '{key}' not found\")\n            return default\n", "Result": "Pass"}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "\ndef convert_to_public(self):\n    \"\"\"\n    Convert an SSHKey object containing a private key into one that contains only the corresponding public key.\n    \n    Parameters:\n        self (SSHKey): The SSHKey instance containing a private key.\n    \n    Returns:\n        SSHKey: The converted SSHKey object containing only the public key.\n    \"\"\"\n    # Ensure the current key is a private key before proceeding\n    if not self._key:\n        raise ValueError(\"Provided SSHKey does not contain a private key.\")\n    \n    # Extract public key components from the private key\n    public_key_params = self._key.get_public_key_params()\n    \n    # Generate a new SSHKey object using the extracted public key components\n    public_key = self.__class__.make_public(public_key_params)\n    \n    # Copy over the comment and filename from the original key to the public key\n    public_key.set_comment(self._comment)\n    public_key.set_filename(self._filename)\n    \n    return public_key\n", "Result": "Error"}
{"namespace": "rows.fields.DateField.deserialize", "completion": "\nclass DateField(Field):\n    \"\"\"Field class to represent date\n\n    Is not locale-aware (does not need to be)\n    \"\"\"\n\n    TYPE = (datetime.date,)\n    INPUT_FORMAT = \"%Y-%m-%d\"\n    OUTPUT_FORMAT = \"%Y-%m-%d\"\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            date_obj = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        except ValueError:\n            raise ValueError(\"Invalid date format. Expected {}\".format(cls.INPUT_FORMAT))\n        \n        return datetime.date(date_obj.year, date_obj.month, date_obj.day)\n", "Result": "Error"}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "\nimport re\n\nclass EmailField(TextField):\n    EMAIL_REGEXP = re.compile(\n        r\"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]+$\", flags=re.IGNORECASE\n    )\n\n    @classmethod\n    def serialize(cls, value, *args, **kwargs):\n        if value is None:\n            return \"\"\n\n        return six.text_type(value)\n\n    @classmethod\n    def deserialize(cls, value, *args, **kwargs):\n        value = super(EmailField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif not value:\n            return None\n        \n        match = cls.EMAIL_REGEXP.match(value)\n        if match:\n            return match.group(0)\n        else:\n            raise ValueError(\"Invalid email address\")\n\n# Example usage:\nemail_field = EmailField()\nprint(email_field.deserialize(\"example@example.com\"))  # Output: example@example.com\nprint(email_field.deserialize(\"\"))  # Output: None\nprint(email_field.deserialize(\"invalid-email\"))  # Raises ValueError: Invalid email address\n", "Result": "Error"}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "\ndef freeze(self, queryset, format='csv', filename=None, file_obj=None, encoding='utf8', **kwargs):\n    \"\"\"Freeze the dataset by exporting it to a file in the specified format.\"\"\"\n    \n    # Check the arguments\n    self._check_arguments(filename, file_obj, format, self._export_formats)\n\n    # Create an exporter instance based on the format\n    exporter = self._export_formats[format](queryset, **kwargs)\n\n    # Open the file if a filename is provided\n    if filename:\n        file_obj = open_file(filename, 'w', encoding)\n\n    # Export the dataset to the file\n    exporter.export(file_obj)\n\n    # Close the file if it was opened\n    if filename:\n        file_obj.close()\n", "Result": "Pass"}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "\nimport functools\nimport struct\nfrom pycoin.satoshi.satoshi_struct import parse_struct\nfrom pycoin.encoding.hexbytes import h2b\nfrom pycoin.intbytes import iterbytes\n\nIP4_HEADER = h2b(\"00000000000000000000FFFF\")\n\ndef ip_bin_to_ip6_addr(ip_bin):\n    return \":\".join(\"%x\" % v for v in struct.unpack(\">HHHHHHHH\", ip_bin))\n\ndef ip_bin_to_ip4_addr(ip_bin):\n    return \".\".join(map(str, iterbytes(ip_bin[-4:])))\n\n@functools.total_ordering\nclass PeerAddress(object):\n    def __init__(self, services, ip_bin, port):\n        self.services = int(services)\n        assert isinstance(ip_bin, bytes)\n        if len(ip_bin) == 4:\n            ip_bin = IP4_HEADER + ip_bin\n        assert len(ip_bin) == 16\n        self.ip_bin = ip_bin\n        self.port = port\n\n    def __repr__(self):\n        return \"%s/%d\" % (self.host(), self.port)\n\n    def stream(self, f):\n        f.write(struct.pack(\"<Q\", self.services))\n        f.write(self.ip_bin)\n        f.write(struct.pack(\"!H\", self.port))\n\n    @classmethod\n    def parse(cls, f):\n        services, ip_bin, port = parse_struct(\"Q@h\", f)\n        return cls(services, ip_bin, port)\n\n    def __lt__(self, other):\n        return (self.ip_bin, self.port, self.services) < (other.ip_bin, other.port, other.services)\n\n    def __eq__(self, other):\n        return self.services == other.services and \\\n               self.ip_bin == other.ip_bin and self.port == other.port\n\n# Function to determine the host address based on the IP binary string\ndef host(self):\n    if self.ip_bin.startswith(IP4_HEADER):\n        return ip_bin_to_ip4_addr(self.ip_bin[12:])\n    else:\n        return ip_bin_to_ip6_addr(self.ip_bin)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a PeerAddress instance with an IPv4 address\n    ipv4_peer = PeerAddress(0, b'\\x00\\x00\\x00\\x01', 8333)\n    print(ipv4_peer.host())  # Output: \"1.0.0.0\"\n\n    # Create a PeerAddress instance with an IPv6 address\n    ipv6_peer = PeerAddress(0, b'\\x20\\x01\\x0d\\xb8\\x85\\xa3\\x00\\x00\\x00\\x00\\x8a\\x2e\\x03\\x70\\x73\\x73', 8333)\n    print(ipv6_peer.host())  # Output: \"2001:db8:85:a3::8a2e:3703:733\"\n", "Result": "Pass"}
