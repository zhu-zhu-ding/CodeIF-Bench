{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "\ndef read_value(self, key):\n    \"\"\"Read the value corresponding to the given key from the MmapedDict instance.\n    \n    If the key is not found in the instance, it initializes the value and then returns it.\n    \n    Args:\n        key: The key to read the value from the instance.\n        \n    Returns:\n        tuple: A tuple containing (value, timestamp) corresponding to the key.\n    \"\"\"\n    if key not in self._positions:\n        self._init_value(key)\n    pos = self._positions[key]\n    return _unpack_two_doubles(self._m, pos)\n", "Result": "Pass"}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "\ndef add_rule(self, ip_protocol, from_port, to_port,\n             src_group_name, src_group_owner_id, cidr_ip,\n             src_group_group_id, dry_run=False):\n    \"\"\"\n    Add a rule to the SecurityGroup object. Note that this method\n    only changes the local version of the object. No information\n    is sent to EC2.\n\n    :param ip_protocol: String. The IP protocol for the rule\n    :param from_port: Integer. The starting port range for the rule\n    :param to_port: Integer. The ending port range for the rule\n    :param src_group_name: String. The name of the source security group\n    :param src_group_owner_id: String. The ID of the owner of the source security group\n    :param cidr_ip: String. The CIDR IP range for the rule\n    :param src_group_group_id: String. The ID of the source security group\n    :param dry_run: Bool. Whether to perform a dry run. Defaults to False\n    \"\"\"\n    # Find existing rule or create new one\n    target_rule = None\n    for rule in self.rules:\n        if (rule.ip_protocol == ip_protocol and\n            rule.from_port == from_port and\n            rule.to_port == to_port):\n            target_rule = rule\n            break\n\n    if target_rule is None:\n        target_rule = IPPermissions(self.rules)\n        target_rule.ip_protocol = ip_protocol\n        target_rule.from_port = from_port\n        target_rule.to_port = to_port\n        self.rules.append(target_rule)\n\n    # Add the grant to the rule\n    target_rule.add_grant(\n        name=src_group_name,\n        owner_id=src_group_owner_id,\n        cidr_ip=cidr_ip,\n        group_id=src_group_group_id,\n        dry_run=dry_run\n    )\n", "Result": "Pass"}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "\n@classmethod\ndef from_batch_payloads(\n    cls,\n    payloads: t.Sequence[Payload],\n    batch_dim: int = 0,\n) -> tuple[list[t.Any], list[int]]:\n    \"\"\"\n    Convert a sequence of payloads into a single batch.\n\n    Args:\n        payloads: Sequence of Payload objects to be converted\n        batch_dim: The dimension along which the batches will be combined (default: 0)\n\n    Returns:\n        A tuple containing:\n        - The combined batch as a list\n        - A list of integers representing the batch sizes/indices\n    \"\"\"\n    assert batch_dim == 0, \"Default Runner DataContainer does not support batch_dim other than 0\"\n    \n    # Convert each payload back to its original form\n    batches = [cls.from_payload(payload) for payload in payloads]\n    \n    # Combine the batches into a single batch\n    return cls.batches_to_batch(batches, batch_dim)\n", "Result": "Error"}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "\ndef commit(self, blocking=True):\n    \"\"\"Persist all data to disk.\n\n    :param blocking: Bool. Whether to block until the commit is complete. Defaults to True.\n    \"\"\"\n    if self.flag == 'r':\n        raise RuntimeError('Refusing to commit to read-only SqliteDict')\n\n    if self.conn is not None:\n        self.conn.commit(blocking=blocking)\n", "Result": "Pass"}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "\n@classmethod\ndef from_column_and_tablename(\n    cls, \n    schema: Optional[str], \n    tname: str, \n    col: Column[Any]\n) -> DropColumnOp:\n    \"\"\"Create a drop column operation from a Column object and table name.\n    \n    :param schema: Optional schema name\n    :param tname: String name of the table\n    :param col: Column object representing the column to be dropped\n    :return: New DropColumnOp instance\n    \"\"\"\n    return cls(\n        tname,\n        col.name,\n        schema=schema,\n        _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col)\n    )\n", "Result": "Error"}
{"namespace": "pyramid.i18n.Translations.add", "completion": "\ndef add(self, translations, merge=True):\n    \"\"\"Add the given translations to the catalog.\n    \n    If the domain of the translations is different from the current catalog,\n    they are added as a separate catalog. Provides option to merge translations\n    for message domains that have already been added.\n    \n    :param translations: The Translations instance with the messages to add\n    :param merge: Whether translations for message domains that have already been \n                 added should be merged with the existing translations. Defaults to True.\n    :return: The Translations instance (self) so that merge calls can be easily chained\n    :rtype: Translations\n    \"\"\"\n    if not isinstance(translations, gettext.GNUTranslations):\n        return self\n        \n    domain = getattr(translations, 'domain', self.DEFAULT_DOMAIN)\n    \n    if domain == self.domain:\n        if merge:\n            self.merge(translations)\n    else:\n        existing = self._domains.get(domain)\n        if existing is not None and merge:\n            existing.merge(translations)\n        else:\n            self._domains[domain] = translations\n            \n    if isinstance(translations, Translations):\n        self.files.extend(translations.files)\n        \n    return self\n", "Result": "Error"}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "\ndef darwin_checker(self):\n    return (\n        self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name, installed=True)\n        is not None\n    )\n", "Result": "Pass"}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "\ndef pop(self, key, default=_MISSING):\n    \"\"\"Pop the key in the LRI instance and return the corresponding value.\n    \n    Args:\n        key: The key to remove in the instance\n        default: The value to return if the key is not found. Defaults to _MISSING.\n    \n    Returns:\n        The value corresponding to the key.\n        \n    Raises:\n        KeyError: If the key is not found and default is _MISSING.\n    \"\"\"\n    with self._lock:\n        try:\n            value = super(LRI, self).pop(key)\n            self._remove_from_ll(key)\n            return value\n        except KeyError:\n            if default is _MISSING:\n                raise\n            return default\n", "Result": "Pass"}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "\ndef expunge(self, messages=None):\n    \"\"\"Expunge messages from the currently selected folder.\n    \n    If messages is not specified, removes all messages with the \n    \\\\Deleted flag set. If messages is specified, removes the specified\n    messages that have the \\\\Deleted flag set.\n    \n    :param messages: Optional list of message UIDs to expunge\n    :type messages: List[int] or List[str] or None\n    :returns: The server response message followed by a list of expunge \n             responses if no messages specified, None if messages specified\n    :rtype: Tuple[bytes, List[Tuple]] or None\n    \"\"\"\n    if messages:\n        # Use UID EXPUNGE if messages specified and UIDPLUS capability available\n        if self.has_capability('UIDPLUS'):\n            return self.uid_expunge(messages)\n        else:\n            raise exceptions.CapabilityError(\n                \"Server does not support UIDPLUS capability required for expunging \"\n                \"specific messages\"\n            )\n    \n    # Regular EXPUNGE command for all messages with \\Deleted flag\n    tag = self._imap._command('EXPUNGE')\n    return self._consume_until_tagged_response(tag, 'EXPUNGE')\n", "Result": "Error"}
{"namespace": "mopidy.ext.load_extensions", "completion": "\ndef load_extensions() -> List[ExtensionData]:\n    \"\"\"Find all installed extensions.\n\n    Returns:\n        List of installed extensions, where each extension is represented by\n        an ExtensionData object.\n    \"\"\"\n    installed_extensions = []\n\n    logger.debug('Loading extensions from entry point mopidy.ext')\n\n    for entry_point in pkg_resources.iter_entry_points('mopidy.ext'):\n        try:\n            extension_class = entry_point.load()\n        except Exception:\n            logger.exception(\n                'Failed to load extension %s', entry_point.name\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                logger.error(\n                    'Entry point %s does not point to an Extension subclass.',\n                    entry_point.name\n                )\n                continue\n\n            extension = extension_class()\n            config_schema = extension.get_config_schema()\n            config_defaults = extension.get_default_config()\n            command = extension.get_command()\n\n            extension_data = ExtensionData(\n                extension=extension,\n                entry_point=entry_point,\n                config_schema=config_schema,\n                config_defaults=config_defaults,\n                command=command\n            )\n\n            if validate_extension_data(extension_data):\n                installed_extensions.append(extension_data)\n                logger.debug(\n                    'Loaded extension: %s %s',\n                    extension.dist_name,\n                    extension.version\n                )\n            else:\n                logger.debug(\n                    'Did not load extension: %s', entry_point.name\n                )\n\n        except Exception:\n            logger.exception(\n                'Failed to initialize extension %s', entry_point.name\n            )\n            continue\n\n    return installed_extensions\n", "Result": "Error"}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "\ndef section_by_title(self, title: str) -> Optional[WikipediaPageSection]:\n    \"\"\"\n    Returns the last section of the current page with the given title.\n\n    :param title: str. The title of the section to retrieve.\n    :return: Optional[WikipediaPageSection]. The last section of the current page with the given title.\n    \"\"\"\n    if not self._called[\"extracts\"]:\n        self._fetch(\"extracts\")\n    \n    sections = self._section_mapping.get(title)\n    if sections:\n        return sections[-1]\n    return None\n", "Result": "Pass"}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "\ndef run(self):\n    \"\"\"\n    A wrapper around the main loop of a utility. Handles opening and closing files.\n    \n    :param self: CSVKitUtility. An instance of the CSVKitUtility class.\n    :return: No return values.\n    \"\"\"\n    # Only open input file if -f flag is not present in override flags\n    if 'f' not in self.override_flags:\n        self.input_file = self._open_input_file(self.args.input_path)\n\n    # Execute main loop, ignoring warnings about column names if no_header_row is True\n    with warnings.catch_warnings():\n        if getattr(self.args, 'no_header_row', None):\n            warnings.filterwarnings('ignore', category=UserWarning, message='Column names were not specified')\n        self.main()\n\n    # Close input file if -f flag is not present in override flags\n    if 'f' not in self.override_flags:\n        self.input_file.close()\n", "Result": "Pass"}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "\ndef write_value(self, key, value, timestamp):\n    \"\"\"Write a value and timestamp for the given key to the memory-mapped file.\n    \n    Args:\n        key: The key to write the value to\n        value: The value to be written (float)\n        timestamp: The timestamp associated with the value (float)\n    \"\"\"\n    # Initialize the key if it doesn't exist\n    if key not in self._positions:\n        self._init_value(key)\n    \n    # Get position and write the value and timestamp\n    pos = self._positions[key]\n    _pack_two_doubles(self._m, pos, value, timestamp)\n", "Result": "Pass"}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "\ndef serialize(self, value, display=False):\n    \"\"\"Serialize a value based on the LogLevel instance.\n\n    :param value: The value to be serialized\n    :param display: Whether to display the serialized value. Defaults to False\n    :return: The serialized value or an empty string if the value is not found\n    \"\"\"\n    for level_name, level_value in self.levels.items():\n        if value == level_value:\n            return level_name\n    return \"\"\n", "Result": "Pass"}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "\ndef pop(self, key, default=Sentinel):\n    \"\"\"\n    Remove the specified key and return its value.\n    \n    :param key: The key to be removed from the instance\n    :param default: Optional value to return if key is not found (defaults to Sentinel)\n    :return: The value corresponding to the key, or default if provided\n    :raises KeyError: If key is not found and no default is provided\n    \"\"\"\n    with self._database.atomic():\n        try:\n            # Get the value before deleting\n            value = self[key]\n            # Delete the key-value pair\n            del self[key]\n            return value\n        except KeyError:\n            if default is Sentinel:\n                raise\n            return default\n", "Result": "Pass"}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "\ndef convert_to_public(self) -> 'SSHKey':\n    \"\"\"Convert a private key into a public key\n    \n    This method converts an SSHKey object that contains a private key into \n    one that contains only the corresponding public key.\n    \n    :param self: SSHKey. An instance of the SSHKey class.\n    :return: SSHKey. The SSHKey object that contains only the corresponding public key.\n    \"\"\"\n    \n    # Create new SSHKey instance with only the public key data\n    key = self.__class__.make_public(self._key.public_key_params())\n    \n    # Copy over the algorithm, comment and filename\n    key.algorithm = self.algorithm\n    key._comment = self._comment\n    key._filename = self._filename\n    \n    return key\n", "Result": "Error"}
{"namespace": "rows.fields.DateField.deserialize", "completion": "\n@classmethod\ndef deserialize(cls, value, *args, **kwargs):\n    \"\"\"Deserialize a value into a date instance\n    \n    :param cls: Class. The DateField class\n    :param value: Object. The value to be deserialized into a DateField instance\n    :param args: Object. Additional positional arguments\n    :param kwargs: Object. Additional keyword arguments\n    :return: date. The deserialized date instance\n    \"\"\"\n    value = super(DateField, cls).deserialize(value)\n    if value is None or isinstance(value, cls.TYPE):\n        return value\n\n    value = as_string(value)\n    dt = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n    return datetime.date(dt.year, dt.month, dt.day)\n", "Result": "Error"}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "\n@classmethod\ndef deserialize(cls, value, *args, **kwargs):\n    \"\"\"Deserialize and validate an email field value.\n    \n    :param cls: Class. The class object itself.\n    :param value: Any. The value to be deserialized and validated as an email field.\n    :param *args: Any. Additional positional arguments.\n    :param **kwargs: Any. Additional keyword arguments.\n    :return: Object. The deserialized and validated email value, or None if empty.\n    \"\"\"\n    value = super(EmailField, cls).deserialize(value, *args, **kwargs)\n    \n    if value is None or not value:\n        return None\n        \n    match = cls.EMAIL_REGEXP.match(value)\n    if not match:\n        raise ValueError(\"Invalid email format\")\n        \n    return match.group()\n", "Result": "Error"}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "\ndef freeze(self, query, format='csv', filename=None, file_obj=None,\n          encoding='utf8', **kwargs):\n    \"\"\"\n    Freeze the dataset by exporting it to a file in the specified format.\n    \n    :param self: DataSet instance\n    :param query: The query to export\n    :param format: String. Export format ('csv', 'json', or 'tsv'). Defaults to 'csv'\n    :param filename: String. Name of file to export to\n    :param file_obj: File object to export to\n    :param encoding: String. File encoding. Defaults to 'utf8'\n    :param kwargs: Additional keyword arguments for the exporter\n    \"\"\"\n    # Validate arguments and format\n    self._check_arguments(filename, file_obj, format, self._export_formats)\n    \n    # Open file if filename is provided\n    if filename:\n        file_obj = open_file(filename, 'w', encoding)\n    \n    try:\n        # Create exporter instance\n        exporter = self._export_formats[format](query)\n        \n        # Export the data\n        exporter.export(file_obj, **kwargs)\n    finally:\n        # Close file if we opened it\n        if filename:\n            file_obj.close()\n", "Result": "Pass"}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "\ndef host(self):\n    \"\"\"\n    Determines the host address based on the IP binary string.\n    \n    :param self: PeerAddress instance\n    :return: The host address as a string (either IPv4 or IPv6 format)\n    \"\"\"\n    if self.ip_bin.startswith(IP4_HEADER):\n        return ip_bin_to_ip4_addr(self.ip_bin)\n    return ip_bin_to_ip6_addr(self.ip_bin)\n", "Result": "Pass"}
