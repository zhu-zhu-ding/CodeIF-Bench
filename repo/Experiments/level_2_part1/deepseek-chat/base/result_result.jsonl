{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "\ndef setmaxsize(self, maxsize):\n    \"\"\"\n    Set the maximum size for receiving netstrings in the NetstringSocket instance.\n    \n    :param self: NetstringSocket. An instance of the NetstringSocket class.\n    :param maxsize: The maximum size for receiving netstrings.\n    :return: No return values.\n    \"\"\"\n    self.maxsize = maxsize\n    self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)\n", "Result": "Pass"}
{"namespace": "gunicorn.config.Config.__str__", "completion": "\ndef __str__(self):\n    # Initialize a list to hold the formatted key-value pairs\n    lines = []\n\n    # Determine the maximum length of the keys for alignment\n    key_max_length = max(len(key) for key in self.settings.keys()) if self.settings else 0\n\n    # Iterate through the settings dictionary\n    for key, value in self.settings.items():\n        # Get the actual value from the setting\n        setting_value = value.get()\n\n        # Format callable values\n        if callable(setting_value):\n            formatted_value = f\"<{setting_value.__qualname__}()>\"\n        else:\n            formatted_value = str(setting_value)\n\n        # Format the key-value pair and append it to the list\n        formatted_line = f\"{key:{key_max_length}} = {formatted_value}\"\n        lines.append(formatted_line)\n\n    # Join all lines with a newline character and return the resulting string\n    return \"\\n\".join(lines)\n", "Result": "Pass"}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "\ndef remove(self, category_name, discriminator):\n    \"\"\"\n    Remove an introspection object from the Introspector instance.\n\n    :param self: Introspector. An instance of the Introspector class.\n    :param category_name: str. The name of the category where the introspection object belongs.\n    :param discriminator: The discriminator of the introspection object.\n    :return: No return values.\n    \"\"\"\n    # Retrieve the category dictionary\n    category = self._categories.get(category_name)\n    if category is None:\n        return  # Category does not exist, nothing to remove\n\n    # Retrieve the introspection object\n    intr = category.get(discriminator)\n    if intr is None:\n        return  # Introspection object does not exist, nothing to remove\n\n    # Remove all references to the introspection object\n    if intr in self._refs:\n        # Remove references from other introspection objects\n        for related_intr in self._refs[intr]:\n            if related_intr in self._refs:\n                self._refs[related_intr].remove(intr)\n        # Remove the introspection object from the references dictionary\n        del self._refs[intr]\n\n    # Remove the introspection object from the category dictionary\n    del category[discriminator]\n    if hasattr(intr, 'discriminator_hash'):\n        del category[intr.discriminator_hash]\n\n    # If the category is empty after removal, delete the category\n    if not category:\n        del self._categories[category_name]\n", "Result": "Error"}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "\ndef set_status(self, msg):\n    \"\"\"\n    Sets the job status in Hadoop streaming by printing a message to the standard error stream.\n    This function is also used as a keepalive mechanism to prevent the job from timing out.\n\n    :param self: MRJob. An instance of the MRJob class.\n    :param msg: String. The message to set as the job status.\n    :return: No return values.\n    \"\"\"\n    # Construct the status message in the required format\n    status_message = f\"reporter:status:{msg}\\n\"\n    \n    # Ensure the message is in bytes if necessary\n    if not isinstance(status_message, bytes):\n        status_message = status_message.encode('utf-8')\n    \n    # Write the status message to stderr\n    self.stderr.write(status_message)\n    self.stderr.flush()\n", "Result": "Error"}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "\ndef discriminator_hash(self):\n    \"\"\"\n    Calculate the hash of the discriminator of the Introspectable instance.\n\n    :param self: Introspectable. An instance of the Introspectable class.\n    :return: int. The hash value of the discriminator in the instance.\n    \"\"\"\n    self._assert_resolved()  # Ensure the discriminator is resolved\n    return hash(self.discriminator)\n", "Result": "Pass"}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "\ndef add_passthru_arg(self, *args, **kwargs):\n    \"\"\"\n    Add a command-line argument that both the job runner and the job itself will respect.\n    This creates options that can be used by the job to configure its behavior.\n    The options are added to the argument parser of the job.\n\n    :param self: MRJob. An instance of the MRJob class.\n    :param *args: Variable length argument list. The arguments to be passed to the argument parser.\n    :param **kwargs: Arbitrary keyword arguments. The keyword arguments to be passed to the argument parser.\n    :return: No return values.\n    \"\"\"\n    # Add the argument to the argument parser\n    pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n    \n    # Add the destination of the argument to the set of passthrough argument destinations\n    self._passthru_arg_dests.add(pass_opt.dest)\n", "Result": "Error"}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "\nfrom math import pow\n\ndef to_hertz(self, standard_pitch=440.0):\n    \"\"\"\n    Convert a given Note instance to Hertz (frequency in cycles per second).\n\n    :param self: Note. An instance of the Note class.\n    :param standard_pitch: float. The pitch of A-4, from which the rest of the notes are calculated. It defaults to 440 if not specified.\n    :return: float. The frequency of the Note in Hertz.\n    \"\"\"\n    # Calculate the number of semitones between A4 and the current note\n    # A4 is 69 semitones above C0 (since C0 is 0, C1 is 12, ..., A4 is 69)\n    semitones_from_A4 = int(self) - 69\n    \n    # Calculate the frequency using the formula f = f_ref * 2^(n/12)\n    frequency = standard_pitch * pow(2, semitones_from_A4 / 12.0)\n    \n    return frequency\n\n# Example usage:\n# Assuming the Note class is already defined as provided in the context\n# note = Note('C', 4)\n# print(to_hertz(note))  # Output: 261.6255653005986\n", "Result": "Error"}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "\ndef __repr__(self) -> str:\n    \"\"\"\n    Returns a string representation of the WikipediaPage object.\n\n    If any recorded methods have been called, the string includes the title, pageid, and ns.\n    Otherwise, it includes only the title and ns attributes.\n\n    :return: String representation of the WikipediaPage object.\n    \"\"\"\n    title = self._attributes.get(\"title\", \"Unknown Title\")\n    ns = self._attributes.get(\"ns\", \"??\")\n    \n    # Check if any recorded methods have been called\n    if any(self._called.values()):\n        pageid = self._attributes.get(\"pageid\", \"??\")\n        return f\"{title} (id: {pageid}, ns: {ns})\"\n    else:\n        return f\"{title} (id: ??, ns: {ns})\"\n", "Result": "Pass"}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "\ndef delete_parameter(self, button):\n    \"\"\"Delete the selected parameter from the parameters dictionary and clear the input widgets.\n\n    Parameters\n    ----------\n    button : ipywidgets.Button\n        The button that triggered the delete action. It is not used in the function.\n    \"\"\"\n    # Get the currently selected parameter name from the dropdown\n    selected_parameter = self.parameter_dropdown.value\n\n    # If no parameter is selected, do nothing\n    if not selected_parameter:\n        return\n\n    # Remove the parameter from the parameters dictionary\n    if selected_parameter in self.param_container.parameters:\n        del self.param_container.parameters[selected_parameter]\n\n    # Update the dropdown options to reflect the deletion\n    self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n\n    # Clear the input widgets\n    self._blank_parameter()\n\n    # Set the changed data flag to True\n    self._changed_data = True\n", "Result": "Error"}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "\ndef _traverse(self):\n    \"\"\"\n    Traverses the tree structure of a Component instance and yields the second value in each item in the tree.\n\n    :param self: Component. An instance of the Component class.\n    :return: Yields each item in the tree.\n    \"\"\"\n    children = getattr(self, \"children\", None)\n\n    # If children is a single Component, yield it and traverse its children\n    if isinstance(children, Component):\n        yield children\n        yield from children._traverse()\n\n    # If children is a list of Components, yield each and traverse their children\n    elif isinstance(children, (tuple, MutableSequence)):\n        for child in children:\n            if isinstance(child, Component):\n                yield child\n                yield from child._traverse()\n\n    # If children is a string, number, or None, yield it\n    else:\n        yield children\n", "Result": "Error"}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "\ndef tuple_for_index(self, index):\n    \"\"\"\n    Returns a tuple containing information about a block in the blockchain at the given index.\n\n    :param self: BlockChain. An instance of the BlockChain class.\n    :param index: Integer. The index of the block to retrieve.\n    :return: Tuple. A tuple containing the block's hash, parent hash, and weight.\n    \"\"\"\n    # Adjust negative index to be positive relative to the end of the blockchain\n    if index < 0:\n        index += self.length()\n\n    # Check if the index is within the range of the locked chain\n    if index < self.locked_length():\n        # Return the corresponding block from the locked chain\n        return self._locked_chain[index]\n\n    # Calculate the index relative to the unlocked chain\n    unlocked_index = index - self.locked_length()\n\n    # Retrieve the longest local block chain\n    longest_chain = self._longest_local_block_chain()\n\n    # Check if the index is within the range of the longest local block chain\n    if unlocked_index < len(longest_chain):\n        # Get the block hash from the longest local block chain\n        block_hash = longest_chain[unlocked_index]\n\n        # Get the parent hash and weight from the unlocked block storage and weight lookup\n        parent_hash = self.unlocked_block_storage[block_hash].previous_block_hash\n        weight = self.weight_lookup.get(block_hash, 0)\n\n        # Return the tuple containing the block's hash, parent hash, and weight\n        return (block_hash, parent_hash, weight)\n\n    # If the index is outside the range of both the locked chain and the longest local block chain,\n    # return None or raise an exception, depending on the desired behavior\n    raise IndexError(\"Index out of range\")\n", "Result": "Error"}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "\ndef redirect(cls, path, status=302, headers=None):\n    \"\"\"\n    Creates a redirect response.\n\n    :param cls: Class. The class of the Response instance.\n    :param path: String. The path to redirect to.\n    :param status: Integer. The status code for the response. Defaults to 302.\n    :param headers: Dictionary. Additional headers to include in the response. Defaults to an empty dictionary.\n    :return: Response. The created redirect response instance.\n    \"\"\"\n    if headers is None:\n        headers = {}\n    headers[\"Location\"] = path\n    return cls(\"\", status=status, headers=headers)\n", "Result": "Error"}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "\nimport xml.dom.minidom\n\ndef as_xml(self, filename=None, pretty=False):\n    \"\"\"\n    Returns the XML representation of the document.\n\n    :param filename: String [optional]. The name of the file to save the XML representation. Defaults to None.\n    :param pretty: Bool. Whether to format the XML representation with indentation and line breaks. Defaults to False.\n    :return: String. The XML representation of the document.\n    \"\"\"\n    if pretty:\n        # Format the XML with indentation and line breaks\n        xml_str = self._element.toprettyxml(indent=\"  \", encoding=\"utf-8\").decode(\"utf-8\")\n    else:\n        # Return the XML without any formatting\n        xml_str = self._element.toxml(encoding=\"utf-8\").decode(\"utf-8\")\n\n    # Save to file if filename is provided\n    if filename:\n        with open(filename, \"w\", encoding=\"utf-8\") as file:\n            file.write(xml_str)\n\n    return xml_str\n", "Result": "Error"}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "\ndef get_note_names(self):\n    \"\"\"Return a list of unique note names from the current note container.\n\n    :return: List. A list containing the unique note names from the current note container.\n    \"\"\"\n    # Extract the note names from the Note objects in the container\n    note_names = [note.name for note in self.notes]\n    \n    # Use a set to ensure uniqueness, then convert back to a list\n    unique_note_names = list(set(note_names))\n    \n    return unique_note_names\n", "Result": "Error"}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "\nclass EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, word_freq_dict):\n        \"\"\"\n        the subset of 'word_freq_dict' that appear in the dictionary of word_freq_dict\n        :param word_freq_dict:\n        :param limit_count:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in word_freq_dict if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        generate possible spelling corrections for word.\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n\n    def correct_word(self, word):\n        \"\"\"\n        Corrects the spelling of a given word by finding the most probable spelling correction.\n        :param word: String. The word to be corrected.\n        :return: String. The most probable spelling correction for the word.\n        \"\"\"\n        self.check_init()\n        \n        # Get all possible candidate corrections for the word\n        candidate_corrections = self.candidates(word)\n        \n        # Calculate the probability of each candidate\n        candidate_probabilities = {candidate: self.probability(candidate) for candidate in candidate_corrections}\n        \n        # Sort candidates by probability in ascending order\n        sorted_candidates = sorted(candidate_probabilities.items(), key=lambda x: x[1], reverse=True)\n        \n        # Return the candidate with the highest probability\n        if sorted_candidates:\n            return sorted_candidates[0][0]\n        else:\n            return word\n\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "Result": "Error"}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "\nimport json\n\ndef to_plotly_json(self):\n    \"\"\"\n    Convert a Component instance into a JSON object that can be used by Plotly.\n\n    :param self: Component. An instance of the Component class.\n    :return: JSON. The JSON representation of the Component instance.\n    \"\"\"\n    # Initialize the JSON object with the component's type and namespace\n    json_obj = {\n        \"type\": self._type,\n        \"namespace\": self._namespace,\n        \"props\": {}\n    }\n\n    # Extract normal properties\n    for prop in self._prop_names:\n        if hasattr(self, prop):\n            json_obj[\"props\"][prop] = getattr(self, prop)\n\n    # Extract wildcard properties (properties starting with \"data-\" or \"aria-\")\n    for attr in self.__dict__:\n        if any(attr.startswith(wc_attr) for wc_attr in self._valid_wildcard_attributes):\n            json_obj[\"props\"][attr] = getattr(self, attr)\n\n    # Convert the JSON object to a JSON string and then back to a dictionary\n    # to ensure it's serializable\n    return json.loads(json.dumps(json_obj))\n", "Result": "Error"}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "\ndef get_attr_by_channel(self, channel):\n    \"\"\"\n    Retrieves the attribute based on the given channel from the inferred intent list.\n    It filters the list based on the channel and value attributes of each object in the list and returns the filtered list.\n\n    Parameters\n    ----------\n    channel : str\n        The channel to filter the inferred intent list.\n\n    Returns\n    -------\n    List\n        The filtered list of objects from the inferred intent list.\n    \"\"\"\n    return list(\n        filter(\n            lambda x: hasattr(x, \"channel\") and x.channel == channel and x.value == \"\",\n            self._inferred_intent,\n        )\n    )\n", "Result": "Pass"}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "\nimport re\n\nclass VegaLite(Element):\n    # ... (existing code)\n\n    @property\n    def vegalite_major_version(self):\n        \"\"\"\n        Extracts the major version number of the Vega-Lite schema used in the instance.\n\n        Returns\n        -------\n        int\n            The major version number of the Vega-Lite schema.\n        \"\"\"\n        # Get the $schema attribute from the data\n        schema_url = self.data.get(\"$schema\", \"\")\n        \n        # Use a regular expression to extract the major version number\n        match = re.search(r\"vega-lite/v(\\d+)\", schema_url)\n        if match:\n            return int(match.group(1))\n        else:\n            # Default to version 2 if no schema is specified\n            return 2\n\n    # ... (existing code)\n", "Result": "Error"}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "\nclass EnSpell(object):\n    def __init__(self, word_freq_dict={}):\n        # Word freq dict, k=word, v=int(freq)\n        self.word_freq_dict = word_freq_dict\n        self.custom_confusion = {}\n\n    def _init(self):\n        with gzip.open(config.en_dict_path, \"rb\") as f:\n            all_word_freq_dict = json.loads(f.read())\n            word_freq = {}\n            for k, v in all_word_freq_dict.items():\n                # \u82f1\u8bed\u5e38\u7528\u5355\u8bcd3\u4e07\u4e2a\uff0c\u53d6\u8bcd\u9891\u9ad8\u4e8e400\n                if v > 400:\n                    word_freq[k] = v\n            self.word_freq_dict = word_freq\n            logger.debug(\"load en spell data: %s, size: %d\" % (config.en_dict_path,\n                                                               len(self.word_freq_dict)))\n\n    def check_init(self):\n        if not self.word_freq_dict:\n            self._init()\n\n    @staticmethod\n    def edits1(word):\n        \"\"\"\n        all edits that are one edit away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(self, word):\n        \"\"\"\n        all edit that are two edits away from 'word'\n        :param word:\n        :return:\n        \"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def known(self, words):\n        \"\"\"\n        the subset of 'words' that appear in the dictionary of word_freq_dict\n        :param words:\n        :return:\n        \"\"\"\n        self.check_init()\n        return set(w for w in words if w in self.word_freq_dict)\n\n    def probability(self, word):\n        \"\"\"\n        probability of word\n        :param word:\n        :return:float\n        \"\"\"\n        self.check_init()\n        N = sum(self.word_freq_dict.values())\n        return self.word_freq_dict.get(word, 0) / N\n\n    def candidates(self, word):\n        \"\"\"\n        Generate possible spelling corrections for a given word.\n        :param word: String. The word for which spelling corrections need to be generated.\n        :return: Set of strings. The set of possible spelling corrections for the word.\n        \"\"\"\n        self.check_init()\n        \n        # If the word is already correct, return it\n        if word in self.word_freq_dict:\n            return {word}\n        \n        # Generate candidates with one edit\n        candidates_1edit = self.known(self.edits1(word))\n        if candidates_1edit:\n            return candidates_1edit\n        \n        # Generate candidates with two edits\n        candidates_2edits = self.known(self.edits2(word))\n        if candidates_2edits:\n            return candidates_2edits\n        \n        # If no corrections are found, return the original word\n        return {word}\n\n    def correct_word(self, word):\n        \"\"\"\n        most probable spelling correction for word\n        :param word:\n        :return:\n        \"\"\"\n        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]\n\n    @staticmethod\n    def _get_custom_confusion_dict(path):\n        \"\"\"\n        \u53d6\u81ea\u5b9a\u4e49\u56f0\u60d1\u96c6\n        :param path:\n        :return: dict, {variant: origin}, eg: {\"\u4ea4\u901a\u5148\u884c\": \"\u4ea4\u901a\u9650\u884c\"}\n        \"\"\"\n        confusion = {}\n        if path and os.path.exists(path):\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    wrong = terms[0]\n                    right = terms[1]\n                    confusion[wrong] = right\n        return confusion\n\n    def set_en_custom_confusion_dict(self, path):\n        \"\"\"\n        \u8bbe\u7f6e\u6df7\u6dc6\u7ea0\u9519\u8bcd\u5178\n        :param path:\n        :return:\n        \"\"\"\n        self.check_init()\n        self.custom_confusion = self._get_custom_confusion_dict(path)\n        logger.debug('Loaded en spell confusion path: %s, size: %d' % (path, len(self.custom_confusion)))\n\n    def correct(self, text, include_symbol=True):\n        \"\"\"\n        most probable spelling correction for text\n        :param text: input query\n        :param include_symbol: True, default\n        :return: corrected_text, details [(wrong_word, right_word, begin_idx, end_idx), ...]\n        example:\n        cann you speling it? [['cann', 'can'], ['speling', 'spelling']]\n        \"\"\"\n        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details\n", "Result": "Error"}
