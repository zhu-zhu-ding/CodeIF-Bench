{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "type": "method", "project_path": "System/mrjob", "completion_path": "System/mrjob/mrjob/hadoop.py", "signature_position": [546, 546], "body_position": [548, 555], "dependency": {"intra_class": ["mrjob.hadoop.HadoopJobRunner._hadoop_log_dirs", "mrjob.hadoop.HadoopJobRunner.fs"], "intra_file": ["mrjob.hadoop.log"], "cross_file": ["mrjob.logs.mixin.LogInterpretationMixin._read_logs", "mrjob.logs.wrap._logs_exist", "mrjob.util.unique"]}, "requirement": {"Functionality": "This function yields lists of directories to search for the history log in. It first checks if logs should be read, and then iterates over unique log directories obtained from the hadoop log directories. If the directory exists, it logs an info message: 'Looking for history log in {directory}...'. It then yields a list containing the directory.", "Arguments": ":param self: HadoopJobRunner. An instance of the HadoopJobRunner class.\n:param output_dir: str. The output directory to search for the history log. Defaults to None.\n:return: Generator. Yields lists of directories to search for the history log in."}, "tests": ["tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_output_dir", "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_basic", "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_io_error_from_fs_exists", "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_empty", "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_no_read_logs"], "indent": 8, "domain": "System", "code": "    def _stream_history_log_dirs(self, output_dir=None):\n        \"\"\"Yield lists of directories to look for the history log in.\"\"\"\n        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for history log in %s...' % log_dir)\n                # logs aren't always in a subdir named history/\n                yield [log_dir]\n", "intra_context": "# Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017 Yelp\n# Copyright 2018 Yelp and Google, Inc.\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport getpass\nimport logging\nimport os\nimport posixpath\nimport re\nfrom subprocess import CalledProcessError\nfrom subprocess import Popen\nfrom subprocess import PIPE\n\ntry:\n    import pty\n    pty  # quiet \"redefinition of unused ...\" warning from pyflakes\nexcept ImportError:\n    pty = None\n\nfrom mrjob.bin import MRJobBinRunner\nfrom mrjob.compat import uses_yarn\nfrom mrjob.conf import combine_dicts\nfrom mrjob.fs.composite import CompositeFilesystem\n\n\nfrom mrjob.logs.counters import _pick_counters\nfrom mrjob.logs.errors import _log_probable_cause_of_failure\nfrom mrjob.logs.mixin import LogInterpretationMixin\nfrom mrjob.logs.step import _eio_to_eof\nfrom mrjob.logs.step import _interpret_hadoop_jar_command_stderr\nfrom mrjob.logs.step import _is_counter_log4j_record\nfrom mrjob.logs.step import _log_line_from_driver\nfrom mrjob.logs.step import _log_log4j_record\nfrom mrjob.logs.wrap import _logs_exist\n\nfrom mrjob.py2 import to_unicode\nfrom mrjob.runner import _fix_env\nfrom mrjob.setup import UploadDirManager\nfrom mrjob.step import StepFailedException\nfrom mrjob.step import _is_spark_step_type\nfrom mrjob.util import cmd_line\nfrom mrjob.util import unique\nfrom mrjob.util import which\n\n\nlog = logging.getLogger(__name__)\n\n# don't look for the hadoop streaming jar here!\n_BAD_HADOOP_HOMES = ['/', '/usr', '/usr/local']\n\n# where YARN stores history logs, etc. on HDFS by default\n_DEFAULT_YARN_HDFS_LOG_DIR = 'hdfs:///tmp/hadoop-yarn/staging'\n\n# places to look for the Hadoop streaming jar if we're inside EMR\n_EMR_HADOOP_STREAMING_JAR_DIRS = [\n    # for the 2.x and 3.x AMIs (the 2.x AMIs also set $HADOOP_HOME properly)\n    '/home/hadoop/contrib',\n    # for the 4.x AMIs\n    '/usr/lib/hadoop-mapreduce',\n]\n\n# these are fairly standard places to keep Hadoop logs\n_FALLBACK_HADOOP_LOG_DIRS = [\n    '/var/log/hadoop',\n    '/mnt/var/log/hadoop',  # EMR's 2.x and 3.x AMIs use this\n]\n\n# fairly standard places to keep YARN logs (see #1339)\n_FALLBACK_HADOOP_YARN_LOG_DIRS = [\n    '/var/log/hadoop-yarn',\n    '/mnt/var/log/hadoop-yarn',\n]\n\n# start of Counters printed by Hadoop\n_HADOOP_COUNTERS_START_RE = re.compile(br'^Counters: (?P<amount>\\d+)\\s*$')\n\n# header for a group of counters\n_HADOOP_COUNTER_GROUP_RE = re.compile(br'^(?P<indent>\\s+)(?P<group>.*)$')\n\n# line for a counter\n_HADOOP_COUNTER_RE = re.compile(\n    br'^(?P<indent>\\s+)(?P<counter>.*)=(?P<amount>\\d+)\\s*$')\n\n# the one thing Hadoop streaming prints to stderr not in log format\n_HADOOP_NON_LOG_LINE_RE = re.compile(r'^Streaming Command Failed!')\n\n# if we see this from Hadoop, it actually came from stdout and shouldn't\n# be logged\n_HADOOP_STDOUT_RE = re.compile(br'^packageJobJar: ')\n\n# match the filename of a hadoop streaming jar\n_HADOOP_STREAMING_JAR_RE = re.compile(\n    r'^hadoop.*streaming.*(?<!-sources)\\.jar$')\n\n\ndef fully_qualify_hdfs_path(path):\n    \"\"\"If path isn't an ``hdfs://`` URL, turn it into one.\"\"\"\n    from mrjob.parse import is_uri\n    if is_uri(path):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)\n\n\nclass HadoopJobRunner(MRJobBinRunner, LogInterpretationMixin):\n    \"\"\"Runs an :py:class:`~mrjob.job.MRJob` on your Hadoop cluster.\n    Invoked when you run your job with ``-r hadoop``.\n\n    Input and support files can be either local or on HDFS; use ``hdfs://...``\n    URLs to refer to files on HDFS.\n    \"\"\"\n    alias = 'hadoop'\n\n    OPT_NAMES = MRJobBinRunner.OPT_NAMES | {\n        'hadoop_bin',\n        'hadoop_extra_args',\n        'hadoop_log_dirs',\n        'hadoop_streaming_jar',\n        'hadoop_tmp_dir',\n        'spark_deploy_mode',\n        'spark_master',\n    }\n\n    # supports everything (so far)\n    _STEP_TYPES = {\n        'jar', 'spark', 'spark_jar', 'spark_script', 'streaming'}\n\n    def __init__(self, **kwargs):\n        \"\"\":py:class:`~mrjob.hadoop.HadoopJobRunner` takes the same arguments\n        as :py:class:`~mrjob.runner.MRJobRunner`, plus some additional options\n        which can be defaulted in :ref:`mrjob.conf <mrjob.conf>`.\n        \"\"\"\n        super(HadoopJobRunner, self).__init__(**kwargs)\n\n        self._hadoop_tmp_dir = fully_qualify_hdfs_path(\n            posixpath.join(\n                self._opts['hadoop_tmp_dir'], self._job_key))\n\n        # Keep track of local files to upload to HDFS. We'll add them\n        # to this manager just before we need them.\n        hdfs_files_dir = posixpath.join(self._hadoop_tmp_dir, 'files', '')\n        self._upload_mgr = UploadDirManager(hdfs_files_dir)\n\n        # Set output dir if it wasn't set explicitly\n        self._output_dir = fully_qualify_hdfs_path(\n            self._output_dir or\n            posixpath.join(self._hadoop_tmp_dir, 'output'))\n\n        # Fully qualify step_output_dir, if set\n        if self._step_output_dir:\n            self._step_output_dir = fully_qualify_hdfs_path(\n                self._step_output_dir)\n\n        # Track job and (YARN) application ID to enable log parsing\n        self._application_id = None\n        self._job_id = None\n\n        # Keep track of where the hadoop streaming jar is\n        self._hadoop_streaming_jar = self._opts['hadoop_streaming_jar']\n        self._searched_for_hadoop_streaming_jar = False\n\n        # List of dicts (one for each step) potentially containing\n        # the keys 'history', 'step', and 'task' ('step' will always\n        # be filled because it comes from the hadoop jar command output,\n        # others will be filled as needed)\n        self._log_interpretations = []\n\n    @classmethod\n    def _default_opts(cls):\n        return combine_dicts(\n            super(HadoopJobRunner, cls)._default_opts(),\n            dict(\n                hadoop_tmp_dir='tmp/mrjob',\n            )\n        )\n\n    @property\n    def fs(self):\n        \"\"\":py:class:`mrjob.fs.base.Filesystem` object for HDFS and the local\n        filesystem.\n        \"\"\"\n        from mrjob.fs.local import LocalFilesystem\n        from mrjob.fs.hadoop import HadoopFilesystem\n        if self._fs is None:\n            self._fs = CompositeFilesystem()\n\n            # don't pass [] to fs; this means not to use hadoop until\n            # fs.set_hadoop_bin() is called (used for running hadoop over SSH).\n            hadoop_bin = self._opts['hadoop_bin'] or None\n\n            self._fs.add_fs('hadoop',\n                            HadoopFilesystem(hadoop_bin))\n            self._fs.add_fs('local', LocalFilesystem())\n\n        return self._fs\n\n    def get_hadoop_version(self):\n        \"\"\"Invoke the hadoop executable to determine its version\"\"\"\n        return self.fs.hadoop.get_hadoop_version()\n\n    def get_hadoop_bin(self):\n        \"\"\"Find the hadoop binary. A list: binary followed by arguments.\"\"\"\n        return self.fs.hadoop.get_hadoop_bin()\n\n    def get_hadoop_streaming_jar(self):\n        \"\"\"Find the path of the hadoop streaming jar, or None if not found.\"\"\"\n        if not (self._hadoop_streaming_jar or\n                self._searched_for_hadoop_streaming_jar):\n\n            self._hadoop_streaming_jar = self._find_hadoop_streaming_jar()\n\n            if self._hadoop_streaming_jar:\n                log.info('Found Hadoop streaming jar: %s' %\n                         self._hadoop_streaming_jar)\n            else:\n                log.warning('Hadoop streaming jar not found. Use'\n                            ' --hadoop-streaming-jar')\n\n            self._searched_for_hadoop_streaming_jar = True\n\n        return self._hadoop_streaming_jar\n\n    def _find_hadoop_streaming_jar(self):\n        \"\"\"Search for the hadoop streaming jar. See\n        :py:meth:`_hadoop_streaming_jar_dirs` for where we search.\"\"\"\n        for path in unique(self._hadoop_streaming_jar_dirs()):\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n\n            streaming_jars = []\n            for path in self.fs.ls(path):\n                if _HADOOP_STREAMING_JAR_RE.match(posixpath.basename(path)):\n                    streaming_jars.append(path)\n\n            if streaming_jars:\n                # prefer shorter names and shallower paths\n                def sort_key(p):\n                    return (len(p.split('/')),\n                            len(posixpath.basename(p)),\n                            p)\n\n                streaming_jars.sort(key=sort_key)\n\n                return streaming_jars[0]\n\n        return None\n\n    def _hadoop_dirs(self):\n        \"\"\"Yield all possible hadoop directories (used for streaming jar\n        and logs). May yield duplicates\"\"\"\n        for name in ('HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL',\n                     'HADOOP_MAPRED_HOME'):\n            path = os.environ.get(name)\n            if path:\n                yield path\n\n        # guess it from the path of the Hadoop binary\n        hadoop_home = _hadoop_prefix_from_bin(self.get_hadoop_bin()[0])\n        if hadoop_home:\n            yield hadoop_home\n\n        # try HADOOP_*_HOME\n        for name, path in sorted(os.environ.items()):\n            if name.startswith('HADOOP_') and name.endswith('_HOME'):\n                yield path\n\n    def _hadoop_streaming_jar_dirs(self):\n        \"\"\"Yield all possible places to look for the Hadoop streaming jar.\n        May yield duplicates.\n        \"\"\"\n        for hadoop_dir in self._hadoop_dirs():\n            yield hadoop_dir\n\n        # use hard-coded paths to work out-of-the-box on EMR\n        for path in _EMR_HADOOP_STREAMING_JAR_DIRS:\n            yield path\n\n    def _hadoop_log_dirs(self, output_dir=None):\n        \"\"\"Yield all possible places to look for hadoop logs.\"\"\"\n        # hadoop_log_dirs opt overrides all this\n        if self._opts['hadoop_log_dirs']:\n            for path in self._opts['hadoop_log_dirs']:\n                yield path\n            return\n\n        hadoop_log_dir = os.environ.get('HADOOP_LOG_DIR')\n        if hadoop_log_dir:\n            yield hadoop_log_dir\n\n        yarn = uses_yarn(self.get_hadoop_version())\n\n        if yarn:\n            yarn_log_dir = os.environ.get('YARN_LOG_DIR')\n            if yarn_log_dir:\n                yield yarn_log_dir\n\n            yield _DEFAULT_YARN_HDFS_LOG_DIR\n\n        if output_dir:\n            # Cloudera style of logging\n            yield posixpath.join(output_dir, '_logs')\n\n        for hadoop_dir in self._hadoop_dirs():\n            yield posixpath.join(hadoop_dir, 'logs')\n\n        # hard-coded fallback paths\n        if yarn:\n            for path in _FALLBACK_HADOOP_YARN_LOG_DIRS:\n                yield path\n\n        for path in _FALLBACK_HADOOP_LOG_DIRS:\n            yield path\n\n    def _run(self):\n        self._find_binaries_and_jars()\n        self._create_setup_wrapper_scripts()\n        self._add_job_files_for_upload()\n        self._upload_local_files()\n        self._run_job_in_hadoop()\n\n    def _find_binaries_and_jars(self):\n        \"\"\"Find hadoop and (if needed) spark-submit bin up-front, before\n        continuing with the job.\n\n        (This is just for user-interaction purposes; these would otherwise\n        lazy-load as needed.)\n        \"\"\"\n        # this triggers looking for Hadoop binary\n        self.get_hadoop_version()\n\n        if self._has_hadoop_streaming_steps():\n            self.get_hadoop_streaming_jar()\n\n        if self._has_spark_steps():\n            self.get_spark_submit_bin()\n\n    def _add_job_files_for_upload(self):\n        \"\"\"Add files needed for running the job (setup and input)\n        to self._upload_mgr.\"\"\"\n        for path in self._py_files():\n            self._upload_mgr.add(path)\n\n    def _dump_stdin_to_local_file(self):\n        \"\"\"Dump sys.stdin to a local file, and return the path to it.\"\"\"\n        stdin_path = posixpath.join(self._get_local_tmp_dir(), 'STDIN')\n         # prompt user, so they don't think the process has stalled\n        log.info('reading from STDIN')\n\n        log.debug('dumping stdin to local file %s...' % stdin_path)\n        stdin_file = open(stdin_path, 'wb')\n        for line in self._stdin:\n            stdin_file.write(line)\n\n        return stdin_path\n\n    def _run_job_in_hadoop(self):\n        for step_num, step in enumerate(self._get_steps()):\n            step_type = step['type']\n            step_args = self._args_for_step(step_num)\n            env = _fix_env(self._env_for_step(step_num))\n\n            # log this *after* _args_for_step(), which can start a search\n            # for the Hadoop streaming jar\n            log.info('Running step %d of %d...' %\n                     (step_num + 1, self._num_steps()))\n            log.debug('> %s' % cmd_line(step_args))\n            log.debug('  with environment: %r' % sorted(env.items()))\n\n            log_interpretation = {}\n            self._log_interpretations.append(log_interpretation)\n\n            if self._step_type_uses_spark(step_type):\n                returncode, step_interpretation = self._run_spark_submit(\n                    step_args, env, record_callback=_log_log4j_record)\n            else:\n                returncode, step_interpretation = self._run_hadoop(\n                    step_args, env, record_callback=_log_record_from_hadoop)\n\n            # make sure output_dir is filled (used for history log)\n            if 'output_dir' not in step_interpretation:\n                step_interpretation['output_dir'] = (\n                    self._step_output_uri(step_num))\n\n            log_interpretation['step'] = step_interpretation\n\n            self._log_counters(log_interpretation, step_num)\n\n            if returncode:\n                error = self._pick_error(log_interpretation, step_type)\n                if error:\n                    _log_probable_cause_of_failure(log, error)\n\n                # use CalledProcessError's well-known message format\n                reason = str(CalledProcessError(returncode, step_args))\n                raise StepFailedException(\n                    reason=reason, step_num=step_num,\n                    num_steps=self._num_steps())\n\n    def _run_hadoop(self, hadoop_args, env, record_callback):\n        # try to use a PTY if it's available\n        try:\n            pid, master_fd = pty.fork()\n        except (AttributeError, OSError):\n            # no PTYs, just use Popen\n\n            # user won't get much feedback for a while, so tell them\n            # Hadoop is running\n            log.debug('No PTY available, using Popen() to invoke Hadoop')\n\n            step_proc = Popen(hadoop_args, stdout=PIPE, stderr=PIPE, env=env)\n\n            step_interpretation = _interpret_hadoop_jar_command_stderr(\n                step_proc.stderr,\n                record_callback=_log_record_from_hadoop)\n\n            # there shouldn't be much output to STDOUT\n            for line in step_proc.stdout:\n                _log_line_from_driver(to_unicode(line).strip('\\r\\n'))\n\n            step_proc.stdout.close()\n            step_proc.stderr.close()\n\n            returncode = step_proc.wait()\n        else:\n            # we have PTYs\n            if pid == 0:  # we are the child process\n                try:\n                    os.execvpe(hadoop_args[0], hadoop_args, env)\n                    # now we are no longer Python\n                except OSError as ex:\n                    # use _exit() so we don't do cleanup, etc. that's\n                    # the parent process's job\n                    os._exit(ex.errno)\n                finally:\n                    # if we got some other exception, still exit hard\n                    os._exit(-1)\n            else:\n                log.debug('Invoking Hadoop via PTY')\n\n                with os.fdopen(master_fd, 'rb') as master:\n                    # reading from master gives us the subprocess's\n                    # stderr and stdout (it's a fake terminal)\n                    step_interpretation = (\n                        _interpret_hadoop_jar_command_stderr(\n                            _eio_to_eof(master),\n                            record_callback=_log_record_from_hadoop))\n                    _, returncode = os.waitpid(pid, 0)\n\n        return returncode, step_interpretation\n\n    def _spark_master(self):\n        return self._opts['spark_master'] or 'yarn'\n\n    def _args_for_step(self, step_num):\n        step = self._get_step(step_num)\n\n        if step['type'] == 'streaming':\n            return self._args_for_streaming_step(step_num)\n        elif step['type'] == 'jar':\n            return self._args_for_jar_step(step_num)\n        elif _is_spark_step_type(step['type']):\n            return self._args_for_spark_step(step_num)\n        else:\n            raise ValueError('Bad step type: %r' % (step['type'],))\n\n    def _args_for_streaming_step(self, step_num):\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        return (self.get_hadoop_bin() + ['jar', hadoop_streaming_jar] +\n                self._hadoop_streaming_jar_args(step_num))\n\n    def _args_for_jar_step(self, step_num):\n        step = self._get_step(step_num)\n\n        args = []\n\n        args.extend(self.get_hadoop_bin())\n\n        # special case for consistency with EMR runner.\n        #\n        # This might look less like duplicated code if we ever\n        # implement #780 (fetching jars from URIs)\n        if step['jar'].startswith('file:///'):\n            jar = step['jar'][7:]  # keep leading slash\n        else:\n            jar = step['jar']\n\n        args.extend(['jar', jar])\n\n        if step.get('main_class'):\n            args.append(step['main_class'])\n\n        if step.get('args'):\n            args.extend(\n                self._interpolate_jar_step_args(step['args'], step_num))\n\n        return args\n\n    def _env_for_step(self, step_num):\n        step = self._get_step(step_num)\n\n        env = dict(os.environ)\n\n        # when running spark-submit, set its environment directly. See #1464\n        if _is_spark_step_type(step['type']):\n            env.update(self._spark_cmdenv(step_num))\n\n        return env\n\n    def _default_step_output_dir(self):\n        return posixpath.join(self._hadoop_tmp_dir, 'step-output')\n\n    def _cleanup_hadoop_tmp(self):\n        if self._hadoop_tmp_dir:\n            log.info('Removing HDFS temp directory %s...' %\n                     self._hadoop_tmp_dir)\n            try:\n                self.fs.rm(self._hadoop_tmp_dir)\n            except Exception as e:\n                log.exception(e)\n\n    def _manifest_download_commands(self):\n        cp_to_local = self.get_hadoop_bin() + ['fs', '-copyToLocal']\n\n        return [\n            ('*://*', cmd_line(cp_to_local)),\n        ]\n\n    ### LOG (implementation of LogInterpretationMixin) ###\n\n###The function: _stream_history_log_dirs###\n    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\n        \"\"\"Yield lists of directories to look for the task logs in.\"\"\"\n        # Note: this is unlikely to be super-helpful on \"real\" (multi-node)\n        # pre-YARN Hadoop because task logs aren't generally shipped to a\n        # local directory. It's a start, anyways. See #1201.\n        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                path = self.fs.join(log_dir, 'userlogs', application_id)\n            else:\n                path = self.fs.join(log_dir, 'userlogs')\n\n            if _logs_exist(self.fs, path):\n                log.info('Looking for task syslogs in %s...' % path)\n                yield [path]\n\n    def counters(self):\n        return [_pick_counters(log_interpretation)\n                for log_interpretation in self._log_interpretations]\n\n\n# These don't require state from HadoopJobRunner, so making them functions.\n# Feel free to convert them back into methods as need be\n\n\ndef _hadoop_prefix_from_bin(hadoop_bin):\n    \"\"\"Given a path to the hadoop binary, return the path of the implied\n    hadoop home, or None if we don't know.\n\n    Don't return the parent directory of directories in the default\n    path (not ``/``, ``/usr``, or ``/usr/local``).\n    \"\"\"\n    # resolve unqualified binary name (relative paths are okay)\n    if '/' not in hadoop_bin:\n        hadoop_bin = which(hadoop_bin)\n        if not hadoop_bin:\n            return None\n\n    # use parent of hadoop_bin's directory\n    hadoop_home = posixpath.abspath(\n        posixpath.join(posixpath.realpath(posixpath.dirname(hadoop_bin)), '..')\n    )\n\n    if hadoop_home in _BAD_HADOOP_HOMES:\n        return None\n\n    return hadoop_home\n\n\ndef _log_record_from_hadoop(record):\n    \"\"\"Log log4j record parsed from hadoop stderr.\"\"\"\n    if not _is_counter_log4j_record(record):  # counters are printed separately\n        _log_log4j_record(record)\n", "cross_context": [{"mrjob.logs.mixin.LogInterpretationMixin._read_logs": "# -*- coding: utf-8 -*-\n# Copyright 2016 Yelp and Contributors\n# Copyright 2017-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Runner mixin for counters and probable cause of failure.\n\nThis relies on passing around a *log_interpretation* dictionary, which\nis described in detail in :py:mod:`mrjob.logs`.\n\nThis mixin doesn't yet handle step logs because the EMR and Hadoop runners\nhandle them so differently. It's up to you to fill in the 'step' field\nof your log interpretation; the mixin can't do much without it because\nit needs it for the job/application ID.\n\nYour runner should generally have one log interpretation per step,\nthough the mixin doesn't care how or where you store them.\n\"\"\"\nfrom logging import getLogger\n\nfrom mrjob.compat import uses_yarn\nfrom mrjob.logs.counters import _format_counters\nfrom mrjob.logs.counters import _pick_counters\n\n\nfrom mrjob.logs.history import _interpret_history_log\nfrom mrjob.logs.history import _ls_history_logs\nfrom mrjob.logs.spark import _interpret_spark_logs\nfrom mrjob.logs.task import _interpret_task_logs\nfrom mrjob.logs.task import _ls_task_logs\nfrom mrjob.logs.task import _ls_spark_task_logs\n\nlog = getLogger(__name__)\n\n\n# a callback for _interpret_task_logs(). Breaking it out to make\n# testing easier\ndef _log_parsing_task_log(log_path):\n    log.info('  Parsing task log: %s' % log_path)\n\n\nclass LogInterpretationMixin(object):\n    \"\"\"Mix this in to your runner class to simplify log interpretation.\"\"\"\n    # this mixin is meant to be tightly bound to MRJobRunner, but\n    # currently it only relies on self.fs and self.get_hadoop_version()\n\n    ### stuff to redefine ###\n\n    def _stream_history_log_dirs(self, output_dir=None):\n        \"\"\"Yield lists of directories (usually, URIs) to search for history\n        logs in.\n\n        Usually, you'll want to add logging messages (e.g.\n        'Searching for history logs in ...'\n\n        :param output_dir: Output directory for step (optional), to look\n            for logs (e.g. on Cloudera).\n        \"\"\"\n        return ()\n\n    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\n        \"\"\"Yield lists of directories (usually, URIs) to search for task\n        logs in.\n\n        Usually, you'll want to add logging messages (e.g.\n        'Searching for task syslogs in...')\n\n        :param application_id: YARN application ID (optional), so we can ls\n            the relevant subdirectory of `userlogs/` rather than the whole\n            thing\n        :param output_dir: Output directory for step (optional), to look\n            for logs (e.g. on Cloudera).\n        \"\"\"\n        # sometimes pre-YARN logs are organized by job ID, but not always,\n        # so we don't bother with job_id; just ls() the entire userlogs\n        # dir and depend on regexes to find the right subdir.\n        return ()\n\n    def _get_step_log_interpretation(self, log_interpretation, step_type):\n        \"\"\"Return interpretation of the step log. Either implement\n        this, or fill ``'step'`` yourself (e.g. from Hadoop binary's\n        output.\"\"\"\n        return None\n\n    ### stuff to call ###\n\n    def _pick_counters(self, log_interpretation, step_type):\n        \"\"\"Pick counters from our log interpretation, interpreting\n        history logs if need be.\"\"\"\n        if self._step_type_uses_spark(step_type):\n            return {}\n\n        counters = _pick_counters(log_interpretation)\n\n        if self._read_logs():\n            if not counters:\n                log.info('Attempting to fetch counters from logs...')\n                self._interpret_step_logs(log_interpretation, step_type)\n                counters = _pick_counters(log_interpretation)\n\n            if not counters:\n                self._interpret_history_log(log_interpretation)\n                counters = _pick_counters(log_interpretation)\n\n        return counters\n\n    def _pick_error(self, log_interpretation, step_type):\n        \"\"\"Pick probable cause of failure (only call this if job fails).\"\"\"\n        from mrjob.logs.errors import _pick_error_attempt_ids\n        from mrjob.logs.errors import _pick_error\n        logs_needed = self._logs_needed_to_pick_error(step_type)\n\n        if self._read_logs() and not all(\n                log_type in log_interpretation for log_type in logs_needed):\n            log.info('Scanning logs for probable cause of failure...')\n\n            if 'step' in logs_needed:\n                self._interpret_step_logs(log_interpretation, step_type)\n\n            if 'history' in logs_needed:\n                self._interpret_history_log(log_interpretation)\n\n            if 'task' in logs_needed:\n                error_attempt_ids = _pick_error_attempt_ids(log_interpretation)\n\n                self._interpret_task_logs(\n                    log_interpretation, step_type, error_attempt_ids)\n\n        return _pick_error(log_interpretation)\n\n    def _logs_needed_to_pick_error(self, step_type):\n        \"\"\"We don't need all the logs when interpreting Spark steps\"\"\"\n        if self._step_type_uses_spark(step_type):\n            if self._spark_deploy_mode() == 'cluster':\n                return ('step', 'task')\n            else:\n                return ('step',)\n        else:\n            return ('step', 'history', 'task')\n\n    ### stuff that should just work ###\n\n    def _interpret_history_log(self, log_interpretation):\n        \"\"\"Fetch history log and add 'history' to log_interpretation.\"\"\"\n        if 'history' in log_interpretation:\n            return   # already interpreted\n\n        if not self._read_logs():\n            return  # nothing to do\n\n        step_interpretation = log_interpretation.get('step') or {}\n\n        job_id = step_interpretation.get('job_id')\n        if not job_id:\n            if not log_interpretation.get('no_job'):\n                log.warning(\"Can't fetch history log; missing job ID\")\n            return\n\n        output_dir = step_interpretation.get('output_dir')\n\n        log_interpretation['history'] = _interpret_history_log(\n            self.fs, self._ls_history_logs(\n                job_id=job_id, output_dir=output_dir))\n\n    def _ls_history_logs(self, job_id=None, output_dir=None):\n        \"\"\"Yield history log matches, logging a message for each one.\"\"\"\n        if not self._read_logs():\n            return\n\n        for match in _ls_history_logs(\n                self.fs,\n                self._stream_history_log_dirs(output_dir=output_dir),\n                job_id=job_id):\n            log.info('  Parsing history log: %s' % match['path'])\n            yield match\n\n    def _interpret_step_logs(self, log_interpretation, step_type):\n        \"\"\"Add *step* to the log interpretation, if it's not already there.\"\"\"\n        if 'step' in log_interpretation:\n            return\n\n        if not self._read_logs():\n            return\n\n        step_interpretation = self._get_step_log_interpretation(\n            log_interpretation, step_type)\n        if step_interpretation:\n            log_interpretation['step'] = step_interpretation\n\n    def _interpret_task_logs(\n            self, log_interpretation, step_type, error_attempt_ids=(),\n            partial=True):\n        \"\"\"Fetch task syslogs and stderr, and add 'task' to interpretation.\"\"\"\n        if 'task' in log_interpretation and (\n                partial or not log_interpretation['task'].get('partial')):\n            return   # already interpreted\n\n        if not self._read_logs():\n            return\n\n        step_interpretation = log_interpretation.get('step') or {}\n\n        application_id = step_interpretation.get('application_id')\n        job_id = step_interpretation.get('job_id')\n        output_dir = step_interpretation.get('output_dir')\n\n        yarn = uses_yarn(self.get_hadoop_version())\n\n        attempt_to_container_id = log_interpretation.get('history', {}).get(\n            'attempt_to_container_id', {})\n\n        if yarn:\n            if not application_id:\n                if not log_interpretation.get('no_job'):\n                    log.warning(\n                        \"Can't fetch task logs; missing application ID\")\n                return\n        else:\n            if not job_id:\n                if not log_interpretation.get('no_job'):\n                    log.warning(\"Can't fetch task logs; missing job ID\")\n                return\n\n        if self._step_type_uses_spark(step_type):\n            interpret_func = _interpret_spark_logs\n        else:\n            interpret_func = _interpret_task_logs\n\n        log_interpretation['task'] = interpret_func(\n            self.fs,\n            self._ls_task_logs(\n                step_type,\n                application_id=application_id,\n                job_id=job_id,\n                output_dir=output_dir,\n                error_attempt_ids=error_attempt_ids,\n                attempt_to_container_id=attempt_to_container_id,\n            ),\n            partial=partial,\n            log_callback=_log_parsing_task_log)\n\n    def _ls_task_logs(self, step_type,\n                      application_id=None, job_id=None, output_dir=None,\n                      error_attempt_ids=None, attempt_to_container_id=None):\n        \"\"\"Yield task log matches.\"\"\"\n        if not self._read_logs():\n            return\n\n        if self._step_type_uses_spark(step_type):\n            ls_func = _ls_spark_task_logs\n        else:\n            ls_func = _ls_task_logs\n\n        # logging messages are handled by a callback in _interpret_task_logs()\n        matches = ls_func(\n            self.fs,\n            self._stream_task_log_dirs(\n                application_id=application_id, output_dir=output_dir),\n            application_id=application_id,\n            job_id=job_id,\n            error_attempt_ids=error_attempt_ids,\n            attempt_to_container_id=attempt_to_container_id,\n        )\n\n        for match in matches:\n            yield match\n\n    def _log_counters(self, log_interpretation, step_num):\n        \"\"\"Utility for logging counters (if any) for a step.\"\"\"\n        step_type = self._get_step(step_num)['type']\n\n        if not self._step_type_uses_spark(step_type):\n            counters = self._pick_counters(\n                log_interpretation, step_type)\n            if counters:\n                log.info(_format_counters(counters))\n            elif self._read_logs():\n                # should only log this if we actually looked for counters\n                log.warning('No counters found')\n\n    def _read_logs(self):\n        \"\"\"If this is false, we shouldn't attempt to list or cat logs.\"\"\"\n        return self._opts.get('read_logs', True)\n"}, {"mrjob.logs.wrap._logs_exist": "# -*- coding: utf-8 -*-\n# Copyright 2015-2017 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities for ls()ing and cat()ing logs without raising exceptions.\"\"\"\nfrom logging import getLogger\n\nfrom mrjob.py2 import to_unicode\nfrom mrjob.util import to_lines\n\nfrom .ids import _sort_by_recency\nfrom .ids import _sort_for_spark\n\nlog = getLogger(__name__)\n\n\ndef _cat_log_lines(fs, path):\n    \"\"\"Yield lines from the given log.\n\n    Log errors rather than raising them.\n    \"\"\"\n    try:\n        if not fs.exists(path):\n            return\n        for line in to_lines(fs.cat(path)):\n            yield to_unicode(line)\n    except (IOError, OSError) as e:\n        log.warning(\"couldn't cat() %s: %r\" % (path, e))\n\n\ndef _ls_logs(fs, log_dir_stream, matcher, is_spark=False, **kwargs):\n    \"\"\"Return a list matches against log files. Used to implement\n    ``_ls_*_logs()`` functions.\n\n    This yields dictionaries with ``path`` set to matching log path, and\n    other information (e.g. corresponding job_id) returned by *matcher*\n\n    *fs* is a :py:class:`mrjob.fs.Filesystem`\n\n    *log_dir_stream* is a sequence of lists of log dirs. The idea is that\n    there may be copies of the same logs in multiple places (e.g.\n    on S3 and by SSHing into nodes) and we want to list them all without\n    finding duplicate copies. This function will go through the lists of\n    log dirs in turn, stopping if it finds any matches from a list.\n\n    *matcher* is a function that takes (log_path, **kwargs)\n    and returns either None (no match) or a dictionary with information\n    about the path (e.g. the corresponding job_id). It's okay to return\n    an empty dict.\n    \"\"\"\n    # wrapper for fs.ls() that turns IOErrors into warnings\n    def _fs_ls(path):\n        try:\n            log.debug('    listing logs in %s' % log_dir)\n            if fs.exists(log_dir):\n                for path in fs.ls(log_dir):\n                    yield path\n        except (IOError, OSError) as e:\n            log.warning(\"couldn't ls() %s: %r\" % (log_dir, e))\n\n    for log_dirs in log_dir_stream:\n        if isinstance(log_dirs, str):\n            raise TypeError\n\n        matched = False\n\n        for log_dir in log_dirs:\n            matches = []\n\n            for path in _fs_ls(log_dir):\n                match = matcher(path, **kwargs)\n                if match is not None:\n                    match['path'] = path\n                    matches.append(match)\n\n            if matches:\n                matched = True\n\n                if is_spark:\n                    matches = _sort_for_spark(matches)\n                else:\n                    matches = _sort_by_recency(matches)\n\n                for match in matches:\n                    yield match\n\n        if matched:\n            return  # e.g. don't check S3 if we can get logs via SSH\n\n\ndef _logs_exist(fs, path):\n    \"\"\"Do ``fs.exists(path)``, and return ``None`` if it raises ``IOError``\"\"\"\n    try:\n        return fs.exists(path)\n    except IOError:\n        return None\n"}, {"mrjob.util.unique": "# Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utility functions for MRJob\n\"\"\"\n# don't add imports here that aren't part of the standard Python library,\n# since MRJobs need to run in Amazon's generic EMR environment\nimport logging\nimport os\nimport os.path\nimport pipes\nimport random\nimport shlex\nimport shutil\nimport sys\nimport tarfile\nfrom contextlib import contextmanager\nfrom datetime import timedelta\nfrom distutils.spawn import find_executable\nfrom logging import getLogger\nfrom zipfile import ZIP_DEFLATED\nfrom zipfile import ZIP_STORED\nfrom zipfile import ZipFile\nfrom zipfile import is_zipfile\n\n\n\n\nlog = getLogger(__name__)\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\ndef cmd_line(args):\n    \"\"\"build a command line that works in a shell.\n    \"\"\"\n    args = [str(x) for x in args]\n    return ' '.join(pipes.quote(x) for x in args)\n\n\ndef expand_path(path):\n    \"\"\"Resolve ``~`` (home dir) and environment variables in *path*.\n\n    If *path* is ``None``, return ``None``.\n    \"\"\"\n    if path is None:\n        return None\n    else:\n        return os.path.expanduser(os.path.expandvars(path))\n\n\ndef file_ext(filename):\n    \"\"\"return the file extension, including the ``.``\n\n    >>> file_ext('foo.tar.gz')\n    '.tar.gz'\n\n    >>> file_ext('.emacs')\n    ''\n\n    >>> file_ext('.mrjob.conf')\n    '.conf'\n    \"\"\"\n    stripped_name = filename.lstrip('.')\n    dot_index = stripped_name.find('.')\n\n    if dot_index == -1:\n        return ''\n    return stripped_name[dot_index:]\n\n\ndef log_to_null(name=None):\n    \"\"\"Set up a null handler for the given stream, to suppress\n    \"no handlers could be found\" warnings.\"\"\"\n    logger = logging.getLogger(name)\n    logger.addHandler(NullHandler())\n\n\ndef log_to_stream(name=None, stream=None, format=None, level=None,\n                  debug=False):\n    \"\"\"Set up logging.\n\n    :type name: str\n    :param name: name of the logger, or ``None`` for the root logger\n    :type stream: file object\n    :param stream:  stream to log to (default is ``sys.stderr``)\n    :type format: str\n    :param format: log message format (default is '%(message)s')\n    :param level: log level to use\n    :type debug: bool\n    :param debug: quick way of setting the log level: if true, use\n                  ``logging.DEBUG``, otherwise use ``logging.INFO``\n    \"\"\"\n    if level is None:\n        level = logging.DEBUG if debug else logging.INFO\n\n    if format is None:\n        format = '%(message)s'\n\n    if stream is None:\n        stream = sys.stderr\n\n    handler = logging.StreamHandler(stream)\n    handler.setLevel(level)\n    handler.setFormatter(logging.Formatter(format))\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    logger.addHandler(handler)\n\n\ndef random_identifier():\n    \"\"\"A random 16-digit hex string.\"\"\"\n    return '%016x' % random.randint(0, 2 ** 64 - 1)\n\n\n# Thanks to http://lybniz2.sourceforge.net/safeeval.html for\n# explaining how to do this!\ndef safeeval(expr, globals=None, locals=None):\n    \"\"\"Like eval, but with nearly everything in the environment\n    blanked out, so that it's difficult to cause mischief.\n\n    *globals* and *locals* are optional dictionaries mapping names to\n    values for those names (just like in :py:func:`eval`).\n    \"\"\"\n    # blank out builtins, but keep None, True, and False\n    from mrjob.py2 import PY2\n    safe_globals = {\n        'False': False,\n        'None': None,\n        'True': True,\n        '__builtin__': None,\n        '__builtins__': None,\n        'set': set\n    }\n\n    # xrange is range in Python 3\n    if PY2:\n        safe_globals['xrange'] = xrange\n    else:\n        safe_globals['range'] = range\n\n    # PyPy needs special magic\n    def open(*args, **kwargs):\n        raise NameError(\"name 'open' is not defined\")\n    safe_globals['open'] = open\n\n    # add the user-specified global variables\n    if globals:\n        safe_globals.update(globals)\n\n    return eval(expr, safe_globals, locals)\n\n\n@contextmanager\ndef save_current_environment():\n    \"\"\" Context manager that saves os.environ and loads\n        it back again after execution\n    \"\"\"\n    original_environ = os.environ.copy()\n\n    try:\n        yield\n\n    finally:\n        os.environ.clear()\n        os.environ.update(original_environ)\n\n\n@contextmanager\ndef save_cwd():\n    \"\"\"Context manager that saves the current working directory,\n    and chdir's back to it after execution.\"\"\"\n    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n\n@contextmanager\ndef save_sys_std():\n    \"\"\"Context manager that saves the current values of `sys.stdin`,\n    `sys.stdout`, and `sys.stderr`, and flushes these filehandles before\n    and after switching them out.\"\"\"\n\n    stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n\n    try:\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        yield\n\n        # at this point, sys.stdout/stderr may have been patched. Don't\n        # raise an exception if flush() fails\n        try:\n            sys.stdout.flush()\n        except:\n            pass\n\n        try:\n            sys.stderr.flush()\n        except:\n            pass\n    finally:\n        sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\n\n\n@contextmanager\ndef save_sys_path():\n    \"\"\"Context manager that saves sys.path and restores it after execution.\"\"\"\n    original_sys_path = list(sys.path)\n\n    try:\n        yield\n\n    finally:\n        sys.path = original_sys_path\n\n\ndef shlex_split(s):\n    \"\"\"Wrapper around shlex.split(), but convert to str if Python version <\n    2.7.3 when unicode support was added.\n    \"\"\"\n    if sys.version_info < (2, 7, 3):\n        return shlex.split(str(s))\n    else:\n        return shlex.split(s)\n\n\ndef strip_microseconds(delta):\n    \"\"\"Return the given :py:class:`datetime.timedelta`, without microseconds.\n\n    Useful for printing :py:class:`datetime.timedelta` objects.\n    \"\"\"\n    return timedelta(delta.days, delta.seconds)\n\n\ndef to_lines(chunks):\n    \"\"\"Take in data as a sequence of bytes, and yield it, one line at a time.\n\n    Only breaks lines on ``\\\\n`` (not ``\\\\r``), and does not add\n    a trailing newline.\n\n    For efficiency, passes through anything with a ``readline()`` attribute.\n    \"\"\"\n    # hopefully this is good enough for anything mrjob will encounter\n    if hasattr(chunks, 'readline'):\n        return chunks\n    else:\n        return _to_lines(chunks)\n\n\ndef _to_lines(chunks):\n    \"\"\"Take in data as a sequence of bytes, and yield it, one line at a time.\n\n    Only breaks lines on ``\\\\n`` (not ``\\\\r``), and does not add\n    a trailing newline.\n\n    Exception: if we encounter an empty bytestring ``b''``, immediately yield\n    what we have so far rather than joining it to the next chunk. This allows\n    us to handle bytes from multiple files without joining the end of one\n    file to the beginning of the next one.\n\n    Optimizes for:\n\n    * chunks bigger than lines (e.g. reading test files)\n    * chunks that are lines (idempotency)\n    \"\"\"\n    # list of chunks with no final newline\n    leftovers = []\n\n    for chunk in chunks:\n        # special case for b'' standing for EOF\n        if chunk == b'':\n            if leftovers:\n                yield b''.join(leftovers)\n                leftovers = []\n\n            continue\n\n        start = 0\n\n        while start < len(chunk):\n            end = chunk.find(b'\\n', start) + 1\n\n            if end == 0:  # no newlines found\n                leftovers.append(chunk[start:])\n                break\n\n            if leftovers:\n                leftovers.append(chunk[start:end])\n                yield b''.join(leftovers)\n                leftovers = []\n            else:\n                yield chunk[start:end]\n\n            start = end\n\n    if leftovers:\n        yield b''.join(leftovers)\n\n\ndef unique(items):\n    \"\"\"Yield items from *item* in order, skipping duplicates.\"\"\"\n    seen = set()\n\n    for item in items:\n        if item in seen:\n            continue\n        else:\n            yield item\n            seen.add(item)\n\n\ndef unarchive(archive_path, dest):\n    \"\"\"Extract the contents of a tar or zip file at *archive_path* into the\n    directory *dest*.\n\n    :type archive_path: str\n    :param archive_path: path to archive file\n    :type dest: str\n    :param dest: path to directory where archive will be extracted\n\n    *dest* will be created if it doesn't already exist.\n\n    tar files can be gzip compressed, bzip2 compressed, or uncompressed. Files\n    within zip files can be deflated or stored.\n    \"\"\"\n    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r') as archive:\n            archive.extractall(dest)\n    elif is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as archive:\n            for name in archive.namelist():\n                # the zip spec specifies that front slashes are always\n                # used as directory separators\n                dest_path = os.path.join(dest, *name.split('/'))\n\n                # now, split out any dirname and filename and create\n                # one and/or the other\n                dirname, filename = os.path.split(dest_path)\n                if dirname and not os.path.exists(dirname):\n                    os.makedirs(dirname)\n                if filename:\n                    with open(dest_path, 'wb') as dest_file:\n                        dest_file.write(archive.read(name))\n    else:\n        raise IOError('Unknown archive type: %s' % (archive_path,))\n\n\ndef which(cmd, path=None):\n    \"\"\"Like the UNIX which command: search in *path* for the executable named\n    *cmd*. *path* defaults to :envvar:`PATH`. Returns ``None`` if no\n    such executable found.\n\n    This is basically ``shutil.which()`` (which was introduced in Python 3.3)\n    without the *mode* argument. Best practice is to always specify *path*\n    as a keyword argument.\n    \"\"\"\n    if hasattr(shutil, 'which'):\n        # added in Python 3.3\n        return shutil.which(cmd, path=path)\n    elif path is None and os.environ.get('PATH') is None:\n        # find_executable() errors if neither path nor $PATH is set\n        return None\n    else:\n        return find_executable(cmd, path=path)\n\n\ndef zip_dir(dir, out_path, filter=None, prefix=''):\n    \"\"\"Compress the given *dir* into a zip file at *out_path*.\n\n    If we encounter symlinks, include the actual file, not the symlink.\n\n    :type dir: str\n    :param dir: dir to tar up\n    :type out_path: str\n    :param out_path: where to write the tarball too\n    :param filter: if defined, a function that takes paths (relative to *dir*\n                   and returns ``True`` if we should keep them\n    :type prefix: str\n    :param prefix: subdirectory inside the tarball to put everything into (e.g.\n                   ``'mrjob'``)\n    \"\"\"\n    if not os.path.isdir(dir):\n        raise IOError('Not a directory: %r' % (dir,))\n\n    if not filter:\n        filter = lambda path: True\n\n    with _create_zip_file(out_path) as zip_file:\n        for dirpath, dirnames, filenames in os.walk(dir, followlinks=True):\n            for filename in filenames:\n                path = os.path.join(dirpath, filename)\n                rel_path = os.path.relpath(path, dir)\n\n                if filter(rel_path):\n                    # copy over real files, not symlinks\n                    real_path = os.path.realpath(path)\n                    path_in_zip_file = os.path.join(prefix, rel_path)\n                    zip_file.write(real_path, arcname=path_in_zip_file)\n\n\n# this is also used by spark runner\ndef _create_zip_file(path):\n    try:\n        return ZipFile(path, mode='w', compression=ZIP_DEFLATED)\n    except RuntimeError:  # zlib not available\n        return ZipFile(path, mode='w', compression=ZIP_STORED)\n"}], "prompt": "Please write a python function called '_stream_history_log_dirs' base the context. This function yields lists of directories to search for the history log in. It first checks if logs should be read, and then iterates over unique log directories obtained from the hadoop log directories. If the directory exists, it logs an info message: 'Looking for history log in {directory}...'. It then yields a list containing the directory.:param self: HadoopJobRunner. An instance of the HadoopJobRunner class.\n:param output_dir: str. The output directory to search for the history log. Defaults to None.\n:return: Generator. Yields lists of directories to search for the history log in..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # Copyright 2009-2016 Yelp and Contributors\n# Copyright 2017 Yelp\n# Copyright 2018 Yelp and Google, Inc.\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport getpass\nimport logging\nimport os\nimport posixpath\nimport re\nfrom subprocess import CalledProcessError\nfrom subprocess import Popen\nfrom subprocess import PIPE\n\ntry:\n    import pty\n    pty  # quiet \"redefinition of unused ...\" warning from pyflakes\nexcept ImportError:\n    pty = None\n\nfrom mrjob.bin import MRJobBinRunner\nfrom mrjob.compat import uses_yarn\nfrom mrjob.conf import combine_dicts\nfrom mrjob.fs.composite import CompositeFilesystem\n\n\nfrom mrjob.logs.counters import _pick_counters\nfrom mrjob.logs.errors import _log_probable_cause_of_failure\nfrom mrjob.logs.mixin import LogInterpretationMixin\nfrom mrjob.logs.step import _eio_to_eof\nfrom mrjob.logs.step import _interpret_hadoop_jar_command_stderr\nfrom mrjob.logs.step import _is_counter_log4j_record\nfrom mrjob.logs.step import _log_line_from_driver\nfrom mrjob.logs.step import _log_log4j_record\nfrom mrjob.logs.wrap import _logs_exist\n\nfrom mrjob.py2 import to_unicode\nfrom mrjob.runner import _fix_env\nfrom mrjob.setup import UploadDirManager\nfrom mrjob.step import StepFailedException\nfrom mrjob.step import _is_spark_step_type\nfrom mrjob.util import cmd_line\nfrom mrjob.util import unique\nfrom mrjob.util import which\n\n\nlog = logging.getLogger(__name__)\n\n# don't look for the hadoop streaming jar here!\n_BAD_HADOOP_HOMES = ['/', '/usr', '/usr/local']\n\n# where YARN stores history logs, etc. on HDFS by default\n_DEFAULT_YARN_HDFS_LOG_DIR = 'hdfs:///tmp/hadoop-yarn/staging'\n\n# places to look for the Hadoop streaming jar if we're inside EMR\n_EMR_HADOOP_STREAMING_JAR_DIRS = [\n    # for the 2.x and 3.x AMIs (the 2.x AMIs also set $HADOOP_HOME properly)\n    '/home/hadoop/contrib',\n    # for the 4.x AMIs\n    '/usr/lib/hadoop-mapreduce',\n]\n\n# these are fairly standard places to keep Hadoop logs\n_FALLBACK_HADOOP_LOG_DIRS = [\n    '/var/log/hadoop',\n    '/mnt/var/log/hadoop',  # EMR's 2.x and 3.x AMIs use this\n]\n\n# fairly standard places to keep YARN logs (see #1339)\n_FALLBACK_HADOOP_YARN_LOG_DIRS = [\n    '/var/log/hadoop-yarn',\n    '/mnt/var/log/hadoop-yarn',\n]\n\n# start of Counters printed by Hadoop\n_HADOOP_COUNTERS_START_RE = re.compile(br'^Counters: (?P<amount>\\d+)\\s*$')\n\n# header for a group of counters\n_HADOOP_COUNTER_GROUP_RE = re.compile(br'^(?P<indent>\\s+)(?P<group>.*)$')\n\n# line for a counter\n_HADOOP_COUNTER_RE = re.compile(\n    br'^(?P<indent>\\s+)(?P<counter>.*)=(?P<amount>\\d+)\\s*$')\n\n# the one thing Hadoop streaming prints to stderr not in log format\n_HADOOP_NON_LOG_LINE_RE = re.compile(r'^Streaming Command Failed!')\n\n# if we see this from Hadoop, it actually came from stdout and shouldn't\n# be logged\n_HADOOP_STDOUT_RE = re.compile(br'^packageJobJar: ')\n\n# match the filename of a hadoop streaming jar\n_HADOOP_STREAMING_JAR_RE = re.compile(\n    r'^hadoop.*streaming.*(?<!-sources)\\.jar$')\n\n\ndef fully_qualify_hdfs_path(path):\n    \"\"\"If path isn't an ``hdfs://`` URL, turn it into one.\"\"\"\n    from mrjob.parse import is_uri\n    if is_uri(path):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)\n\n\nclass HadoopJobRunner(MRJobBinRunner, LogInterpretationMixin):\n    \"\"\"Runs an :py:class:`~mrjob.job.MRJob` on your Hadoop cluster.\n    Invoked when you run your job with ``-r hadoop``.\n\n    Input and support files can be either local or on HDFS; use ``hdfs://...``\n    URLs to refer to files on HDFS.\n    \"\"\"\n    alias = 'hadoop'\n\n    OPT_NAMES = MRJobBinRunner.OPT_NAMES | {\n        'hadoop_bin',\n        'hadoop_extra_args',\n        'hadoop_log_dirs',\n        'hadoop_streaming_jar',\n        'hadoop_tmp_dir',\n        'spark_deploy_mode',\n        'spark_master',\n    }\n\n    # supports everything (so far)\n    _STEP_TYPES = {\n        'jar', 'spark', 'spark_jar', 'spark_script', 'streaming'}\n\n    def __init__(self, **kwargs):\n        \"\"\":py:class:`~mrjob.hadoop.HadoopJobRunner` takes the same arguments\n        as :py:class:`~mrjob.runner.MRJobRunner`, plus some additional options\n        which can be defaulted in :ref:`mrjob.conf <mrjob.conf>`.\n        \"\"\"\n        super(HadoopJobRunner, self).__init__(**kwargs)\n\n        self._hadoop_tmp_dir = fully_qualify_hdfs_path(\n            posixpath.join(\n                self._opts['hadoop_tmp_dir'], self._job_key))\n\n        # Keep track of local files to upload to HDFS. We'll add them\n        # to this manager just before we need them.\n        hdfs_files_dir = posixpath.join(self._hadoop_tmp_dir, 'files', '')\n        self._upload_mgr = UploadDirManager(hdfs_files_dir)\n\n        # Set output dir if it wasn't set explicitly\n        self._output_dir = fully_qualify_hdfs_path(\n            self._output_dir or\n            posixpath.join(self._hadoop_tmp_dir, 'output'))\n\n        # Fully qualify step_output_dir, if set\n        if self._step_output_dir:\n            self._step_output_dir = fully_qualify_hdfs_path(\n                self._step_output_dir)\n\n        # Track job and (YARN) application ID to enable log parsing\n        self._application_id = None\n        self._job_id = None\n\n        # Keep track of where the hadoop streaming jar is\n        self._hadoop_streaming_jar = self._opts['hadoop_streaming_jar']\n        self._searched_for_hadoop_streaming_jar = False\n\n        # List of dicts (one for each step) potentially containing\n        # the keys 'history', 'step', and 'task' ('step' will always\n        # be filled because it comes from the hadoop jar command output,\n        # others will be filled as needed)\n        self._log_interpretations = []\n\n    @classmethod\n    def _default_opts(cls):\n        return combine_dicts(\n            super(HadoopJobRunner, cls)._default_opts(),\n            dict(\n                hadoop_tmp_dir='tmp/mrjob',\n            )\n        )\n\n    @property\n    def fs(self):\n        \"\"\":py:class:`mrjob.fs.base.Filesystem` object for HDFS and the local\n        filesystem.\n        \"\"\"\n        from mrjob.fs.local import LocalFilesystem\n        from mrjob.fs.hadoop import HadoopFilesystem\n        if self._fs is None:\n            self._fs = CompositeFilesystem()\n\n            # don't pass [] to fs; this means not to use hadoop until\n            # fs.set_hadoop_bin() is called (used for running hadoop over SSH).\n            hadoop_bin = self._opts['hadoop_bin'] or None\n\n            self._fs.add_fs('hadoop',\n                            HadoopFilesystem(hadoop_bin))\n            self._fs.add_fs('local', LocalFilesystem())\n\n        return self._fs\n\n    def get_hadoop_version(self):\n        \"\"\"Invoke the hadoop executable to determine its version\"\"\"\n        return self.fs.hadoop.get_hadoop_version()\n\n    def get_hadoop_bin(self):\n        \"\"\"Find the hadoop binary. A list: binary followed by arguments.\"\"\"\n        return self.fs.hadoop.get_hadoop_bin()\n\n    def get_hadoop_streaming_jar(self):\n        \"\"\"Find the path of the hadoop streaming jar, or None if not found.\"\"\"\n        if not (self._hadoop_streaming_jar or\n                self._searched_for_hadoop_streaming_jar):\n\n            self._hadoop_streaming_jar = self._find_hadoop_streaming_jar()\n\n            if self._hadoop_streaming_jar:\n                log.info('Found Hadoop streaming jar: %s' %\n                         self._hadoop_streaming_jar)\n            else:\n                log.warning('Hadoop streaming jar not found. Use'\n                            ' --hadoop-streaming-jar')\n\n            self._searched_for_hadoop_streaming_jar = True\n\n        return self._hadoop_streaming_jar\n\n    def _find_hadoop_streaming_jar(self):\n        \"\"\"Search for the hadoop streaming jar. See\n        :py:meth:`_hadoop_streaming_jar_dirs` for where we search.\"\"\"\n        for path in unique(self._hadoop_streaming_jar_dirs()):\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n\n            streaming_jars = []\n            for path in self.fs.ls(path):\n                if _HADOOP_STREAMING_JAR_RE.match(posixpath.basename(path)):\n                    streaming_jars.append(path)\n\n            if streaming_jars:\n                # prefer shorter names and shallower paths\n                def sort_key(p):\n                    return (len(p.split('/')),\n                            len(posixpath.basename(p)),\n                            p)\n\n                streaming_jars.sort(key=sort_key)\n\n                return streaming_jars[0]\n\n        return None\n\n    def _hadoop_dirs(self):\n        \"\"\"Yield all possible hadoop directories (used for streaming jar\n        and logs). May yield duplicates\"\"\"\n        for name in ('HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL',\n                     'HADOOP_MAPRED_HOME'):\n            path = os.environ.get(name)\n            if path:\n                yield path\n\n        # guess it from the path of the Hadoop binary\n        hadoop_home = _hadoop_prefix_from_bin(self.get_hadoop_bin()[0])\n        if hadoop_home:\n            yield hadoop_home\n\n        # try HADOOP_*_HOME\n        for name, path in sorted(os.environ.items()):\n            if name.startswith('HADOOP_') and name.endswith('_HOME'):\n                yield path\n\n    def _hadoop_streaming_jar_dirs(self):\n        \"\"\"Yield all possible places to look for the Hadoop streaming jar.\n        May yield duplicates.\n        \"\"\"\n        for hadoop_dir in self._hadoop_dirs():\n            yield hadoop_dir\n\n        # use hard-coded paths to work out-of-the-box on EMR\n        for path in _EMR_HADOOP_STREAMING_JAR_DIRS:\n            yield path\n\n    def _hadoop_log_dirs(self, output_dir=None):\n        \"\"\"Yield all possible places to look for hadoop logs.\"\"\"\n        # hadoop_log_dirs opt overrides all this\n        if self._opts['hadoop_log_dirs']:\n            for path in self._opts['hadoop_log_dirs']:\n                yield path\n            return\n\n        hadoop_log_dir = os.environ.get('HADOOP_LOG_DIR')\n        if hadoop_log_dir:\n            yield hadoop_log_dir\n\n        yarn = uses_yarn(self.get_hadoop_version())\n\n        if yarn:\n            yarn_log_dir = os.environ.get('YARN_LOG_DIR')\n            if yarn_log_dir:\n                yield yarn_log_dir\n\n            yield _DEFAULT_YARN_HDFS_LOG_DIR\n\n        if output_dir:\n            # Cloudera style of logging\n            yield posixpath.join(output_dir, '_logs')\n\n        for hadoop_dir in self._hadoop_dirs():\n            yield posixpath.join(hadoop_dir, 'logs')\n\n        # hard-coded fallback paths\n        if yarn:\n            for path in _FALLBACK_HADOOP_YARN_LOG_DIRS:\n                yield path\n\n        for path in _FALLBACK_HADOOP_LOG_DIRS:\n            yield path\n\n    def _run(self):\n        self._find_binaries_and_jars()\n        self._create_setup_wrapper_scripts()\n        self._add_job_files_for_upload()\n        self._upload_local_files()\n        self._run_job_in_hadoop()\n\n    def _find_binaries_and_jars(self):\n        \"\"\"Find hadoop and (if needed) spark-submit bin up-front, before\n        continuing with the job.\n\n        (This is just for user-interaction purposes; these would otherwise\n        lazy-load as needed.)\n        \"\"\"\n        # this triggers looking for Hadoop binary\n        self.get_hadoop_version()\n\n        if self._has_hadoop_streaming_steps():\n            self.get_hadoop_streaming_jar()\n\n        if self._has_spark_steps():\n            self.get_spark_submit_bin()\n\n    def _add_job_files_for_upload(self):\n        \"\"\"Add files needed for running the job (setup and input)\n        to self._upload_mgr.\"\"\"\n        for path in self._py_files():\n            self._upload_mgr.add(path)\n\n    def _dump_stdin_to_local_file(self):\n        \"\"\"Dump sys.stdin to a local file, and return the path to it.\"\"\"\n        stdin_path = posixpath.join(self._get_local_tmp_dir(), 'STDIN')\n         # prompt user, so they don't think the process has stalled\n        log.info('reading from STDIN')\n\n        log.debug('dumping stdin to local file %s...' % stdin_path)\n        stdin_file = open(stdin_path, 'wb')\n        for line in self._stdin:\n            stdin_file.write(line)\n\n        return stdin_path\n\n    def _run_job_in_hadoop(self):\n        for step_num, step in enumerate(self._get_steps()):\n            step_type = step['type']\n            step_args = self._args_for_step(step_num)\n            env = _fix_env(self._env_for_step(step_num))\n\n            # log this *after* _args_for_step(), which can start a search\n            # for the Hadoop streaming jar\n            log.info('Running step %d of %d...' %\n                     (step_num + 1, self._num_steps()))\n            log.debug('> %s' % cmd_line(step_args))\n            log.debug('  with environment: %r' % sorted(env.items()))\n\n            log_interpretation = {}\n            self._log_interpretations.append(log_interpretation)\n\n            if self._step_type_uses_spark(step_type):\n                returncode, step_interpretation = self._run_spark_submit(\n                    step_args, env, record_callback=_log_log4j_record)\n            else:\n                returncode, step_interpretation = self._run_hadoop(\n                    step_args, env, record_callback=_log_record_from_hadoop)\n\n            # make sure output_dir is filled (used for history log)\n            if 'output_dir' not in step_interpretation:\n                step_interpretation['output_dir'] = (\n                    self._step_output_uri(step_num))\n\n            log_interpretation['step'] = step_interpretation\n\n            self._log_counters(log_interpretation, step_num)\n\n            if returncode:\n                error = self._pick_error(log_interpretation, step_type)\n                if error:\n                    _log_probable_cause_of_failure(log, error)\n\n                # use CalledProcessError's well-known message format\n                reason = str(CalledProcessError(returncode, step_args))\n                raise StepFailedException(\n                    reason=reason, step_num=step_num,\n                    num_steps=self._num_steps())\n\n    def _run_hadoop(self, hadoop_args, env, record_callback):\n        # try to use a PTY if it's available\n        try:\n            pid, master_fd = pty.fork()\n        except (AttributeError, OSError):\n            # no PTYs, just use Popen\n\n            # user won't get much feedback for a while, so tell them\n            # Hadoop is running\n            log.debug('No PTY available, using Popen() to invoke Hadoop')\n\n            step_proc = Popen(hadoop_args, stdout=PIPE, stderr=PIPE, env=env)\n\n            step_interpretation = _interpret_hadoop_jar_command_stderr(\n                step_proc.stderr,\n                record_callback=_log_record_from_hadoop)\n\n            # there shouldn't be much output to STDOUT\n            for line in step_proc.stdout:\n                _log_line_from_driver(to_unicode(line).strip('\\r\\n'))\n\n            step_proc.stdout.close()\n            step_proc.stderr.close()\n\n            returncode = step_proc.wait()\n        else:\n            # we have PTYs\n            if pid == 0:  # we are the child process\n                try:\n                    os.execvpe(hadoop_args[0], hadoop_args, env)\n                    # now we are no longer Python\n                except OSError as ex:\n                    # use _exit() so we don't do cleanup, etc. that's\n                    # the parent process's job\n                    os._exit(ex.errno)\n                finally:\n                    # if we got some other exception, still exit hard\n                    os._exit(-1)\n            else:\n                log.debug('Invoking Hadoop via PTY')\n\n                with os.fdopen(master_fd, 'rb') as master:\n                    # reading from master gives us the subprocess's\n                    # stderr and stdout (it's a fake terminal)\n                    step_interpretation = (\n                        _interpret_hadoop_jar_command_stderr(\n                            _eio_to_eof(master),\n                            record_callback=_log_record_from_hadoop))\n                    _, returncode = os.waitpid(pid, 0)\n\n        return returncode, step_interpretation\n\n    def _spark_master(self):\n        return self._opts['spark_master'] or 'yarn'\n\n    def _args_for_step(self, step_num):\n        step = self._get_step(step_num)\n\n        if step['type'] == 'streaming':\n            return self._args_for_streaming_step(step_num)\n        elif step['type'] == 'jar':\n            return self._args_for_jar_step(step_num)\n        elif _is_spark_step_type(step['type']):\n            return self._args_for_spark_step(step_num)\n        else:\n            raise ValueError('Bad step type: %r' % (step['type'],))\n\n    def _args_for_streaming_step(self, step_num):\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        return (self.get_hadoop_bin() + ['jar', hadoop_streaming_jar] +\n                self._hadoop_streaming_jar_args(step_num))\n\n    def _args_for_jar_step(self, step_num):\n        step = self._get_step(step_num)\n\n        args = []\n\n        args.extend(self.get_hadoop_bin())\n\n        # special case for consistency with EMR runner.\n        #\n        # This might look less like duplicated code if we ever\n        # implement #780 (fetching jars from URIs)\n        if step['jar'].startswith('file:///'):\n            jar = step['jar'][7:]  # keep leading slash\n        else:\n            jar = step['jar']\n\n        args.extend(['jar', jar])\n\n        if step.get('main_class'):\n            args.append(step['main_class'])\n\n        if step.get('args'):\n            args.extend(\n                self._interpolate_jar_step_args(step['args'], step_num))\n\n        return args\n\n    def _env_for_step(self, step_num):\n        step = self._get_step(step_num)\n\n        env = dict(os.environ)\n\n        # when running spark-submit, set its environment directly. See #1464\n        if _is_spark_step_type(step['type']):\n            env.update(self._spark_cmdenv(step_num))\n\n        return env\n\n    def _default_step_output_dir(self):\n        return posixpath.join(self._hadoop_tmp_dir, 'step-output')\n\n    def _cleanup_hadoop_tmp(self):\n        if self._hadoop_tmp_dir:\n            log.info('Removing HDFS temp directory %s...' %\n                     self._hadoop_tmp_dir)\n            try:\n                self.fs.rm(self._hadoop_tmp_dir)\n            except Exception as e:\n                log.exception(e)\n\n    def _manifest_download_commands(self):\n        cp_to_local = self.get_hadoop_bin() + ['fs', '-copyToLocal']\n\n        return [\n            ('*://*', cmd_line(cp_to_local)),\n        ]\n\n    ### LOG (implementation of LogInterpretationMixin) ###\n\n###The function: _stream_history_log_dirs###\n    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\n        \"\"\"Yield lists of directories to look for the task logs in.\"\"\"\n        # Note: this is unlikely to be super-helpful on \"real\" (multi-node)\n        # pre-YARN Hadoop because task logs aren't generally shipped to a\n        # local directory. It's a start, anyways. See #1201.\n        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                path = self.fs.join(log_dir, 'userlogs', application_id)\n            else:\n                path = self.fs.join(log_dir, 'userlogs')\n\n            if _logs_exist(self.fs, path):\n                log.info('Looking for task syslogs in %s...' % path)\n                yield [path]\n\n    def counters(self):\n        return [_pick_counters(log_interpretation)\n                for log_interpretation in self._log_interpretations]\n\n\n# These don't require state from HadoopJobRunner, so making them functions.\n# Feel free to convert them back into methods as need be\n\n\ndef _hadoop_prefix_from_bin(hadoop_bin):\n    \"\"\"Given a path to the hadoop binary, return the path of the implied\n    hadoop home, or None if we don't know.\n\n    Don't return the parent directory of directories in the default\n    path (not ``/``, ``/usr``, or ``/usr/local``).\n    \"\"\"\n    # resolve unqualified binary name (relative paths are okay)\n    if '/' not in hadoop_bin:\n        hadoop_bin = which(hadoop_bin)\n        if not hadoop_bin:\n            return None\n\n    # use parent of hadoop_bin's directory\n    hadoop_home = posixpath.abspath(\n        posixpath.join(posixpath.realpath(posixpath.dirname(hadoop_bin)), '..')\n    )\n\n    if hadoop_home in _BAD_HADOOP_HOMES:\n        return None\n\n    return hadoop_home\n\n\ndef _log_record_from_hadoop(record):\n    \"\"\"Log log4j record parsed from hadoop stderr.\"\"\"\n    if not _is_counter_log4j_record(record):  # counters are printed separately\n        _log_log4j_record(record)\n\n        ####cross_file_context:\n        [{'mrjob.logs.mixin.LogInterpretationMixin._read_logs': '# -*- coding: utf-8 -*-\\n# Copyright 2016 Yelp and Contributors\\n# Copyright 2017-2018 Yelp\\n# Copyright 2019 Yelp\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Runner mixin for counters and probable cause of failure.\\n\\nThis relies on passing around a *log_interpretation* dictionary, which\\nis described in detail in :py:mod:`mrjob.logs`.\\n\\nThis mixin doesn\\'t yet handle step logs because the EMR and Hadoop runners\\nhandle them so differently. It\\'s up to you to fill in the \\'step\\' field\\nof your log interpretation; the mixin can\\'t do much without it because\\nit needs it for the job/application ID.\\n\\nYour runner should generally have one log interpretation per step,\\nthough the mixin doesn\\'t care how or where you store them.\\n\"\"\"\\nfrom logging import getLogger\\n\\nfrom mrjob.compat import uses_yarn\\nfrom mrjob.logs.counters import _format_counters\\nfrom mrjob.logs.counters import _pick_counters\\n\\n\\nfrom mrjob.logs.history import _interpret_history_log\\nfrom mrjob.logs.history import _ls_history_logs\\nfrom mrjob.logs.spark import _interpret_spark_logs\\nfrom mrjob.logs.task import _interpret_task_logs\\nfrom mrjob.logs.task import _ls_task_logs\\nfrom mrjob.logs.task import _ls_spark_task_logs\\n\\nlog = getLogger(__name__)\\n\\n\\n# a callback for _interpret_task_logs(). Breaking it out to make\\n# testing easier\\ndef _log_parsing_task_log(log_path):\\n    log.info(\\'  Parsing task log: %s\\' % log_path)\\n\\n\\nclass LogInterpretationMixin(object):\\n    \"\"\"Mix this in to your runner class to simplify log interpretation.\"\"\"\\n    # this mixin is meant to be tightly bound to MRJobRunner, but\\n    # currently it only relies on self.fs and self.get_hadoop_version()\\n\\n    ### stuff to redefine ###\\n\\n    def _stream_history_log_dirs(self, output_dir=None):\\n        \"\"\"Yield lists of directories (usually, URIs) to search for history\\n        logs in.\\n\\n        Usually, you\\'ll want to add logging messages (e.g.\\n        \\'Searching for history logs in ...\\'\\n\\n        :param output_dir: Output directory for step (optional), to look\\n            for logs (e.g. on Cloudera).\\n        \"\"\"\\n        return ()\\n\\n    def _stream_task_log_dirs(self, application_id=None, output_dir=None):\\n        \"\"\"Yield lists of directories (usually, URIs) to search for task\\n        logs in.\\n\\n        Usually, you\\'ll want to add logging messages (e.g.\\n        \\'Searching for task syslogs in...\\')\\n\\n        :param application_id: YARN application ID (optional), so we can ls\\n            the relevant subdirectory of `userlogs/` rather than the whole\\n            thing\\n        :param output_dir: Output directory for step (optional), to look\\n            for logs (e.g. on Cloudera).\\n        \"\"\"\\n        # sometimes pre-YARN logs are organized by job ID, but not always,\\n        # so we don\\'t bother with job_id; just ls() the entire userlogs\\n        # dir and depend on regexes to find the right subdir.\\n        return ()\\n\\n    def _get_step_log_interpretation(self, log_interpretation, step_type):\\n        \"\"\"Return interpretation of the step log. Either implement\\n        this, or fill ``\\'step\\'`` yourself (e.g. from Hadoop binary\\'s\\n        output.\"\"\"\\n        return None\\n\\n    ### stuff to call ###\\n\\n    def _pick_counters(self, log_interpretation, step_type):\\n        \"\"\"Pick counters from our log interpretation, interpreting\\n        history logs if need be.\"\"\"\\n        if self._step_type_uses_spark(step_type):\\n            return {}\\n\\n        counters = _pick_counters(log_interpretation)\\n\\n        if self._read_logs():\\n            if not counters:\\n                log.info(\\'Attempting to fetch counters from logs...\\')\\n                self._interpret_step_logs(log_interpretation, step_type)\\n                counters = _pick_counters(log_interpretation)\\n\\n            if not counters:\\n                self._interpret_history_log(log_interpretation)\\n                counters = _pick_counters(log_interpretation)\\n\\n        return counters\\n\\n    def _pick_error(self, log_interpretation, step_type):\\n        \"\"\"Pick probable cause of failure (only call this if job fails).\"\"\"\\n        from mrjob.logs.errors import _pick_error_attempt_ids\\n        from mrjob.logs.errors import _pick_error\\n        logs_needed = self._logs_needed_to_pick_error(step_type)\\n\\n        if self._read_logs() and not all(\\n                log_type in log_interpretation for log_type in logs_needed):\\n            log.info(\\'Scanning logs for probable cause of failure...\\')\\n\\n            if \\'step\\' in logs_needed:\\n                self._interpret_step_logs(log_interpretation, step_type)\\n\\n            if \\'history\\' in logs_needed:\\n                self._interpret_history_log(log_interpretation)\\n\\n            if \\'task\\' in logs_needed:\\n                error_attempt_ids = _pick_error_attempt_ids(log_interpretation)\\n\\n                self._interpret_task_logs(\\n                    log_interpretation, step_type, error_attempt_ids)\\n\\n        return _pick_error(log_interpretation)\\n\\n    def _logs_needed_to_pick_error(self, step_type):\\n        \"\"\"We don\\'t need all the logs when interpreting Spark steps\"\"\"\\n        if self._step_type_uses_spark(step_type):\\n            if self._spark_deploy_mode() == \\'cluster\\':\\n                return (\\'step\\', \\'task\\')\\n            else:\\n                return (\\'step\\',)\\n        else:\\n            return (\\'step\\', \\'history\\', \\'task\\')\\n\\n    ### stuff that should just work ###\\n\\n    def _interpret_history_log(self, log_interpretation):\\n        \"\"\"Fetch history log and add \\'history\\' to log_interpretation.\"\"\"\\n        if \\'history\\' in log_interpretation:\\n            return   # already interpreted\\n\\n        if not self._read_logs():\\n            return  # nothing to do\\n\\n        step_interpretation = log_interpretation.get(\\'step\\') or {}\\n\\n        job_id = step_interpretation.get(\\'job_id\\')\\n        if not job_id:\\n            if not log_interpretation.get(\\'no_job\\'):\\n                log.warning(\"Can\\'t fetch history log; missing job ID\")\\n            return\\n\\n        output_dir = step_interpretation.get(\\'output_dir\\')\\n\\n        log_interpretation[\\'history\\'] = _interpret_history_log(\\n            self.fs, self._ls_history_logs(\\n                job_id=job_id, output_dir=output_dir))\\n\\n    def _ls_history_logs(self, job_id=None, output_dir=None):\\n        \"\"\"Yield history log matches, logging a message for each one.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        for match in _ls_history_logs(\\n                self.fs,\\n                self._stream_history_log_dirs(output_dir=output_dir),\\n                job_id=job_id):\\n            log.info(\\'  Parsing history log: %s\\' % match[\\'path\\'])\\n            yield match\\n\\n    def _interpret_step_logs(self, log_interpretation, step_type):\\n        \"\"\"Add *step* to the log interpretation, if it\\'s not already there.\"\"\"\\n        if \\'step\\' in log_interpretation:\\n            return\\n\\n        if not self._read_logs():\\n            return\\n\\n        step_interpretation = self._get_step_log_interpretation(\\n            log_interpretation, step_type)\\n        if step_interpretation:\\n            log_interpretation[\\'step\\'] = step_interpretation\\n\\n    def _interpret_task_logs(\\n            self, log_interpretation, step_type, error_attempt_ids=(),\\n            partial=True):\\n        \"\"\"Fetch task syslogs and stderr, and add \\'task\\' to interpretation.\"\"\"\\n        if \\'task\\' in log_interpretation and (\\n                partial or not log_interpretation[\\'task\\'].get(\\'partial\\')):\\n            return   # already interpreted\\n\\n        if not self._read_logs():\\n            return\\n\\n        step_interpretation = log_interpretation.get(\\'step\\') or {}\\n\\n        application_id = step_interpretation.get(\\'application_id\\')\\n        job_id = step_interpretation.get(\\'job_id\\')\\n        output_dir = step_interpretation.get(\\'output_dir\\')\\n\\n        yarn = uses_yarn(self.get_hadoop_version())\\n\\n        attempt_to_container_id = log_interpretation.get(\\'history\\', {}).get(\\n            \\'attempt_to_container_id\\', {})\\n\\n        if yarn:\\n            if not application_id:\\n                if not log_interpretation.get(\\'no_job\\'):\\n                    log.warning(\\n                        \"Can\\'t fetch task logs; missing application ID\")\\n                return\\n        else:\\n            if not job_id:\\n                if not log_interpretation.get(\\'no_job\\'):\\n                    log.warning(\"Can\\'t fetch task logs; missing job ID\")\\n                return\\n\\n        if self._step_type_uses_spark(step_type):\\n            interpret_func = _interpret_spark_logs\\n        else:\\n            interpret_func = _interpret_task_logs\\n\\n        log_interpretation[\\'task\\'] = interpret_func(\\n            self.fs,\\n            self._ls_task_logs(\\n                step_type,\\n                application_id=application_id,\\n                job_id=job_id,\\n                output_dir=output_dir,\\n                error_attempt_ids=error_attempt_ids,\\n                attempt_to_container_id=attempt_to_container_id,\\n            ),\\n            partial=partial,\\n            log_callback=_log_parsing_task_log)\\n\\n    def _ls_task_logs(self, step_type,\\n                      application_id=None, job_id=None, output_dir=None,\\n                      error_attempt_ids=None, attempt_to_container_id=None):\\n        \"\"\"Yield task log matches.\"\"\"\\n        if not self._read_logs():\\n            return\\n\\n        if self._step_type_uses_spark(step_type):\\n            ls_func = _ls_spark_task_logs\\n        else:\\n            ls_func = _ls_task_logs\\n\\n        # logging messages are handled by a callback in _interpret_task_logs()\\n        matches = ls_func(\\n            self.fs,\\n            self._stream_task_log_dirs(\\n                application_id=application_id, output_dir=output_dir),\\n            application_id=application_id,\\n            job_id=job_id,\\n            error_attempt_ids=error_attempt_ids,\\n            attempt_to_container_id=attempt_to_container_id,\\n        )\\n\\n        for match in matches:\\n            yield match\\n\\n    def _log_counters(self, log_interpretation, step_num):\\n        \"\"\"Utility for logging counters (if any) for a step.\"\"\"\\n        step_type = self._get_step(step_num)[\\'type\\']\\n\\n        if not self._step_type_uses_spark(step_type):\\n            counters = self._pick_counters(\\n                log_interpretation, step_type)\\n            if counters:\\n                log.info(_format_counters(counters))\\n            elif self._read_logs():\\n                # should only log this if we actually looked for counters\\n                log.warning(\\'No counters found\\')\\n\\n    def _read_logs(self):\\n        \"\"\"If this is false, we shouldn\\'t attempt to list or cat logs.\"\"\"\\n        return self._opts.get(\\'read_logs\\', True)\\n'}, {'mrjob.logs.wrap._logs_exist': '# -*- coding: utf-8 -*-\\n# Copyright 2015-2017 Yelp\\n# Copyright 2019 Yelp\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Utilities for ls()ing and cat()ing logs without raising exceptions.\"\"\"\\nfrom logging import getLogger\\n\\nfrom mrjob.py2 import to_unicode\\nfrom mrjob.util import to_lines\\n\\nfrom .ids import _sort_by_recency\\nfrom .ids import _sort_for_spark\\n\\nlog = getLogger(__name__)\\n\\n\\ndef _cat_log_lines(fs, path):\\n    \"\"\"Yield lines from the given log.\\n\\n    Log errors rather than raising them.\\n    \"\"\"\\n    try:\\n        if not fs.exists(path):\\n            return\\n        for line in to_lines(fs.cat(path)):\\n            yield to_unicode(line)\\n    except (IOError, OSError) as e:\\n        log.warning(\"couldn\\'t cat() %s: %r\" % (path, e))\\n\\n\\ndef _ls_logs(fs, log_dir_stream, matcher, is_spark=False, **kwargs):\\n    \"\"\"Return a list matches against log files. Used to implement\\n    ``_ls_*_logs()`` functions.\\n\\n    This yields dictionaries with ``path`` set to matching log path, and\\n    other information (e.g. corresponding job_id) returned by *matcher*\\n\\n    *fs* is a :py:class:`mrjob.fs.Filesystem`\\n\\n    *log_dir_stream* is a sequence of lists of log dirs. The idea is that\\n    there may be copies of the same logs in multiple places (e.g.\\n    on S3 and by SSHing into nodes) and we want to list them all without\\n    finding duplicate copies. This function will go through the lists of\\n    log dirs in turn, stopping if it finds any matches from a list.\\n\\n    *matcher* is a function that takes (log_path, **kwargs)\\n    and returns either None (no match) or a dictionary with information\\n    about the path (e.g. the corresponding job_id). It\\'s okay to return\\n    an empty dict.\\n    \"\"\"\\n    # wrapper for fs.ls() that turns IOErrors into warnings\\n    def _fs_ls(path):\\n        try:\\n            log.debug(\\'    listing logs in %s\\' % log_dir)\\n            if fs.exists(log_dir):\\n                for path in fs.ls(log_dir):\\n                    yield path\\n        except (IOError, OSError) as e:\\n            log.warning(\"couldn\\'t ls() %s: %r\" % (log_dir, e))\\n\\n    for log_dirs in log_dir_stream:\\n        if isinstance(log_dirs, str):\\n            raise TypeError\\n\\n        matched = False\\n\\n        for log_dir in log_dirs:\\n            matches = []\\n\\n            for path in _fs_ls(log_dir):\\n                match = matcher(path, **kwargs)\\n                if match is not None:\\n                    match[\\'path\\'] = path\\n                    matches.append(match)\\n\\n            if matches:\\n                matched = True\\n\\n                if is_spark:\\n                    matches = _sort_for_spark(matches)\\n                else:\\n                    matches = _sort_by_recency(matches)\\n\\n                for match in matches:\\n                    yield match\\n\\n        if matched:\\n            return  # e.g. don\\'t check S3 if we can get logs via SSH\\n\\n\\ndef _logs_exist(fs, path):\\n    \"\"\"Do ``fs.exists(path)``, and return ``None`` if it raises ``IOError``\"\"\"\\n    try:\\n        return fs.exists(path)\\n    except IOError:\\n        return None\\n'}, {'mrjob.util.unique': '# Copyright 2009-2016 Yelp and Contributors\\n# Copyright 2017-2018 Yelp\\n# Copyright 2019 Yelp\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Utility functions for MRJob\\n\"\"\"\\n# don\\'t add imports here that aren\\'t part of the standard Python library,\\n# since MRJobs need to run in Amazon\\'s generic EMR environment\\nimport logging\\nimport os\\nimport os.path\\nimport pipes\\nimport random\\nimport shlex\\nimport shutil\\nimport sys\\nimport tarfile\\nfrom contextlib import contextmanager\\nfrom datetime import timedelta\\nfrom distutils.spawn import find_executable\\nfrom logging import getLogger\\nfrom zipfile import ZIP_DEFLATED\\nfrom zipfile import ZIP_STORED\\nfrom zipfile import ZipFile\\nfrom zipfile import is_zipfile\\n\\n\\n\\n\\nlog = getLogger(__name__)\\n\\n\\nclass NullHandler(logging.Handler):\\n    def emit(self, record):\\n        pass\\n\\n\\ndef cmd_line(args):\\n    \"\"\"build a command line that works in a shell.\\n    \"\"\"\\n    args = [str(x) for x in args]\\n    return \\' \\'.join(pipes.quote(x) for x in args)\\n\\n\\ndef expand_path(path):\\n    \"\"\"Resolve ``~`` (home dir) and environment variables in *path*.\\n\\n    If *path* is ``None``, return ``None``.\\n    \"\"\"\\n    if path is None:\\n        return None\\n    else:\\n        return os.path.expanduser(os.path.expandvars(path))\\n\\n\\ndef file_ext(filename):\\n    \"\"\"return the file extension, including the ``.``\\n\\n    >>> file_ext(\\'foo.tar.gz\\')\\n    \\'.tar.gz\\'\\n\\n    >>> file_ext(\\'.emacs\\')\\n    \\'\\'\\n\\n    >>> file_ext(\\'.mrjob.conf\\')\\n    \\'.conf\\'\\n    \"\"\"\\n    stripped_name = filename.lstrip(\\'.\\')\\n    dot_index = stripped_name.find(\\'.\\')\\n\\n    if dot_index == -1:\\n        return \\'\\'\\n    return stripped_name[dot_index:]\\n\\n\\ndef log_to_null(name=None):\\n    \"\"\"Set up a null handler for the given stream, to suppress\\n    \"no handlers could be found\" warnings.\"\"\"\\n    logger = logging.getLogger(name)\\n    logger.addHandler(NullHandler())\\n\\n\\ndef log_to_stream(name=None, stream=None, format=None, level=None,\\n                  debug=False):\\n    \"\"\"Set up logging.\\n\\n    :type name: str\\n    :param name: name of the logger, or ``None`` for the root logger\\n    :type stream: file object\\n    :param stream:  stream to log to (default is ``sys.stderr``)\\n    :type format: str\\n    :param format: log message format (default is \\'%(message)s\\')\\n    :param level: log level to use\\n    :type debug: bool\\n    :param debug: quick way of setting the log level: if true, use\\n                  ``logging.DEBUG``, otherwise use ``logging.INFO``\\n    \"\"\"\\n    if level is None:\\n        level = logging.DEBUG if debug else logging.INFO\\n\\n    if format is None:\\n        format = \\'%(message)s\\'\\n\\n    if stream is None:\\n        stream = sys.stderr\\n\\n    handler = logging.StreamHandler(stream)\\n    handler.setLevel(level)\\n    handler.setFormatter(logging.Formatter(format))\\n\\n    logger = logging.getLogger(name)\\n    logger.setLevel(level)\\n    logger.addHandler(handler)\\n\\n\\ndef random_identifier():\\n    \"\"\"A random 16-digit hex string.\"\"\"\\n    return \\'%016x\\' % random.randint(0, 2 ** 64 - 1)\\n\\n\\n# Thanks to http://lybniz2.sourceforge.net/safeeval.html for\\n# explaining how to do this!\\ndef safeeval(expr, globals=None, locals=None):\\n    \"\"\"Like eval, but with nearly everything in the environment\\n    blanked out, so that it\\'s difficult to cause mischief.\\n\\n    *globals* and *locals* are optional dictionaries mapping names to\\n    values for those names (just like in :py:func:`eval`).\\n    \"\"\"\\n    # blank out builtins, but keep None, True, and False\\n    from mrjob.py2 import PY2\\n    safe_globals = {\\n        \\'False\\': False,\\n        \\'None\\': None,\\n        \\'True\\': True,\\n        \\'__builtin__\\': None,\\n        \\'__builtins__\\': None,\\n        \\'set\\': set\\n    }\\n\\n    # xrange is range in Python 3\\n    if PY2:\\n        safe_globals[\\'xrange\\'] = xrange\\n    else:\\n        safe_globals[\\'range\\'] = range\\n\\n    # PyPy needs special magic\\n    def open(*args, **kwargs):\\n        raise NameError(\"name \\'open\\' is not defined\")\\n    safe_globals[\\'open\\'] = open\\n\\n    # add the user-specified global variables\\n    if globals:\\n        safe_globals.update(globals)\\n\\n    return eval(expr, safe_globals, locals)\\n\\n\\n@contextmanager\\ndef save_current_environment():\\n    \"\"\" Context manager that saves os.environ and loads\\n        it back again after execution\\n    \"\"\"\\n    original_environ = os.environ.copy()\\n\\n    try:\\n        yield\\n\\n    finally:\\n        os.environ.clear()\\n        os.environ.update(original_environ)\\n\\n\\n@contextmanager\\ndef save_cwd():\\n    \"\"\"Context manager that saves the current working directory,\\n    and chdir\\'s back to it after execution.\"\"\"\\n    original_cwd = os.getcwd()\\n\\n    try:\\n        yield\\n\\n    finally:\\n        os.chdir(original_cwd)\\n\\n\\n@contextmanager\\ndef save_sys_std():\\n    \"\"\"Context manager that saves the current values of `sys.stdin`,\\n    `sys.stdout`, and `sys.stderr`, and flushes these filehandles before\\n    and after switching them out.\"\"\"\\n\\n    stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\\n\\n    try:\\n        sys.stdout.flush()\\n        sys.stderr.flush()\\n\\n        yield\\n\\n        # at this point, sys.stdout/stderr may have been patched. Don\\'t\\n        # raise an exception if flush() fails\\n        try:\\n            sys.stdout.flush()\\n        except:\\n            pass\\n\\n        try:\\n            sys.stderr.flush()\\n        except:\\n            pass\\n    finally:\\n        sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\\n\\n\\n@contextmanager\\ndef save_sys_path():\\n    \"\"\"Context manager that saves sys.path and restores it after execution.\"\"\"\\n    original_sys_path = list(sys.path)\\n\\n    try:\\n        yield\\n\\n    finally:\\n        sys.path = original_sys_path\\n\\n\\ndef shlex_split(s):\\n    \"\"\"Wrapper around shlex.split(), but convert to str if Python version <\\n    2.7.3 when unicode support was added.\\n    \"\"\"\\n    if sys.version_info < (2, 7, 3):\\n        return shlex.split(str(s))\\n    else:\\n        return shlex.split(s)\\n\\n\\ndef strip_microseconds(delta):\\n    \"\"\"Return the given :py:class:`datetime.timedelta`, without microseconds.\\n\\n    Useful for printing :py:class:`datetime.timedelta` objects.\\n    \"\"\"\\n    return timedelta(delta.days, delta.seconds)\\n\\n\\ndef to_lines(chunks):\\n    \"\"\"Take in data as a sequence of bytes, and yield it, one line at a time.\\n\\n    Only breaks lines on ``\\\\\\\\n`` (not ``\\\\\\\\r``), and does not add\\n    a trailing newline.\\n\\n    For efficiency, passes through anything with a ``readline()`` attribute.\\n    \"\"\"\\n    # hopefully this is good enough for anything mrjob will encounter\\n    if hasattr(chunks, \\'readline\\'):\\n        return chunks\\n    else:\\n        return _to_lines(chunks)\\n\\n\\ndef _to_lines(chunks):\\n    \"\"\"Take in data as a sequence of bytes, and yield it, one line at a time.\\n\\n    Only breaks lines on ``\\\\\\\\n`` (not ``\\\\\\\\r``), and does not add\\n    a trailing newline.\\n\\n    Exception: if we encounter an empty bytestring ``b\\'\\'``, immediately yield\\n    what we have so far rather than joining it to the next chunk. This allows\\n    us to handle bytes from multiple files without joining the end of one\\n    file to the beginning of the next one.\\n\\n    Optimizes for:\\n\\n    * chunks bigger than lines (e.g. reading test files)\\n    * chunks that are lines (idempotency)\\n    \"\"\"\\n    # list of chunks with no final newline\\n    leftovers = []\\n\\n    for chunk in chunks:\\n        # special case for b\\'\\' standing for EOF\\n        if chunk == b\\'\\':\\n            if leftovers:\\n                yield b\\'\\'.join(leftovers)\\n                leftovers = []\\n\\n            continue\\n\\n        start = 0\\n\\n        while start < len(chunk):\\n            end = chunk.find(b\\'\\\\n\\', start) + 1\\n\\n            if end == 0:  # no newlines found\\n                leftovers.append(chunk[start:])\\n                break\\n\\n            if leftovers:\\n                leftovers.append(chunk[start:end])\\n                yield b\\'\\'.join(leftovers)\\n                leftovers = []\\n            else:\\n                yield chunk[start:end]\\n\\n            start = end\\n\\n    if leftovers:\\n        yield b\\'\\'.join(leftovers)\\n\\n\\ndef unique(items):\\n    \"\"\"Yield items from *item* in order, skipping duplicates.\"\"\"\\n    seen = set()\\n\\n    for item in items:\\n        if item in seen:\\n            continue\\n        else:\\n            yield item\\n            seen.add(item)\\n\\n\\ndef unarchive(archive_path, dest):\\n    \"\"\"Extract the contents of a tar or zip file at *archive_path* into the\\n    directory *dest*.\\n\\n    :type archive_path: str\\n    :param archive_path: path to archive file\\n    :type dest: str\\n    :param dest: path to directory where archive will be extracted\\n\\n    *dest* will be created if it doesn\\'t already exist.\\n\\n    tar files can be gzip compressed, bzip2 compressed, or uncompressed. Files\\n    within zip files can be deflated or stored.\\n    \"\"\"\\n    if tarfile.is_tarfile(archive_path):\\n        with tarfile.open(archive_path, \\'r\\') as archive:\\n            archive.extractall(dest)\\n    elif is_zipfile(archive_path):\\n        with ZipFile(archive_path, \\'r\\') as archive:\\n            for name in archive.namelist():\\n                # the zip spec specifies that front slashes are always\\n                # used as directory separators\\n                dest_path = os.path.join(dest, *name.split(\\'/\\'))\\n\\n                # now, split out any dirname and filename and create\\n                # one and/or the other\\n                dirname, filename = os.path.split(dest_path)\\n                if dirname and not os.path.exists(dirname):\\n                    os.makedirs(dirname)\\n                if filename:\\n                    with open(dest_path, \\'wb\\') as dest_file:\\n                        dest_file.write(archive.read(name))\\n    else:\\n        raise IOError(\\'Unknown archive type: %s\\' % (archive_path,))\\n\\n\\ndef which(cmd, path=None):\\n    \"\"\"Like the UNIX which command: search in *path* for the executable named\\n    *cmd*. *path* defaults to :envvar:`PATH`. Returns ``None`` if no\\n    such executable found.\\n\\n    This is basically ``shutil.which()`` (which was introduced in Python 3.3)\\n    without the *mode* argument. Best practice is to always specify *path*\\n    as a keyword argument.\\n    \"\"\"\\n    if hasattr(shutil, \\'which\\'):\\n        # added in Python 3.3\\n        return shutil.which(cmd, path=path)\\n    elif path is None and os.environ.get(\\'PATH\\') is None:\\n        # find_executable() errors if neither path nor $PATH is set\\n        return None\\n    else:\\n        return find_executable(cmd, path=path)\\n\\n\\ndef zip_dir(dir, out_path, filter=None, prefix=\\'\\'):\\n    \"\"\"Compress the given *dir* into a zip file at *out_path*.\\n\\n    If we encounter symlinks, include the actual file, not the symlink.\\n\\n    :type dir: str\\n    :param dir: dir to tar up\\n    :type out_path: str\\n    :param out_path: where to write the tarball too\\n    :param filter: if defined, a function that takes paths (relative to *dir*\\n                   and returns ``True`` if we should keep them\\n    :type prefix: str\\n    :param prefix: subdirectory inside the tarball to put everything into (e.g.\\n                   ``\\'mrjob\\'``)\\n    \"\"\"\\n    if not os.path.isdir(dir):\\n        raise IOError(\\'Not a directory: %r\\' % (dir,))\\n\\n    if not filter:\\n        filter = lambda path: True\\n\\n    with _create_zip_file(out_path) as zip_file:\\n        for dirpath, dirnames, filenames in os.walk(dir, followlinks=True):\\n            for filename in filenames:\\n                path = os.path.join(dirpath, filename)\\n                rel_path = os.path.relpath(path, dir)\\n\\n                if filter(rel_path):\\n                    # copy over real files, not symlinks\\n                    real_path = os.path.realpath(path)\\n                    path_in_zip_file = os.path.join(prefix, rel_path)\\n                    zip_file.write(real_path, arcname=path_in_zip_file)\\n\\n\\n# this is also used by spark runner\\ndef _create_zip_file(path):\\n    try:\\n        return ZipFile(path, mode=\\'w\\', compression=ZIP_DEFLATED)\\n    except RuntimeError:  # zlib not available\\n        return ZipFile(path, mode=\\'w\\', compression=ZIP_STORED)\\n'}]", "test_list": ["def test_output_dir(self):\n    output_dir = 'hdfs:///path/to/output'\n    self.runner._hadoop_log_dirs.return_value = [output_dir]\n    results = self.runner._stream_history_log_dirs(output_dir=output_dir)\n    self.assertEqual(next(results), [output_dir])\n    self.runner._hadoop_log_dirs.assert_called_with(output_dir=output_dir)\n    self.assertRaises(StopIteration, next, results)", "def test_basic(self):\n    self.runner._hadoop_log_dirs.return_value = ['/mnt/var/logs/hadoop', 'hdfs:///logs']\n    results = self.runner._stream_history_log_dirs()\n    self.assertFalse(self.log.info.called)\n    self.assertEqual(next(results), ['/mnt/var/logs/hadoop'])\n    self.assertEqual(self.log.info.call_count, 1)\n    self.assertIn('/mnt/var/logs/hadoop', self.log.info.call_args[0][0])\n    self.assertEqual(next(results), ['hdfs:///logs'])\n    self.assertEqual(self.log.info.call_count, 2)\n    self.assertIn('hdfs:///logs', self.log.info.call_args[0][0])\n    self.assertRaises(StopIteration, next, results)", "def test_io_error_from_fs_exists(self):\n    self.runner._hadoop_log_dirs.return_value = ['hdfs:///tmp/output/_logs']\n    self.runner.fs.exists.side_effect = IOError\n    results = self.runner._stream_history_log_dirs()\n    self.assertRaises(StopIteration, next, results)", "def test_empty(self):\n    results = self.runner._stream_history_log_dirs()\n    self.assertFalse(self.log.info.called)\n    self.assertRaises(StopIteration, next, results)", "def test_no_read_logs(self):\n    self.runner._opts['read_logs'] = False\n    self.runner._hadoop_log_dirs.return_value = ['/mnt/var/logs/hadoop', 'hdfs:///logs']\n    results = self.runner._stream_history_log_dirs()\n    self.assertRaises(StopIteration, next, results)\n    self.assertFalse(self.log.info.called)\n    self.assertFalse(self.runner._hadoop_log_dirs.called)"], "requirements": {"Input-Output Conditions": {"requirement": "The function '_stream_history_log_dirs' should accept 'output_dir' as a string or None and return a generator yielding lists of directories as strings.", "unit_test": ["def test_output_dir_type(self):\n    output_dir = 'hdfs:///path/to/output'\n    results = self.runner._stream_history_log_dirs(output_dir=output_dir)\n    self.assertIsInstance(next(results), list)\n    self.assertIsInstance(next(results)[0], str)\n    self.assertRaises(StopIteration, next, results)"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_output_dir_type"}, "Exception Handling": {"requirement": "The function '_stream_history_log_dirs' should handle IOError exceptions gracefully when checking if a directory exists.", "unit_test": ["def test_io_error_handling(self):\n    self.runner._hadoop_log_dirs.return_value = ['hdfs:///tmp/output/_logs']\n    self.runner.fs.exists.side_effect = IOError\n    results = self.runner._stream_history_log_dirs()\n    self.assertRaises(StopIteration, next, results)"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_io_error_handling"}, "Edge Case Handling": {"requirement": "The function '_stream_history_log_dirs' should handle the case where 'output_dir' is None and still function correctly.", "unit_test": ["def test_none_output_dir(self):\n    self.runner._hadoop_log_dirs.return_value = ['/mnt/var/logs/hadoop', 'hdfs:///logs']\n    results = self.runner._stream_history_log_dirs(output_dir=None)\n    self.assertEqual(next(results), ['/mnt/var/logs/hadoop'])\n    self.assertEqual(next(results), ['hdfs:///logs'])\n    self.assertRaises(StopIteration, next, results)"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_none_output_dir"}, "Functionality Extension": {"requirement": "Extend the function '_stream_history_log_dirs' to log a warning message: 'No directories found' if no directories are yielded.", "unit_test": ["def test_no_directories_warning(self):\n    self.runner._hadoop_log_dirs.return_value = []\n    results = self.runner._stream_history_log_dirs()\n    self.assertRaises(StopIteration, next, results)\n    self.assertTrue(self.log.warning.called)\n    self.assertIn('No directories found', self.log.warning.call_args[0][0])"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_no_directories_warning"}, "Annotation Coverage": {"requirement": "Ensure that the function '_stream_history_log_dirs' has complete parameter and return type annotations.", "unit_test": ["def test_function_annotations(self):\n    from typing import Generator\n    annotations = self.runner._stream_history_log_dirs.__annotations__\n    self.assertIn('output_dir', annotations)\n    self.assertEqual(annotations['output_dir'], str)\n    self.assertIn('return', annotations)\n    self.assertEqual(annotations['return'], Generator)"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_function_annotations"}, "Code Complexity": {"requirement": "The function '_stream_history_log_dirs' should maintain a cyclomatic complexity of 5 or less.", "unit_test": ["def test_cyclomatic_complexity(self):\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(self.runner._stream_history_log_dirs)\n    complexity = cc_visit(source_code)\n    self.assertLessEqual(complexity[0].complexity, 5)"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_cyclomatic_complexity"}, "Code Standard": {"requirement": "Ensure the function '_stream_history_log_dirs' follows PEP 8 guidelines, including proper indentation and spacing.", "unit_test": ["def test_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/your/module.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that the function '_stream_history_log_dirs' uses the 'mrjob.hadoop.HadoopJobRunner._hadoop_log_dirs','mrjob.hadoop.HadoopJobRunner.fs','mrjob.hadoop.log','mrjob.logs.mixin.LogInterpretationMixin._read_logs','mrjob.logs.wrap._logs_exist'and 'mrjob.util.unique'.", "unit_test": ["def test_stream_history_log_dirs_method_calls(self):\n    self.runner._hadoop_log_dirs = mock.Mock(return_value=['/mnt/var/logs/hadoop'])\n    results = self.runner._stream_history_log_dirs()\n    self.runner._hadoop_log_dirs.assert_called_once()\n    self.assertEqual(next(results), ['/mnt/var/logs/hadoop'])"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_stream_history_log_dirs_method_calls"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the '_stream_history_log_dirs' method is correctly use the 'mrjob.hadoop.HadoopJobRunner._hadoop_log_dirs','mrjob.hadoop.HadoopJobRunner.fs','mrjob.hadoop.log','mrjob.logs.mixin.LogInterpretationMixin._read_logs','mrjob.logs.wrap._logs_exist'and 'mrjob.util.unique'", "unit_test": ["def test_correct_hadoop_log_dirs_usage(self):\n    self.runner._hadoop_log_dirs.return_value = ['/mnt/var/logs/hadoop', 'hdfs:///logs']\n    results = self.runner._stream_history_log_dirs()\n    self.assertEqual(next(results), ['/mnt/var/logs/hadoop'])\n    self.assertEqual(next(results), ['hdfs:///logs'])\n    self.assertRaises(StopIteration, next, results)"], "test": "tests/test_hadoop.py::StreamHistoryLogDirsTestCase::test_correct_hadoop_log_dirs_usage"}}}
{"namespace": "falcon.http_error.HTTPError.to_json", "type": "method", "project_path": "Internet/falcon", "completion_path": "Internet/falcon/falcon/http_error.py", "signature_position": [178, 178], "body_position": [191, 195], "dependency": {"intra_class": ["falcon.http_error.HTTPError.to_dict"], "intra_file": ["falcon.http_error._DEFAULT_JSON_HANDLER"], "cross_file": ["falcon.constants.MEDIA_JSON", "falcon.media.json.JSONHandler.serialize"]}, "requirement": {"Functionality": "This function converts the HTTPError instance into a JSON representation. It takes an optional handler object to customize the serialization process. If no handler is provided, a default handler using the built-in JSON library is used.", "Arguments": ":param self: HTTPError. An instance of the HTTPError class.\n:param handler: Handler object. An optional handler object that will be used to serialize the representation of this error to JSON. Defaults to None.\n:return: bytes. A JSON document representing the error."}, "tests": ["tests/test_httperror.py::TestHTTPError::test_to_json_dumps"], "indent": 8, "domain": "Internet", "code": "    def to_json(self, handler=None):\n        \"\"\"Return a JSON representation of the error.\n\n        Args:\n            handler: Handler object that will be used to serialize the representation\n                of this error to JSON. When not provided, a default handler using\n                the builtin JSON library will be used (default ``None``).\n\n        Returns:\n            bytes: A JSON document for the error.\n\n        \"\"\"\n\n        from falcon.constants import MEDIA_JSON\n        obj = self.to_dict(OrderedDict)\n        if handler is None:\n            handler = _DEFAULT_JSON_HANDLER\n        return handler.serialize(obj, MEDIA_JSON)\n", "intra_context": "# Copyright 2013 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"HTTPError exception class.\"\"\"\n\nfrom collections import OrderedDict\nimport xml.etree.ElementTree as et\n\n\nfrom falcon.util import uri\nfrom falcon.util.deprecation import deprecated\nfrom falcon.util.deprecation import deprecated_args\n\n\nclass HTTPError(Exception):\n    \"\"\"Represents a generic HTTP error.\n\n    Raise an instance or subclass of ``HTTPError`` to have Falcon return\n    a formatted error response and an appropriate HTTP status code\n    to the client when something goes wrong. JSON and XML media types\n    are supported by default.\n\n    To customize the error presentation, implement a custom error\n    serializer and set it on the :class:`~.App` instance via\n    :meth:`~.App.set_error_serializer`.\n\n    To customize what data is passed to the serializer, subclass\n    ``HTTPError`` and override the ``to_dict()`` method (``to_json()``\n    is implemented via ``to_dict()``). To also support XML, override\n    the ``to_xml()`` method.\n\n    Note:\n        ``status`` is the only positional argument allowed, the other\n        arguments should be used as keyword only. Using them as positional\n        arguments will raise a deprecation warning and will result in an\n        error in a future version of falcon.\n\n    Args:\n        status (str): HTTP status code and text, such as \"400 Bad Request\"\n\n    Keyword Args:\n        title (str): Human-friendly error title. If not provided, defaults\n            to the HTTP status line as determined by the ``status`` argument.\n        description (str): Human-friendly description of the error, along with\n            a helpful suggestion or two (default ``None``).\n        headers (dict or list): A ``dict`` of header names and values\n            to set, or a ``list`` of (*name*, *value*) tuples. Both *name* and\n            *value* must be of type ``str`` or ``StringType``, and only\n            character values 0x00 through 0xFF may be used on platforms that\n            use wide characters.\n\n            Note:\n                The Content-Type header, if present, will be overridden. If\n                you wish to return custom error messages, you can create\n                your own HTTP error class, and install an error handler\n                to convert it into an appropriate HTTP response for the\n                client\n\n            Note:\n                Falcon can process a list of ``tuple`` slightly faster\n                than a ``dict``.\n\n        href (str): A URL someone can visit to find out more information\n            (default ``None``). Unicode characters are percent-encoded.\n        href_text (str): If href is given, use this as the friendly\n            title/description for the link (default 'App documentation\n            for this error').\n        code (int): An internal code that customers can reference in their\n            support request or to help them when searching for knowledge\n            base articles related to this error (default ``None``).\n\n    Attributes:\n        status (str): HTTP status line, e.g. '748 Confounded by Ponies'.\n        title (str): Error title to send to the client.\n        description (str): Description of the error to send to the client.\n        headers (dict): Extra headers to add to the response.\n        link (str): An href that the client can provide to the user for\n            getting help.\n        code (int): An internal application code that a user can reference when\n            requesting support for the error.\n    \"\"\"\n\n    __slots__ = (\n        'status',\n        'title',\n        'description',\n        'headers',\n        'link',\n        'code',\n    )\n\n    @deprecated_args(allowed_positional=1)\n    def __init__(\n        self,\n        status,\n        title=None,\n        description=None,\n        headers=None,\n        href=None,\n        href_text=None,\n        code=None,\n    ):\n        self.status = status\n\n        # TODO(kgriffs): HTTP/2 does away with the \"reason phrase\". Eventually\n        #   we'll probably switch over to making everything code-based to more\n        #   easily support HTTP/2. When that happens, should we continue to\n        #   include the reason phrase in the title?\n        self.title = title or status\n\n        self.description = description\n        self.headers = headers\n        self.code = code\n\n        if href:\n            link = self.link = OrderedDict()\n            link['text'] = href_text or 'Documentation related to this error'\n            link['href'] = uri.encode(href)\n            link['rel'] = 'help'\n        else:\n            self.link = None\n\n    def __repr__(self):\n        return '<%s: %s>' % (self.__class__.__name__, self.status)\n\n    __str__ = __repr__\n\n    @property  # type: ignore\n    @deprecated(\n        'has_representation is deprecated and is currently unused by falcon',\n        is_property=True,\n    )\n    def has_representation(self):\n        return True\n\n    def to_dict(self, obj_type=dict):\n        \"\"\"Return a basic dictionary representing the error.\n\n        This method can be useful when serializing the error to hash-like\n        media types, such as YAML, JSON, and MessagePack.\n\n        Args:\n            obj_type: A dict-like type that will be used to store the\n                error information (default ``dict``).\n\n        Returns:\n            dict: A dictionary populated with the error's title,\n            description, etc.\n\n        \"\"\"\n\n        obj = obj_type()\n\n        obj['title'] = self.title\n\n        if self.description is not None:\n            obj['description'] = self.description\n\n        if self.code is not None:\n            obj['code'] = self.code\n\n        if self.link is not None:\n            obj['link'] = self.link\n\n        return obj\n\n###The function: to_json###\n    def to_xml(self):\n        \"\"\"Return an XML-encoded representation of the error.\n\n        Returns:\n            bytes: An XML document for the error.\n\n        \"\"\"\n\n        error_element = et.Element('error')\n\n        et.SubElement(error_element, 'title').text = self.title\n\n        if self.description is not None:\n            et.SubElement(error_element, 'description').text = self.description\n\n        if self.code is not None:\n            et.SubElement(error_element, 'code').text = str(self.code)\n\n        if self.link is not None:\n            link_element = et.SubElement(error_element, 'link')\n\n            for key in ('text', 'href', 'rel'):\n                et.SubElement(link_element, key).text = self.link[key]\n\n        return b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>' + et.tostring(\n            error_element, encoding='utf-8'\n        )\n\n\n# NOTE: initialized in falcon.media.json, that is always imported since Request/Respose\n# are imported by falcon init.\n_DEFAULT_JSON_HANDLER = None\n\n\nclass NoRepresentation:\n    \"\"\"Mixin for ``HTTPError`` child classes that have no representation.\n\n    This class can be mixed in when inheriting from ``HTTPError``, in order\n    to override the `has_representation` property such that it always\n    returns ``False``. This, in turn, will cause Falcon to return an empty\n    response body to the client.\n\n    You can use this mixin when defining errors that either should not have\n    a body (as dictated by HTTP standards or common practice), or in the\n    case that a detailed error response may leak information to an attacker.\n\n    Note:\n        This mixin class must appear before ``HTTPError`` in the base class\n        list when defining the child; otherwise, it will not override the\n        `has_representation` property as expected.\n\n    Warning:\n        As of Falcon 3.0, this mixin class is no longer used since all Falcon\n        errors have a representation. This class is considered deprecated and\n        will be removed in a future release.\n    \"\"\"\n\n    @property  # type: ignore\n    @deprecated(\n        'has_representation is deprecated and is currently unused by falcon. '\n        'The class NoRepresentation is deprecated and will be removed in a '\n        'future release',\n        is_property=True,\n    )\n    def has_representation(self):\n        return False\n\n\nclass OptionalRepresentation:\n    \"\"\"Mixin for ``HTTPError`` child classes that may have a representation.\n\n    This class can be mixed in when inheriting from ``HTTPError`` in order\n    to override the `has_representation` property, such that it will\n    return ``False`` when the error instance has no description\n    (i.e., the `description` kwarg was not set).\n\n    You can use this mixin when defining errors that do not include\n    a body in the HTTP response by default, serializing details only when\n    the web developer provides a description of the error.\n\n    Note:\n        This mixin class must appear before ``HTTPError`` in the base class\n        list when defining the child; otherwise, it will not override the\n        `has_representation` property as expected.\n\n    Warning:\n        As of Falcon 3.0, this mixin class is no longer used since all Falcon\n        errors have a representation. This class is considered deprecated and\n        will be removed in a future release.\n    \"\"\"\n\n    @property  # type: ignore\n    @deprecated(\n        'has_representation is deprecated and is currently unused by falcon. '\n        'The class OptionalRepresentation is deprecated and will be removed '\n        'in a future release',\n        is_property=True,\n    )\n    def has_representation(self):\n        return self.description is not None\n", "cross_context": [{"falcon.constants.MEDIA_JSON": "from enum import Enum\nimport os\nimport sys\n\n\nPYPY = sys.implementation.name == 'pypy'\n\"\"\"Evaluates to ``True`` when the current Python implementation is PyPy.\"\"\"\n\nPYTHON_VERSION = tuple(sys.version_info[:3])\n\"\"\"Python version information triplet: (major, minor, micro).\"\"\"\n# If FALCON_TESTING_MOCK_PY35 is defined in the env, pretend that we are\n# on 3.5.0. Only intended for testing of the framework itself.\n_is_py35 = PYTHON_VERSION[:2] == (3, 5)\n_mock_py35 = os.environ.get('FALCON_TESTING_MOCK_PY35')\n\n# TODO(vytas): Remove these hacks in 4.0 once we have dropped 3.5/3.6 support.\n# NOTE(vytas): Do not alter the microversion if already on 3.5\n#   (read: a hack to hit coverage on 3.5 as well).\nif _mock_py35 or _is_py35:\n    PYTHON_VERSION = (3, 5, PYTHON_VERSION[-1] if _is_py35 else 0)\n\nASGI_SUPPORTED = PYTHON_VERSION >= (3, 6)\n\"\"\"Evaluates to ``True`` when ASGI is supported for the current Python version.\"\"\"\n\n# RFC 7231, 5789 methods\nHTTP_METHODS = [\n    'CONNECT',\n    'DELETE',\n    'GET',\n    'HEAD',\n    'OPTIONS',\n    'PATCH',\n    'POST',\n    'PUT',\n    'TRACE',\n]\n\n# RFC 2518 and 4918 methods\nWEBDAV_METHODS = [\n    'CHECKIN',\n    'CHECKOUT',\n    'COPY',\n    'LOCK',\n    'MKCOL',\n    'MOVE',\n    'PROPFIND',\n    'PROPPATCH',\n    'REPORT',\n    'UNCHECKIN',\n    'UNLOCK',\n    'UPDATE',\n    'VERSION-CONTROL',\n]\n\n# if FALCON_CUSTOM_HTTP_METHODS is defined, treat it as a comma-\n# delimited string of additional supported methods in this env.\nFALCON_CUSTOM_HTTP_METHODS = [\n    method.strip().upper()\n    for method in os.environ.get('FALCON_CUSTOM_HTTP_METHODS', '').split(',')\n    if method.strip() != ''\n]\n\n_META_METHODS = [\n    'WEBSOCKET',\n]\n\nCOMBINED_METHODS = (\n    HTTP_METHODS + WEBDAV_METHODS + FALCON_CUSTOM_HTTP_METHODS + _META_METHODS\n)\n\n# NOTE(kgriffs): According to RFC 7159, most JSON parsers assume\n# UTF-8 and so it is the recommended default charset going forward,\n# and indeed, other charsets should not be specified to ensure\n# maximum interoperability.\nMEDIA_JSON = 'application/json'\n\n# NOTE(kgriffs): An internet media type for MessagePack has not\n# yet been registered. 'application/x-msgpack' is commonly used,\n# but the use of the 'x-' prefix is discouraged by RFC 6838.\nMEDIA_MSGPACK = 'application/msgpack'\n\nMEDIA_MULTIPART = 'multipart/form-data'\n\nMEDIA_URLENCODED = 'application/x-www-form-urlencoded'\n\n# NOTE(kgriffs): An internet media type for YAML has not been\n# registered. RoR uses 'application/x-yaml', but since use of\n# 'x-' is discouraged by RFC 6838, we don't use it in Falcon.\n#\n# The YAML specification requires that parsers deduce the character\n# encoding by examining the first few bytes of the document itself.\n# Therefore, it does not make sense to include the charset in the\n# media type string.\nMEDIA_YAML = 'application/yaml'\n\n# NOTE(kgriffs): According to RFC 7303, when the charset is\n# omitted, preference is given to the encoding specified in the\n# document itself (either via a BOM, or via the XML declaration). If\n# the document does not explicitly specify the encoding, UTF-8 is\n# assumed. We do not specify the charset here, because many parsers\n# ignore it anyway and just use what is specified in the document,\n# contrary to the RFCs.\nMEDIA_XML = 'application/xml'\n\n# NOTE(kgriffs): RFC 4329 recommends application/* over text/.\n# futhermore, parsers are required to respect the Unicode\n# encoding signature, if present in the document, and to default\n# to UTF-8 when not present. Note, however, that implementations\n# are not required to support anything besides UTF-8, so it is\n# unclear how much utility an encoding signature (or the charset\n# parameter for that matter) has in practice.\nMEDIA_JS = 'application/javascript'\n\n# NOTE(kgriffs): According to RFC 6838, most text media types should\n# include the charset parameter.\nMEDIA_HTML = 'text/html; charset=utf-8'\nMEDIA_TEXT = 'text/plain; charset=utf-8'\n\nMEDIA_JPEG = 'image/jpeg'\nMEDIA_PNG = 'image/png'\nMEDIA_GIF = 'image/gif'\n\nDEFAULT_MEDIA_TYPE = MEDIA_JSON\n\n# NOTE(kgriffs): We do not expect more than one of these in the request\nSINGLETON_HEADERS = frozenset(\n    [\n        'content-length',\n        'content-type',\n        'cookie',\n        'expect',\n        'from',\n        'host',\n        'max-forwards',\n        'referer',\n        'user-agent',\n    ]\n)\n\n# NOTE(kgriffs): Special singleton to be used internally whenever using\n#   None would be ambiguous.\n_UNSET = object()\n\nWebSocketPayloadType = Enum('WebSocketPayloadType', 'TEXT BINARY')\n\"\"\"Enum representing the two possible WebSocket payload types.\"\"\"\n"}, {"falcon.media.json.JSONHandler.serialize": "from functools import partial\nimport json\n\nfrom falcon import errors\nfrom falcon import http_error\nfrom falcon.media.base import BaseHandler\nfrom falcon.media.base import TextBaseHandlerWS\n\n\nclass JSONHandler(BaseHandler):\n    \"\"\"JSON media handler.\n\n    This handler uses Python's standard :py:mod:`json` library by default, but\n    can be easily configured to use any of a number of third-party JSON\n    libraries, depending on your needs. For example, you can often\n    realize a significant performance boost under CPython by using an\n    alternative library. Good options in this respect include `orjson`,\n    `python-rapidjson`, and `mujson`.\n\n    This handler will raise a :class:`falcon.MediaNotFoundError` when attempting\n    to parse an empty body, or a :class:`falcon.MediaMalformedError`\n    if an error happens while parsing the body.\n\n    Note:\n        If you are deploying to PyPy, we recommend sticking with the standard\n        library's JSON implementation, since it will be faster in most cases\n        as compared to a third-party library.\n\n    .. rubric:: Custom JSON library\n\n    You can replace the default JSON handler by using a custom JSON library\n    (see also: :ref:`custom_media_handlers`). Overriding the default JSON\n    implementation is simply a matter of specifying the desired ``dumps`` and\n    ``loads`` functions::\n\n        import falcon\n        from falcon import media\n\n        import rapidjson\n\n        json_handler = media.JSONHandler(\n            dumps=rapidjson.dumps,\n            loads=rapidjson.loads,\n        )\n        extra_handlers = {\n            'application/json': json_handler,\n        }\n\n        app = falcon.App()\n        app.req_options.media_handlers.update(extra_handlers)\n        app.resp_options.media_handlers.update(extra_handlers)\n\n    .. rubric:: Custom serialization parameters\n\n    Even if you decide to stick with the stdlib's :any:`json.dumps` and\n    :any:`json.loads`, you can wrap them using :any:`functools.partial` to\n    provide custom serialization or deserialization parameters supported by the\n    ``dumps`` and ``loads`` functions, respectively\n    (see also: :ref:`prettifying-json-responses`)::\n\n        import falcon\n        from falcon import media\n\n        from functools import partial\n\n        json_handler = media.JSONHandler(\n            dumps=partial(\n                json.dumps,\n                default=str,\n                sort_keys=True,\n            ),\n        )\n        extra_handlers = {\n            'application/json': json_handler,\n        }\n\n        app = falcon.App()\n        app.req_options.media_handlers.update(extra_handlers)\n        app.resp_options.media_handlers.update(extra_handlers)\n\n    By default, ``ensure_ascii`` is passed to the ``json.dumps`` function.\n    If you override the ``dumps`` function, you will need to explicitly set\n    ``ensure_ascii`` to ``False`` in order to enable the serialization of\n    Unicode characters to UTF-8. This is easily done by using\n    :any:`functools.partial` to apply the desired keyword argument. As also\n    demonstrated in the previous paragraph, you can use this same technique to\n    customize any option supported by the ``dumps`` and ``loads`` functions::\n\n        from functools import partial\n\n        from falcon import media\n        import rapidjson\n\n        json_handler = media.JSONHandler(\n            dumps=partial(\n                rapidjson.dumps,\n                ensure_ascii=False, sort_keys=True\n            ),\n        )\n\n    .. _custom-media-json-encoder:\n\n    .. rubric:: Custom JSON encoder\n\n    You can also override the default :class:`~json.JSONEncoder` by using a\n    custom Encoder and updating the media handlers for ``application/json``\n    type to use that::\n\n        import json\n        from datetime import datetime\n        from functools import partial\n\n        import falcon\n        from falcon import media\n\n        class DatetimeEncoder(json.JSONEncoder):\n            \\\"\\\"\\\"Json Encoder that supports datetime objects.\\\"\\\"\\\"\n\n            def default(self, obj):\n                if isinstance(obj, datetime):\n                    return obj.isoformat()\n                return super().default(obj)\n\n        app = falcon.App()\n\n        json_handler = media.JSONHandler(\n            dumps=partial(json.dumps, cls=DatetimeEncoder),\n        )\n        extra_handlers = {\n            'application/json': json_handler,\n        }\n\n        app.req_options.media_handlers.update(extra_handlers)\n        app.resp_options.media_handlers.update(extra_handlers)\n\n\n    Keyword Arguments:\n        dumps (func): Function to use when serializing JSON responses.\n        loads (func): Function to use when deserializing JSON requests.\n    \"\"\"\n\n    def __init__(self, dumps=None, loads=None):\n        self._dumps = dumps or partial(json.dumps, ensure_ascii=False)\n        self._loads = loads or json.loads\n\n        # PERF(kgriffs): Test dumps once up front so we can set the\n        #     proper serialize implementation.\n        result = self._dumps({'message': 'Hello World'})\n        if isinstance(result, str):\n            self.serialize = self._serialize_s\n            self.serialize_async = self._serialize_async_s\n        else:\n            self.serialize = self._serialize_b\n            self.serialize_async = self._serialize_async_b\n\n        # NOTE(kgriffs): To be safe, only enable the optimized protocol when\n        #   not subclassed.\n        if type(self) is JSONHandler:\n            self._serialize_sync = self.serialize\n            self._deserialize_sync = self._deserialize\n\n    def _deserialize(self, data):\n        if not data:\n            raise errors.MediaNotFoundError('JSON')\n        try:\n            return self._loads(data.decode())\n        except ValueError as err:\n            raise errors.MediaMalformedError('JSON') from err\n\n    def deserialize(self, stream, content_type, content_length):\n        return self._deserialize(stream.read())\n\n    async def deserialize_async(self, stream, content_type, content_length):\n        return self._deserialize(await stream.read())\n\n    # NOTE(kgriffs): Make content_type a kwarg to support the\n    #   Request.render_body() shortcut optimization.\n    def _serialize_s(self, media, content_type=None) -> bytes:\n        return self._dumps(media).encode()\n\n    async def _serialize_async_s(self, media, content_type) -> bytes:\n        return self._dumps(media).encode()\n\n    # NOTE(kgriffs): Make content_type a kwarg to support the\n    #   Request.render_body() shortcut optimization.\n    def _serialize_b(self, media, content_type=None) -> bytes:\n        return self._dumps(media)\n\n    async def _serialize_async_b(self, media, content_type) -> bytes:\n        return self._dumps(media)\n\n\nclass JSONHandlerWS(TextBaseHandlerWS):\n    \"\"\"WebSocket media handler for de(serializing) JSON to/from TEXT payloads.\n\n    This handler uses Python's standard :py:mod:`json` library by default, but\n    can be easily configured to use any of a number of third-party JSON\n    libraries, depending on your needs. For example, you can often\n    realize a significant performance boost under CPython by using an\n    alternative library. Good options in this respect include `orjson`,\n    `python-rapidjson`, and `mujson`.\n\n    Note:\n        If you are deploying to PyPy, we recommend sticking with the standard\n        library's JSON implementation, since it will be faster in most cases\n        as compared to a third-party library.\n\n    Overriding the default JSON implementation is simply a matter of specifying\n    the desired ``dumps`` and ``loads`` functions::\n\n        import falcon\n        from falcon import media\n\n        import rapidjson\n\n        json_handler = media.JSONHandlerWS(\n            dumps=rapidjson.dumps,\n            loads=rapidjson.loads,\n        )\n\n        app = falcon.asgi.App()\n        app.ws_options.media_handlers[falcon.WebSocketPayloadType.TEXT] = json_handler\n\n    By default, ``ensure_ascii`` is passed to the ``json.dumps`` function.\n    If you override the ``dumps`` function, you will need to explicitly set\n    ``ensure_ascii`` to ``False`` in order to enable the serialization of\n    Unicode characters to UTF-8. This is easily done by using\n    :any:`functools.partial` to apply the desired keyword argument. In fact, you\n    can use this same technique to customize any option supported by the\n    ``dumps`` and ``loads`` functions::\n\n        from functools import partial\n\n        from falcon import media\n        import rapidjson\n\n        json_handler = media.JSONHandlerWS(\n            dumps=partial(\n                rapidjson.dumps,\n                ensure_ascii=False, sort_keys=True\n            ),\n        )\n\n    Keyword Arguments:\n        dumps (func): Function to use when serializing JSON.\n        loads (func): Function to use when deserializing JSON.\n    \"\"\"\n\n    __slots__ = ['dumps', 'loads']\n\n    def __init__(self, dumps=None, loads=None):\n        self._dumps = dumps or partial(json.dumps, ensure_ascii=False)\n        self._loads = loads or json.loads\n\n    def serialize(self, media: object) -> str:\n        return self._dumps(media)\n\n    def deserialize(self, payload: str) -> object:\n        return self._loads(payload)\n\n\nhttp_error._DEFAULT_JSON_HANDLER = _DEFAULT_JSON_HANDLER = JSONHandler()  # type: ignore\n"}], "prompt": "Please write a python function called 'to_json' base the context. This function converts the HTTPError instance into a JSON representation. It takes an optional handler object to customize the serialization process. If no handler is provided, a default handler using the built-in JSON library is used.:param self: HTTPError. An instance of the HTTPError class.\n:param handler: Handler object. An optional handler object that will be used to serialize the representation of this error to JSON. Defaults to None.\n:return: bytes. A JSON document representing the error..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # Copyright 2013 by Rackspace Hosting, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"HTTPError exception class.\"\"\"\n\nfrom collections import OrderedDict\nimport xml.etree.ElementTree as et\n\n\nfrom falcon.util import uri\nfrom falcon.util.deprecation import deprecated\nfrom falcon.util.deprecation import deprecated_args\n\n\nclass HTTPError(Exception):\n    \"\"\"Represents a generic HTTP error.\n\n    Raise an instance or subclass of ``HTTPError`` to have Falcon return\n    a formatted error response and an appropriate HTTP status code\n    to the client when something goes wrong. JSON and XML media types\n    are supported by default.\n\n    To customize the error presentation, implement a custom error\n    serializer and set it on the :class:`~.App` instance via\n    :meth:`~.App.set_error_serializer`.\n\n    To customize what data is passed to the serializer, subclass\n    ``HTTPError`` and override the ``to_dict()`` method (``to_json()``\n    is implemented via ``to_dict()``). To also support XML, override\n    the ``to_xml()`` method.\n\n    Note:\n        ``status`` is the only positional argument allowed, the other\n        arguments should be used as keyword only. Using them as positional\n        arguments will raise a deprecation warning and will result in an\n        error in a future version of falcon.\n\n    Args:\n        status (str): HTTP status code and text, such as \"400 Bad Request\"\n\n    Keyword Args:\n        title (str): Human-friendly error title. If not provided, defaults\n            to the HTTP status line as determined by the ``status`` argument.\n        description (str): Human-friendly description of the error, along with\n            a helpful suggestion or two (default ``None``).\n        headers (dict or list): A ``dict`` of header names and values\n            to set, or a ``list`` of (*name*, *value*) tuples. Both *name* and\n            *value* must be of type ``str`` or ``StringType``, and only\n            character values 0x00 through 0xFF may be used on platforms that\n            use wide characters.\n\n            Note:\n                The Content-Type header, if present, will be overridden. If\n                you wish to return custom error messages, you can create\n                your own HTTP error class, and install an error handler\n                to convert it into an appropriate HTTP response for the\n                client\n\n            Note:\n                Falcon can process a list of ``tuple`` slightly faster\n                than a ``dict``.\n\n        href (str): A URL someone can visit to find out more information\n            (default ``None``). Unicode characters are percent-encoded.\n        href_text (str): If href is given, use this as the friendly\n            title/description for the link (default 'App documentation\n            for this error').\n        code (int): An internal code that customers can reference in their\n            support request or to help them when searching for knowledge\n            base articles related to this error (default ``None``).\n\n    Attributes:\n        status (str): HTTP status line, e.g. '748 Confounded by Ponies'.\n        title (str): Error title to send to the client.\n        description (str): Description of the error to send to the client.\n        headers (dict): Extra headers to add to the response.\n        link (str): An href that the client can provide to the user for\n            getting help.\n        code (int): An internal application code that a user can reference when\n            requesting support for the error.\n    \"\"\"\n\n    __slots__ = (\n        'status',\n        'title',\n        'description',\n        'headers',\n        'link',\n        'code',\n    )\n\n    @deprecated_args(allowed_positional=1)\n    def __init__(\n        self,\n        status,\n        title=None,\n        description=None,\n        headers=None,\n        href=None,\n        href_text=None,\n        code=None,\n    ):\n        self.status = status\n\n        # TODO(kgriffs): HTTP/2 does away with the \"reason phrase\". Eventually\n        #   we'll probably switch over to making everything code-based to more\n        #   easily support HTTP/2. When that happens, should we continue to\n        #   include the reason phrase in the title?\n        self.title = title or status\n\n        self.description = description\n        self.headers = headers\n        self.code = code\n\n        if href:\n            link = self.link = OrderedDict()\n            link['text'] = href_text or 'Documentation related to this error'\n            link['href'] = uri.encode(href)\n            link['rel'] = 'help'\n        else:\n            self.link = None\n\n    def __repr__(self):\n        return '<%s: %s>' % (self.__class__.__name__, self.status)\n\n    __str__ = __repr__\n\n    @property  # type: ignore\n    @deprecated(\n        'has_representation is deprecated and is currently unused by falcon',\n        is_property=True,\n    )\n    def has_representation(self):\n        return True\n\n    def to_dict(self, obj_type=dict):\n        \"\"\"Return a basic dictionary representing the error.\n\n        This method can be useful when serializing the error to hash-like\n        media types, such as YAML, JSON, and MessagePack.\n\n        Args:\n            obj_type: A dict-like type that will be used to store the\n                error information (default ``dict``).\n\n        Returns:\n            dict: A dictionary populated with the error's title,\n            description, etc.\n\n        \"\"\"\n\n        obj = obj_type()\n\n        obj['title'] = self.title\n\n        if self.description is not None:\n            obj['description'] = self.description\n\n        if self.code is not None:\n            obj['code'] = self.code\n\n        if self.link is not None:\n            obj['link'] = self.link\n\n        return obj\n\n###The function: to_json###\n    def to_xml(self):\n        \"\"\"Return an XML-encoded representation of the error.\n\n        Returns:\n            bytes: An XML document for the error.\n\n        \"\"\"\n\n        error_element = et.Element('error')\n\n        et.SubElement(error_element, 'title').text = self.title\n\n        if self.description is not None:\n            et.SubElement(error_element, 'description').text = self.description\n\n        if self.code is not None:\n            et.SubElement(error_element, 'code').text = str(self.code)\n\n        if self.link is not None:\n            link_element = et.SubElement(error_element, 'link')\n\n            for key in ('text', 'href', 'rel'):\n                et.SubElement(link_element, key).text = self.link[key]\n\n        return b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>' + et.tostring(\n            error_element, encoding='utf-8'\n        )\n\n\n# NOTE: initialized in falcon.media.json, that is always imported since Request/Respose\n# are imported by falcon init.\n_DEFAULT_JSON_HANDLER = None\n\n\nclass NoRepresentation:\n    \"\"\"Mixin for ``HTTPError`` child classes that have no representation.\n\n    This class can be mixed in when inheriting from ``HTTPError``, in order\n    to override the `has_representation` property such that it always\n    returns ``False``. This, in turn, will cause Falcon to return an empty\n    response body to the client.\n\n    You can use this mixin when defining errors that either should not have\n    a body (as dictated by HTTP standards or common practice), or in the\n    case that a detailed error response may leak information to an attacker.\n\n    Note:\n        This mixin class must appear before ``HTTPError`` in the base class\n        list when defining the child; otherwise, it will not override the\n        `has_representation` property as expected.\n\n    Warning:\n        As of Falcon 3.0, this mixin class is no longer used since all Falcon\n        errors have a representation. This class is considered deprecated and\n        will be removed in a future release.\n    \"\"\"\n\n    @property  # type: ignore\n    @deprecated(\n        'has_representation is deprecated and is currently unused by falcon. '\n        'The class NoRepresentation is deprecated and will be removed in a '\n        'future release',\n        is_property=True,\n    )\n    def has_representation(self):\n        return False\n\n\nclass OptionalRepresentation:\n    \"\"\"Mixin for ``HTTPError`` child classes that may have a representation.\n\n    This class can be mixed in when inheriting from ``HTTPError`` in order\n    to override the `has_representation` property, such that it will\n    return ``False`` when the error instance has no description\n    (i.e., the `description` kwarg was not set).\n\n    You can use this mixin when defining errors that do not include\n    a body in the HTTP response by default, serializing details only when\n    the web developer provides a description of the error.\n\n    Note:\n        This mixin class must appear before ``HTTPError`` in the base class\n        list when defining the child; otherwise, it will not override the\n        `has_representation` property as expected.\n\n    Warning:\n        As of Falcon 3.0, this mixin class is no longer used since all Falcon\n        errors have a representation. This class is considered deprecated and\n        will be removed in a future release.\n    \"\"\"\n\n    @property  # type: ignore\n    @deprecated(\n        'has_representation is deprecated and is currently unused by falcon. '\n        'The class OptionalRepresentation is deprecated and will be removed '\n        'in a future release',\n        is_property=True,\n    )\n    def has_representation(self):\n        return self.description is not None\n\n        ####cross_file_context:\n        [{'falcon.constants.MEDIA_JSON': 'from enum import Enum\\nimport os\\nimport sys\\n\\n\\nPYPY = sys.implementation.name == \\'pypy\\'\\n\"\"\"Evaluates to ``True`` when the current Python implementation is PyPy.\"\"\"\\n\\nPYTHON_VERSION = tuple(sys.version_info[:3])\\n\"\"\"Python version information triplet: (major, minor, micro).\"\"\"\\n# If FALCON_TESTING_MOCK_PY35 is defined in the env, pretend that we are\\n# on 3.5.0. Only intended for testing of the framework itself.\\n_is_py35 = PYTHON_VERSION[:2] == (3, 5)\\n_mock_py35 = os.environ.get(\\'FALCON_TESTING_MOCK_PY35\\')\\n\\n# TODO(vytas): Remove these hacks in 4.0 once we have dropped 3.5/3.6 support.\\n# NOTE(vytas): Do not alter the microversion if already on 3.5\\n#   (read: a hack to hit coverage on 3.5 as well).\\nif _mock_py35 or _is_py35:\\n    PYTHON_VERSION = (3, 5, PYTHON_VERSION[-1] if _is_py35 else 0)\\n\\nASGI_SUPPORTED = PYTHON_VERSION >= (3, 6)\\n\"\"\"Evaluates to ``True`` when ASGI is supported for the current Python version.\"\"\"\\n\\n# RFC 7231, 5789 methods\\nHTTP_METHODS = [\\n    \\'CONNECT\\',\\n    \\'DELETE\\',\\n    \\'GET\\',\\n    \\'HEAD\\',\\n    \\'OPTIONS\\',\\n    \\'PATCH\\',\\n    \\'POST\\',\\n    \\'PUT\\',\\n    \\'TRACE\\',\\n]\\n\\n# RFC 2518 and 4918 methods\\nWEBDAV_METHODS = [\\n    \\'CHECKIN\\',\\n    \\'CHECKOUT\\',\\n    \\'COPY\\',\\n    \\'LOCK\\',\\n    \\'MKCOL\\',\\n    \\'MOVE\\',\\n    \\'PROPFIND\\',\\n    \\'PROPPATCH\\',\\n    \\'REPORT\\',\\n    \\'UNCHECKIN\\',\\n    \\'UNLOCK\\',\\n    \\'UPDATE\\',\\n    \\'VERSION-CONTROL\\',\\n]\\n\\n# if FALCON_CUSTOM_HTTP_METHODS is defined, treat it as a comma-\\n# delimited string of additional supported methods in this env.\\nFALCON_CUSTOM_HTTP_METHODS = [\\n    method.strip().upper()\\n    for method in os.environ.get(\\'FALCON_CUSTOM_HTTP_METHODS\\', \\'\\').split(\\',\\')\\n    if method.strip() != \\'\\'\\n]\\n\\n_META_METHODS = [\\n    \\'WEBSOCKET\\',\\n]\\n\\nCOMBINED_METHODS = (\\n    HTTP_METHODS + WEBDAV_METHODS + FALCON_CUSTOM_HTTP_METHODS + _META_METHODS\\n)\\n\\n# NOTE(kgriffs): According to RFC 7159, most JSON parsers assume\\n# UTF-8 and so it is the recommended default charset going forward,\\n# and indeed, other charsets should not be specified to ensure\\n# maximum interoperability.\\nMEDIA_JSON = \\'application/json\\'\\n\\n# NOTE(kgriffs): An internet media type for MessagePack has not\\n# yet been registered. \\'application/x-msgpack\\' is commonly used,\\n# but the use of the \\'x-\\' prefix is discouraged by RFC 6838.\\nMEDIA_MSGPACK = \\'application/msgpack\\'\\n\\nMEDIA_MULTIPART = \\'multipart/form-data\\'\\n\\nMEDIA_URLENCODED = \\'application/x-www-form-urlencoded\\'\\n\\n# NOTE(kgriffs): An internet media type for YAML has not been\\n# registered. RoR uses \\'application/x-yaml\\', but since use of\\n# \\'x-\\' is discouraged by RFC 6838, we don\\'t use it in Falcon.\\n#\\n# The YAML specification requires that parsers deduce the character\\n# encoding by examining the first few bytes of the document itself.\\n# Therefore, it does not make sense to include the charset in the\\n# media type string.\\nMEDIA_YAML = \\'application/yaml\\'\\n\\n# NOTE(kgriffs): According to RFC 7303, when the charset is\\n# omitted, preference is given to the encoding specified in the\\n# document itself (either via a BOM, or via the XML declaration). If\\n# the document does not explicitly specify the encoding, UTF-8 is\\n# assumed. We do not specify the charset here, because many parsers\\n# ignore it anyway and just use what is specified in the document,\\n# contrary to the RFCs.\\nMEDIA_XML = \\'application/xml\\'\\n\\n# NOTE(kgriffs): RFC 4329 recommends application/* over text/.\\n# futhermore, parsers are required to respect the Unicode\\n# encoding signature, if present in the document, and to default\\n# to UTF-8 when not present. Note, however, that implementations\\n# are not required to support anything besides UTF-8, so it is\\n# unclear how much utility an encoding signature (or the charset\\n# parameter for that matter) has in practice.\\nMEDIA_JS = \\'application/javascript\\'\\n\\n# NOTE(kgriffs): According to RFC 6838, most text media types should\\n# include the charset parameter.\\nMEDIA_HTML = \\'text/html; charset=utf-8\\'\\nMEDIA_TEXT = \\'text/plain; charset=utf-8\\'\\n\\nMEDIA_JPEG = \\'image/jpeg\\'\\nMEDIA_PNG = \\'image/png\\'\\nMEDIA_GIF = \\'image/gif\\'\\n\\nDEFAULT_MEDIA_TYPE = MEDIA_JSON\\n\\n# NOTE(kgriffs): We do not expect more than one of these in the request\\nSINGLETON_HEADERS = frozenset(\\n    [\\n        \\'content-length\\',\\n        \\'content-type\\',\\n        \\'cookie\\',\\n        \\'expect\\',\\n        \\'from\\',\\n        \\'host\\',\\n        \\'max-forwards\\',\\n        \\'referer\\',\\n        \\'user-agent\\',\\n    ]\\n)\\n\\n# NOTE(kgriffs): Special singleton to be used internally whenever using\\n#   None would be ambiguous.\\n_UNSET = object()\\n\\nWebSocketPayloadType = Enum(\\'WebSocketPayloadType\\', \\'TEXT BINARY\\')\\n\"\"\"Enum representing the two possible WebSocket payload types.\"\"\"\\n'}, {'falcon.media.json.JSONHandler.serialize': 'from functools import partial\\nimport json\\n\\nfrom falcon import errors\\nfrom falcon import http_error\\nfrom falcon.media.base import BaseHandler\\nfrom falcon.media.base import TextBaseHandlerWS\\n\\n\\nclass JSONHandler(BaseHandler):\\n    \"\"\"JSON media handler.\\n\\n    This handler uses Python\\'s standard :py:mod:`json` library by default, but\\n    can be easily configured to use any of a number of third-party JSON\\n    libraries, depending on your needs. For example, you can often\\n    realize a significant performance boost under CPython by using an\\n    alternative library. Good options in this respect include `orjson`,\\n    `python-rapidjson`, and `mujson`.\\n\\n    This handler will raise a :class:`falcon.MediaNotFoundError` when attempting\\n    to parse an empty body, or a :class:`falcon.MediaMalformedError`\\n    if an error happens while parsing the body.\\n\\n    Note:\\n        If you are deploying to PyPy, we recommend sticking with the standard\\n        library\\'s JSON implementation, since it will be faster in most cases\\n        as compared to a third-party library.\\n\\n    .. rubric:: Custom JSON library\\n\\n    You can replace the default JSON handler by using a custom JSON library\\n    (see also: :ref:`custom_media_handlers`). Overriding the default JSON\\n    implementation is simply a matter of specifying the desired ``dumps`` and\\n    ``loads`` functions::\\n\\n        import falcon\\n        from falcon import media\\n\\n        import rapidjson\\n\\n        json_handler = media.JSONHandler(\\n            dumps=rapidjson.dumps,\\n            loads=rapidjson.loads,\\n        )\\n        extra_handlers = {\\n            \\'application/json\\': json_handler,\\n        }\\n\\n        app = falcon.App()\\n        app.req_options.media_handlers.update(extra_handlers)\\n        app.resp_options.media_handlers.update(extra_handlers)\\n\\n    .. rubric:: Custom serialization parameters\\n\\n    Even if you decide to stick with the stdlib\\'s :any:`json.dumps` and\\n    :any:`json.loads`, you can wrap them using :any:`functools.partial` to\\n    provide custom serialization or deserialization parameters supported by the\\n    ``dumps`` and ``loads`` functions, respectively\\n    (see also: :ref:`prettifying-json-responses`)::\\n\\n        import falcon\\n        from falcon import media\\n\\n        from functools import partial\\n\\n        json_handler = media.JSONHandler(\\n            dumps=partial(\\n                json.dumps,\\n                default=str,\\n                sort_keys=True,\\n            ),\\n        )\\n        extra_handlers = {\\n            \\'application/json\\': json_handler,\\n        }\\n\\n        app = falcon.App()\\n        app.req_options.media_handlers.update(extra_handlers)\\n        app.resp_options.media_handlers.update(extra_handlers)\\n\\n    By default, ``ensure_ascii`` is passed to the ``json.dumps`` function.\\n    If you override the ``dumps`` function, you will need to explicitly set\\n    ``ensure_ascii`` to ``False`` in order to enable the serialization of\\n    Unicode characters to UTF-8. This is easily done by using\\n    :any:`functools.partial` to apply the desired keyword argument. As also\\n    demonstrated in the previous paragraph, you can use this same technique to\\n    customize any option supported by the ``dumps`` and ``loads`` functions::\\n\\n        from functools import partial\\n\\n        from falcon import media\\n        import rapidjson\\n\\n        json_handler = media.JSONHandler(\\n            dumps=partial(\\n                rapidjson.dumps,\\n                ensure_ascii=False, sort_keys=True\\n            ),\\n        )\\n\\n    .. _custom-media-json-encoder:\\n\\n    .. rubric:: Custom JSON encoder\\n\\n    You can also override the default :class:`~json.JSONEncoder` by using a\\n    custom Encoder and updating the media handlers for ``application/json``\\n    type to use that::\\n\\n        import json\\n        from datetime import datetime\\n        from functools import partial\\n\\n        import falcon\\n        from falcon import media\\n\\n        class DatetimeEncoder(json.JSONEncoder):\\n            \\\\\"\\\\\"\\\\\"Json Encoder that supports datetime objects.\\\\\"\\\\\"\\\\\"\\n\\n            def default(self, obj):\\n                if isinstance(obj, datetime):\\n                    return obj.isoformat()\\n                return super().default(obj)\\n\\n        app = falcon.App()\\n\\n        json_handler = media.JSONHandler(\\n            dumps=partial(json.dumps, cls=DatetimeEncoder),\\n        )\\n        extra_handlers = {\\n            \\'application/json\\': json_handler,\\n        }\\n\\n        app.req_options.media_handlers.update(extra_handlers)\\n        app.resp_options.media_handlers.update(extra_handlers)\\n\\n\\n    Keyword Arguments:\\n        dumps (func): Function to use when serializing JSON responses.\\n        loads (func): Function to use when deserializing JSON requests.\\n    \"\"\"\\n\\n    def __init__(self, dumps=None, loads=None):\\n        self._dumps = dumps or partial(json.dumps, ensure_ascii=False)\\n        self._loads = loads or json.loads\\n\\n        # PERF(kgriffs): Test dumps once up front so we can set the\\n        #     proper serialize implementation.\\n        result = self._dumps({\\'message\\': \\'Hello World\\'})\\n        if isinstance(result, str):\\n            self.serialize = self._serialize_s\\n            self.serialize_async = self._serialize_async_s\\n        else:\\n            self.serialize = self._serialize_b\\n            self.serialize_async = self._serialize_async_b\\n\\n        # NOTE(kgriffs): To be safe, only enable the optimized protocol when\\n        #   not subclassed.\\n        if type(self) is JSONHandler:\\n            self._serialize_sync = self.serialize\\n            self._deserialize_sync = self._deserialize\\n\\n    def _deserialize(self, data):\\n        if not data:\\n            raise errors.MediaNotFoundError(\\'JSON\\')\\n        try:\\n            return self._loads(data.decode())\\n        except ValueError as err:\\n            raise errors.MediaMalformedError(\\'JSON\\') from err\\n\\n    def deserialize(self, stream, content_type, content_length):\\n        return self._deserialize(stream.read())\\n\\n    async def deserialize_async(self, stream, content_type, content_length):\\n        return self._deserialize(await stream.read())\\n\\n    # NOTE(kgriffs): Make content_type a kwarg to support the\\n    #   Request.render_body() shortcut optimization.\\n    def _serialize_s(self, media, content_type=None) -> bytes:\\n        return self._dumps(media).encode()\\n\\n    async def _serialize_async_s(self, media, content_type) -> bytes:\\n        return self._dumps(media).encode()\\n\\n    # NOTE(kgriffs): Make content_type a kwarg to support the\\n    #   Request.render_body() shortcut optimization.\\n    def _serialize_b(self, media, content_type=None) -> bytes:\\n        return self._dumps(media)\\n\\n    async def _serialize_async_b(self, media, content_type) -> bytes:\\n        return self._dumps(media)\\n\\n\\nclass JSONHandlerWS(TextBaseHandlerWS):\\n    \"\"\"WebSocket media handler for de(serializing) JSON to/from TEXT payloads.\\n\\n    This handler uses Python\\'s standard :py:mod:`json` library by default, but\\n    can be easily configured to use any of a number of third-party JSON\\n    libraries, depending on your needs. For example, you can often\\n    realize a significant performance boost under CPython by using an\\n    alternative library. Good options in this respect include `orjson`,\\n    `python-rapidjson`, and `mujson`.\\n\\n    Note:\\n        If you are deploying to PyPy, we recommend sticking with the standard\\n        library\\'s JSON implementation, since it will be faster in most cases\\n        as compared to a third-party library.\\n\\n    Overriding the default JSON implementation is simply a matter of specifying\\n    the desired ``dumps`` and ``loads`` functions::\\n\\n        import falcon\\n        from falcon import media\\n\\n        import rapidjson\\n\\n        json_handler = media.JSONHandlerWS(\\n            dumps=rapidjson.dumps,\\n            loads=rapidjson.loads,\\n        )\\n\\n        app = falcon.asgi.App()\\n        app.ws_options.media_handlers[falcon.WebSocketPayloadType.TEXT] = json_handler\\n\\n    By default, ``ensure_ascii`` is passed to the ``json.dumps`` function.\\n    If you override the ``dumps`` function, you will need to explicitly set\\n    ``ensure_ascii`` to ``False`` in order to enable the serialization of\\n    Unicode characters to UTF-8. This is easily done by using\\n    :any:`functools.partial` to apply the desired keyword argument. In fact, you\\n    can use this same technique to customize any option supported by the\\n    ``dumps`` and ``loads`` functions::\\n\\n        from functools import partial\\n\\n        from falcon import media\\n        import rapidjson\\n\\n        json_handler = media.JSONHandlerWS(\\n            dumps=partial(\\n                rapidjson.dumps,\\n                ensure_ascii=False, sort_keys=True\\n            ),\\n        )\\n\\n    Keyword Arguments:\\n        dumps (func): Function to use when serializing JSON.\\n        loads (func): Function to use when deserializing JSON.\\n    \"\"\"\\n\\n    __slots__ = [\\'dumps\\', \\'loads\\']\\n\\n    def __init__(self, dumps=None, loads=None):\\n        self._dumps = dumps or partial(json.dumps, ensure_ascii=False)\\n        self._loads = loads or json.loads\\n\\n    def serialize(self, media: object) -> str:\\n        return self._dumps(media)\\n\\n    def deserialize(self, payload: str) -> object:\\n        return self._loads(payload)\\n\\n\\nhttp_error._DEFAULT_JSON_HANDLER = _DEFAULT_JSON_HANDLER = JSONHandler()  # type: ignore\\n'}]", "test_list": ["def test_to_json_dumps(self):\n    e = falcon.HTTPError(status=falcon.HTTP_418, title='foo', description='bar')\n    assert e.to_json() == b'{\"title\": \"foo\", \"description\": \"bar\"}'\n\n    class Handler:\n\n        def serialize(self, obj, type):\n            assert type == falcon.MEDIA_JSON\n            return b'{\"a\": \"b\"}'\n    assert e.to_json(Handler()) == b'{\"a\": \"b\"}'"], "requirements": {"Input-Output Conditions": {"requirement": "The 'to_json' function should return a JSON representation of the HTTPError instance as bytes, and should correctly handle the optional handler object for serialization.", "unit_test": ["def test_to_json_with_custom_handler():\n    class CustomHandler:\n        def serialize(self, obj, media_type):\n            assert media_type == falcon.MEDIA_JSON\n            return b'{\"custom\": \"handler\"}'\n\n    e = falcon.HTTPError(status=falcon.HTTP_400, title='Bad Request')\n    assert e.to_json(CustomHandler()) == b'{\"custom\": \"handler\"}'"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_with_custom_handler'"}, "Exception Handling": {"requirement": "The 'to_json' function should raise a AttributeError: 'Handler object must have a 'serialize' method' if the handler object does not have a 'serialize' method.", "unit_test": ["def test_to_json_invalid_handler():\n    class InvalidHandler:\n        pass\n\n    e = falcon.HTTPError(status=falcon.HTTP_400, title='Bad Request')\n    try:\n        e.to_json(InvalidHandler())\n    except TypeError as ex:\n        assert str(ex) == \"Handler object must have a 'serialize' method\""], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_invalid_handler'"}, "Edge Case Handling": {"requirement": "The 'to_json' function should handle cases where the HTTPError instance has no title or description, returning an empty JSON object.", "unit_test": ["def test_to_json_no_title_description():\n    e = falcon.HTTPError(status=falcon.HTTP_400)\n    assert e.to_json() == b'{}'"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_no_title_description'"}, "Functionality Extension": {"requirement": "Extend the 'to_json' function to include an optional 'indent' parameter for pretty-printing the JSON output.", "unit_test": ["def test_to_json_with_indent():\n    e = falcon.HTTPError(status=falcon.HTTP_400, title='Bad Request')\n    json_output = e.to_json(indent=4)\n    assert json_output == b'{\\n    \"title\": \"Bad Request\"\\n}'"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_with_indent'"}, "Annotation Coverage": {"requirement": "Ensure that the 'to_json' function is fully annotated with parameter and return types.", "unit_test": ["def test_to_json_annotations():\n    from typing import get_type_hints\n    hints = get_type_hints(falcon.HTTPError.to_json)\n    assert hints['return'] == bytes"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_annotations'"}, "Code Complexity": {"requirement": "The 'to_json' function should maintain a cyclomatic complexity of 5 or less.", "unit_test": ["def test_to_json_complexity():\n    import radon.complexity as rc\n    complexity = rc.cc_visit(falcon.HTTPError.to_json.__code__)\n    assert complexity[0].complexity <= 5"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_complexity'"}, "Code Standard": {"requirement": "The 'to_json' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['falcon/http_error.py'])\n    assert result.total_errors == 0"], "test": "'tests/test_httperror.py::TestHTTPError::test_check_code_style'"}, "Context Usage Verification": {"requirement": "The 'to_json' function should utilize the 'to_dict' ,'Handler' and 'falcon.http_error._DEFAULT_JSON_HANDLER.serialize' method from the HTTPError class to obtain the error details.", "unit_test": ["def test_to_json_uses_to_dict():\n    e = falcon.HTTPError(status=falcon.HTTP_400, title='Bad Request')\n    e.to_dict = lambda: {'title': 'Bad Request'}\n    assert e.to_json() == b'{\"title\": \"Bad Request\"}'"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_uses_to_dict'"}, "Context Usage Correctness Verification": {"requirement": "Verify that the 'to_json' function correctly uses the 'to_dict' ,'Handler' and 'falcon.http_error._DEFAULT_JSON_HANDLER.serialize' method to serialize the error details into JSON format.", "unit_test": ["def test_to_json_correct_dict_usage():\n    e = falcon.HTTPError(status=falcon.HTTP_400, title='Bad Request')\n    dict_representation = e.to_dict()\n    json_representation = e.to_json()\n    assert json.loads(json_representation.decode()) == dict_representation"], "test": "'tests/test_httperror.py::TestHTTPError::test_to_json_correct_dict_usage'"}}}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "type": "method", "project_path": "System/mrjob", "completion_path": "System/mrjob/mrjob/fs/hadoop.py", "signature_position": [286, 286], "body_position": [287, 298], "dependency": {"intra_class": ["mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_version", "mrjob.fs.hadoop.HadoopFilesystem.invoke_hadoop"], "intra_file": ["mrjob.fs.hadoop._HADOOP_FILE_EXISTS_RE"], "cross_file": ["mrjob.compat.uses_yarn"]}, "requirement": {"Functionality": "Create a directory in the Hadoop filesystem. It uses Hadoop 'fs -mkdir' command (additionally with '-p' option on Hadoop 2) to create the directory. If the command fails except for the case where the directory already exists, it raises an IOError: 'Could not mkdir {path}'.", "Arguments": ":param self: HadoopFilesystem. An instance of the HadoopFilesystem class.\n:param path: str. The path of the directory to be created.\n:return: No return values."}, "tests": ["tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir"], "indent": 8, "domain": "System", "code": "    def mkdir(self, path):\n        version = self.get_hadoop_version()\n\n        # use -p on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)\n", "intra_context": "# Copyright 2009-2012 Yelp and Contributors\n# Copyright 2013 David Marin\n# Copyright 2015-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport os.path\nimport re\nfrom io import BytesIO\nfrom subprocess import Popen\nfrom subprocess import PIPE\nfrom subprocess import CalledProcessError\n\nfrom mrjob.cat import decompress\nfrom mrjob.compat import uses_yarn\nfrom mrjob.fs.base import Filesystem\nfrom mrjob.py2 import to_unicode\nfrom mrjob.parse import is_uri\nfrom mrjob.parse import urlparse\nfrom mrjob.util import cmd_line\nfrom mrjob.util import unique\nfrom mrjob.util import which\n\n\nlog = logging.getLogger(__name__)\n\n# used by mkdir()\n_HADOOP_FILE_EXISTS_RE = re.compile(br'.*File exists.*')\n\n# used by ls() and exists()\n_HADOOP_LS_NO_SUCH_FILE = re.compile(br'^lsr?: .*No such file.*$')\n\n# used by rm() (see below)\n_HADOOP_RM_NO_SUCH_FILE = re.compile(br'^rmr?: .*No such file.*$')\n\n# find version string in \"Hadoop 0.20.203\" etc.\n_HADOOP_VERSION_RE = re.compile(br'^.*?(?P<version>(\\d|\\.)+).*?$')\n\n\nclass HadoopFilesystem(Filesystem):\n    \"\"\"Filesystem for URIs accepted by ``hadoop fs``. Typically you will get\n    one of these via ``HadoopJobRunner().fs``, composed with\n    :py:class:`~mrjob.fs.local.LocalFilesystem`.\n\n    This also helps with other invocations of the ``hadoop`` binary, such\n    as ``hadoop version`` (see :py:meth:`invoke_hadoop`).\n    \"\"\"\n\n    def __init__(self, hadoop_bin=None):\n        \"\"\"Create a Hadoop filesystem\n\n        :param hadoop_bin: ``hadoop`` binary, as a list of args. If set to\n                           ``None``, we'll auto-detect the Hadoop binary.\n                           If set to ``[]``, this FS will be disabled\n                           until you call :py:meth:`set_hadoop_bin`.\n        \"\"\"\n        super(HadoopFilesystem, self).__init__()\n        self._hadoop_bin = hadoop_bin\n        self._hadoop_version = None  # cache for get_hadoop_version()\n\n    def can_handle_path(self, path):\n        if not (self._hadoop_bin or self._hadoop_bin is None):\n            return False\n\n        return is_uri(path)\n\n    def get_hadoop_bin(self):\n        \"\"\"Return the hadoop binary, searching for it if need be.\"\"\"\n        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin\n\n    def set_hadoop_bin(self, hadoop_bin):\n        \"\"\"Manually set the hadoop binary, as a list of args.\"\"\"\n        self._hadoop_bin = hadoop_bin\n\n    def _find_hadoop_bin(self):\n        \"\"\"Look for the hadoop binary in any plausible place. If all\n        else fails, return ``['hadoop']``.\n        \"\"\"\n        def yield_paths():\n            for name in 'HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL':\n                path = os.environ.get(name)\n                if path:\n                    yield os.path.join(path, 'bin')\n\n            # They use $HADOOP_INSTALL/hadoop/bin here:\n            # https://wiki.apache.org/hadoop/GettingStartedWithHadoop\n            if os.environ.get('HADOOP_INSTALL'):\n                yield os.path.join(\n                    os.environ['HADOOP_INSTALL'], 'hadoop', 'bin')\n\n            yield None  # use $PATH\n\n            # Maybe it's in $HADOOP_MAPRED_HOME? $HADOOP_YARN_HOME? Don't give\n            # up. Don't worry about duplicates; they're de-duplicated below\n            for name, path in sorted(os.environ.items()):\n                if name.startswith('HADOOP_') and name.endswith('_HOME'):\n                    yield os.path.join(path, 'bin')\n\n        for path in unique(yield_paths()):\n            log.info('Looking for hadoop binary in %s...' % (path or '$PATH'))\n\n            hadoop_bin = which('hadoop', path=path)\n\n            if hadoop_bin:\n                log.info('Found hadoop binary: %s' % hadoop_bin)\n                return [hadoop_bin]\n        else:\n            log.info(\"Falling back to 'hadoop'\")\n            return ['hadoop']\n\n    def get_hadoop_version(self):\n        \"\"\"Invoke the hadoop executable to determine its version\"\"\"\n        # mkdir() needs this\n        if not self._hadoop_version:\n            stdout = self.invoke_hadoop(['version'], return_stdout=True)\n            if stdout:\n                first_line = stdout.split(b'\\n')[0]\n                m = _HADOOP_VERSION_RE.match(first_line)\n                if m:\n                    self._hadoop_version = to_unicode(m.group('version'))\n                    log.info(\"Using Hadoop version %s\" % self._hadoop_version)\n                else:\n                    raise Exception('Unable to determine Hadoop version.')\n\n        return self._hadoop_version\n\n    def invoke_hadoop(self, args, ok_returncodes=None, ok_stderr=None,\n                      return_stdout=False):\n        \"\"\"Run the given hadoop command, raising an exception on non-zero\n        return code. This only works for commands whose output we don't\n        care about.\n\n        Args:\n        ok_returncodes -- a list/tuple/set of return codes we expect to\n            get back from hadoop (e.g. [0,1]). By default, we only expect 0.\n            If we get an unexpected return code, we raise a CalledProcessError.\n        ok_stderr -- don't log STDERR or raise CalledProcessError if stderr\n            matches a regex in this list (even if the returncode is bad)\n        return_stdout -- return the stdout from the hadoop command rather\n            than logging it. If this is False, we return the returncode\n            instead.\n        \"\"\"\n        args = self.get_hadoop_bin() + args\n\n        log.debug('> %s' % cmd_line(args))\n\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = proc.communicate()\n\n        log_func = log.debug if proc.returncode == 0 else log.error\n        if not return_stdout:\n            for line in BytesIO(stdout):\n                log_func('STDOUT: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        # check if STDERR is okay\n        stderr_is_ok = False\n        if ok_stderr:\n            for stderr_re in ok_stderr:\n                if stderr_re.match(stderr):\n                    stderr_is_ok = True\n                    break\n\n        if not stderr_is_ok:\n            for line in BytesIO(stderr):\n                log_func('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        ok_returncodes = ok_returncodes or [0]\n\n        if not stderr_is_ok and proc.returncode not in ok_returncodes:\n            raise CalledProcessError(proc.returncode, args)\n\n        if return_stdout:\n            return stdout\n        else:\n            return proc.returncode\n\n    def du(self, path_glob):\n        \"\"\"Get the size of a file or directory (recursively), or 0\n        if it doesn't exist.\"\"\"\n        try:\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob],\n                                        return_stdout=True,\n                                        ok_returncodes=[0, 1, 255])\n        except CalledProcessError:\n            return 0\n\n        try:\n            return sum(int(line.split()[0])\n                       for line in stdout.split(b'\\n')\n                       if line.strip())\n        except (ValueError, TypeError, IndexError):\n            raise IOError(\n                'Unexpected output from hadoop fs -du: %r' % stdout)\n\n    def ls(self, path_glob):\n        components = urlparse(path_glob)\n        hdfs_prefix = '%s://%s' % (components.scheme, components.netloc)\n\n        version = self.get_hadoop_version()\n\n        # use ls -R on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = ['fs', '-ls', '-R', path_glob]\n        else:\n            args = ['fs', '-lsr', path_glob]\n\n        try:\n            stdout = self.invoke_hadoop(args, return_stdout=True,\n                                        ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not ls %s\" % path_glob)\n\n        for line in BytesIO(stdout):\n            line = line.rstrip(b'\\r\\n')\n\n            # ignore total item count\n            if line.startswith(b'Found '):\n                continue\n\n            fields = line.split(b' ')\n\n            # Throw out directories\n            if fields[0].startswith(b'd'):\n                continue\n\n            # Try to figure out which part of the line is the path\n            # Expected lines:\n            #\n            # HDFS:\n            # -rw-r--r--   3 dave users       3276 2010-01-13 14:00 /foo/bar\n            #\n            # S3:\n            # -rwxrwxrwx   1          3276 010-01-13 14:00 /foo/bar\n            path_index = None\n            for index, field in enumerate(fields):\n                # look for time field, and pick one after that\n                # (can't use field[2] because that's an int in Python 3)\n                if len(field) == 5 and field[2:3] == b':':\n                    path_index = (index + 1)\n            if not path_index:\n                raise IOError(\"Could not locate path in string %r\" % line)\n\n            path = to_unicode(line.split(b' ', path_index)[-1])\n            # handle fully qualified URIs from newer versions of Hadoop ls\n            # (see Pull Request #577)\n            if is_uri(path):\n                yield path\n            else:\n                yield hdfs_prefix + path\n\n    def _cat_file(self, path):\n        # stream from HDFS\n        cat_args = self.get_hadoop_bin() + ['fs', '-cat', path]\n        log.debug('> %s' % cmd_line(cat_args))\n\n        cat_proc = Popen(cat_args, stdout=PIPE, stderr=PIPE)\n\n        for chunk in decompress(cat_proc.stdout, path):\n            yield chunk\n\n        # this does someties happen; see #1396\n        for line in cat_proc.stderr:\n            log.error('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        cat_proc.stdout.close()\n        cat_proc.stderr.close()\n\n        returncode = cat_proc.wait()\n\n        if returncode != 0:\n            raise IOError(\"Could not stream %s\" % path)\n\n###The function: mkdir###\n    def exists(self, path_glob):\n        \"\"\"Does the given path exist?\n\n        If dest is a directory (ends with a \"/\"), we check if there are\n        any files starting with that path.\n        \"\"\"\n        try:\n            return_code = self.invoke_hadoop(\n                ['fs', '-ls', path_glob],\n                ok_returncodes=[0, -1, 255],\n                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n\n            return (return_code == 0)\n        except CalledProcessError:\n            raise IOError(\"Could not check path %s\" % path_glob)\n\n    def put(self, src, path):\n        # don't inadvertently support cp syntax\n        if path.endswith('/'):\n            raise ValueError('put() destination may not be a directory')\n\n        self.invoke_hadoop(['fs', '-put', src, path])\n\n    def rm(self, path_glob):\n        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n\n        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-R', '-f', '-skipTrash', path_glob]\n        else:\n            args = ['fs', '-rmr', '-skipTrash', path_glob]\n\n        try:\n            self.invoke_hadoop(\n                args,\n                return_stdout=True, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)\n\n    def touchz(self, path):\n        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)\n", "cross_context": [{"mrjob.compat.uses_yarn": "# -*- coding: utf-8 -*-\n# Copyright 2009-2012 Yelp\n# Copyright 2013-2014 Yelp and Contributors\n# Copyright 2015-2016 Yelp\n# Copyright 2018 Ben Dalling\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utility functions for compatibility with different version of hadoop.\"\"\"\nfrom distutils.version import LooseVersion\nimport logging\nimport os\n\nfrom mrjob.py2 import string_types\n\n# lists alternative names for jobconf variables\n# full listing thanks to translation table in\n# http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html # noqa\n\nlog = logging.getLogger(__name__)\n\n_JOBCONF_DICT_LIST = [\n    {'1.0': 'StorageId',\n     '2.0': 'dfs.datanode.StorageId'},\n    {'1.0': 'create.empty.dir.if.nonexist',\n     '2.0': 'mapreduce.jobcontrol.createdir.ifnotexist'},\n    {'1.0': 'dfs.access.time.precision',\n     '2.0': 'dfs.namenode.accesstime.precision'},\n    {'1.0': 'dfs.backup.address',\n     '2.0': 'dfs.namenode.backup.address'},\n    {'1.0': 'dfs.backup.http.address',\n     '2.0': 'dfs.namenode.backup.http-address'},\n    {'1.0': 'dfs.balance.bandwidthPerSec',\n     '2.0': 'dfs.datanode.balance.bandwidthPerSec'},\n    {'1.0': 'dfs.block.size',\n     '2.0': 'dfs.blocksize'},\n    {'1.0': 'dfs.client.buffer.dir',\n     '2.0': 'fs.client.buffer.dir'},\n    {'1.0': 'dfs.data.dir',\n     '2.0': 'dfs.datanode.data.dir'},\n    {'1.0': 'dfs.datanode.max.xcievers',\n     '2.0': 'dfs.datanode.max.transfer.threads'},\n    {'1.0': 'dfs.df.interval',\n     '2.0': 'fs.df.interval'},\n    {'1.0': 'dfs.http.address',\n     '2.0': 'dfs.namenode.http-address'},\n    {'1.0': 'dfs.https.address',\n     '2.0': 'dfs.namenode.https-address'},\n    {'1.0': 'dfs.https.client.keystore.resource',\n     '2.0': 'dfs.client.https.keystore.resource'},\n    {'1.0': 'dfs.https.need.client.auth',\n     '2.0': 'dfs.client.https.need-auth'},\n    {'1.0': 'dfs.max-repl-streams',\n     '2.0': 'dfs.namenode.replication.max-streams'},\n    {'1.0': 'dfs.max.objects',\n     '2.0': 'dfs.namenode.max.objects'},\n    {'1.0': 'dfs.name.dir',\n     '2.0': 'dfs.namenode.name.dir'},\n    {'1.0': 'dfs.name.dir.restore',\n     '2.0': 'dfs.namenode.name.dir.restore'},\n    {'1.0': 'dfs.name.edits.dir',\n     '2.0': 'dfs.namenode.edits.dir'},\n    {'1.0': 'dfs.permissions',\n     '2.0': 'dfs.permissions.enabled'},\n    {'1.0': 'dfs.permissions.supergroup',\n     '2.0': 'dfs.permissions.superusergroup'},\n    {'1.0': 'dfs.read.prefetch.size',\n     '2.0': 'dfs.client.read.prefetch.size'},\n    {'1.0': 'dfs.replication.considerLoad',\n     '2.0': 'dfs.namenode.replication.considerLoad'},\n    {'1.0': 'dfs.replication.interval',\n     '2.0': 'dfs.namenode.replication.interval'},\n    {'1.0': 'dfs.replication.min',\n     '2.0': 'dfs.namenode.replication.min'},\n    {'1.0': 'dfs.replication.pending.timeout.sec',\n     '2.0': 'dfs.namenode.replication.pending.timeout-sec'},\n    {'1.0': 'dfs.safemode.extension',\n     '2.0': 'dfs.namenode.safemode.extension'},\n    {'1.0': 'dfs.safemode.threshold.pct',\n     '2.0': 'dfs.namenode.safemode.threshold-pct'},\n    {'1.0': 'dfs.secondary.http.address',\n     '2.0': 'dfs.namenode.secondary.http-address'},\n    {'1.0': 'dfs.socket.timeout',\n     '2.0': 'dfs.client.socket-timeout'},\n    {'1.0': 'dfs.upgrade.permission',\n     '2.0': 'dfs.namenode.upgrade.permission'},\n    {'1.0': 'dfs.write.packet.size',\n     '2.0': 'dfs.client-write-packet-size'},\n    {'1.0': 'fs.checkpoint.dir',\n     '2.0': 'dfs.namenode.checkpoint.dir'},\n    {'1.0': 'fs.checkpoint.edits.dir',\n     '2.0': 'dfs.namenode.checkpoint.edits.dir'},\n    {'1.0': 'fs.checkpoint.period',\n     '2.0': 'dfs.namenode.checkpoint.period'},\n    {'1.0': 'fs.default.name',\n     '2.0': 'fs.defaultFS'},\n    {'1.0': 'hadoop.configured.node.mapping',\n     '2.0': 'net.topology.configured.node.mapping'},\n    {'1.0': 'hadoop.job.history.location',\n     '2.0': 'mapreduce.jobtracker.jobhistory.location'},\n    {'1.0': 'hadoop.native.lib',\n     '2.0': 'io.native.lib.available'},\n    {'1.0': 'hadoop.net.static.resolutions',\n     '2.0': 'mapreduce.tasktracker.net.static.resolutions'},\n    {'1.0': 'hadoop.pipes.command-file.keep',\n     '2.0': 'mapreduce.pipes.commandfile.preserve'},\n    {'1.0': 'hadoop.pipes.executable',\n     '2.0': 'mapreduce.pipes.executable'},\n    {'1.0': 'hadoop.pipes.executable.interpretor',\n     '2.0': 'mapreduce.pipes.executable.interpretor'},\n    {'1.0': 'hadoop.pipes.java.mapper',\n     '2.0': 'mapreduce.pipes.isjavamapper'},\n    {'1.0': 'hadoop.pipes.java.recordreader',\n     '2.0': 'mapreduce.pipes.isjavarecordreader'},\n    {'1.0': 'hadoop.pipes.java.recordwriter',\n     '2.0': 'mapreduce.pipes.isjavarecordwriter'},\n    {'1.0': 'hadoop.pipes.java.reducer',\n     '2.0': 'mapreduce.pipes.isjavareducer'},\n    {'1.0': 'hadoop.pipes.partitioner',\n     '2.0': 'mapreduce.pipes.partitioner'},\n    {'1.0': 'heartbeat.recheck.interval',\n     '2.0': 'dfs.namenode.heartbeat.recheck-interval'},\n    {'1.0': 'io.bytes.per.checksum',\n     '2.0': 'dfs.bytes-per-checksum'},\n    {'1.0': 'io.sort.factor',\n     '2.0': 'mapreduce.task.io.sort.factor'},\n    {'1.0': 'io.sort.mb',\n     '2.0': 'mapreduce.task.io.sort.mb'},\n    {'1.0': 'io.sort.spill.percent',\n     '2.0': 'mapreduce.map.sort.spill.percent'},\n    {'1.0': 'job.end.notification.url',\n     '2.0': 'mapreduce.job.end-notification.url'},\n    {'1.0': 'job.end.retry.attempts',\n     '2.0': 'mapreduce.job.end-notification.retry.attempts'},\n    {'1.0': 'job.end.retry.interval',\n     '2.0': 'mapreduce.job.end-notification.retry.interval'},\n    {'1.0': 'job.local.dir',\n     '2.0': 'mapreduce.job.local.dir'},\n    {'1.0': 'jobclient.completion.poll.interval',\n     '2.0': 'mapreduce.client.completion.pollinterval'},\n    {'1.0': 'jobclient.output.filter',\n     '2.0': 'mapreduce.client.output.filter'},\n    {'1.0': 'jobclient.progress.monitor.poll.interval',\n     '2.0': 'mapreduce.client.progressmonitor.pollinterval'},\n    {'1.0': 'keep.failed.task.files',\n     '2.0': 'mapreduce.task.files.preserve.failedtasks'},\n    {'1.0': 'keep.task.files.pattern',\n     '2.0': 'mapreduce.task.files.preserve.filepattern'},\n    {'1.0': 'key.value.separator.in.input.line',\n     '2.0': 'mapreduce.input.keyvaluelinerecordreader.key.value.separator'},\n    {'1.0': 'local.cache.size',\n     '2.0': 'mapreduce.tasktracker.cache.local.size'},\n    {'1.0': 'map.input.file',\n     '2.0': 'mapreduce.map.input.file'},\n    {'1.0': 'map.input.length',\n     '2.0': 'mapreduce.map.input.length'},\n    {'1.0': 'map.input.start',\n     '2.0': 'mapreduce.map.input.start'},\n    {'1.0': 'map.output.key.field.separator',\n     '2.0': 'mapreduce.map.output.key.field.separator'},\n    {'1.0': 'map.output.key.value.fields.spec',\n     '2.0': 'mapreduce.fieldsel.map.output.key.value.fields.spec'},\n    {'1.0': 'mapred.acls.enabled',\n     '2.0': 'mapreduce.cluster.acls.enabled'},\n    {'1.0': 'mapred.binary.partitioner.left.offset',\n     '2.0': 'mapreduce.partition.binarypartitioner.left.offset'},\n    {'1.0': 'mapred.binary.partitioner.right.offset',\n     '2.0': 'mapreduce.partition.binarypartitioner.right.offset'},\n    {'1.0': 'mapred.cache.archives',\n     '2.0': 'mapreduce.job.cache.archives'},\n    {'1.0': 'mapred.cache.archives.timestamps',\n     '2.0': 'mapreduce.job.cache.archives.timestamps'},\n    {'1.0': 'mapred.cache.files',\n     '2.0': 'mapreduce.job.cache.files'},\n    {'1.0': 'mapred.cache.files.timestamps',\n     '2.0': 'mapreduce.job.cache.files.timestamps'},\n    {'1.0': 'mapred.cache.localArchives',\n     '2.0': 'mapreduce.job.cache.local.archives'},\n    {'1.0': 'mapred.cache.localFiles',\n     '2.0': 'mapreduce.job.cache.local.files'},\n    {'1.0': 'mapred.child.tmp',\n     '2.0': 'mapreduce.task.tmp.dir'},\n    {'1.0': 'mapred.cluster.average.blacklist.threshold',\n     '2.0': 'mapreduce.jobtracker.blacklist.average.threshold'},\n    {'1.0': 'mapred.cluster.map.memory.mb',\n     '2.0': 'mapreduce.cluster.mapmemory.mb'},\n    {'1.0': 'mapred.cluster.max.map.memory.mb',\n     '2.0': 'mapreduce.jobtracker.maxmapmemory.mb'},\n    {'1.0': 'mapred.cluster.max.reduce.memory.mb',\n     '2.0': 'mapreduce.jobtracker.maxreducememory.mb'},\n    {'1.0': 'mapred.cluster.reduce.memory.mb',\n     '2.0': 'mapreduce.cluster.reducememory.mb'},\n    {'1.0': 'mapred.committer.job.setup.cleanup.needed',\n     '2.0': 'mapreduce.job.committer.setup.cleanup.needed'},\n    {'1.0': 'mapred.compress.map.output',\n     '2.0': 'mapreduce.map.output.compress'},\n    {'1.0': 'mapred.create.symlink',\n     '2.0': 'mapreduce.job.cache.symlink.create'},\n    {'1.0': 'mapred.data.field.separator',\n     '2.0': 'mapreduce.fieldsel.data.field.separator'},\n    {'1.0': 'mapred.debug.out.lines',\n     '2.0': 'mapreduce.task.debugout.lines'},\n    {'1.0': 'mapred.healthChecker.interval',\n     '2.0': 'mapreduce.tasktracker.healthchecker.interval'},\n    {'1.0': 'mapred.healthChecker.script.args',\n     '2.0': 'mapreduce.tasktracker.healthchecker.script.args'},\n    {'1.0': 'mapred.healthChecker.script.path',\n     '2.0': 'mapreduce.tasktracker.healthchecker.script.path'},\n    {'1.0': 'mapred.healthChecker.script.timeout',\n     '2.0': 'mapreduce.tasktracker.healthchecker.script.timeout'},\n    {'1.0': 'mapred.heartbeats.in.second',\n     '2.0': 'mapreduce.jobtracker.heartbeats.in.second'},\n    {'1.0': 'mapred.hosts',\n     '2.0': 'mapreduce.jobtracker.hosts.filename'},\n    {'1.0': 'mapred.hosts.exclude',\n     '2.0': 'mapreduce.jobtracker.hosts.exclude.filename'},\n    {'1.0': 'mapred.inmem.merge.threshold',\n     '2.0': 'mapreduce.reduce.merge.inmem.threshold'},\n    {'1.0': 'mapred.input.dir',\n     '2.0': 'mapreduce.input.fileinputformat.inputdir'},\n    {'1.0': 'mapred.input.dir.formats',\n     '2.0': 'mapreduce.input.multipleinputs.dir.formats'},\n    {'1.0': 'mapred.input.dir.mappers',\n     '2.0': 'mapreduce.input.multipleinputs.dir.mappers'},\n    {'1.0': 'mapred.input.pathFilter.class',\n     '2.0': 'mapreduce.input.pathFilter.class'},\n    {'1.0': 'mapred.jar',\n     '2.0': 'mapreduce.job.jar'},\n    {'1.0': 'mapred.job.classpath.archives',\n     '2.0': 'mapreduce.job.classpath.archives'},\n    {'1.0': 'mapred.job.classpath.files',\n     '2.0': 'mapreduce.job.classpath.files'},\n    {'1.0': 'mapred.job.id',\n     '2.0': 'mapreduce.job.id'},\n    {'1.0': 'mapred.job.map.memory.mb',\n     '2.0': 'mapreduce.map.memory.mb'},\n    {'1.0': 'mapred.job.name',\n     '2.0': 'mapreduce.job.name'},\n    {'1.0': 'mapred.job.priority',\n     '2.0': 'mapreduce.job.priority'},\n    {'1.0': 'mapred.job.queue.name',\n     '2.0': 'mapreduce.job.queuename'},\n    {'1.0': 'mapred.job.reduce.input.buffer.percent',\n     '2.0': 'mapreduce.reduce.input.buffer.percent'},\n    {'1.0': 'mapred.job.reduce.markreset.buffer.percent',\n     '2.0': 'mapreduce.reduce.markreset.buffer.percent'},\n    {'1.0': 'mapred.job.reduce.memory.mb',\n     '2.0': 'mapreduce.reduce.memory.mb'},\n    {'1.0': 'mapred.job.reduce.total.mem.bytes',\n     '2.0': 'mapreduce.reduce.memory.totalbytes'},\n    {'1.0': 'mapred.job.reuse.jvm.num.tasks',\n     '2.0': 'mapreduce.job.jvm.numtasks'},\n    {'1.0': 'mapred.job.shuffle.input.buffer.percent',\n     '2.0': 'mapreduce.reduce.shuffle.input.buffer.percent'},\n    {'1.0': 'mapred.job.shuffle.merge.percent',\n     '2.0': 'mapreduce.reduce.shuffle.merge.percent'},\n    {'1.0': 'mapred.job.tracker',\n     '2.0': 'mapreduce.jobtracker.address'},\n    {'1.0': 'mapred.job.tracker.handler.count',\n     '2.0': 'mapreduce.jobtracker.handler.count'},\n    {'1.0': 'mapred.job.tracker.history.completed.location',\n     '2.0': 'mapreduce.jobtracker.jobhistory.completed.location'},\n    {'1.0': 'mapred.job.tracker.http.address',\n     '2.0': 'mapreduce.jobtracker.http.address'},\n    {'1.0': 'mapred.job.tracker.jobhistory.lru.cache.size',\n     '2.0': 'mapreduce.jobtracker.jobhistory.lru.cache.size'},\n    {'1.0': 'mapred.job.tracker.persist.jobstatus.active',\n     '2.0': 'mapreduce.jobtracker.persist.jobstatus.active'},\n    {'1.0': 'mapred.job.tracker.persist.jobstatus.dir',\n     '2.0': 'mapreduce.jobtracker.persist.jobstatus.dir'},\n    {'1.0': 'mapred.job.tracker.persist.jobstatus.hours',\n     '2.0': 'mapreduce.jobtracker.persist.jobstatus.hours'},\n    {'1.0': 'mapred.job.tracker.retire.jobs',\n     '2.0': 'mapreduce.jobtracker.retirejobs'},\n    {'1.0': 'mapred.job.tracker.retiredjobs.cache.size',\n     '2.0': 'mapreduce.jobtracker.retiredjobs.cache.size'},\n    {'1.0': 'mapred.jobinit.threads',\n     '2.0': 'mapreduce.jobtracker.jobinit.threads'},\n    {'1.0': 'mapred.jobtracker.instrumentation',\n     '2.0': 'mapreduce.jobtracker.instrumentation'},\n    {'1.0': 'mapred.jobtracker.job.history.block.size',\n     '2.0': 'mapreduce.jobtracker.jobhistory.block.size'},\n    {'1.0': 'mapred.jobtracker.maxtasks.per.job',\n     '2.0': 'mapreduce.jobtracker.maxtasks.perjob'},\n    {'1.0': 'mapred.jobtracker.restart.recover',\n     '2.0': 'mapreduce.jobtracker.restart.recover'},\n    {'1.0': 'mapred.jobtracker.taskScheduler',\n     '2.0': 'mapreduce.jobtracker.taskscheduler'},\n    {'1.0': 'mapred.jobtracker.taskScheduler.maxRunningTasksPerJob',\n     '2.0': 'mapreduce.jobtracker.taskscheduler.maxrunningtasks.perjob'},\n    {'1.0': 'mapred.jobtracker.taskalloc.capacitypad',\n     '2.0': 'mapreduce.jobtracker.taskscheduler.taskalloc.capacitypad'},\n    {'1.0': 'mapred.join.expr',\n     '2.0': 'mapreduce.join.expr'},\n    {'1.0': 'mapred.join.keycomparator',\n     '2.0': 'mapreduce.join.keycomparator'},\n    {'1.0': 'mapred.lazy.output.format',\n     '2.0': 'mapreduce.output.lazyoutputformat.outputformat'},\n    {'1.0': 'mapred.line.input.format.linespermap',\n     '2.0': 'mapreduce.input.lineinputformat.linespermap'},\n    {'1.0': 'mapred.linerecordreader.maxlength',\n     '2.0': 'mapreduce.input.linerecordreader.line.maxlength'},\n    {'1.0': 'mapred.local.dir',\n     '2.0': 'mapreduce.cluster.local.dir'},\n    {'1.0': 'mapred.local.dir.minspacekill',\n     '2.0': 'mapreduce.tasktracker.local.dir.minspacekill'},\n    {'1.0': 'mapred.local.dir.minspacestart',\n     '2.0': 'mapreduce.tasktracker.local.dir.minspacestart'},\n    {'1.0': 'mapred.map.child.env',\n     '2.0': 'mapreduce.map.env'},\n    {'1.0': 'mapred.map.child.java.opts',\n     '2.0': 'mapreduce.map.java.opts'},\n    {'1.0': 'mapred.map.child.log.level',\n     '2.0': 'mapreduce.map.log.level'},\n    {'1.0': 'mapred.map.max.attempts',\n     '2.0': 'mapreduce.map.maxattempts'},\n    {'1.0': 'mapred.map.output.compression.codec',\n     '2.0': 'mapreduce.map.output.compress.codec'},\n    {'1.0': 'mapred.map.task.debug.script',\n     '2.0': 'mapreduce.map.debug.script'},\n    {'1.0': 'mapred.map.tasks',\n     '2.0': 'mapreduce.job.maps'},\n    {'1.0': 'mapred.map.tasks.speculative.execution',\n     '2.0': 'mapreduce.map.speculative'},\n    {'1.0': 'mapred.mapoutput.key.class',\n     '2.0': 'mapreduce.map.output.key.class'},\n    {'1.0': 'mapred.mapoutput.value.class',\n     '2.0': 'mapreduce.map.output.value.class'},\n    {'1.0': 'mapred.mapper.regex',\n     '2.0': 'mapreduce.mapper.regex'},\n    {'1.0': 'mapred.mapper.regex.group',\n     '2.0': 'mapreduce.mapper.regexmapper..group'},\n    {'1.0': 'mapred.max.map.failures.percent',\n     '2.0': 'mapreduce.map.failures.maxpercent'},\n    {'1.0': 'mapred.max.reduce.failures.percent',\n     '2.0': 'mapreduce.reduce.failures.maxpercent'},\n    {'1.0': 'mapred.max.split.size',\n     '2.0': 'mapreduce.input.fileinputformat.split.maxsize'},\n    {'1.0': 'mapred.max.tracker.blacklists',\n     '2.0': 'mapreduce.jobtracker.tasktracker.maxblacklists'},\n    {'1.0': 'mapred.max.tracker.failures',\n     '2.0': 'mapreduce.job.maxtaskfailures.per.tracker'},\n    {'1.0': 'mapred.merge.recordsBeforeProgress',\n     '2.0': 'mapreduce.task.merge.progress.records'},\n    {'1.0': 'mapred.min.split.size',\n     '2.0': 'mapreduce.input.fileinputformat.split.minsize'},\n    {'1.0': 'mapred.min.split.size.per.node',\n     '2.0': 'mapreduce.input.fileinputformat.split.minsize.per.node'},\n    {'1.0': 'mapred.min.split.size.per.rack',\n     '2.0': 'mapreduce.input.fileinputformat.split.minsize.per.rack'},\n    {'1.0': 'mapred.output.compress',\n     '2.0': 'mapreduce.output.fileoutputformat.compress'},\n    {'1.0': 'mapred.output.compression.codec',\n     '2.0': 'mapreduce.output.fileoutputformat.compress.codec'},\n    {'1.0': 'mapred.output.compression.type',\n     '2.0': 'mapreduce.output.fileoutputformat.compress.type'},\n    {'1.0': 'mapred.output.dir',\n     '2.0': 'mapreduce.output.fileoutputformat.outputdir'},\n    {'1.0': 'mapred.output.key.class',\n     '2.0': 'mapreduce.job.output.key.class'},\n    {'1.0': 'mapred.output.key.comparator.class',\n     '2.0': 'mapreduce.job.output.key.comparator.class'},\n    {'1.0': 'mapred.output.value.class',\n     '2.0': 'mapreduce.job.output.value.class'},\n    {'1.0': 'mapred.output.value.groupfn.class',\n     '2.0': 'mapreduce.job.output.group.comparator.class'},\n    {'1.0': 'mapred.permissions.supergroup',\n     '2.0': 'mapreduce.cluster.permissions.supergroup'},\n    {'1.0': 'mapred.pipes.user.inputformat',\n     '2.0': 'mapreduce.pipes.inputformat'},\n    {'1.0': 'mapred.reduce.child.env',\n     '2.0': 'mapreduce.reduce.env'},\n    {'1.0': 'mapred.reduce.child.java.opts',\n     '2.0': 'mapreduce.reduce.java.opts'},\n    {'1.0': 'mapred.reduce.child.log.level',\n     '2.0': 'mapreduce.reduce.log.level'},\n    {'1.0': 'mapred.reduce.max.attempts',\n     '2.0': 'mapreduce.reduce.maxattempts'},\n    {'1.0': 'mapred.reduce.parallel.copies',\n     '2.0': 'mapreduce.reduce.shuffle.parallelcopies'},\n    {'1.0': 'mapred.reduce.slowstart.completed.maps',\n     '2.0': 'mapreduce.job.reduce.slowstart.completedmaps'},\n    {'1.0': 'mapred.reduce.task.debug.script',\n     '2.0': 'mapreduce.reduce.debug.script'},\n    {'1.0': 'mapred.reduce.tasks',\n     '2.0': 'mapreduce.job.reduces'},\n    {'1.0': 'mapred.reduce.tasks.speculative.execution',\n     '2.0': 'mapreduce.reduce.speculative'},\n    {'1.0': 'mapred.seqbinary.output.key.class',\n     '2.0': 'mapreduce.output.seqbinaryoutputformat.key.class'},\n    {'1.0': 'mapred.seqbinary.output.value.class',\n     '2.0': 'mapreduce.output.seqbinaryoutputformat.value.class'},\n    {'1.0': 'mapred.shuffle.connect.timeout',\n     '2.0': 'mapreduce.reduce.shuffle.connect.timeout'},\n    {'1.0': 'mapred.shuffle.read.timeout',\n     '2.0': 'mapreduce.reduce.shuffle.read.timeout'},\n    {'1.0': 'mapred.skip.attempts.to.start.skipping',\n     '2.0': 'mapreduce.task.skip.start.attempts'},\n    {'1.0': 'mapred.skip.map.auto.incr.proc.count',\n     '2.0': 'mapreduce.map.skip.proc-count.auto-incr'},\n    {'1.0': 'mapred.skip.map.max.skip.records',\n     '2.0': 'mapreduce.map.skip.maxrecords'},\n    {'1.0': 'mapred.skip.on',\n     '2.0': 'mapreduce.job.skiprecords'},\n    {'1.0': 'mapred.skip.out.dir',\n     '2.0': 'mapreduce.job.skip.outdir'},\n    {'1.0': 'mapred.skip.reduce.auto.incr.proc.count',\n     '2.0': 'mapreduce.reduce.skip.proc-count.auto-incr'},\n    {'1.0': 'mapred.skip.reduce.max.skip.groups',\n     '2.0': 'mapreduce.reduce.skip.maxgroups'},\n    {'1.0': 'mapred.speculative.execution.slowNodeThreshold',\n     '2.0': 'mapreduce.job.speculative.slownodethreshold'},\n    {'1.0': 'mapred.speculative.execution.slowTaskThreshold',\n     '2.0': 'mapreduce.job.speculative.slowtaskthreshold'},\n    {'1.0': 'mapred.speculative.execution.speculativeCap',\n     '2.0': 'mapreduce.job.speculative.speculativecap'},\n    {'1.0': 'mapred.submit.replication',\n     '2.0': 'mapreduce.client.submit.file.replication'},\n    {'1.0': 'mapred.system.dir',\n     '2.0': 'mapreduce.jobtracker.system.dir'},\n    {'1.0': 'mapred.task.cache.levels',\n     '2.0': 'mapreduce.jobtracker.taskcache.levels'},\n    {'1.0': 'mapred.task.id',\n     '2.0': 'mapreduce.task.attempt.id'},\n    {'1.0': 'mapred.task.is.map',\n     '2.0': 'mapreduce.task.ismap'},\n    {'1.0': 'mapred.task.partition',\n     '2.0': 'mapreduce.task.partition'},\n    {'1.0': 'mapred.task.profile',\n     '2.0': 'mapreduce.task.profile'},\n    {'1.0': 'mapred.task.profile.maps',\n     '2.0': 'mapreduce.task.profile.maps'},\n    {'1.0': 'mapred.task.profile.params',\n     '2.0': 'mapreduce.task.profile.params'},\n    {'1.0': 'mapred.task.profile.reduces',\n     '2.0': 'mapreduce.task.profile.reduces'},\n    {'1.0': 'mapred.task.timeout',\n     '2.0': 'mapreduce.task.timeout'},\n    {'1.0': 'mapred.task.tracker.http.address',\n     '2.0': 'mapreduce.tasktracker.http.address'},\n    {'1.0': 'mapred.task.tracker.report.address',\n     '2.0': 'mapreduce.tasktracker.report.address'},\n    {'1.0': 'mapred.task.tracker.task-controller',\n     '2.0': 'mapreduce.tasktracker.taskcontroller'},\n    {'1.0': 'mapred.tasktracker.dns.interface',\n     '2.0': 'mapreduce.tasktracker.dns.interface'},\n    {'1.0': 'mapred.tasktracker.dns.nameserver',\n     '2.0': 'mapreduce.tasktracker.dns.nameserver'},\n    {'1.0': 'mapred.tasktracker.events.batchsize',\n     '2.0': 'mapreduce.tasktracker.events.batchsize'},\n    {'1.0': 'mapred.tasktracker.expiry.interval',\n     '2.0': 'mapreduce.jobtracker.expire.trackers.interval'},\n    {'1.0': 'mapred.tasktracker.indexcache.mb',\n     '2.0': 'mapreduce.tasktracker.indexcache.mb'},\n    {'1.0': 'mapred.tasktracker.instrumentation',\n     '2.0': 'mapreduce.tasktracker.instrumentation'},\n    {'1.0': 'mapred.tasktracker.map.tasks.maximum',\n     '2.0': 'mapreduce.tasktracker.map.tasks.maximum'},\n    {'1.0': 'mapred.tasktracker.memory_calculator_plugin',\n     '2.0': 'mapreduce.tasktracker.resourcecalculatorplugin'},\n    {'1.0': 'mapred.tasktracker.memorycalculatorplugin',\n     '2.0': 'mapreduce.tasktracker.resourcecalculatorplugin'},\n    {'1.0': 'mapred.tasktracker.reduce.tasks.maximum',\n     '2.0': 'mapreduce.tasktracker.reduce.tasks.maximum'},\n    {'1.0': 'mapred.tasktracker.taskmemorymanager.monitoring-interval',\n     '2.0': 'mapreduce.tasktracker.taskmemorymanager.monitoringinterval'},\n    {'1.0': 'mapred.tasktracker.tasks.sleeptime-before-sigkill',\n     '2.0': 'mapreduce.tasktracker.tasks.sleeptimebeforesigkill'},\n    {'1.0': 'mapred.temp.dir',\n     '2.0': 'mapreduce.cluster.temp.dir'},\n    {'1.0': 'mapred.text.key.comparator.options',\n     '2.0': 'mapreduce.partition.keycomparator.options'},\n    {'1.0': 'mapred.text.key.partitioner.options',\n     '2.0': 'mapreduce.partition.keypartitioner.options'},\n    {'1.0': 'mapred.textoutputformat.separator',\n     '2.0': 'mapreduce.output.textoutputformat.separator'},\n    {'1.0': 'mapred.tip.id',\n     '2.0': 'mapreduce.task.id'},\n    {'1.0': 'mapred.used.genericoptionsparser',\n     '2.0': 'mapreduce.client.genericoptionsparser.used'},\n    {'1.0': 'mapred.userlog.limit.kb',\n     '2.0': 'mapreduce.task.userlog.limit.kb'},\n    {'1.0': 'mapred.userlog.retain.hours',\n     '2.0': 'mapreduce.job.userlog.retain.hours'},\n    {'1.0': 'mapred.work.output.dir',\n     '2.0': 'mapreduce.task.output.dir'},\n    {'1.0': 'mapred.working.dir',\n     '2.0': 'mapreduce.job.working.dir'},\n    {'1.0': 'mapreduce.combine.class',\n     '2.0': 'mapreduce.job.combine.class'},\n    {'1.0': 'mapreduce.inputformat.class',\n     '2.0': 'mapreduce.job.inputformat.class'},\n    {'1.0': 'mapreduce.jobtracker.permissions.supergroup',\n     '2.0': 'mapreduce.cluster.permissions.supergroup'},\n    {'1.0': 'mapreduce.map.class',\n     '2.0': 'mapreduce.job.map.class'},\n    {'1.0': 'mapreduce.outputformat.class',\n     '2.0': 'mapreduce.job.outputformat.class'},\n    {'1.0': 'mapreduce.partitioner.class',\n     '2.0': 'mapreduce.job.partitioner.class'},\n    {'1.0': 'mapreduce.reduce.class',\n     '2.0': 'mapreduce.job.reduce.class'},\n    {'1.0': 'min.num.spills.for.combine',\n     '2.0': 'mapreduce.map.combine.minspills'},\n    {'1.0': 'reduce.output.key.value.fields.spec',\n     '2.0': 'mapreduce.fieldsel.reduce.output.key.value.fields.spec'},\n    {'1.0': 'security.job.submission.protocol.acl',\n     '2.0': 'security.job.client.protocol.acl'},\n    {'1.0': 'security.task.umbilical.protocol.acl',\n     '2.0': 'security.job.task.protocol.acl'},\n    {'1.0': 'sequencefile.filter.class',\n     '2.0': 'mapreduce.input.sequencefileinputfilter.class'},\n    {'1.0': 'sequencefile.filter.frequency',\n     '2.0': 'mapreduce.input.sequencefileinputfilter.frequency'},\n    {'1.0': 'sequencefile.filter.regex',\n     '2.0': 'mapreduce.input.sequencefileinputfilter.regex'},\n    {'1.0': 'session.id',\n     '2.0': 'dfs.metrics.session-id'},\n    {'1.0': 'slave.host.name',\n     '2.0': 'dfs.datanode.hostname'},\n    {'1.0': 'slave.host.name',\n     '2.0': 'mapreduce.tasktracker.host.name'},\n    {'1.0': 'tasktracker.contention.tracking',\n     '2.0': 'mapreduce.tasktracker.contention.tracking'},\n    {'1.0': 'tasktracker.http.threads',\n     '2.0': 'mapreduce.tasktracker.http.threads'},\n    {'1.0': 'topology.node.switch.mapping.impl',\n     '2.0': 'net.topology.node.switch.mapping.impl'},\n    {'1.0': 'topology.script.file.name',\n     '2.0': 'net.topology.script.file.name'},\n    {'1.0': 'topology.script.number.args',\n     '2.0': 'net.topology.script.number.args'},\n    {'1.0': 'user.name',\n     '2.0': 'mapreduce.job.user.name'},\n    {'1.0': 'webinterface.private.actions',\n     '2.0': 'mapreduce.jobtracker.webinterface.trusted'},\n]\n\n# Handle compatibility for 0.x versions of Hadoop too\nfor jobconf_dict in _JOBCONF_DICT_LIST:\n    jobconf_dict['0.20'] = jobconf_dict['1.0']\n    jobconf_dict['0.21'] = jobconf_dict['2.0']\n\n\ndef _dict_list_to_compat_map(dict_list):\n    # compat_map = {\n    #   ...\n    #   a: {'1.0': a, '2.0': b}\n    #   ..\n    # }\n    compat_map = {}\n    for version_dict in dict_list:\n        for value in version_dict.values():\n            compat_map[value] = version_dict\n    return compat_map\n\n\n_JOBCONF_MAP = _dict_list_to_compat_map(_JOBCONF_DICT_LIST)\n\n\ndef jobconf_from_env(variable, default=None):\n    \"\"\"Get the value of a jobconf variable from the runtime environment.\n\n    For example, a :py:class:`~mrjob.job.MRJob` could use\n    ``jobconf_from_env('map.input.file')`` to get the name of the file a\n    mapper is reading input from.\n\n    If the name of the jobconf variable is different in different versions of\n    Hadoop (e.g. in Hadoop 2.0, ``map.input.file`` is\n    ``mapreduce.map.input.file``), we'll automatically try all variants before\n    giving up.\n\n    Return *default* if that jobconf variable isn't set.\n    \"\"\"\n    # try variable verbatim first\n    name = variable.replace('.', '_')\n    if name in os.environ:\n        return os.environ[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(variable, {}).values():\n        name = var.replace('.', '_')\n        if name in os.environ:\n            return os.environ[name]\n\n    return default\n\n\ndef jobconf_from_dict(jobconf, name, default=None):\n    \"\"\"Get the value of a jobconf variable from the given dictionary.\n\n    :param dict jobconf: jobconf dictionary\n    :param string name: name of the jobconf variable (e.g. ``'user.name'``)\n    :param default: fallback value\n\n    If the name of the jobconf variable is different in different versions of\n    Hadoop (e.g. in Hadoop 2, ``map.input.file`` is\n    ``mapreduce.map.input.file``), we'll automatically try all variants before\n    giving up.\n\n    Return *default* if that jobconf variable isn't set    \"\"\"\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(name, {}).values():\n        if alternative in jobconf:\n            return jobconf[alternative]\n\n    return default\n\n\ndef map_version(version, version_map):\n    \"\"\"Allows you to look up something by version (e.g. which jobconf variable\n    to use, specifying only the versions where that value changed.\n\n    *version* is a string\n\n    *version_map* is a map from version (as a string) that a value changed\n    to the new value.\n\n    For efficiency, *version_map* can also be a list of tuples of\n    ``(LooseVersion(version_as_string), value)``, with oldest versions first.\n\n    If *version* is less than any version in *version_map*, use the value for\n    the earliest version in *version_map*.\n    \"\"\"\n    if version is None:\n        raise TypeError\n\n    if not version_map:\n        raise ValueError\n\n    if isinstance(version_map, dict):\n        version_map = sorted((LooseVersion(k), v)\n                             for k, v in version_map.items())\n\n    req_version = LooseVersion(version)\n\n    for min_version, value in reversed(version_map):\n        if req_version >= min_version:\n            return value\n    else:\n        return version_map[0][1]\n\n\ndef translate_jobconf(variable, version):\n    \"\"\"Translate *variable* to Hadoop version *version*. If it's not\n    a variable we recognize, leave as-is.\n    \"\"\"\n    if version is None:\n        raise TypeError\n\n    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable\n\n\ndef translate_jobconf_for_all_versions(variable):\n    \"\"\"Get all known variants of the given jobconf variable.\n    Unlike :py:func:`translate_jobconf`, returns a list.\"\"\"\n    return sorted(\n        set([variable] + list(_JOBCONF_MAP.get(variable, {}).values())))\n\n\ndef translate_jobconf_dict(jobconf, hadoop_version=None):\n    \"\"\"Translates the configuration property name to match those that\n    are accepted in hadoop_version. Prints a warning message if any\n    configuration property name does not match the name in the hadoop\n    version. Combines the original jobconf with the translated jobconf.\n\n    :return: a map consisting of the original and translated configuration\n             property names and values.\n    \"\"\"\n    translated_jobconf = jobconf.copy()\n    translation_warnings = {}\n\n    for variable, value in jobconf.items():\n        if hadoop_version:\n            variants = [translate_jobconf(variable, hadoop_version)]\n        else:\n            variants = translate_jobconf_for_all_versions(variable)\n\n        for variant in variants:\n            if variant in jobconf:\n                # this happens if variant == variable or\n                # if the variant was in jobconf to start with\n                continue\n\n            translated_jobconf[variant] = value\n\n            if hadoop_version:\n                translation_warnings[variable] = variant\n\n    if translation_warnings:\n        log.warning(\"Detected hadoop configuration property names that\"\n                    \" do not match hadoop version %s:\"\n                    \"\\nThe have been translated as follows\\n %s\",\n                    hadoop_version,\n                    '\\n'.join([\n                        \"%s: %s\" % (variable, variant) for variable, variant\n                        in sorted(translation_warnings.items())]))\n\n    return translated_jobconf\n\n\ndef uses_yarn(version):\n    \"\"\"Basically, is this Hadoop 2? This also handles versions in the\n    zero series (0.23+) where YARN originated.\"\"\"\n    return (version_gte(version, '2') or\n            version_gte(version, '0.23') and not version_gte(version, '1'))\n\n\ndef version_gte(version, cmp_version_str):\n    \"\"\"Return ``True`` if version >= *cmp_version_str*.\"\"\"\n\n    if not isinstance(version, string_types):\n        raise TypeError('%r is not a string' % version)\n\n    if not isinstance(cmp_version_str, string_types):\n        raise TypeError('%r is not a string' % cmp_version_str)\n\n    return LooseVersion(version) >= LooseVersion(cmp_version_str)\n"}], "prompt": "Please write a python function called 'mkdir' base the context. Create a directory in the Hadoop filesystem. It uses Hadoop 'fs -mkdir' command (additionally with '-p' option on Hadoop 2) to create the directory. If the command fails except for the case where the directory already exists, it raises an IOError: 'Could not mkdir {path}'.:param self: HadoopFilesystem. An instance of the HadoopFilesystem class.\n:param path: str. The path of the directory to be created.\n:return: No return values..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # Copyright 2009-2012 Yelp and Contributors\n# Copyright 2013 David Marin\n# Copyright 2015-2018 Yelp\n# Copyright 2019 Yelp\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport logging\nimport os.path\nimport re\nfrom io import BytesIO\nfrom subprocess import Popen\nfrom subprocess import PIPE\nfrom subprocess import CalledProcessError\n\nfrom mrjob.cat import decompress\nfrom mrjob.compat import uses_yarn\nfrom mrjob.fs.base import Filesystem\nfrom mrjob.py2 import to_unicode\nfrom mrjob.parse import is_uri\nfrom mrjob.parse import urlparse\nfrom mrjob.util import cmd_line\nfrom mrjob.util import unique\nfrom mrjob.util import which\n\n\nlog = logging.getLogger(__name__)\n\n# used by mkdir()\n_HADOOP_FILE_EXISTS_RE = re.compile(br'.*File exists.*')\n\n# used by ls() and exists()\n_HADOOP_LS_NO_SUCH_FILE = re.compile(br'^lsr?: .*No such file.*$')\n\n# used by rm() (see below)\n_HADOOP_RM_NO_SUCH_FILE = re.compile(br'^rmr?: .*No such file.*$')\n\n# find version string in \"Hadoop 0.20.203\" etc.\n_HADOOP_VERSION_RE = re.compile(br'^.*?(?P<version>(\\d|\\.)+).*?$')\n\n\nclass HadoopFilesystem(Filesystem):\n    \"\"\"Filesystem for URIs accepted by ``hadoop fs``. Typically you will get\n    one of these via ``HadoopJobRunner().fs``, composed with\n    :py:class:`~mrjob.fs.local.LocalFilesystem`.\n\n    This also helps with other invocations of the ``hadoop`` binary, such\n    as ``hadoop version`` (see :py:meth:`invoke_hadoop`).\n    \"\"\"\n\n    def __init__(self, hadoop_bin=None):\n        \"\"\"Create a Hadoop filesystem\n\n        :param hadoop_bin: ``hadoop`` binary, as a list of args. If set to\n                           ``None``, we'll auto-detect the Hadoop binary.\n                           If set to ``[]``, this FS will be disabled\n                           until you call :py:meth:`set_hadoop_bin`.\n        \"\"\"\n        super(HadoopFilesystem, self).__init__()\n        self._hadoop_bin = hadoop_bin\n        self._hadoop_version = None  # cache for get_hadoop_version()\n\n    def can_handle_path(self, path):\n        if not (self._hadoop_bin or self._hadoop_bin is None):\n            return False\n\n        return is_uri(path)\n\n    def get_hadoop_bin(self):\n        \"\"\"Return the hadoop binary, searching for it if need be.\"\"\"\n        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin\n\n    def set_hadoop_bin(self, hadoop_bin):\n        \"\"\"Manually set the hadoop binary, as a list of args.\"\"\"\n        self._hadoop_bin = hadoop_bin\n\n    def _find_hadoop_bin(self):\n        \"\"\"Look for the hadoop binary in any plausible place. If all\n        else fails, return ``['hadoop']``.\n        \"\"\"\n        def yield_paths():\n            for name in 'HADOOP_PREFIX', 'HADOOP_HOME', 'HADOOP_INSTALL':\n                path = os.environ.get(name)\n                if path:\n                    yield os.path.join(path, 'bin')\n\n            # They use $HADOOP_INSTALL/hadoop/bin here:\n            # https://wiki.apache.org/hadoop/GettingStartedWithHadoop\n            if os.environ.get('HADOOP_INSTALL'):\n                yield os.path.join(\n                    os.environ['HADOOP_INSTALL'], 'hadoop', 'bin')\n\n            yield None  # use $PATH\n\n            # Maybe it's in $HADOOP_MAPRED_HOME? $HADOOP_YARN_HOME? Don't give\n            # up. Don't worry about duplicates; they're de-duplicated below\n            for name, path in sorted(os.environ.items()):\n                if name.startswith('HADOOP_') and name.endswith('_HOME'):\n                    yield os.path.join(path, 'bin')\n\n        for path in unique(yield_paths()):\n            log.info('Looking for hadoop binary in %s...' % (path or '$PATH'))\n\n            hadoop_bin = which('hadoop', path=path)\n\n            if hadoop_bin:\n                log.info('Found hadoop binary: %s' % hadoop_bin)\n                return [hadoop_bin]\n        else:\n            log.info(\"Falling back to 'hadoop'\")\n            return ['hadoop']\n\n    def get_hadoop_version(self):\n        \"\"\"Invoke the hadoop executable to determine its version\"\"\"\n        # mkdir() needs this\n        if not self._hadoop_version:\n            stdout = self.invoke_hadoop(['version'], return_stdout=True)\n            if stdout:\n                first_line = stdout.split(b'\\n')[0]\n                m = _HADOOP_VERSION_RE.match(first_line)\n                if m:\n                    self._hadoop_version = to_unicode(m.group('version'))\n                    log.info(\"Using Hadoop version %s\" % self._hadoop_version)\n                else:\n                    raise Exception('Unable to determine Hadoop version.')\n\n        return self._hadoop_version\n\n    def invoke_hadoop(self, args, ok_returncodes=None, ok_stderr=None,\n                      return_stdout=False):\n        \"\"\"Run the given hadoop command, raising an exception on non-zero\n        return code. This only works for commands whose output we don't\n        care about.\n\n        Args:\n        ok_returncodes -- a list/tuple/set of return codes we expect to\n            get back from hadoop (e.g. [0,1]). By default, we only expect 0.\n            If we get an unexpected return code, we raise a CalledProcessError.\n        ok_stderr -- don't log STDERR or raise CalledProcessError if stderr\n            matches a regex in this list (even if the returncode is bad)\n        return_stdout -- return the stdout from the hadoop command rather\n            than logging it. If this is False, we return the returncode\n            instead.\n        \"\"\"\n        args = self.get_hadoop_bin() + args\n\n        log.debug('> %s' % cmd_line(args))\n\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = proc.communicate()\n\n        log_func = log.debug if proc.returncode == 0 else log.error\n        if not return_stdout:\n            for line in BytesIO(stdout):\n                log_func('STDOUT: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        # check if STDERR is okay\n        stderr_is_ok = False\n        if ok_stderr:\n            for stderr_re in ok_stderr:\n                if stderr_re.match(stderr):\n                    stderr_is_ok = True\n                    break\n\n        if not stderr_is_ok:\n            for line in BytesIO(stderr):\n                log_func('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        ok_returncodes = ok_returncodes or [0]\n\n        if not stderr_is_ok and proc.returncode not in ok_returncodes:\n            raise CalledProcessError(proc.returncode, args)\n\n        if return_stdout:\n            return stdout\n        else:\n            return proc.returncode\n\n    def du(self, path_glob):\n        \"\"\"Get the size of a file or directory (recursively), or 0\n        if it doesn't exist.\"\"\"\n        try:\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob],\n                                        return_stdout=True,\n                                        ok_returncodes=[0, 1, 255])\n        except CalledProcessError:\n            return 0\n\n        try:\n            return sum(int(line.split()[0])\n                       for line in stdout.split(b'\\n')\n                       if line.strip())\n        except (ValueError, TypeError, IndexError):\n            raise IOError(\n                'Unexpected output from hadoop fs -du: %r' % stdout)\n\n    def ls(self, path_glob):\n        components = urlparse(path_glob)\n        hdfs_prefix = '%s://%s' % (components.scheme, components.netloc)\n\n        version = self.get_hadoop_version()\n\n        # use ls -R on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = ['fs', '-ls', '-R', path_glob]\n        else:\n            args = ['fs', '-lsr', path_glob]\n\n        try:\n            stdout = self.invoke_hadoop(args, return_stdout=True,\n                                        ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not ls %s\" % path_glob)\n\n        for line in BytesIO(stdout):\n            line = line.rstrip(b'\\r\\n')\n\n            # ignore total item count\n            if line.startswith(b'Found '):\n                continue\n\n            fields = line.split(b' ')\n\n            # Throw out directories\n            if fields[0].startswith(b'd'):\n                continue\n\n            # Try to figure out which part of the line is the path\n            # Expected lines:\n            #\n            # HDFS:\n            # -rw-r--r--   3 dave users       3276 2010-01-13 14:00 /foo/bar\n            #\n            # S3:\n            # -rwxrwxrwx   1          3276 010-01-13 14:00 /foo/bar\n            path_index = None\n            for index, field in enumerate(fields):\n                # look for time field, and pick one after that\n                # (can't use field[2] because that's an int in Python 3)\n                if len(field) == 5 and field[2:3] == b':':\n                    path_index = (index + 1)\n            if not path_index:\n                raise IOError(\"Could not locate path in string %r\" % line)\n\n            path = to_unicode(line.split(b' ', path_index)[-1])\n            # handle fully qualified URIs from newer versions of Hadoop ls\n            # (see Pull Request #577)\n            if is_uri(path):\n                yield path\n            else:\n                yield hdfs_prefix + path\n\n    def _cat_file(self, path):\n        # stream from HDFS\n        cat_args = self.get_hadoop_bin() + ['fs', '-cat', path]\n        log.debug('> %s' % cmd_line(cat_args))\n\n        cat_proc = Popen(cat_args, stdout=PIPE, stderr=PIPE)\n\n        for chunk in decompress(cat_proc.stdout, path):\n            yield chunk\n\n        # this does someties happen; see #1396\n        for line in cat_proc.stderr:\n            log.error('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        cat_proc.stdout.close()\n        cat_proc.stderr.close()\n\n        returncode = cat_proc.wait()\n\n        if returncode != 0:\n            raise IOError(\"Could not stream %s\" % path)\n\n###The function: mkdir###\n    def exists(self, path_glob):\n        \"\"\"Does the given path exist?\n\n        If dest is a directory (ends with a \"/\"), we check if there are\n        any files starting with that path.\n        \"\"\"\n        try:\n            return_code = self.invoke_hadoop(\n                ['fs', '-ls', path_glob],\n                ok_returncodes=[0, -1, 255],\n                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n\n            return (return_code == 0)\n        except CalledProcessError:\n            raise IOError(\"Could not check path %s\" % path_glob)\n\n    def put(self, src, path):\n        # don't inadvertently support cp syntax\n        if path.endswith('/'):\n            raise ValueError('put() destination may not be a directory')\n\n        self.invoke_hadoop(['fs', '-put', src, path])\n\n    def rm(self, path_glob):\n        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n\n        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-R', '-f', '-skipTrash', path_glob]\n        else:\n            args = ['fs', '-rmr', '-skipTrash', path_glob]\n\n        try:\n            self.invoke_hadoop(\n                args,\n                return_stdout=True, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)\n\n    def touchz(self, path):\n        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)\n\n        ####cross_file_context:\n        [{'mrjob.compat.uses_yarn': '# -*- coding: utf-8 -*-\\n# Copyright 2009-2012 Yelp\\n# Copyright 2013-2014 Yelp and Contributors\\n# Copyright 2015-2016 Yelp\\n# Copyright 2018 Ben Dalling\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n# http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\n\"\"\"Utility functions for compatibility with different version of hadoop.\"\"\"\\nfrom distutils.version import LooseVersion\\nimport logging\\nimport os\\n\\nfrom mrjob.py2 import string_types\\n\\n# lists alternative names for jobconf variables\\n# full listing thanks to translation table in\\n# http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html # noqa\\n\\nlog = logging.getLogger(__name__)\\n\\n_JOBCONF_DICT_LIST = [\\n    {\\'1.0\\': \\'StorageId\\',\\n     \\'2.0\\': \\'dfs.datanode.StorageId\\'},\\n    {\\'1.0\\': \\'create.empty.dir.if.nonexist\\',\\n     \\'2.0\\': \\'mapreduce.jobcontrol.createdir.ifnotexist\\'},\\n    {\\'1.0\\': \\'dfs.access.time.precision\\',\\n     \\'2.0\\': \\'dfs.namenode.accesstime.precision\\'},\\n    {\\'1.0\\': \\'dfs.backup.address\\',\\n     \\'2.0\\': \\'dfs.namenode.backup.address\\'},\\n    {\\'1.0\\': \\'dfs.backup.http.address\\',\\n     \\'2.0\\': \\'dfs.namenode.backup.http-address\\'},\\n    {\\'1.0\\': \\'dfs.balance.bandwidthPerSec\\',\\n     \\'2.0\\': \\'dfs.datanode.balance.bandwidthPerSec\\'},\\n    {\\'1.0\\': \\'dfs.block.size\\',\\n     \\'2.0\\': \\'dfs.blocksize\\'},\\n    {\\'1.0\\': \\'dfs.client.buffer.dir\\',\\n     \\'2.0\\': \\'fs.client.buffer.dir\\'},\\n    {\\'1.0\\': \\'dfs.data.dir\\',\\n     \\'2.0\\': \\'dfs.datanode.data.dir\\'},\\n    {\\'1.0\\': \\'dfs.datanode.max.xcievers\\',\\n     \\'2.0\\': \\'dfs.datanode.max.transfer.threads\\'},\\n    {\\'1.0\\': \\'dfs.df.interval\\',\\n     \\'2.0\\': \\'fs.df.interval\\'},\\n    {\\'1.0\\': \\'dfs.http.address\\',\\n     \\'2.0\\': \\'dfs.namenode.http-address\\'},\\n    {\\'1.0\\': \\'dfs.https.address\\',\\n     \\'2.0\\': \\'dfs.namenode.https-address\\'},\\n    {\\'1.0\\': \\'dfs.https.client.keystore.resource\\',\\n     \\'2.0\\': \\'dfs.client.https.keystore.resource\\'},\\n    {\\'1.0\\': \\'dfs.https.need.client.auth\\',\\n     \\'2.0\\': \\'dfs.client.https.need-auth\\'},\\n    {\\'1.0\\': \\'dfs.max-repl-streams\\',\\n     \\'2.0\\': \\'dfs.namenode.replication.max-streams\\'},\\n    {\\'1.0\\': \\'dfs.max.objects\\',\\n     \\'2.0\\': \\'dfs.namenode.max.objects\\'},\\n    {\\'1.0\\': \\'dfs.name.dir\\',\\n     \\'2.0\\': \\'dfs.namenode.name.dir\\'},\\n    {\\'1.0\\': \\'dfs.name.dir.restore\\',\\n     \\'2.0\\': \\'dfs.namenode.name.dir.restore\\'},\\n    {\\'1.0\\': \\'dfs.name.edits.dir\\',\\n     \\'2.0\\': \\'dfs.namenode.edits.dir\\'},\\n    {\\'1.0\\': \\'dfs.permissions\\',\\n     \\'2.0\\': \\'dfs.permissions.enabled\\'},\\n    {\\'1.0\\': \\'dfs.permissions.supergroup\\',\\n     \\'2.0\\': \\'dfs.permissions.superusergroup\\'},\\n    {\\'1.0\\': \\'dfs.read.prefetch.size\\',\\n     \\'2.0\\': \\'dfs.client.read.prefetch.size\\'},\\n    {\\'1.0\\': \\'dfs.replication.considerLoad\\',\\n     \\'2.0\\': \\'dfs.namenode.replication.considerLoad\\'},\\n    {\\'1.0\\': \\'dfs.replication.interval\\',\\n     \\'2.0\\': \\'dfs.namenode.replication.interval\\'},\\n    {\\'1.0\\': \\'dfs.replication.min\\',\\n     \\'2.0\\': \\'dfs.namenode.replication.min\\'},\\n    {\\'1.0\\': \\'dfs.replication.pending.timeout.sec\\',\\n     \\'2.0\\': \\'dfs.namenode.replication.pending.timeout-sec\\'},\\n    {\\'1.0\\': \\'dfs.safemode.extension\\',\\n     \\'2.0\\': \\'dfs.namenode.safemode.extension\\'},\\n    {\\'1.0\\': \\'dfs.safemode.threshold.pct\\',\\n     \\'2.0\\': \\'dfs.namenode.safemode.threshold-pct\\'},\\n    {\\'1.0\\': \\'dfs.secondary.http.address\\',\\n     \\'2.0\\': \\'dfs.namenode.secondary.http-address\\'},\\n    {\\'1.0\\': \\'dfs.socket.timeout\\',\\n     \\'2.0\\': \\'dfs.client.socket-timeout\\'},\\n    {\\'1.0\\': \\'dfs.upgrade.permission\\',\\n     \\'2.0\\': \\'dfs.namenode.upgrade.permission\\'},\\n    {\\'1.0\\': \\'dfs.write.packet.size\\',\\n     \\'2.0\\': \\'dfs.client-write-packet-size\\'},\\n    {\\'1.0\\': \\'fs.checkpoint.dir\\',\\n     \\'2.0\\': \\'dfs.namenode.checkpoint.dir\\'},\\n    {\\'1.0\\': \\'fs.checkpoint.edits.dir\\',\\n     \\'2.0\\': \\'dfs.namenode.checkpoint.edits.dir\\'},\\n    {\\'1.0\\': \\'fs.checkpoint.period\\',\\n     \\'2.0\\': \\'dfs.namenode.checkpoint.period\\'},\\n    {\\'1.0\\': \\'fs.default.name\\',\\n     \\'2.0\\': \\'fs.defaultFS\\'},\\n    {\\'1.0\\': \\'hadoop.configured.node.mapping\\',\\n     \\'2.0\\': \\'net.topology.configured.node.mapping\\'},\\n    {\\'1.0\\': \\'hadoop.job.history.location\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.jobhistory.location\\'},\\n    {\\'1.0\\': \\'hadoop.native.lib\\',\\n     \\'2.0\\': \\'io.native.lib.available\\'},\\n    {\\'1.0\\': \\'hadoop.net.static.resolutions\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.net.static.resolutions\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.command-file.keep\\',\\n     \\'2.0\\': \\'mapreduce.pipes.commandfile.preserve\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.executable\\',\\n     \\'2.0\\': \\'mapreduce.pipes.executable\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.executable.interpretor\\',\\n     \\'2.0\\': \\'mapreduce.pipes.executable.interpretor\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.java.mapper\\',\\n     \\'2.0\\': \\'mapreduce.pipes.isjavamapper\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.java.recordreader\\',\\n     \\'2.0\\': \\'mapreduce.pipes.isjavarecordreader\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.java.recordwriter\\',\\n     \\'2.0\\': \\'mapreduce.pipes.isjavarecordwriter\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.java.reducer\\',\\n     \\'2.0\\': \\'mapreduce.pipes.isjavareducer\\'},\\n    {\\'1.0\\': \\'hadoop.pipes.partitioner\\',\\n     \\'2.0\\': \\'mapreduce.pipes.partitioner\\'},\\n    {\\'1.0\\': \\'heartbeat.recheck.interval\\',\\n     \\'2.0\\': \\'dfs.namenode.heartbeat.recheck-interval\\'},\\n    {\\'1.0\\': \\'io.bytes.per.checksum\\',\\n     \\'2.0\\': \\'dfs.bytes-per-checksum\\'},\\n    {\\'1.0\\': \\'io.sort.factor\\',\\n     \\'2.0\\': \\'mapreduce.task.io.sort.factor\\'},\\n    {\\'1.0\\': \\'io.sort.mb\\',\\n     \\'2.0\\': \\'mapreduce.task.io.sort.mb\\'},\\n    {\\'1.0\\': \\'io.sort.spill.percent\\',\\n     \\'2.0\\': \\'mapreduce.map.sort.spill.percent\\'},\\n    {\\'1.0\\': \\'job.end.notification.url\\',\\n     \\'2.0\\': \\'mapreduce.job.end-notification.url\\'},\\n    {\\'1.0\\': \\'job.end.retry.attempts\\',\\n     \\'2.0\\': \\'mapreduce.job.end-notification.retry.attempts\\'},\\n    {\\'1.0\\': \\'job.end.retry.interval\\',\\n     \\'2.0\\': \\'mapreduce.job.end-notification.retry.interval\\'},\\n    {\\'1.0\\': \\'job.local.dir\\',\\n     \\'2.0\\': \\'mapreduce.job.local.dir\\'},\\n    {\\'1.0\\': \\'jobclient.completion.poll.interval\\',\\n     \\'2.0\\': \\'mapreduce.client.completion.pollinterval\\'},\\n    {\\'1.0\\': \\'jobclient.output.filter\\',\\n     \\'2.0\\': \\'mapreduce.client.output.filter\\'},\\n    {\\'1.0\\': \\'jobclient.progress.monitor.poll.interval\\',\\n     \\'2.0\\': \\'mapreduce.client.progressmonitor.pollinterval\\'},\\n    {\\'1.0\\': \\'keep.failed.task.files\\',\\n     \\'2.0\\': \\'mapreduce.task.files.preserve.failedtasks\\'},\\n    {\\'1.0\\': \\'keep.task.files.pattern\\',\\n     \\'2.0\\': \\'mapreduce.task.files.preserve.filepattern\\'},\\n    {\\'1.0\\': \\'key.value.separator.in.input.line\\',\\n     \\'2.0\\': \\'mapreduce.input.keyvaluelinerecordreader.key.value.separator\\'},\\n    {\\'1.0\\': \\'local.cache.size\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.cache.local.size\\'},\\n    {\\'1.0\\': \\'map.input.file\\',\\n     \\'2.0\\': \\'mapreduce.map.input.file\\'},\\n    {\\'1.0\\': \\'map.input.length\\',\\n     \\'2.0\\': \\'mapreduce.map.input.length\\'},\\n    {\\'1.0\\': \\'map.input.start\\',\\n     \\'2.0\\': \\'mapreduce.map.input.start\\'},\\n    {\\'1.0\\': \\'map.output.key.field.separator\\',\\n     \\'2.0\\': \\'mapreduce.map.output.key.field.separator\\'},\\n    {\\'1.0\\': \\'map.output.key.value.fields.spec\\',\\n     \\'2.0\\': \\'mapreduce.fieldsel.map.output.key.value.fields.spec\\'},\\n    {\\'1.0\\': \\'mapred.acls.enabled\\',\\n     \\'2.0\\': \\'mapreduce.cluster.acls.enabled\\'},\\n    {\\'1.0\\': \\'mapred.binary.partitioner.left.offset\\',\\n     \\'2.0\\': \\'mapreduce.partition.binarypartitioner.left.offset\\'},\\n    {\\'1.0\\': \\'mapred.binary.partitioner.right.offset\\',\\n     \\'2.0\\': \\'mapreduce.partition.binarypartitioner.right.offset\\'},\\n    {\\'1.0\\': \\'mapred.cache.archives\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.archives\\'},\\n    {\\'1.0\\': \\'mapred.cache.archives.timestamps\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.archives.timestamps\\'},\\n    {\\'1.0\\': \\'mapred.cache.files\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.files\\'},\\n    {\\'1.0\\': \\'mapred.cache.files.timestamps\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.files.timestamps\\'},\\n    {\\'1.0\\': \\'mapred.cache.localArchives\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.local.archives\\'},\\n    {\\'1.0\\': \\'mapred.cache.localFiles\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.local.files\\'},\\n    {\\'1.0\\': \\'mapred.child.tmp\\',\\n     \\'2.0\\': \\'mapreduce.task.tmp.dir\\'},\\n    {\\'1.0\\': \\'mapred.cluster.average.blacklist.threshold\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.blacklist.average.threshold\\'},\\n    {\\'1.0\\': \\'mapred.cluster.map.memory.mb\\',\\n     \\'2.0\\': \\'mapreduce.cluster.mapmemory.mb\\'},\\n    {\\'1.0\\': \\'mapred.cluster.max.map.memory.mb\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.maxmapmemory.mb\\'},\\n    {\\'1.0\\': \\'mapred.cluster.max.reduce.memory.mb\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.maxreducememory.mb\\'},\\n    {\\'1.0\\': \\'mapred.cluster.reduce.memory.mb\\',\\n     \\'2.0\\': \\'mapreduce.cluster.reducememory.mb\\'},\\n    {\\'1.0\\': \\'mapred.committer.job.setup.cleanup.needed\\',\\n     \\'2.0\\': \\'mapreduce.job.committer.setup.cleanup.needed\\'},\\n    {\\'1.0\\': \\'mapred.compress.map.output\\',\\n     \\'2.0\\': \\'mapreduce.map.output.compress\\'},\\n    {\\'1.0\\': \\'mapred.create.symlink\\',\\n     \\'2.0\\': \\'mapreduce.job.cache.symlink.create\\'},\\n    {\\'1.0\\': \\'mapred.data.field.separator\\',\\n     \\'2.0\\': \\'mapreduce.fieldsel.data.field.separator\\'},\\n    {\\'1.0\\': \\'mapred.debug.out.lines\\',\\n     \\'2.0\\': \\'mapreduce.task.debugout.lines\\'},\\n    {\\'1.0\\': \\'mapred.healthChecker.interval\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.healthchecker.interval\\'},\\n    {\\'1.0\\': \\'mapred.healthChecker.script.args\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.healthchecker.script.args\\'},\\n    {\\'1.0\\': \\'mapred.healthChecker.script.path\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.healthchecker.script.path\\'},\\n    {\\'1.0\\': \\'mapred.healthChecker.script.timeout\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.healthchecker.script.timeout\\'},\\n    {\\'1.0\\': \\'mapred.heartbeats.in.second\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.heartbeats.in.second\\'},\\n    {\\'1.0\\': \\'mapred.hosts\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.hosts.filename\\'},\\n    {\\'1.0\\': \\'mapred.hosts.exclude\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.hosts.exclude.filename\\'},\\n    {\\'1.0\\': \\'mapred.inmem.merge.threshold\\',\\n     \\'2.0\\': \\'mapreduce.reduce.merge.inmem.threshold\\'},\\n    {\\'1.0\\': \\'mapred.input.dir\\',\\n     \\'2.0\\': \\'mapreduce.input.fileinputformat.inputdir\\'},\\n    {\\'1.0\\': \\'mapred.input.dir.formats\\',\\n     \\'2.0\\': \\'mapreduce.input.multipleinputs.dir.formats\\'},\\n    {\\'1.0\\': \\'mapred.input.dir.mappers\\',\\n     \\'2.0\\': \\'mapreduce.input.multipleinputs.dir.mappers\\'},\\n    {\\'1.0\\': \\'mapred.input.pathFilter.class\\',\\n     \\'2.0\\': \\'mapreduce.input.pathFilter.class\\'},\\n    {\\'1.0\\': \\'mapred.jar\\',\\n     \\'2.0\\': \\'mapreduce.job.jar\\'},\\n    {\\'1.0\\': \\'mapred.job.classpath.archives\\',\\n     \\'2.0\\': \\'mapreduce.job.classpath.archives\\'},\\n    {\\'1.0\\': \\'mapred.job.classpath.files\\',\\n     \\'2.0\\': \\'mapreduce.job.classpath.files\\'},\\n    {\\'1.0\\': \\'mapred.job.id\\',\\n     \\'2.0\\': \\'mapreduce.job.id\\'},\\n    {\\'1.0\\': \\'mapred.job.map.memory.mb\\',\\n     \\'2.0\\': \\'mapreduce.map.memory.mb\\'},\\n    {\\'1.0\\': \\'mapred.job.name\\',\\n     \\'2.0\\': \\'mapreduce.job.name\\'},\\n    {\\'1.0\\': \\'mapred.job.priority\\',\\n     \\'2.0\\': \\'mapreduce.job.priority\\'},\\n    {\\'1.0\\': \\'mapred.job.queue.name\\',\\n     \\'2.0\\': \\'mapreduce.job.queuename\\'},\\n    {\\'1.0\\': \\'mapred.job.reduce.input.buffer.percent\\',\\n     \\'2.0\\': \\'mapreduce.reduce.input.buffer.percent\\'},\\n    {\\'1.0\\': \\'mapred.job.reduce.markreset.buffer.percent\\',\\n     \\'2.0\\': \\'mapreduce.reduce.markreset.buffer.percent\\'},\\n    {\\'1.0\\': \\'mapred.job.reduce.memory.mb\\',\\n     \\'2.0\\': \\'mapreduce.reduce.memory.mb\\'},\\n    {\\'1.0\\': \\'mapred.job.reduce.total.mem.bytes\\',\\n     \\'2.0\\': \\'mapreduce.reduce.memory.totalbytes\\'},\\n    {\\'1.0\\': \\'mapred.job.reuse.jvm.num.tasks\\',\\n     \\'2.0\\': \\'mapreduce.job.jvm.numtasks\\'},\\n    {\\'1.0\\': \\'mapred.job.shuffle.input.buffer.percent\\',\\n     \\'2.0\\': \\'mapreduce.reduce.shuffle.input.buffer.percent\\'},\\n    {\\'1.0\\': \\'mapred.job.shuffle.merge.percent\\',\\n     \\'2.0\\': \\'mapreduce.reduce.shuffle.merge.percent\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.address\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.handler.count\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.handler.count\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.history.completed.location\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.jobhistory.completed.location\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.http.address\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.http.address\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.jobhistory.lru.cache.size\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.jobhistory.lru.cache.size\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.persist.jobstatus.active\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.persist.jobstatus.active\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.persist.jobstatus.dir\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.persist.jobstatus.dir\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.persist.jobstatus.hours\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.persist.jobstatus.hours\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.retire.jobs\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.retirejobs\\'},\\n    {\\'1.0\\': \\'mapred.job.tracker.retiredjobs.cache.size\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.retiredjobs.cache.size\\'},\\n    {\\'1.0\\': \\'mapred.jobinit.threads\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.jobinit.threads\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.instrumentation\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.instrumentation\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.job.history.block.size\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.jobhistory.block.size\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.maxtasks.per.job\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.maxtasks.perjob\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.restart.recover\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.restart.recover\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.taskScheduler\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.taskscheduler\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.taskScheduler.maxRunningTasksPerJob\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.taskscheduler.maxrunningtasks.perjob\\'},\\n    {\\'1.0\\': \\'mapred.jobtracker.taskalloc.capacitypad\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.taskscheduler.taskalloc.capacitypad\\'},\\n    {\\'1.0\\': \\'mapred.join.expr\\',\\n     \\'2.0\\': \\'mapreduce.join.expr\\'},\\n    {\\'1.0\\': \\'mapred.join.keycomparator\\',\\n     \\'2.0\\': \\'mapreduce.join.keycomparator\\'},\\n    {\\'1.0\\': \\'mapred.lazy.output.format\\',\\n     \\'2.0\\': \\'mapreduce.output.lazyoutputformat.outputformat\\'},\\n    {\\'1.0\\': \\'mapred.line.input.format.linespermap\\',\\n     \\'2.0\\': \\'mapreduce.input.lineinputformat.linespermap\\'},\\n    {\\'1.0\\': \\'mapred.linerecordreader.maxlength\\',\\n     \\'2.0\\': \\'mapreduce.input.linerecordreader.line.maxlength\\'},\\n    {\\'1.0\\': \\'mapred.local.dir\\',\\n     \\'2.0\\': \\'mapreduce.cluster.local.dir\\'},\\n    {\\'1.0\\': \\'mapred.local.dir.minspacekill\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.local.dir.minspacekill\\'},\\n    {\\'1.0\\': \\'mapred.local.dir.minspacestart\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.local.dir.minspacestart\\'},\\n    {\\'1.0\\': \\'mapred.map.child.env\\',\\n     \\'2.0\\': \\'mapreduce.map.env\\'},\\n    {\\'1.0\\': \\'mapred.map.child.java.opts\\',\\n     \\'2.0\\': \\'mapreduce.map.java.opts\\'},\\n    {\\'1.0\\': \\'mapred.map.child.log.level\\',\\n     \\'2.0\\': \\'mapreduce.map.log.level\\'},\\n    {\\'1.0\\': \\'mapred.map.max.attempts\\',\\n     \\'2.0\\': \\'mapreduce.map.maxattempts\\'},\\n    {\\'1.0\\': \\'mapred.map.output.compression.codec\\',\\n     \\'2.0\\': \\'mapreduce.map.output.compress.codec\\'},\\n    {\\'1.0\\': \\'mapred.map.task.debug.script\\',\\n     \\'2.0\\': \\'mapreduce.map.debug.script\\'},\\n    {\\'1.0\\': \\'mapred.map.tasks\\',\\n     \\'2.0\\': \\'mapreduce.job.maps\\'},\\n    {\\'1.0\\': \\'mapred.map.tasks.speculative.execution\\',\\n     \\'2.0\\': \\'mapreduce.map.speculative\\'},\\n    {\\'1.0\\': \\'mapred.mapoutput.key.class\\',\\n     \\'2.0\\': \\'mapreduce.map.output.key.class\\'},\\n    {\\'1.0\\': \\'mapred.mapoutput.value.class\\',\\n     \\'2.0\\': \\'mapreduce.map.output.value.class\\'},\\n    {\\'1.0\\': \\'mapred.mapper.regex\\',\\n     \\'2.0\\': \\'mapreduce.mapper.regex\\'},\\n    {\\'1.0\\': \\'mapred.mapper.regex.group\\',\\n     \\'2.0\\': \\'mapreduce.mapper.regexmapper..group\\'},\\n    {\\'1.0\\': \\'mapred.max.map.failures.percent\\',\\n     \\'2.0\\': \\'mapreduce.map.failures.maxpercent\\'},\\n    {\\'1.0\\': \\'mapred.max.reduce.failures.percent\\',\\n     \\'2.0\\': \\'mapreduce.reduce.failures.maxpercent\\'},\\n    {\\'1.0\\': \\'mapred.max.split.size\\',\\n     \\'2.0\\': \\'mapreduce.input.fileinputformat.split.maxsize\\'},\\n    {\\'1.0\\': \\'mapred.max.tracker.blacklists\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.tasktracker.maxblacklists\\'},\\n    {\\'1.0\\': \\'mapred.max.tracker.failures\\',\\n     \\'2.0\\': \\'mapreduce.job.maxtaskfailures.per.tracker\\'},\\n    {\\'1.0\\': \\'mapred.merge.recordsBeforeProgress\\',\\n     \\'2.0\\': \\'mapreduce.task.merge.progress.records\\'},\\n    {\\'1.0\\': \\'mapred.min.split.size\\',\\n     \\'2.0\\': \\'mapreduce.input.fileinputformat.split.minsize\\'},\\n    {\\'1.0\\': \\'mapred.min.split.size.per.node\\',\\n     \\'2.0\\': \\'mapreduce.input.fileinputformat.split.minsize.per.node\\'},\\n    {\\'1.0\\': \\'mapred.min.split.size.per.rack\\',\\n     \\'2.0\\': \\'mapreduce.input.fileinputformat.split.minsize.per.rack\\'},\\n    {\\'1.0\\': \\'mapred.output.compress\\',\\n     \\'2.0\\': \\'mapreduce.output.fileoutputformat.compress\\'},\\n    {\\'1.0\\': \\'mapred.output.compression.codec\\',\\n     \\'2.0\\': \\'mapreduce.output.fileoutputformat.compress.codec\\'},\\n    {\\'1.0\\': \\'mapred.output.compression.type\\',\\n     \\'2.0\\': \\'mapreduce.output.fileoutputformat.compress.type\\'},\\n    {\\'1.0\\': \\'mapred.output.dir\\',\\n     \\'2.0\\': \\'mapreduce.output.fileoutputformat.outputdir\\'},\\n    {\\'1.0\\': \\'mapred.output.key.class\\',\\n     \\'2.0\\': \\'mapreduce.job.output.key.class\\'},\\n    {\\'1.0\\': \\'mapred.output.key.comparator.class\\',\\n     \\'2.0\\': \\'mapreduce.job.output.key.comparator.class\\'},\\n    {\\'1.0\\': \\'mapred.output.value.class\\',\\n     \\'2.0\\': \\'mapreduce.job.output.value.class\\'},\\n    {\\'1.0\\': \\'mapred.output.value.groupfn.class\\',\\n     \\'2.0\\': \\'mapreduce.job.output.group.comparator.class\\'},\\n    {\\'1.0\\': \\'mapred.permissions.supergroup\\',\\n     \\'2.0\\': \\'mapreduce.cluster.permissions.supergroup\\'},\\n    {\\'1.0\\': \\'mapred.pipes.user.inputformat\\',\\n     \\'2.0\\': \\'mapreduce.pipes.inputformat\\'},\\n    {\\'1.0\\': \\'mapred.reduce.child.env\\',\\n     \\'2.0\\': \\'mapreduce.reduce.env\\'},\\n    {\\'1.0\\': \\'mapred.reduce.child.java.opts\\',\\n     \\'2.0\\': \\'mapreduce.reduce.java.opts\\'},\\n    {\\'1.0\\': \\'mapred.reduce.child.log.level\\',\\n     \\'2.0\\': \\'mapreduce.reduce.log.level\\'},\\n    {\\'1.0\\': \\'mapred.reduce.max.attempts\\',\\n     \\'2.0\\': \\'mapreduce.reduce.maxattempts\\'},\\n    {\\'1.0\\': \\'mapred.reduce.parallel.copies\\',\\n     \\'2.0\\': \\'mapreduce.reduce.shuffle.parallelcopies\\'},\\n    {\\'1.0\\': \\'mapred.reduce.slowstart.completed.maps\\',\\n     \\'2.0\\': \\'mapreduce.job.reduce.slowstart.completedmaps\\'},\\n    {\\'1.0\\': \\'mapred.reduce.task.debug.script\\',\\n     \\'2.0\\': \\'mapreduce.reduce.debug.script\\'},\\n    {\\'1.0\\': \\'mapred.reduce.tasks\\',\\n     \\'2.0\\': \\'mapreduce.job.reduces\\'},\\n    {\\'1.0\\': \\'mapred.reduce.tasks.speculative.execution\\',\\n     \\'2.0\\': \\'mapreduce.reduce.speculative\\'},\\n    {\\'1.0\\': \\'mapred.seqbinary.output.key.class\\',\\n     \\'2.0\\': \\'mapreduce.output.seqbinaryoutputformat.key.class\\'},\\n    {\\'1.0\\': \\'mapred.seqbinary.output.value.class\\',\\n     \\'2.0\\': \\'mapreduce.output.seqbinaryoutputformat.value.class\\'},\\n    {\\'1.0\\': \\'mapred.shuffle.connect.timeout\\',\\n     \\'2.0\\': \\'mapreduce.reduce.shuffle.connect.timeout\\'},\\n    {\\'1.0\\': \\'mapred.shuffle.read.timeout\\',\\n     \\'2.0\\': \\'mapreduce.reduce.shuffle.read.timeout\\'},\\n    {\\'1.0\\': \\'mapred.skip.attempts.to.start.skipping\\',\\n     \\'2.0\\': \\'mapreduce.task.skip.start.attempts\\'},\\n    {\\'1.0\\': \\'mapred.skip.map.auto.incr.proc.count\\',\\n     \\'2.0\\': \\'mapreduce.map.skip.proc-count.auto-incr\\'},\\n    {\\'1.0\\': \\'mapred.skip.map.max.skip.records\\',\\n     \\'2.0\\': \\'mapreduce.map.skip.maxrecords\\'},\\n    {\\'1.0\\': \\'mapred.skip.on\\',\\n     \\'2.0\\': \\'mapreduce.job.skiprecords\\'},\\n    {\\'1.0\\': \\'mapred.skip.out.dir\\',\\n     \\'2.0\\': \\'mapreduce.job.skip.outdir\\'},\\n    {\\'1.0\\': \\'mapred.skip.reduce.auto.incr.proc.count\\',\\n     \\'2.0\\': \\'mapreduce.reduce.skip.proc-count.auto-incr\\'},\\n    {\\'1.0\\': \\'mapred.skip.reduce.max.skip.groups\\',\\n     \\'2.0\\': \\'mapreduce.reduce.skip.maxgroups\\'},\\n    {\\'1.0\\': \\'mapred.speculative.execution.slowNodeThreshold\\',\\n     \\'2.0\\': \\'mapreduce.job.speculative.slownodethreshold\\'},\\n    {\\'1.0\\': \\'mapred.speculative.execution.slowTaskThreshold\\',\\n     \\'2.0\\': \\'mapreduce.job.speculative.slowtaskthreshold\\'},\\n    {\\'1.0\\': \\'mapred.speculative.execution.speculativeCap\\',\\n     \\'2.0\\': \\'mapreduce.job.speculative.speculativecap\\'},\\n    {\\'1.0\\': \\'mapred.submit.replication\\',\\n     \\'2.0\\': \\'mapreduce.client.submit.file.replication\\'},\\n    {\\'1.0\\': \\'mapred.system.dir\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.system.dir\\'},\\n    {\\'1.0\\': \\'mapred.task.cache.levels\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.taskcache.levels\\'},\\n    {\\'1.0\\': \\'mapred.task.id\\',\\n     \\'2.0\\': \\'mapreduce.task.attempt.id\\'},\\n    {\\'1.0\\': \\'mapred.task.is.map\\',\\n     \\'2.0\\': \\'mapreduce.task.ismap\\'},\\n    {\\'1.0\\': \\'mapred.task.partition\\',\\n     \\'2.0\\': \\'mapreduce.task.partition\\'},\\n    {\\'1.0\\': \\'mapred.task.profile\\',\\n     \\'2.0\\': \\'mapreduce.task.profile\\'},\\n    {\\'1.0\\': \\'mapred.task.profile.maps\\',\\n     \\'2.0\\': \\'mapreduce.task.profile.maps\\'},\\n    {\\'1.0\\': \\'mapred.task.profile.params\\',\\n     \\'2.0\\': \\'mapreduce.task.profile.params\\'},\\n    {\\'1.0\\': \\'mapred.task.profile.reduces\\',\\n     \\'2.0\\': \\'mapreduce.task.profile.reduces\\'},\\n    {\\'1.0\\': \\'mapred.task.timeout\\',\\n     \\'2.0\\': \\'mapreduce.task.timeout\\'},\\n    {\\'1.0\\': \\'mapred.task.tracker.http.address\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.http.address\\'},\\n    {\\'1.0\\': \\'mapred.task.tracker.report.address\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.report.address\\'},\\n    {\\'1.0\\': \\'mapred.task.tracker.task-controller\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.taskcontroller\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.dns.interface\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.dns.interface\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.dns.nameserver\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.dns.nameserver\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.events.batchsize\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.events.batchsize\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.expiry.interval\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.expire.trackers.interval\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.indexcache.mb\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.indexcache.mb\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.instrumentation\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.instrumentation\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.map.tasks.maximum\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.map.tasks.maximum\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.memory_calculator_plugin\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.resourcecalculatorplugin\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.memorycalculatorplugin\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.resourcecalculatorplugin\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.reduce.tasks.maximum\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.reduce.tasks.maximum\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.taskmemorymanager.monitoring-interval\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.taskmemorymanager.monitoringinterval\\'},\\n    {\\'1.0\\': \\'mapred.tasktracker.tasks.sleeptime-before-sigkill\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.tasks.sleeptimebeforesigkill\\'},\\n    {\\'1.0\\': \\'mapred.temp.dir\\',\\n     \\'2.0\\': \\'mapreduce.cluster.temp.dir\\'},\\n    {\\'1.0\\': \\'mapred.text.key.comparator.options\\',\\n     \\'2.0\\': \\'mapreduce.partition.keycomparator.options\\'},\\n    {\\'1.0\\': \\'mapred.text.key.partitioner.options\\',\\n     \\'2.0\\': \\'mapreduce.partition.keypartitioner.options\\'},\\n    {\\'1.0\\': \\'mapred.textoutputformat.separator\\',\\n     \\'2.0\\': \\'mapreduce.output.textoutputformat.separator\\'},\\n    {\\'1.0\\': \\'mapred.tip.id\\',\\n     \\'2.0\\': \\'mapreduce.task.id\\'},\\n    {\\'1.0\\': \\'mapred.used.genericoptionsparser\\',\\n     \\'2.0\\': \\'mapreduce.client.genericoptionsparser.used\\'},\\n    {\\'1.0\\': \\'mapred.userlog.limit.kb\\',\\n     \\'2.0\\': \\'mapreduce.task.userlog.limit.kb\\'},\\n    {\\'1.0\\': \\'mapred.userlog.retain.hours\\',\\n     \\'2.0\\': \\'mapreduce.job.userlog.retain.hours\\'},\\n    {\\'1.0\\': \\'mapred.work.output.dir\\',\\n     \\'2.0\\': \\'mapreduce.task.output.dir\\'},\\n    {\\'1.0\\': \\'mapred.working.dir\\',\\n     \\'2.0\\': \\'mapreduce.job.working.dir\\'},\\n    {\\'1.0\\': \\'mapreduce.combine.class\\',\\n     \\'2.0\\': \\'mapreduce.job.combine.class\\'},\\n    {\\'1.0\\': \\'mapreduce.inputformat.class\\',\\n     \\'2.0\\': \\'mapreduce.job.inputformat.class\\'},\\n    {\\'1.0\\': \\'mapreduce.jobtracker.permissions.supergroup\\',\\n     \\'2.0\\': \\'mapreduce.cluster.permissions.supergroup\\'},\\n    {\\'1.0\\': \\'mapreduce.map.class\\',\\n     \\'2.0\\': \\'mapreduce.job.map.class\\'},\\n    {\\'1.0\\': \\'mapreduce.outputformat.class\\',\\n     \\'2.0\\': \\'mapreduce.job.outputformat.class\\'},\\n    {\\'1.0\\': \\'mapreduce.partitioner.class\\',\\n     \\'2.0\\': \\'mapreduce.job.partitioner.class\\'},\\n    {\\'1.0\\': \\'mapreduce.reduce.class\\',\\n     \\'2.0\\': \\'mapreduce.job.reduce.class\\'},\\n    {\\'1.0\\': \\'min.num.spills.for.combine\\',\\n     \\'2.0\\': \\'mapreduce.map.combine.minspills\\'},\\n    {\\'1.0\\': \\'reduce.output.key.value.fields.spec\\',\\n     \\'2.0\\': \\'mapreduce.fieldsel.reduce.output.key.value.fields.spec\\'},\\n    {\\'1.0\\': \\'security.job.submission.protocol.acl\\',\\n     \\'2.0\\': \\'security.job.client.protocol.acl\\'},\\n    {\\'1.0\\': \\'security.task.umbilical.protocol.acl\\',\\n     \\'2.0\\': \\'security.job.task.protocol.acl\\'},\\n    {\\'1.0\\': \\'sequencefile.filter.class\\',\\n     \\'2.0\\': \\'mapreduce.input.sequencefileinputfilter.class\\'},\\n    {\\'1.0\\': \\'sequencefile.filter.frequency\\',\\n     \\'2.0\\': \\'mapreduce.input.sequencefileinputfilter.frequency\\'},\\n    {\\'1.0\\': \\'sequencefile.filter.regex\\',\\n     \\'2.0\\': \\'mapreduce.input.sequencefileinputfilter.regex\\'},\\n    {\\'1.0\\': \\'session.id\\',\\n     \\'2.0\\': \\'dfs.metrics.session-id\\'},\\n    {\\'1.0\\': \\'slave.host.name\\',\\n     \\'2.0\\': \\'dfs.datanode.hostname\\'},\\n    {\\'1.0\\': \\'slave.host.name\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.host.name\\'},\\n    {\\'1.0\\': \\'tasktracker.contention.tracking\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.contention.tracking\\'},\\n    {\\'1.0\\': \\'tasktracker.http.threads\\',\\n     \\'2.0\\': \\'mapreduce.tasktracker.http.threads\\'},\\n    {\\'1.0\\': \\'topology.node.switch.mapping.impl\\',\\n     \\'2.0\\': \\'net.topology.node.switch.mapping.impl\\'},\\n    {\\'1.0\\': \\'topology.script.file.name\\',\\n     \\'2.0\\': \\'net.topology.script.file.name\\'},\\n    {\\'1.0\\': \\'topology.script.number.args\\',\\n     \\'2.0\\': \\'net.topology.script.number.args\\'},\\n    {\\'1.0\\': \\'user.name\\',\\n     \\'2.0\\': \\'mapreduce.job.user.name\\'},\\n    {\\'1.0\\': \\'webinterface.private.actions\\',\\n     \\'2.0\\': \\'mapreduce.jobtracker.webinterface.trusted\\'},\\n]\\n\\n# Handle compatibility for 0.x versions of Hadoop too\\nfor jobconf_dict in _JOBCONF_DICT_LIST:\\n    jobconf_dict[\\'0.20\\'] = jobconf_dict[\\'1.0\\']\\n    jobconf_dict[\\'0.21\\'] = jobconf_dict[\\'2.0\\']\\n\\n\\ndef _dict_list_to_compat_map(dict_list):\\n    # compat_map = {\\n    #   ...\\n    #   a: {\\'1.0\\': a, \\'2.0\\': b}\\n    #   ..\\n    # }\\n    compat_map = {}\\n    for version_dict in dict_list:\\n        for value in version_dict.values():\\n            compat_map[value] = version_dict\\n    return compat_map\\n\\n\\n_JOBCONF_MAP = _dict_list_to_compat_map(_JOBCONF_DICT_LIST)\\n\\n\\ndef jobconf_from_env(variable, default=None):\\n    \"\"\"Get the value of a jobconf variable from the runtime environment.\\n\\n    For example, a :py:class:`~mrjob.job.MRJob` could use\\n    ``jobconf_from_env(\\'map.input.file\\')`` to get the name of the file a\\n    mapper is reading input from.\\n\\n    If the name of the jobconf variable is different in different versions of\\n    Hadoop (e.g. in Hadoop 2.0, ``map.input.file`` is\\n    ``mapreduce.map.input.file``), we\\'ll automatically try all variants before\\n    giving up.\\n\\n    Return *default* if that jobconf variable isn\\'t set.\\n    \"\"\"\\n    # try variable verbatim first\\n    name = variable.replace(\\'.\\', \\'_\\')\\n    if name in os.environ:\\n        return os.environ[name]\\n\\n    # try alternatives (arbitrary order)\\n    for var in _JOBCONF_MAP.get(variable, {}).values():\\n        name = var.replace(\\'.\\', \\'_\\')\\n        if name in os.environ:\\n            return os.environ[name]\\n\\n    return default\\n\\n\\ndef jobconf_from_dict(jobconf, name, default=None):\\n    \"\"\"Get the value of a jobconf variable from the given dictionary.\\n\\n    :param dict jobconf: jobconf dictionary\\n    :param string name: name of the jobconf variable (e.g. ``\\'user.name\\'``)\\n    :param default: fallback value\\n\\n    If the name of the jobconf variable is different in different versions of\\n    Hadoop (e.g. in Hadoop 2, ``map.input.file`` is\\n    ``mapreduce.map.input.file``), we\\'ll automatically try all variants before\\n    giving up.\\n\\n    Return *default* if that jobconf variable isn\\'t set    \"\"\"\\n    if name in jobconf:\\n        return jobconf[name]\\n\\n    # try alternatives (arbitrary order)\\n    for alternative in _JOBCONF_MAP.get(name, {}).values():\\n        if alternative in jobconf:\\n            return jobconf[alternative]\\n\\n    return default\\n\\n\\ndef map_version(version, version_map):\\n    \"\"\"Allows you to look up something by version (e.g. which jobconf variable\\n    to use, specifying only the versions where that value changed.\\n\\n    *version* is a string\\n\\n    *version_map* is a map from version (as a string) that a value changed\\n    to the new value.\\n\\n    For efficiency, *version_map* can also be a list of tuples of\\n    ``(LooseVersion(version_as_string), value)``, with oldest versions first.\\n\\n    If *version* is less than any version in *version_map*, use the value for\\n    the earliest version in *version_map*.\\n    \"\"\"\\n    if version is None:\\n        raise TypeError\\n\\n    if not version_map:\\n        raise ValueError\\n\\n    if isinstance(version_map, dict):\\n        version_map = sorted((LooseVersion(k), v)\\n                             for k, v in version_map.items())\\n\\n    req_version = LooseVersion(version)\\n\\n    for min_version, value in reversed(version_map):\\n        if req_version >= min_version:\\n            return value\\n    else:\\n        return version_map[0][1]\\n\\n\\ndef translate_jobconf(variable, version):\\n    \"\"\"Translate *variable* to Hadoop version *version*. If it\\'s not\\n    a variable we recognize, leave as-is.\\n    \"\"\"\\n    if version is None:\\n        raise TypeError\\n\\n    if variable in _JOBCONF_MAP:\\n        return map_version(version, _JOBCONF_MAP[variable])\\n    else:\\n        return variable\\n\\n\\ndef translate_jobconf_for_all_versions(variable):\\n    \"\"\"Get all known variants of the given jobconf variable.\\n    Unlike :py:func:`translate_jobconf`, returns a list.\"\"\"\\n    return sorted(\\n        set([variable] + list(_JOBCONF_MAP.get(variable, {}).values())))\\n\\n\\ndef translate_jobconf_dict(jobconf, hadoop_version=None):\\n    \"\"\"Translates the configuration property name to match those that\\n    are accepted in hadoop_version. Prints a warning message if any\\n    configuration property name does not match the name in the hadoop\\n    version. Combines the original jobconf with the translated jobconf.\\n\\n    :return: a map consisting of the original and translated configuration\\n             property names and values.\\n    \"\"\"\\n    translated_jobconf = jobconf.copy()\\n    translation_warnings = {}\\n\\n    for variable, value in jobconf.items():\\n        if hadoop_version:\\n            variants = [translate_jobconf(variable, hadoop_version)]\\n        else:\\n            variants = translate_jobconf_for_all_versions(variable)\\n\\n        for variant in variants:\\n            if variant in jobconf:\\n                # this happens if variant == variable or\\n                # if the variant was in jobconf to start with\\n                continue\\n\\n            translated_jobconf[variant] = value\\n\\n            if hadoop_version:\\n                translation_warnings[variable] = variant\\n\\n    if translation_warnings:\\n        log.warning(\"Detected hadoop configuration property names that\"\\n                    \" do not match hadoop version %s:\"\\n                    \"\\\\nThe have been translated as follows\\\\n %s\",\\n                    hadoop_version,\\n                    \\'\\\\n\\'.join([\\n                        \"%s: %s\" % (variable, variant) for variable, variant\\n                        in sorted(translation_warnings.items())]))\\n\\n    return translated_jobconf\\n\\n\\ndef uses_yarn(version):\\n    \"\"\"Basically, is this Hadoop 2? This also handles versions in the\\n    zero series (0.23+) where YARN originated.\"\"\"\\n    return (version_gte(version, \\'2\\') or\\n            version_gte(version, \\'0.23\\') and not version_gte(version, \\'1\\'))\\n\\n\\ndef version_gte(version, cmp_version_str):\\n    \"\"\"Return ``True`` if version >= *cmp_version_str*.\"\"\"\\n\\n    if not isinstance(version, string_types):\\n        raise TypeError(\\'%r is not a string\\' % version)\\n\\n    if not isinstance(cmp_version_str, string_types):\\n        raise TypeError(\\'%r is not a string\\' % cmp_version_str)\\n\\n    return LooseVersion(version) >= LooseVersion(cmp_version_str)\\n'}]", "test_list": ["def test_mkdir(self):\n    self.fs.mkdir('hdfs:///d/ave')\n    path_in_mock_hdfs = os.path.join(get_mock_hdfs_root(self.env), 'd', 'ave')\n    self.assertEqual(os.path.isdir(path_in_mock_hdfs), True)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'mkdir' function should accept a string as the 'path' parameter and should not return any value.", "unit_test": ["def test_mkdir_input_output(self):\n    result = self.fs.mkdir('hdfs:///d/ave')\n    self.assertIsNone(result)"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_input_output"}, "Exception Handling": {"requirement": "The 'mkdir' function should raise an IOError with the message 'Could not mkdir {path}' if the directory creation fails for reasons other than the directory already existing.", "unit_test": ["def test_mkdir_exception_handling(self):\n    with self.assertRaises(IOError) as cm:\n        self.fs.mkdir('hdfs:///invalid/path')\n    self.assertEqual(str(cm.exception), 'Could not mkdir hdfs:///invalid/path')"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_exception_handling"}, "Edge Case Handling": {"requirement": "The 'mkdir' function should handle edge cases such as creating a directory with special characters in the path.", "unit_test": ["def test_mkdir_edge_case_special_characters(self):\n    self.fs.mkdir('hdfs:///d/ave@123')\n    path_in_mock_hdfs = os.path.join(get_mock_hdfs_root(self.env), 'd', 'ave@123')\n    self.assertEqual(os.path.isdir(path_in_mock_hdfs), True)"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_edge_case_special_characters"}, "Functionality Extension": {"requirement": "Extend the 'mkdir' function to log a message indicating the success or failure of directory creation.", "unit_test": ["def test_mkdir_logging(self):\n    with self.assertLogs('mrjob.fs.hadoop', level='INFO') as cm:\n        self.fs.mkdir('hdfs:///d/ave')\n    self.assertIn('INFO:mrjob.fs.hadoop:Successfully created directory hdfs:///d/ave', cm.output)"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_logging"}, "Annotation Coverage": {"requirement": "Ensure that the 'mkdir' function has comprehensive docstring annotations for parameters and exceptions.", "unit_test": ["def test_mkdir_annotation_coverage(self):\n    self.assertIn(':param path: str', self.fs.mkdir.__doc__)\n    self.assertIn(':raises IOError:', self.fs.mkdir.__doc__)"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_annotation_coverage"}, "Code Complexity": {"requirement": "The 'mkdir' function should maintain a cyclomatic complexity of 5 or lower.", "unit_test": ["def test_mkdir_code_complexity(self):\n    from radon.complexity import cc_visit\n    complexity = cc_visit(self.fs.mkdir.__code__)\n    self.assertLessEqual(complexity[0].complexity, 5)"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_code_complexity"}, "Code Standard": {"requirement": "The 'mkdir' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pep8\n    style_guide = pep8.StyleGuide(quiet=True)\n    result = style_guide.check_files(['mrjob/fs/hadoop.py'])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'mkdir' function should utilize the 'invoke_hadoop', 'get_hadoop_version', 'uses_yarn' method and '_HADOOP_FILE_EXISTS_RE'.", "unit_test": ["def test_mkdir_context_usage(self):\n    with unittest.mock.patch.object(self.fs, 'invoke_hadoop') as mock_invoke:\n        self.fs.mkdir('hdfs:///d/ave')\n    mock_invoke.assert_called_with(['fs', '-mkdir', 'hdfs:///d/ave'], ok_stderr=[_HADOOP_FILE_EXISTS_RE])"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'mkdir' function should correctly utilize the 'invoke_hadoop', 'get_hadoop_version', 'uses_yarn' method and '_HADOOP_FILE_EXISTS_RE' such as 'uses_yarn' depends by 'get_hadoop_version'.", "unit_test": ["def test_mkdir_context_usage_correctness(self):\n    with unittest.mock.patch.object(self.fs, 'get_hadoop_version', return_value='2.7.3'):\n        with unittest.mock.patch.object(self.fs, 'invoke_hadoop') as mock_invoke:\n            self.fs.mkdir('hdfs:///d/ave')\n    mock_invoke.assert_called_with(['fs', '-mkdir', '-p', 'hdfs:///d/ave'], ok_stderr=[_HADOOP_FILE_EXISTS_RE])"], "test": "tests/fs/test_hadoop.py::HadoopFSTestCase::test_mkdir_context_usage_correctness"}}}
{"namespace": "twtxt.config.Config.following", "type": "method", "project_path": "Communications/twtxt", "completion_path": "Communications/twtxt/twtxt/config.py", "signature_position": [101, 101], "body_position": [103, 111], "dependency": {"intra_class": ["twtxt.config.Config.cfg"], "intra_file": ["twtxt.config.logger"], "cross_file": ["twtxt.models.Source"]}, "requirement": {"Functionality": "This function retrieves a list of all Source objects that are stored in the \"following\" section of the Config instance. It iterates over the items in the \"following\" section, creates a Source object for each item, and appends it to the \"following\" list. If the \"following\" section does not exist, it logs a debug message and returns an empty list.", "Arguments": ":param self: Config. An instance of the Config class.\n:return: list. A list of Source objects that are stored in the \"following\" section of the Config instance."}, "tests": ["tests/test_config.py::test_create_config"], "indent": 8, "domain": "Communications", "code": "    def following(self):\n        \"\"\"A :class:`list` of all :class:`Source` objects.\"\"\"\n        following = []\n        try:\n            for (nick, url) in self.cfg.items(\"following\"):\n                source = Source(nick, url)\n                following.append(source)\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return following\n", "intra_context": "\"\"\"\n    twtxt.config\n    ~~~~~~~~~~~~\n\n    This module implements the config file parser/writer.\n\n    :copyright: (c) 2016-2022 by buckket.\n    :license: MIT, see LICENSE for more details.\n\"\"\"\n\nimport configparser\nimport logging\nimport os\n\nimport click\n\nfrom twtxt.models import Source\n\nlogger = logging.getLogger(__name__)\n\n\nclass Config:\n    \"\"\":class:`Config` interacts with the configuration file.\n\n    :param str config_file: full path to the loaded config file\n    :param ~configparser.ConfigParser cfg: a :class:`~configparser.ConfigParser` object with config loaded\n    \"\"\"\n    config_dir = click.get_app_dir(\"twtxt\")\n    config_name = \"config\"\n\n    def __init__(self, config_file, cfg):\n        self.config_file = config_file\n        self.cfg = cfg\n\n    @classmethod\n    def from_file(cls, file):\n        \"\"\"Try loading given config file.\n\n        :param str file: full path to the config file to load\n        \"\"\"\n        if not os.path.exists(file):\n            raise ValueError(\"Config file not found.\")\n\n        try:\n            config_parser = configparser.ConfigParser()\n            config_parser.read(file)\n\n            configuration = cls(file, config_parser)\n            if not configuration.check_config_sanity():\n                raise ValueError(\"Error in config file.\")\n            else:\n                return configuration\n        except configparser.Error:\n            raise ValueError(\"Config file is invalid.\")\n\n    @classmethod\n    def discover(cls):\n        \"\"\"Make a guess about the config file location an try loading it.\"\"\"\n        file = os.path.join(Config.config_dir, Config.config_name)\n        return cls.from_file(file)\n\n    @classmethod\n    def create_config(cls, cfgfile, nick, twtfile, twturl, disclose_identity, add_news):\n        \"\"\"Create a new config file at the default location.\n\n        :param str cfgfile: path to the config file\n        :param str nick: nickname to use for own tweets\n        :param str twtfile: path to the local twtxt file\n        :param str twturl: URL to the remote twtxt file\n        :param bool disclose_identity: if true the users id will be disclosed\n        :param bool add_news: if true follow twtxt news feed\n        \"\"\"\n        cfgfile_dir = os.path.dirname(cfgfile)\n        if not os.path.exists(cfgfile_dir):\n            os.makedirs(cfgfile_dir)\n\n        cfg = configparser.ConfigParser()\n\n        cfg.add_section(\"twtxt\")\n        cfg.set(\"twtxt\", \"nick\", nick)\n        cfg.set(\"twtxt\", \"twtfile\", twtfile)\n        cfg.set(\"twtxt\", \"twturl\", twturl)\n        cfg.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        cfg.set(\"twtxt\", \"character_limit\", \"140\")\n        cfg.set(\"twtxt\", \"character_warning\", \"140\")\n\n        cfg.add_section(\"following\")\n        if add_news:\n            cfg.set(\"following\", \"twtxt\", \"https://buckket.org/twtxt_news.txt\")\n\n        conf = cls(cfgfile, cfg)\n        conf.write_config()\n        return conf\n\n    def write_config(self):\n        \"\"\"Writes `self.cfg` to `self.config_file`.\"\"\"\n        with open(self.config_file, \"w\") as config_file:\n            self.cfg.write(config_file)\n\n    @property\n###The function: following###\n    @property\n    def options(self):\n        \"\"\"A :class:`dict` of all config options.\"\"\"\n        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n            return {}\n\n    @property\n    def nick(self):\n        return self.cfg.get(\"twtxt\", \"nick\", fallback=os.environ.get(\"USER\", \"\").lower())\n\n    @property\n    def twtfile(self):\n        return os.path.expanduser(self.cfg.get(\"twtxt\", \"twtfile\", fallback=\"twtxt.txt\"))\n\n    @property\n    def twturl(self):\n        return self.cfg.get(\"twtxt\", \"twturl\", fallback=None)\n\n    @property\n    def check_following(self):\n        return self.cfg.getboolean(\"twtxt\", \"check_following\", fallback=True)\n\n    @property\n    def use_pager(self):\n        return self.cfg.getboolean(\"twtxt\", \"use_pager\", fallback=False)\n\n    @property\n    def use_cache(self):\n        return self.cfg.getboolean(\"twtxt\", \"use_cache\", fallback=True)\n\n    @property\n    def porcelain(self):\n        return self.cfg.getboolean(\"twtxt\", \"porcelain\", fallback=False)\n\n    @property\n    def disclose_identity(self):\n        return self.cfg.getboolean(\"twtxt\", \"disclose_identity\", fallback=False)\n\n    @property\n    def character_limit(self):\n        return self.cfg.getint(\"twtxt\", \"character_limit\", fallback=None)\n\n    @property\n    def character_warning(self):\n        return self.cfg.getint(\"twtxt\", \"character_warning\", fallback=None)\n\n    @property\n    def limit_timeline(self):\n        return self.cfg.getint(\"twtxt\", \"limit_timeline\", fallback=20)\n\n    @property\n    def timeline_update_interval(self):\n        return self.cfg.getint(\"twtxt\", \"timeline_update_interval\", fallback=10)\n\n    @property\n    def use_abs_time(self):\n        return self.cfg.getboolean(\"twtxt\", \"use_abs_time\", fallback=False)\n\n    @property\n    def timeout(self):\n        return self.cfg.getfloat(\"twtxt\", \"timeout\", fallback=5.0)\n\n    @property\n    def sorting(self):\n        return self.cfg.get(\"twtxt\", \"sorting\", fallback=\"descending\")\n\n    @property\n    def source(self):\n        return Source(self.nick, self.twturl)\n\n    @property\n    def pre_tweet_hook(self):\n        return self.cfg.get(\"twtxt\", \"pre_tweet_hook\", fallback=None)\n\n    @property\n    def post_tweet_hook(self):\n        return self.cfg.get(\"twtxt\", \"post_tweet_hook\", fallback=None)\n\n    def add_source(self, source):\n        \"\"\"Adds a new :class:`Source` to the config\u2019s following section.\"\"\"\n        if not self.cfg.has_section(\"following\"):\n            self.cfg.add_section(\"following\")\n\n        self.cfg.set(\"following\", source.nick, source.url)\n        self.write_config()\n\n    def get_source_by_nick(self, nick):\n        \"\"\"Returns the :class:`Source` of the given nick.\n\n        :param str nick: nickname for which will be searched in the config\n        \"\"\"\n        url = self.cfg.get(\"following\", nick, fallback=None)\n        return Source(nick, url) if url else None\n\n    def remove_source_by_nick(self, nick):\n        \"\"\"Removes a :class:`Source` form the config\u2019s following section.\n\n        :param str nick: nickname for which will be searched in the config\n        \"\"\"\n        if not self.cfg.has_section(\"following\"):\n            return False\n\n        ret_val = self.cfg.remove_option(\"following\", nick)\n        self.write_config()\n        return ret_val\n\n    def build_default_map(self):\n        \"\"\"Maps config options to the default values used by click, returns :class:`dict`.\"\"\"\n        default_map = {\n            \"following\": {\n                \"check\": self.check_following,\n                \"timeout\": self.timeout,\n                \"porcelain\": self.porcelain,\n            },\n            \"tweet\": {\n                \"twtfile\": self.twtfile,\n            },\n            \"timeline\": {\n                \"pager\": self.use_pager,\n                \"cache\": self.use_cache,\n                \"limit\": self.limit_timeline,\n                \"timeout\": self.timeout,\n                \"sorting\": self.sorting,\n                \"porcelain\": self.porcelain,\n                \"twtfile\": self.twtfile,\n                \"update_interval\": self.timeline_update_interval,\n            },\n            \"view\": {\n                \"pager\": self.use_pager,\n                \"cache\": self.use_cache,\n                \"limit\": self.limit_timeline,\n                \"timeout\": self.timeout,\n                \"sorting\": self.sorting,\n                \"porcelain\": self.porcelain,\n                \"update_interval\": self.timeline_update_interval,\n            }\n        }\n        return default_map\n\n    def check_config_sanity(self):\n        \"\"\"Checks if the given values in the config file are sane.\"\"\"\n        is_sane = True\n\n        # This extracts some properties which cannot be checked like \"nick\",\n        # but it is definitely better than writing the property names as a\n        # string literal.\n        properties = [property_name for property_name, obj\n                      in self.__class__.__dict__.items()\n                      if isinstance(obj, property)]\n\n        for property_name in properties:\n            try:\n                getattr(self, property_name)\n            except ValueError as e:\n                click.echo(\"\u2717 Config error on {0} - {1}\".format(property_name, e))\n                is_sane = False\n\n        return is_sane\n", "cross_context": [{"twtxt.models.Source": "\"\"\"\n    twtxt.models\n    ~~~~~~~~~~~~\n\n    This module implements the main models used in twtxt.\n\n    :copyright: (c) 2016-2022 by buckket.\n    :license: MIT, see LICENSE for more details.\n\"\"\"\n\nfrom datetime import datetime, timezone\n\nimport humanize\nfrom dateutil.tz import tzlocal\n\n\nclass Tweet:\n    \"\"\"A :class:`Tweet` represents a single tweet.\n\n    :param str text: text of the tweet in raw format\n    :param ~datetime.datetime created_at: (optional) when the tweet was created, defaults to :meth:`~datetime.datetime.now` when no value is given\n    :param Source source: (optional) the :class:`Source` the tweet is from\n    \"\"\"\n\n    def __init__(self, text, created_at=None, source=None):\n        if text:\n            self.text = text\n        else:\n            raise ValueError(\"empty text\")\n\n        if created_at is None:\n            created_at = datetime.now(tzlocal())\n\n        try:\n            self.created_at = created_at.replace(microsecond=0)\n        except AttributeError:\n            raise TypeError(\"created_at is of invalid type\")\n\n        self.source = source\n\n    @staticmethod\n    def _is_valid_operand(other):\n        return (hasattr(other, \"text\") and\n                hasattr(other, \"created_at\"))\n\n    def __lt__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return self.created_at < other.created_at\n\n    def __le__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return self.created_at < other.created_at or (self.created_at == other.created_at and self.text == other.text)\n\n    def __gt__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return self.created_at > other.created_at\n\n    def __ge__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return self.created_at > other.created_at or (self.created_at == other.created_at and self.text == other.text)\n\n    def __eq__(self, other):\n        if not self._is_valid_operand(other):\n            return NotImplemented\n        return self.created_at == other.created_at and self.text == other.text\n\n    def __str__(self):\n        return \"{created_at}\\t{text}\".format(created_at=self.created_at.isoformat(), text=self.text)\n\n    @property\n    def relative_datetime(self):\n        \"\"\"Return human-readable relative time string.\"\"\"\n        now = datetime.now(timezone.utc)\n        created_at = self.created_at.astimezone(timezone.utc)\n\n        delta = humanize.naturaldelta(abs(created_at - now))\n        tense = \"from now\" if now < created_at else \"ago\"\n\n        return f\"{delta} {tense}\"\n\n    @property\n    def absolute_datetime(self):\n        \"\"\"Return human-readable absolute time string.\"\"\"\n        return self.created_at.strftime(\"%a, %d %b %Y %H:%M:%S\")\n\n\nclass Source:\n    \"\"\"A :class:`Source` represents a twtxt feed, remote as well as local.\n\n    :param str nick: nickname of twtxt user\n    :param str url: URL to remote twtxt file\n    :param str file: path to local twtxt file\n    \"\"\"\n\n    def __init__(self, nick, url=None, file=None):\n        self.nick = nick.lower()\n        self.url = url\n        self.file = file\n"}], "prompt": "Please write a python function called 'following' base the context. This function retrieves a list of all Source objects that are stored in the \"following\" section of the Config instance. It iterates over the items in the \"following\" section, creates a Source object for each item, and appends it to the \"following\" list. If the \"following\" section does not exist, it logs a debug message and returns an empty list.:param self: Config. An instance of the Config class.\n:return: list. A list of Source objects that are stored in the \"following\" section of the Config instance..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        \"\"\"\n    twtxt.config\n    ~~~~~~~~~~~~\n\n    This module implements the config file parser/writer.\n\n    :copyright: (c) 2016-2022 by buckket.\n    :license: MIT, see LICENSE for more details.\n\"\"\"\n\nimport configparser\nimport logging\nimport os\n\nimport click\n\nfrom twtxt.models import Source\n\nlogger = logging.getLogger(__name__)\n\n\nclass Config:\n    \"\"\":class:`Config` interacts with the configuration file.\n\n    :param str config_file: full path to the loaded config file\n    :param ~configparser.ConfigParser cfg: a :class:`~configparser.ConfigParser` object with config loaded\n    \"\"\"\n    config_dir = click.get_app_dir(\"twtxt\")\n    config_name = \"config\"\n\n    def __init__(self, config_file, cfg):\n        self.config_file = config_file\n        self.cfg = cfg\n\n    @classmethod\n    def from_file(cls, file):\n        \"\"\"Try loading given config file.\n\n        :param str file: full path to the config file to load\n        \"\"\"\n        if not os.path.exists(file):\n            raise ValueError(\"Config file not found.\")\n\n        try:\n            config_parser = configparser.ConfigParser()\n            config_parser.read(file)\n\n            configuration = cls(file, config_parser)\n            if not configuration.check_config_sanity():\n                raise ValueError(\"Error in config file.\")\n            else:\n                return configuration\n        except configparser.Error:\n            raise ValueError(\"Config file is invalid.\")\n\n    @classmethod\n    def discover(cls):\n        \"\"\"Make a guess about the config file location an try loading it.\"\"\"\n        file = os.path.join(Config.config_dir, Config.config_name)\n        return cls.from_file(file)\n\n    @classmethod\n    def create_config(cls, cfgfile, nick, twtfile, twturl, disclose_identity, add_news):\n        \"\"\"Create a new config file at the default location.\n\n        :param str cfgfile: path to the config file\n        :param str nick: nickname to use for own tweets\n        :param str twtfile: path to the local twtxt file\n        :param str twturl: URL to the remote twtxt file\n        :param bool disclose_identity: if true the users id will be disclosed\n        :param bool add_news: if true follow twtxt news feed\n        \"\"\"\n        cfgfile_dir = os.path.dirname(cfgfile)\n        if not os.path.exists(cfgfile_dir):\n            os.makedirs(cfgfile_dir)\n\n        cfg = configparser.ConfigParser()\n\n        cfg.add_section(\"twtxt\")\n        cfg.set(\"twtxt\", \"nick\", nick)\n        cfg.set(\"twtxt\", \"twtfile\", twtfile)\n        cfg.set(\"twtxt\", \"twturl\", twturl)\n        cfg.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        cfg.set(\"twtxt\", \"character_limit\", \"140\")\n        cfg.set(\"twtxt\", \"character_warning\", \"140\")\n\n        cfg.add_section(\"following\")\n        if add_news:\n            cfg.set(\"following\", \"twtxt\", \"https://buckket.org/twtxt_news.txt\")\n\n        conf = cls(cfgfile, cfg)\n        conf.write_config()\n        return conf\n\n    def write_config(self):\n        \"\"\"Writes `self.cfg` to `self.config_file`.\"\"\"\n        with open(self.config_file, \"w\") as config_file:\n            self.cfg.write(config_file)\n\n    @property\n###The function: following###\n    @property\n    def options(self):\n        \"\"\"A :class:`dict` of all config options.\"\"\"\n        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n            return {}\n\n    @property\n    def nick(self):\n        return self.cfg.get(\"twtxt\", \"nick\", fallback=os.environ.get(\"USER\", \"\").lower())\n\n    @property\n    def twtfile(self):\n        return os.path.expanduser(self.cfg.get(\"twtxt\", \"twtfile\", fallback=\"twtxt.txt\"))\n\n    @property\n    def twturl(self):\n        return self.cfg.get(\"twtxt\", \"twturl\", fallback=None)\n\n    @property\n    def check_following(self):\n        return self.cfg.getboolean(\"twtxt\", \"check_following\", fallback=True)\n\n    @property\n    def use_pager(self):\n        return self.cfg.getboolean(\"twtxt\", \"use_pager\", fallback=False)\n\n    @property\n    def use_cache(self):\n        return self.cfg.getboolean(\"twtxt\", \"use_cache\", fallback=True)\n\n    @property\n    def porcelain(self):\n        return self.cfg.getboolean(\"twtxt\", \"porcelain\", fallback=False)\n\n    @property\n    def disclose_identity(self):\n        return self.cfg.getboolean(\"twtxt\", \"disclose_identity\", fallback=False)\n\n    @property\n    def character_limit(self):\n        return self.cfg.getint(\"twtxt\", \"character_limit\", fallback=None)\n\n    @property\n    def character_warning(self):\n        return self.cfg.getint(\"twtxt\", \"character_warning\", fallback=None)\n\n    @property\n    def limit_timeline(self):\n        return self.cfg.getint(\"twtxt\", \"limit_timeline\", fallback=20)\n\n    @property\n    def timeline_update_interval(self):\n        return self.cfg.getint(\"twtxt\", \"timeline_update_interval\", fallback=10)\n\n    @property\n    def use_abs_time(self):\n        return self.cfg.getboolean(\"twtxt\", \"use_abs_time\", fallback=False)\n\n    @property\n    def timeout(self):\n        return self.cfg.getfloat(\"twtxt\", \"timeout\", fallback=5.0)\n\n    @property\n    def sorting(self):\n        return self.cfg.get(\"twtxt\", \"sorting\", fallback=\"descending\")\n\n    @property\n    def source(self):\n        return Source(self.nick, self.twturl)\n\n    @property\n    def pre_tweet_hook(self):\n        return self.cfg.get(\"twtxt\", \"pre_tweet_hook\", fallback=None)\n\n    @property\n    def post_tweet_hook(self):\n        return self.cfg.get(\"twtxt\", \"post_tweet_hook\", fallback=None)\n\n    def add_source(self, source):\n        \"\"\"Adds a new :class:`Source` to the config\u2019s following section.\"\"\"\n        if not self.cfg.has_section(\"following\"):\n            self.cfg.add_section(\"following\")\n\n        self.cfg.set(\"following\", source.nick, source.url)\n        self.write_config()\n\n    def get_source_by_nick(self, nick):\n        \"\"\"Returns the :class:`Source` of the given nick.\n\n        :param str nick: nickname for which will be searched in the config\n        \"\"\"\n        url = self.cfg.get(\"following\", nick, fallback=None)\n        return Source(nick, url) if url else None\n\n    def remove_source_by_nick(self, nick):\n        \"\"\"Removes a :class:`Source` form the config\u2019s following section.\n\n        :param str nick: nickname for which will be searched in the config\n        \"\"\"\n        if not self.cfg.has_section(\"following\"):\n            return False\n\n        ret_val = self.cfg.remove_option(\"following\", nick)\n        self.write_config()\n        return ret_val\n\n    def build_default_map(self):\n        \"\"\"Maps config options to the default values used by click, returns :class:`dict`.\"\"\"\n        default_map = {\n            \"following\": {\n                \"check\": self.check_following,\n                \"timeout\": self.timeout,\n                \"porcelain\": self.porcelain,\n            },\n            \"tweet\": {\n                \"twtfile\": self.twtfile,\n            },\n            \"timeline\": {\n                \"pager\": self.use_pager,\n                \"cache\": self.use_cache,\n                \"limit\": self.limit_timeline,\n                \"timeout\": self.timeout,\n                \"sorting\": self.sorting,\n                \"porcelain\": self.porcelain,\n                \"twtfile\": self.twtfile,\n                \"update_interval\": self.timeline_update_interval,\n            },\n            \"view\": {\n                \"pager\": self.use_pager,\n                \"cache\": self.use_cache,\n                \"limit\": self.limit_timeline,\n                \"timeout\": self.timeout,\n                \"sorting\": self.sorting,\n                \"porcelain\": self.porcelain,\n                \"update_interval\": self.timeline_update_interval,\n            }\n        }\n        return default_map\n\n    def check_config_sanity(self):\n        \"\"\"Checks if the given values in the config file are sane.\"\"\"\n        is_sane = True\n\n        # This extracts some properties which cannot be checked like \"nick\",\n        # but it is definitely better than writing the property names as a\n        # string literal.\n        properties = [property_name for property_name, obj\n                      in self.__class__.__dict__.items()\n                      if isinstance(obj, property)]\n\n        for property_name in properties:\n            try:\n                getattr(self, property_name)\n            except ValueError as e:\n                click.echo(\"\u2717 Config error on {0} - {1}\".format(property_name, e))\n                is_sane = False\n\n        return is_sane\n\n        ####cross_file_context:\n        [{'twtxt.models.Source': '\"\"\"\\n    twtxt.models\\n    ~~~~~~~~~~~~\\n\\n    This module implements the main models used in twtxt.\\n\\n    :copyright: (c) 2016-2022 by buckket.\\n    :license: MIT, see LICENSE for more details.\\n\"\"\"\\n\\nfrom datetime import datetime, timezone\\n\\nimport humanize\\nfrom dateutil.tz import tzlocal\\n\\n\\nclass Tweet:\\n    \"\"\"A :class:`Tweet` represents a single tweet.\\n\\n    :param str text: text of the tweet in raw format\\n    :param ~datetime.datetime created_at: (optional) when the tweet was created, defaults to :meth:`~datetime.datetime.now` when no value is given\\n    :param Source source: (optional) the :class:`Source` the tweet is from\\n    \"\"\"\\n\\n    def __init__(self, text, created_at=None, source=None):\\n        if text:\\n            self.text = text\\n        else:\\n            raise ValueError(\"empty text\")\\n\\n        if created_at is None:\\n            created_at = datetime.now(tzlocal())\\n\\n        try:\\n            self.created_at = created_at.replace(microsecond=0)\\n        except AttributeError:\\n            raise TypeError(\"created_at is of invalid type\")\\n\\n        self.source = source\\n\\n    @staticmethod\\n    def _is_valid_operand(other):\\n        return (hasattr(other, \"text\") and\\n                hasattr(other, \"created_at\"))\\n\\n    def __lt__(self, other):\\n        if not self._is_valid_operand(other):\\n            return NotImplemented\\n        return self.created_at < other.created_at\\n\\n    def __le__(self, other):\\n        if not self._is_valid_operand(other):\\n            return NotImplemented\\n        return self.created_at < other.created_at or (self.created_at == other.created_at and self.text == other.text)\\n\\n    def __gt__(self, other):\\n        if not self._is_valid_operand(other):\\n            return NotImplemented\\n        return self.created_at > other.created_at\\n\\n    def __ge__(self, other):\\n        if not self._is_valid_operand(other):\\n            return NotImplemented\\n        return self.created_at > other.created_at or (self.created_at == other.created_at and self.text == other.text)\\n\\n    def __eq__(self, other):\\n        if not self._is_valid_operand(other):\\n            return NotImplemented\\n        return self.created_at == other.created_at and self.text == other.text\\n\\n    def __str__(self):\\n        return \"{created_at}\\\\t{text}\".format(created_at=self.created_at.isoformat(), text=self.text)\\n\\n    @property\\n    def relative_datetime(self):\\n        \"\"\"Return human-readable relative time string.\"\"\"\\n        now = datetime.now(timezone.utc)\\n        created_at = self.created_at.astimezone(timezone.utc)\\n\\n        delta = humanize.naturaldelta(abs(created_at - now))\\n        tense = \"from now\" if now < created_at else \"ago\"\\n\\n        return f\"{delta} {tense}\"\\n\\n    @property\\n    def absolute_datetime(self):\\n        \"\"\"Return human-readable absolute time string.\"\"\"\\n        return self.created_at.strftime(\"%a, %d %b %Y %H:%M:%S\")\\n\\n\\nclass Source:\\n    \"\"\"A :class:`Source` represents a twtxt feed, remote as well as local.\\n\\n    :param str nick: nickname of twtxt user\\n    :param str url: URL to remote twtxt file\\n    :param str file: path to local twtxt file\\n    \"\"\"\\n\\n    def __init__(self, nick, url=None, file=None):\\n        self.nick = nick.lower()\\n        self.url = url\\n        self.file = file\\n'}]", "test_list": ["def test_create_config(config_dir):\n    config_dir_old = Config.config_dir\n    Config.config_dir = str(config_dir.join('new'))\n    conf_w = Config.create_config(os.path.join(Config.config_dir, Config.config_name), 'bar', 'batz.txt', 'https://example.org', False, True)\n    conf_r = Config.discover()\n    assert conf_r.nick == 'bar'\n    assert conf_r.twtfile == 'batz.txt'\n    assert conf_r.twturl == 'https://example.org'\n    assert conf_r.character_limit == 140\n    assert conf_r.character_warning == 140\n    assert conf_r.following[0].nick == 'twtxt'\n    assert conf_r.following[0].url == 'https://buckket.org/twtxt_news.txt'\n    assert set(conf_r.options.keys()) == {'nick', 'twtfile', 'twturl', 'disclose_identity', 'character_limit', 'character_warning'}\n    conf_r.cfg.remove_section('twtxt')\n    assert conf_r.options == {}\n    conf_r.cfg.remove_section('following')\n    assert conf_r.following == []\n    Config.config_dir = config_dir_old"], "requirements": {"Input-Output Conditions": {"requirement": "The 'following' function should return a list of Source objects, where each Source object is constructed with a valid 'nick' and 'url' from the 'following' section of the Config instance.", "unit_test": ["def test_following_output():\n    config_parser = configparser.ConfigParser()\n    config_parser.add_section('following')\n    config_parser.set('following', 'user1', 'http://example.com/user1')\n    config_parser.set('following', 'user2', 'http://example.com/user2')\n    config = Config('dummy_path', config_parser)\n    following_sources = config.following\n    assert len(following_sources) == 2\n    assert following_sources[0].nick == 'user1'\n    assert following_sources[0].url == 'http://example.com/user1'\n    assert following_sources[1].nick == 'user2'\n    assert following_sources[1].url == 'http://example.com/user2'"], "test": "tests/test_config.py::test_following_output"}, "Exception Handling": {"requirement": "The 'following' function should log a debug message and return an empty list if the 'following' section does not exist in the Config instance.", "unit_test": ["def test_following_no_section():\n    config_parser = configparser.ConfigParser()\n    config = Config('dummy_path', config_parser)\n    with self.assertLogs(logger, level='DEBUG') as log:\n        following_sources = config.following\n        assert following_sources == []\n        assert 'No section: following' in log.output[0]"], "test": "tests/test_config.py::test_following_no_section"}, "Edge Case Handling": {"requirement": "The 'following' function should handle cases where the 'following' section exists but contains no entries, returning an empty list.", "unit_test": ["def test_following_empty_section():\n    config_parser = configparser.ConfigParser()\n    config_parser.add_section('following')\n    config = Config('dummy_path', config_parser)\n    following_sources = config.following\n    assert following_sources == []"], "test": "tests/test_config.py::test_following_empty_section"}, "Functionality Extension": {"requirement": "Extend the 'following' function to filter out any Source objects with invalid URLs, ensuring only valid Source objects are returned.", "unit_test": ["def test_following_filter_invalid_urls():\n    config_parser = configparser.ConfigParser()\n    config_parser.add_section('following')\n    config_parser.set('following', 'user1', 'http://example.com/user1')\n    config_parser.set('following', 'user2', 'invalid_url')\n    config = Config('dummy_path', config_parser)\n    following_sources = config.following\n    assert len(following_sources) == 1\n    assert following_sources[0].nick == 'user1'\n    assert following_sources[0].url == 'http://example.com/user1'"], "test": "tests/test_config.py::test_following_filter_invalid_urls"}, "Annotation Coverage": {"requirement": "Ensure that the 'following' function is properly annotated with type hints for its return type.", "unit_test": ["def test_following_annotations():\n    assert hasattr(Config.following, '__annotations__')\n    assert Config.following.__annotations__['return'] == list"], "test": "tests/test_config.py::test_following_annotations"}, "Code Complexity": {"requirement": "The 'following' function should maintain a cyclomatic complexity of 5 or less to ensure readability and maintainability.", "unit_test": ["def test_following_complexity():\n    import radon.complexity as cc\n    source_code = inspect.getsource(Config.following)\n    complexity = cc.cc_visit(source_code)\n    assert complexity[0].complexity <= 5"], "test": "tests/test_config.py::test_following_complexity"}, "Code Standard": {"requirement": "The 'following' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_config_file.py'])\n    assert result.total_errors == 0"], "test": "tests/test_config.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'following' function should utilize the 'cfg' attribute from the Config, configparser.ConfigParser, twtxt.config.logger and twtxt.config.Source.", "unit_test": ["def test_following_uses_cfg():\n    config_parser = configparser.ConfigParser()\n    config_parser.add_section('following')\n    config_parser.set('following', 'user1', 'http://example.com/user1')\n    config = Config('dummy_path', config_parser)\n    following_sources = config.following\n    assert config.cfg.has_section('following')"], "test": "tests/test_config.py::test_following_uses_cfg"}, "Context Usage Correctness Verification": {"requirement": "The 'following' function should correctly utilize the 'cfg' attribute from the Config, configparser.ConfigParser, twtxt.config.logger and twtxt.config.Source.", "unit_test": ["def test_following_correct_parsing():\n    config_parser = configparser.ConfigParser()\n    config_parser.add_section('following')\n    config_parser.set('following', 'user1', 'http://example.com/user1')\n    config = Config('dummy_path', config_parser)\n    following_sources = config.following\n    assert following_sources[0].nick == 'user1'\n    assert following_sources[0].url == 'http://example.com/user1'"], "test": "tests/test_config.py::test_following_correct_parsing"}}}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "type": "method", "project_path": "Communications/IMAPClient", "completion_path": "Communications/IMAPClient/imapclient/imapclient.py", "signature_position": [1203, 1203], "body_position": [1220, 1230], "dependency": {"intra_class": ["imapclient.imapclient.IMAPClient._raw_command_untagged", "imapclient.imapclient.IMAPClient.has_capability"], "intra_file": ["imapclient.imapclient._normalise_search_criteria"], "cross_file": ["imapclient.exceptions", "imapclient.exceptions.CapabilityError", "imapclient.response_parser.parse_response", "imapclient.util.to_bytes"]}, "requirement": {"Functionality": "Return a list of message threads from the currently selected folder that match the specified criteria. Each returned thread is a list of message IDs.\n", "Arguments": ":param algorithm: String, the threading algorithm to use. It defaults to \"REFERENCES\" if not specified.\n:param criteria: String, the search criteria to match the messages. It defaults to \"ALL\" if not specified.\n:param charset: String, the character set to be used. It defaults to \"UTF-8\" if not specified.\n:return: List[Tuple], each tuple represents a message thread, where each element of the tuple is a message ID. For example, \"((1, 2), (3,), (4, 5, 6))\".\n"}, "tests": ["tests/test_thread.py::TestThread::test_unsupported_algorithm", "tests/test_thread.py::TestThread::test_no_thread_support", "tests/test_thread.py::TestThread::test_defaults", "tests/test_thread.py::TestThread::test_all_args"], "indent": 8, "domain": "Communications", "code": "    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of messages threads from the currently\n        selected folder which match *criteria*.\n\n        Each returned thread is a list of messages ids. An example\n        return value containing three message threads::\n\n            ((1, 2), (3,), (4, 5, 6))\n\n        The optional *algorithm* argument specifies the threading\n        algorithm to use.\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for more details.\n        \"\"\"\n        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)\n", "intra_context": "# Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport dataclasses\nimport functools\nimport imaplib\nimport itertools\nimport re\nimport select\nimport socket\nimport ssl as ssl_lib\nimport sys\nimport warnings\nfrom datetime import date, datetime\nfrom logging import getLogger, LoggerAdapter\nfrom operator import itemgetter\nfrom typing import List, Optional\n\nfrom . import exceptions, imap4, tls\nfrom .datetime_util import datetime_to_INTERNALDATE\nfrom .imap_utf7 import decode as decode_utf7\nfrom .imap_utf7 import encode as encode_utf7\nfrom .response_parser import parse_fetch_response, parse_message_list, parse_response\nfrom .util import assert_imap_protocol, to_bytes, to_unicode\n\nif hasattr(select, \"poll\"):\n    POLL_SUPPORT = True\nelse:\n    # Fallback to select() on systems that don't support poll()\n    POLL_SUPPORT = False\n\n\nlogger = getLogger(__name__)\n\n__all__ = [\n    \"IMAPClient\",\n    \"SocketTimeout\",\n    \"DELETED\",\n    \"SEEN\",\n    \"ANSWERED\",\n    \"FLAGGED\",\n    \"DRAFT\",\n    \"RECENT\",\n]\n\n\n# We also offer the gmail-specific XLIST command...\nif \"XLIST\" not in imaplib.Commands:\n    imaplib.Commands[\"XLIST\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ...and IDLE\nif \"IDLE\" not in imaplib.Commands:\n    imaplib.Commands[\"IDLE\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ..and STARTTLS\nif \"STARTTLS\" not in imaplib.Commands:\n    imaplib.Commands[\"STARTTLS\"] = (\"NONAUTH\",)\n\n# ...and ID. RFC2971 says that this command is valid in all states,\n# but not that some servers (*cough* FastMail *cough*) don't seem to\n# accept it in state NONAUTH.\nif \"ID\" not in imaplib.Commands:\n    imaplib.Commands[\"ID\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ... and UNSELECT. RFC3691 does not specify the state but there is no\n# reason to use the command without AUTH state and a mailbox selected.\nif \"UNSELECT\" not in imaplib.Commands:\n    imaplib.Commands[\"UNSELECT\"] = (\"AUTH\", \"SELECTED\")\n\n# .. and ENABLE.\nif \"ENABLE\" not in imaplib.Commands:\n    imaplib.Commands[\"ENABLE\"] = (\"AUTH\",)\n\n# .. and MOVE for RFC6851.\nif \"MOVE\" not in imaplib.Commands:\n    imaplib.Commands[\"MOVE\"] = (\"AUTH\", \"SELECTED\")\n\n# System flags\nDELETED = rb\"\\Deleted\"\nSEEN = rb\"\\Seen\"\nANSWERED = rb\"\\Answered\"\nFLAGGED = rb\"\\Flagged\"\nDRAFT = rb\"\\Draft\"\nRECENT = rb\"\\Recent\"  # This flag is read-only\n\n# Special folders, see RFC6154\n# \\Flagged is omitted because it is the same as the flag defined above\nALL = rb\"\\All\"\nARCHIVE = rb\"\\Archive\"\nDRAFTS = rb\"\\Drafts\"\nJUNK = rb\"\\Junk\"\nSENT = rb\"\\Sent\"\nTRASH = rb\"\\Trash\"\n\n# Personal namespaces that are common among providers\n# used as a fallback when the server does not support the NAMESPACE capability\n_POPULAR_PERSONAL_NAMESPACES = ((\"\", \"\"), (\"INBOX.\", \".\"))\n\n# Names of special folders that are common among providers\n_POPULAR_SPECIAL_FOLDERS = {\n    SENT: (\"Sent\", \"Sent Items\", \"Sent items\"),\n    DRAFTS: (\"Drafts\",),\n    ARCHIVE: (\"Archive\",),\n    TRASH: (\"Trash\", \"Deleted Items\", \"Deleted Messages\", \"Deleted\"),\n    JUNK: (\"Junk\", \"Spam\"),\n}\n\n_RE_SELECT_RESPONSE = re.compile(rb\"\\[(?P<key>[A-Z-]+)( \\((?P<data>.*)\\))?\\]\")\n\n\nclass Namespace(tuple):\n    def __new__(cls, personal, other, shared):\n        return tuple.__new__(cls, (personal, other, shared))\n\n    personal = property(itemgetter(0))\n    other = property(itemgetter(1))\n    shared = property(itemgetter(2))\n\n\n@dataclasses.dataclass\nclass SocketTimeout:\n    \"\"\"Represents timeout configuration for an IMAP connection.\n\n    :ivar connect: maximum time to wait for a connection attempt to remote server\n    :ivar read: maximum time to wait for performing a read/write operation\n\n    As an example, ``SocketTimeout(connect=15, read=60)`` will make the socket\n    timeout if the connection takes more than 15 seconds to establish but\n    read/write operations can take up to 60 seconds once the connection is done.\n    \"\"\"\n\n    connect: float\n    read: float\n\n\n@dataclasses.dataclass\nclass MailboxQuotaRoots:\n    \"\"\"Quota roots associated with a mailbox.\n\n    Represents the response of a GETQUOTAROOT command.\n\n    :ivar mailbox: the mailbox\n    :ivar quota_roots: list of quota roots associated with the mailbox\n    \"\"\"\n\n    mailbox: str\n    quota_roots: List[str]\n\n\n@dataclasses.dataclass\nclass Quota:\n    \"\"\"Resource quota.\n\n    Represents the response of a GETQUOTA command.\n\n    :ivar quota_roots: the quota roots for which the limit apply\n    :ivar resource: the resource being limited (STORAGE, MESSAGES...)\n    :ivar usage: the current usage of the resource\n    :ivar limit: the maximum allowed usage of the resource\n    \"\"\"\n\n    quota_root: str\n    resource: str\n    usage: bytes\n    limit: bytes\n\n\ndef require_capability(capability):\n    \"\"\"Decorator raising CapabilityError when a capability is not available.\"\"\"\n\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def wrapper(client, *args, **kwargs):\n            if not client.has_capability(capability):\n                raise exceptions.CapabilityError(\n                    \"Server does not support {} capability\".format(capability)\n                )\n            return func(client, *args, **kwargs)\n\n        return wrapper\n\n    return actual_decorator\n\n\nclass IMAPClient:\n    \"\"\"A connection to the IMAP server specified by *host* is made when\n    this class is instantiated.\n\n    *port* defaults to 993, or 143 if *ssl* is ``False``.\n\n    If *use_uid* is ``True`` unique message UIDs be used for all calls\n    that accept message ids (defaults to ``True``).\n\n    If *ssl* is ``True`` (the default) a secure connection will be made.\n    Otherwise an insecure connection over plain text will be\n    established.\n\n    If *ssl* is ``True`` the optional *ssl_context* argument can be\n    used to provide an ``ssl.SSLContext`` instance used to\n    control SSL/TLS connection parameters. If this is not provided a\n    sensible default context will be used.\n\n    If *stream* is ``True`` then *host* is used as the command to run\n    to establish a connection to the IMAP server (defaults to\n    ``False``). This is useful for exotic connection or authentication\n    setups.\n\n    Use *timeout* to specify a timeout for the socket connected to the\n    IMAP server. The timeout can be either a float number, or an instance\n    of :py:class:`imapclient.SocketTimeout`.\n\n    * If a single float number is passed, the same timeout delay applies\n      during the  initial connection to the server and for all future socket\n      reads and writes.\n\n    * In case of a ``SocketTimeout``, connection timeout and\n      read/write operations can have distinct timeouts.\n\n    * The default is ``None``, where no timeout is used.\n\n    The *normalise_times* attribute specifies whether datetimes\n    returned by ``fetch()`` are normalised to the local system time\n    and include no timezone information (native), or are datetimes\n    that include timezone information (aware). By default\n    *normalise_times* is True (times are normalised to the local\n    system time). This attribute can be changed between ``fetch()``\n    calls if required.\n\n    Can be used as a context manager to automatically close opened connections:\n\n    >>> with IMAPClient(host=\"imap.foo.org\") as client:\n    ...     client.login(\"bar@foo.org\", \"passwd\")\n\n    \"\"\"\n\n    # Those exceptions are kept for backward-compatibility, since\n    # previous versions included these attributes as references to\n    # imaplib original exceptions\n    Error = exceptions.IMAPClientError\n    AbortError = exceptions.IMAPClientAbortError\n    ReadOnlyError = exceptions.IMAPClientReadOnlyError\n\n    def __init__(\n        self,\n        host: str,\n        port: int = None,\n        use_uid: bool = True,\n        ssl: bool = True,\n        stream: bool = False,\n        ssl_context: Optional[ssl_lib.SSLContext] = None,\n        timeout: Optional[float] = None,\n    ):\n        if stream:\n            if port is not None:\n                raise ValueError(\"can't set 'port' when 'stream' True\")\n            if ssl:\n                raise ValueError(\"can't use 'ssl' when 'stream' is True\")\n        elif port is None:\n            port = ssl and 993 or 143\n\n        if ssl and port == 143:\n            logger.warning(\n                \"Attempting to establish an encrypted connection \"\n                \"to a port (143) often used for unencrypted \"\n                \"connections\"\n            )\n\n        self.host = host\n        self.port = port\n        self.ssl = ssl\n        self.ssl_context = ssl_context\n        self.stream = stream\n        self.use_uid = use_uid\n        self.folder_encode = True\n        self.normalise_times = True\n\n        # If the user gives a single timeout value, assume it is the same for\n        # connection and read/write operations\n        if not isinstance(timeout, SocketTimeout):\n            timeout = SocketTimeout(timeout, timeout)\n\n        self._timeout = timeout\n        self._starttls_done = False\n        self._cached_capabilities = None\n        self._idle_tag = None\n\n        self._imap = self._create_IMAP4()\n        logger.debug(\n            \"Connected to host %s over %s\",\n            self.host,\n            \"SSL/TLS\" if ssl else \"plain text\",\n        )\n\n        self._set_read_timeout()\n        # Small hack to make imaplib log everything to its own logger\n        imaplib_logger = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._imap.debug = 5\n        self._imap._mesg = imaplib_logger.debug\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Logout and closes the connection when exiting the context manager.\n\n        All exceptions during logout and connection shutdown are caught because\n        an error here usually means the connection was already closed.\n        \"\"\"\n        try:\n            self.logout()\n        except Exception:\n            try:\n                self.shutdown()\n            except Exception as e:\n                logger.info(\"Could not close the connection cleanly: %s\", e)\n\n    def _create_IMAP4(self):\n        if self.stream:\n            return imaplib.IMAP4_stream(self.host)\n\n        connect_timeout = getattr(self._timeout, \"connect\", None)\n\n        if self.ssl:\n            return tls.IMAP4_TLS(\n                self.host,\n                self.port,\n                self.ssl_context,\n                connect_timeout,\n            )\n\n        return imap4.IMAP4WithTimeout(self.host, self.port, connect_timeout)\n\n    def _set_read_timeout(self):\n        if self._timeout is not None:\n            self.socket().settimeout(self._timeout.read)\n\n    @property\n    def _sock(self):\n        warnings.warn(\"_sock is deprecated. Use socket().\", DeprecationWarning)\n        return self.socket()\n\n    def socket(self):\n        \"\"\"Returns socket used to connect to server.\n\n        The socket is provided for polling purposes only.\n        It can be used in,\n        for example, :py:meth:`selectors.BaseSelector.register`\n        and :py:meth:`asyncio.loop.add_reader` to wait for data.\n\n        .. WARNING::\n           All other uses of the returned socket are unsupported.\n           This includes reading from and writing to the socket,\n           as they are likely to break internal bookkeeping of messages.\n        \"\"\"\n        # In py2, imaplib has sslobj (for SSL connections), and sock for non-SSL.\n        # In the py3 version it's just sock.\n        return getattr(self._imap, \"sslobj\", self._imap.sock)\n\n    @require_capability(\"STARTTLS\")\n    def starttls(self, ssl_context=None):\n        \"\"\"Switch to an SSL encrypted connection by sending a STARTTLS command.\n\n        The *ssl_context* argument is optional and should be a\n        :py:class:`ssl.SSLContext` object. If no SSL context is given, a SSL\n        context with reasonable default settings will be used.\n\n        You can enable checking of the hostname in the certificate presented\n        by the server  against the hostname which was used for connecting, by\n        setting the *check_hostname* attribute of the SSL context to ``True``.\n        The default SSL context has this setting enabled.\n\n        Raises :py:exc:`Error` if the SSL connection could not be established.\n\n        Raises :py:exc:`AbortError` if the server does not support STARTTLS\n        or an SSL connection is already established.\n        \"\"\"\n        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n\n    def login(self, username: str, password: str):\n        \"\"\"Login using *username* and *password*, returning the\n        server response.\n        \"\"\"\n        try:\n            rv = self._command_and_check(\n                \"login\",\n                to_unicode(username),\n                to_unicode(password),\n                unpack=True,\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n        logger.debug(\"Logged in as %s\", username)\n        return rv\n\n    def oauth2_login(\n        self,\n        user: str,\n        access_token: str,\n        mech: str = \"XOAUTH2\",\n        vendor: Optional[str] = None,\n    ):\n        \"\"\"Authenticate using the OAUTH2 or XOAUTH2 methods.\n\n        Gmail and Yahoo both support the 'XOAUTH2' mechanism, but Yahoo requires\n        the 'vendor' portion in the payload.\n        \"\"\"\n        auth_string = \"user=%s\\1auth=Bearer %s\\1\" % (user, access_token)\n        if vendor:\n            auth_string += \"vendor=%s\\1\" % vendor\n        auth_string += \"\\1\"\n        try:\n            return self._command_and_check(\"authenticate\", mech, lambda x: auth_string)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def oauthbearer_login(self, identity, access_token):\n        \"\"\"Authenticate using the OAUTHBEARER method.\n\n        This is supported by Gmail and is meant to supersede the non-standard\n        'OAUTH2' and 'XOAUTH2' mechanisms.\n        \"\"\"\n        # https://tools.ietf.org/html/rfc5801#section-4\n        # Technically this is the authorization_identity, but at least for Gmail it's\n        # mandatory and practically behaves like the regular username/identity.\n        if identity:\n            gs2_header = \"n,a=%s,\" % identity.replace(\"=\", \"=3D\").replace(\",\", \"=2C\")\n        else:\n            gs2_header = \"n,,\"\n        # https://tools.ietf.org/html/rfc6750#section-2.1\n        http_authz = \"Bearer %s\" % access_token\n        # https://tools.ietf.org/html/rfc7628#section-3.1\n        auth_string = \"%s\\1auth=%s\\1\\1\" % (gs2_header, http_authz)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"OAUTHBEARER\", lambda x: auth_string\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def plain_login(self, identity, password, authorization_identity=None):\n        \"\"\"Authenticate using the PLAIN method (requires server support).\"\"\"\n        if not authorization_identity:\n            authorization_identity = \"\"\n        auth_string = \"%s\\0%s\\0%s\" % (authorization_identity, identity, password)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"PLAIN\", lambda _: auth_string, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def sasl_login(self, mech_name, mech_callable):\n        \"\"\"Authenticate using a provided SASL mechanism (requires server support).\n\n        The *mech_callable* will be called with one parameter (the server\n        challenge as bytes) and must return the corresponding client response\n        (as bytes, or as string which will be automatically encoded).\n\n        It will be called as many times as the server produces challenges,\n        which will depend on the specific SASL mechanism. (If the mechanism is\n        defined as \"client-first\", the server will nevertheless produce a\n        zero-length challenge.)\n\n        For example, PLAIN has just one step with empty challenge, so a handler\n        might look like this::\n\n            plain_mech = lambda _: \"\\\\0%s\\\\0%s\" % (username, password)\n\n            imap.sasl_login(\"PLAIN\", plain_mech)\n\n        A more complex but still stateless handler might look like this::\n\n            def example_mech(challenge):\n                if challenge == b\"Username:\"\n                    return username.encode(\"utf-8\")\n                elif challenge == b\"Password:\"\n                    return password.encode(\"utf-8\")\n                else:\n                    return b\"\"\n\n            imap.sasl_login(\"EXAMPLE\", example_mech)\n\n        A stateful handler might look like this::\n\n            class ScramSha256SaslMechanism():\n                def __init__(self, username, password):\n                    ...\n\n                def __call__(self, challenge):\n                    self.step += 1\n                    if self.step == 1:\n                        response = ...\n                    elif self.step == 2:\n                        response = ...\n                    return response\n\n            scram_mech = ScramSha256SaslMechanism(username, password)\n\n            imap.sasl_login(\"SCRAM-SHA-256\", scram_mech)\n        \"\"\"\n        try:\n            return self._command_and_check(\n                \"authenticate\", mech_name, mech_callable, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def logout(self):\n        \"\"\"Logout, returning the server response.\"\"\"\n        typ, data = self._imap.logout()\n        self._check_resp(\"BYE\", \"logout\", typ, data)\n        logger.debug(\"Logged out, connection closed\")\n        return data[0]\n\n    def shutdown(self) -> None:\n        \"\"\"Close the connection to the IMAP server (without logging out)\n\n        In most cases, :py:meth:`.logout` should be used instead of\n        this. The logout method also shutdown down the connection.\n        \"\"\"\n        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n\n    @require_capability(\"ENABLE\")\n    def enable(self, *capabilities):\n        \"\"\"Activate one or more server side capability extensions.\n\n        Most capabilities do not need to be enabled. This is only\n        required for extensions which introduce backwards incompatible\n        behaviour. Two capabilities which may require enable are\n        ``CONDSTORE`` and ``UTF8=ACCEPT``.\n\n        A list of the requested extensions that were successfully\n        enabled on the server is returned.\n\n        Once enabled each extension remains active until the IMAP\n        connection is closed.\n\n        See :rfc:`5161` for more details.\n        \"\"\"\n        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n\n    @require_capability(\"ID\")\n    def id_(self, parameters=None):\n        \"\"\"Issue the ID command, returning a dict of server implementation\n        fields.\n\n        *parameters* should be specified as a dictionary of field/value pairs,\n        for example: ``{\"name\": \"IMAPClient\", \"version\": \"0.12\"}``\n        \"\"\"\n        if parameters is None:\n            args = \"NIL\"\n        else:\n            if not isinstance(parameters, dict):\n                raise TypeError(\"'parameters' should be a dictionary\")\n            args = seq_to_parenstr(\n                _quote(v) for v in itertools.chain.from_iterable(parameters.items())\n            )\n\n        typ, data = self._imap._simple_command(\"ID\", args)\n        self._checkok(\"id\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"ID\")\n        return parse_response(data)\n\n    def capabilities(self):\n        \"\"\"Returns the server capability list.\n\n        If the session is authenticated and the server has returned an\n        untagged CAPABILITY response at authentication time, this\n        response will be returned. Otherwise, the CAPABILITY command\n        will be issued to the server, with the results cached for\n        future calls.\n\n        If the session is not yet authenticated, the capabilities\n        requested at connection time will be returned.\n        \"\"\"\n        # Ensure cached capabilities aren't used post-STARTTLS. As per\n        # https://tools.ietf.org/html/rfc2595#section-3.1\n        if self._starttls_done and self._imap.state == \"NONAUTH\":\n            self._cached_capabilities = None\n            return self._do_capabilites()\n\n        # If a capability response has been cached, use that.\n        if self._cached_capabilities:\n            return self._cached_capabilities\n\n        # If the server returned an untagged CAPABILITY response\n        # (during authentication), cache it and return that.\n        untagged = _dict_bytes_normaliser(self._imap.untagged_responses)\n        response = untagged.pop(\"CAPABILITY\", None)\n        if response:\n            self._cached_capabilities = self._normalise_capabilites(response[0])\n            return self._cached_capabilities\n\n        # If authenticated, but don't have a capability response, ask for one\n        if self._imap.state in (\"SELECTED\", \"AUTH\"):\n            self._cached_capabilities = self._do_capabilites()\n            return self._cached_capabilities\n\n        # Return capabilities that imaplib requested at connection\n        # time (pre-auth)\n        return tuple(to_bytes(c) for c in self._imap.capabilities)\n\n    def _do_capabilites(self):\n        raw_response = self._command_and_check(\"capability\", unpack=True)\n        return self._normalise_capabilites(raw_response)\n\n    def _normalise_capabilites(self, raw_response):\n        raw_response = to_bytes(raw_response)\n        return tuple(raw_response.upper().split())\n\n    def has_capability(self, capability):\n        \"\"\"Return ``True`` if the IMAP server has the given *capability*.\"\"\"\n        # FIXME: this will not detect capabilities that are backwards\n        # compatible with the current level. For instance the SORT\n        # capabilities may in the future be named SORT2 which is\n        # still compatible with the current standard and will not\n        # be detected by this method.\n        return to_bytes(capability).upper() in self.capabilities()\n\n    @require_capability(\"NAMESPACE\")\n    def namespace(self):\n        \"\"\"Return the namespace for the account as a (personal, other,\n        shared) tuple.\n\n        Each element may be None if no namespace of that type exists,\n        or a sequence of (prefix, separator) pairs.\n\n        For convenience the tuple elements may be accessed\n        positionally or using attributes named *personal*, *other* and\n        *shared*.\n\n        See :rfc:`2342` for more details.\n        \"\"\"\n        data = self._command_and_check(\"namespace\")\n        parts = []\n        for item in parse_response(data):\n            if item is None:\n                parts.append(item)\n            else:\n                converted = []\n                for prefix, separator in item:\n                    if self.folder_encode:\n                        prefix = decode_utf7(prefix)\n                    converted.append((prefix, to_unicode(separator)))\n                parts.append(tuple(converted))\n        return Namespace(*parts)\n\n    def list_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Get a listing of folders on the server as a list of\n        ``(flags, delimiter, name)`` tuples.\n\n        Specifying *directory* will limit returned folders to the\n        given base directory. The directory and any child directories\n        will returned.\n\n        Specifying *pattern* will limit returned folders to those with\n        matching names. The wildcards are supported in\n        *pattern*. ``*`` matches zero or more of any character and\n        ``%`` matches 0 or more characters except the folder\n        delimiter.\n\n        Calling list_folders with no arguments will recursively list\n        all folders available for the logged in user.\n\n        Folder names are always returned as unicode strings, and\n        decoded from modified UTF-7, except if folder_decode is not\n        set.\n        \"\"\"\n        return self._do_list(\"LIST\", directory, pattern)\n\n    @require_capability(\"XLIST\")\n    def xlist_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Execute the XLIST command, returning ``(flags, delimiter,\n        name)`` tuples.\n\n        This method returns special flags for each folder and a\n        localized name for certain folders (e.g. the name of the\n        inbox may be localized and the flags can be used to\n        determine the actual inbox, even if the name has been\n        localized.\n\n        A ``XLIST`` response could look something like::\n\n            [((b'\\\\HasNoChildren', b'\\\\Inbox'), b'/', u'Inbox'),\n             ((b'\\\\Noselect', b'\\\\HasChildren'), b'/', u'[Gmail]'),\n             ((b'\\\\HasNoChildren', b'\\\\AllMail'), b'/', u'[Gmail]/All Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Drafts'), b'/', u'[Gmail]/Drafts'),\n             ((b'\\\\HasNoChildren', b'\\\\Important'), b'/', u'[Gmail]/Important'),\n             ((b'\\\\HasNoChildren', b'\\\\Sent'), b'/', u'[Gmail]/Sent Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Spam'), b'/', u'[Gmail]/Spam'),\n             ((b'\\\\HasNoChildren', b'\\\\Starred'), b'/', u'[Gmail]/Starred'),\n             ((b'\\\\HasNoChildren', b'\\\\Trash'), b'/', u'[Gmail]/Trash')]\n\n        This is a *deprecated* Gmail-specific IMAP extension (See\n        https://developers.google.com/gmail/imap_extensions#xlist_is_deprecated\n        for more information).\n\n        The *directory* and *pattern* arguments are as per\n        list_folders().\n        \"\"\"\n        return self._do_list(\"XLIST\", directory, pattern)\n\n    def list_sub_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Return a list of subscribed folders on the server as\n        ``(flags, delimiter, name)`` tuples.\n\n        The default behaviour will list all subscribed folders. The\n        *directory* and *pattern* arguments are as per list_folders().\n        \"\"\"\n        return self._do_list(\"LSUB\", directory, pattern)\n\n    def _do_list(self, cmd, directory, pattern):\n        directory = self._normalise_folder(directory)\n        pattern = self._normalise_folder(pattern)\n        typ, dat = self._imap._simple_command(cmd, directory, pattern)\n        self._checkok(cmd, typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, cmd)\n        return self._proc_folder_list(dat)\n\n    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n\n    def find_special_folder(self, folder_flag):\n        \"\"\"Try to locate a special folder, like the Sent or Trash folder.\n\n        >>> server.find_special_folder(imapclient.SENT)\n        'INBOX.Sent'\n\n        This function tries its best to find the correct folder (if any) but\n        uses heuristics when the server is unable to precisely tell where\n        special folders are located.\n\n        Returns the name of the folder if found, or None otherwise.\n        \"\"\"\n        # Detect folder by looking for known attributes\n        # TODO: avoid listing all folders by using extended LIST (RFC6154)\n        for folder in self.list_folders():\n            if folder and len(folder[0]) > 0 and folder_flag in folder[0]:\n                return folder[2]\n\n        # Detect folder by looking for common names\n        # We only look for folders in the \"personal\" namespace of the user\n        if self.has_capability(\"NAMESPACE\"):\n            personal_namespaces = self.namespace().personal\n        else:\n            personal_namespaces = _POPULAR_PERSONAL_NAMESPACES\n\n        for personal_namespace in personal_namespaces:\n            for pattern in _POPULAR_SPECIAL_FOLDERS.get(folder_flag, tuple()):\n                pattern = personal_namespace[0] + pattern\n                sent_folders = self.list_folders(pattern=pattern)\n                if sent_folders:\n                    return sent_folders[0][2]\n\n        return None\n\n    def select_folder(self, folder, readonly=False):\n        \"\"\"Set the current folder on the server.\n\n        Future calls to methods such as search and fetch will act on\n        the selected folder.\n\n        Returns a dictionary containing the ``SELECT`` response. At least\n        the ``b'EXISTS'``, ``b'FLAGS'`` and ``b'RECENT'`` keys are guaranteed\n        to exist. An example::\n\n            {b'EXISTS': 3,\n             b'FLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'RECENT': 0,\n             b'PERMANENTFLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'READ-WRITE': True,\n             b'UIDNEXT': 11,\n             b'UIDVALIDITY': 1239278212}\n        \"\"\"\n        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n\n    @require_capability(\"UNSELECT\")\n    def unselect_folder(self):\n        r\"\"\"Unselect the current folder and release associated resources.\n\n        Unlike ``close_folder``, the ``UNSELECT`` command does not expunge\n        the mailbox, keeping messages with \\Deleted flag set for example.\n\n        Returns the UNSELECT response string returned by the server.\n        \"\"\"\n        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n\n    def _process_select_response(self, resp):\n        untagged = _dict_bytes_normaliser(resp)\n        out = {}\n\n        # imaplib doesn't parse these correctly (broken regex) so replace\n        # with the raw values out of the OK section\n        for line in untagged.get(\"OK\", []):\n            match = _RE_SELECT_RESPONSE.match(line)\n            if match:\n                key = match.group(\"key\")\n                if key == b\"PERMANENTFLAGS\":\n                    out[key] = tuple(match.group(\"data\").split())\n\n        for key, value in untagged.items():\n            key = key.upper()\n            if key in (b\"OK\", b\"PERMANENTFLAGS\"):\n                continue  # already handled above\n            if key in (\n                b\"EXISTS\",\n                b\"RECENT\",\n                b\"UIDNEXT\",\n                b\"UIDVALIDITY\",\n                b\"HIGHESTMODSEQ\",\n            ):\n                value = int(value[0])\n            elif key == b\"READ-WRITE\":\n                value = True\n            elif key == b\"FLAGS\":\n                value = tuple(value[0][1:-1].split())\n            out[key] = value\n        return out\n\n    def noop(self):\n        \"\"\"Execute the NOOP command.\n\n        This command returns immediately, returning any server side\n        status updates. It can also be used to reset any auto-logout\n        timers.\n\n        The return value is the server command response message\n        followed by a list of status responses. For example::\n\n            (b'NOOP completed.',\n             [(4, b'EXISTS'),\n              (3, b'FETCH', (b'FLAGS', (b'bar', b'sne'))),\n              (6, b'FETCH', (b'FLAGS', (b'sne',)))])\n\n        \"\"\"\n        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n\n    @require_capability(\"IDLE\")\n    def idle(self):\n        \"\"\"Put the server into IDLE mode.\n\n        In this mode the server will return unsolicited responses\n        about changes to the selected mailbox. This method returns\n        immediately. Use ``idle_check()`` to look for IDLE responses\n        and ``idle_done()`` to stop IDLE mode.\n\n        .. note::\n\n            Any other commands issued while the server is in IDLE\n            mode will fail.\n\n        See :rfc:`2177` for more information about the IDLE extension.\n        \"\"\"\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n    def _poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is more scalable because it ALLOWS your process\n        to have more than 1024 file descriptors.\n        \"\"\"\n        poller = select.poll()\n        poller.register(sock.fileno(), select.POLLIN)\n        timeout = timeout * 1000 if timeout is not None else None\n        return poller.poll(timeout)\n\n    def _select_poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is a fallback because it FAILS if your process\n        has more than 1024 file descriptors.\n        We still need this for Windows and some other niche systems.\n        \"\"\"\n        return select.select([sock], [], [], timeout)[0]\n\n    @require_capability(\"IDLE\")\n    def idle_check(self, timeout=None):\n        \"\"\"Check for any IDLE responses sent by the server.\n\n        This method should only be called if the server is in IDLE\n        mode (see ``idle()``).\n\n        By default, this method will block until an IDLE response is\n        received. If *timeout* is provided, the call will block for at\n        most this many seconds while waiting for an IDLE response.\n\n        The return value is a list of received IDLE responses. These\n        will be parsed with values converted to appropriate types. For\n        example::\n\n            [(b'OK', b'Still here'),\n             (1, b'EXISTS'),\n             (1, b'FETCH', (b'FLAGS', (b'\\\\NotJunk',)))]\n        \"\"\"\n        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n\n    @require_capability(\"IDLE\")\n    def idle_done(self):\n        \"\"\"Take the server out of IDLE mode.\n\n        This method should only be called if the server is already in\n        IDLE mode.\n\n        The return value is of the form ``(command_text,\n        idle_responses)`` where *command_text* is the text sent by the\n        server when the IDLE command finished (eg. ``b'Idle\n        terminated'``) and *idle_responses* is a list of parsed idle\n        responses received since the last call to ``idle_check()`` (if\n        any). These are returned in parsed form as per\n        ``idle_check()``.\n        \"\"\"\n        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n    def folder_status(self, folder, what=None):\n        \"\"\"Return the status of *folder*.\n\n        *what* should be a sequence of status items to query. This\n        defaults to ``('MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY',\n        'UNSEEN')``.\n\n        Returns a dictionary of the status items for the folder with\n        keys matching *what*.\n        \"\"\"\n        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n\n    def close_folder(self):\n        \"\"\"Close the currently selected folder, returning the server\n        response string.\n        \"\"\"\n        return self._command_and_check(\"close\", unpack=True)\n\n    def create_folder(self, folder):\n        \"\"\"Create *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"create\", self._normalise_folder(folder), unpack=True\n        )\n\n    def rename_folder(self, old_name, new_name):\n        \"\"\"Change the name of a folder on the server.\"\"\"\n        return self._command_and_check(\n            \"rename\",\n            self._normalise_folder(old_name),\n            self._normalise_folder(new_name),\n            unpack=True,\n        )\n\n    def delete_folder(self, folder):\n        \"\"\"Delete *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"delete\", self._normalise_folder(folder), unpack=True\n        )\n\n    def folder_exists(self, folder):\n        \"\"\"Return ``True`` if *folder* exists on the server.\"\"\"\n        return len(self.list_folders(\"\", folder)) > 0\n\n    def subscribe_folder(self, folder):\n        \"\"\"Subscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"subscribe\", self._normalise_folder(folder))\n\n    def unsubscribe_folder(self, folder):\n        \"\"\"Unsubscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"unsubscribe\", self._normalise_folder(folder))\n\n    def search(self, criteria=\"ALL\", charset=None):\n        \"\"\"Return a list of messages ids from the currently selected\n        folder matching *criteria*.\n\n        *criteria* should be a sequence of one or more criteria\n        items. Each criteria item may be either unicode or\n        bytes. Example values::\n\n            [u'UNSEEN']\n            [u'SMALLER', 500]\n            [b'NOT', b'DELETED']\n            [u'TEXT', u'foo bar', u'FLAGGED', u'SUBJECT', u'baz']\n            [u'SINCE', date(2005, 4, 3)]\n\n        IMAPClient will perform conversion and quoting as\n        required. The caller shouldn't do this.\n\n        It is also possible (but not recommended) to pass the combined\n        criteria as a single string. In this case IMAPClient won't\n        perform quoting, allowing lower-level specification of\n        criteria. Examples of this style::\n\n            u'UNSEEN'\n            u'SMALLER 500'\n            b'NOT DELETED'\n            u'TEXT \"foo bar\" FLAGGED SUBJECT \"baz\"'\n            b'SINCE 03-Apr-2005'\n\n        To support complex search expressions, criteria lists can be\n        nested. IMAPClient will insert parentheses in the right\n        places. The following will match messages that are both not\n        flagged and do not have \"foo\" in the subject::\n\n            ['NOT', ['SUBJECT', 'foo', 'FLAGGED']]\n\n        *charset* specifies the character set of the criteria. It\n        defaults to US-ASCII as this is the only charset that a server\n        is required to support by the RFC. UTF-8 is commonly supported\n        however.\n\n        Any criteria specified using unicode will be encoded as per\n        *charset*. Specifying a unicode criteria that can not be\n        encoded using *charset* will result in an error.\n\n        Any criteria specified using bytes will be sent as-is but\n        should use an encoding that matches *charset* (the character\n        set given is still passed on to the server).\n\n        See :rfc:`3501#section-6.4.4` for more details.\n\n        Note that criteria arguments that are 8-bit will be\n        transparently sent by IMAPClient as IMAP literals to ensure\n        adherence to IMAP standards.\n\n        The returned list of message ids will have a special *modseq*\n        attribute. This is set if the server included a MODSEQ value\n        to the search response (i.e. if a MODSEQ criteria was included\n        in the search).\n\n        \"\"\"\n        return self._search(criteria, charset)\n\n    @require_capability(\"X-GM-EXT-1\")\n    def gmail_search(self, query, charset=\"UTF-8\"):\n        \"\"\"Search using Gmail's X-GM-RAW attribute.\n\n        *query* should be a valid Gmail search query string. For\n        example: ``has:attachment in:unread``. The search string may\n        be unicode and will be encoded using the specified *charset*\n        (defaulting to UTF-8).\n\n        This method only works for IMAP servers that support X-GM-RAW,\n        which is only likely to be Gmail.\n\n        See https://developers.google.com/gmail/imap_extensions#extension_of_the_search_command_x-gm-raw\n        for more info.\n        \"\"\"\n        return self._search([b\"X-GM-RAW\", query], charset)\n\n    def _search(self, criteria, charset):\n        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n\n    @require_capability(\"SORT\")\n    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of message ids from the currently selected\n        folder, sorted by *sort_criteria* and optionally filtered by\n        *criteria*.\n\n        *sort_criteria* may be specified as a sequence of strings or a\n        single string. IMAPClient will take care any required\n        conversions. Valid *sort_criteria* values::\n\n            ['ARRIVAL']\n            ['SUBJECT', 'ARRIVAL']\n            'ARRIVAL'\n            'REVERSE SIZE'\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for full details.\n\n        Note that SORT is an extension to the IMAP4 standard so it may\n        not be supported by all IMAP servers.\n        \"\"\"\n        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n\n###The function: thread###\n    def get_flags(self, messages):\n        \"\"\"Return the flags set for each message in *messages* from\n        the currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (flag1, flag2, ... ), }``.\n        \"\"\"\n        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n\n    def add_flags(self, messages, flags, silent=False):\n        \"\"\"Add *flags* to *messages* in the currently selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"+FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def remove_flags(self, messages, flags, silent=False):\n        \"\"\"Remove one or more *flags* from *messages* in the currently\n        selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"-FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def set_flags(self, messages, flags, silent=False):\n        \"\"\"Set the *flags* for *messages* in the currently selected\n        folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def get_gmail_labels(self, messages):\n        \"\"\"Return the label set for each message in *messages* in the\n        currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (label1, label2, ... ), }``.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n\n    def add_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Add *labels* to *messages* in the currently selected folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"+X-GM-LABELS\", messages, labels, silent=silent)\n\n    def remove_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Remove one or more *labels* from *messages* in the\n        currently selected folder, or None if *silent* is true.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*).\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"-X-GM-LABELS\", messages, labels, silent=silent)\n\n    def set_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Set the *labels* for *messages* in the currently selected\n        folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"X-GM-LABELS\", messages, labels, silent=silent)\n\n    def delete_messages(self, messages, silent=False):\n        \"\"\"Delete one or more *messages* from the currently selected\n        folder.\n\n        Returns the flags set for each modified message (see\n        *get_flags*).\n        \"\"\"\n        return self.add_flags(messages, DELETED, silent=silent)\n\n    def fetch(self, messages, data, modifiers=None):\n        \"\"\"Retrieve selected *data* associated with one or more\n        *messages* in the currently selected folder.\n\n        *data* should be specified as a sequence of strings, one item\n        per data selector, for example ``['INTERNALDATE',\n        'RFC822']``.\n\n        *modifiers* are required for some extensions to the IMAP\n        protocol (eg. :rfc:`4551`). These should be a sequence of strings\n        if specified, for example ``['CHANGEDSINCE 123']``.\n\n        A dictionary is returned, indexed by message number. Each item\n        in this dictionary is also a dictionary, with an entry\n        corresponding to each item in *data*. Returned values will be\n        appropriately typed. For example, integer values will be returned as\n        Python integers, timestamps will be returned as datetime\n        instances and ENVELOPE responses will be returned as\n        :py:class:`Envelope <imapclient.response_types.Envelope>` instances.\n\n        String data will generally be returned as bytes (Python 3) or\n        str (Python 2).\n\n        In addition to an element for each *data* item, the dict\n        returned for each message also contains a *SEQ* key containing\n        the sequence number for the message. This allows for mapping\n        between the UID and sequence number (when the *use_uid*\n        property is ``True``).\n\n        Example::\n\n            >> c.fetch([3293, 3230], ['INTERNALDATE', 'FLAGS'])\n            {3230: {b'FLAGS': (b'\\\\Seen',),\n                    b'INTERNALDATE': datetime.datetime(2011, 1, 30, 13, 32, 9),\n                    b'SEQ': 84},\n             3293: {b'FLAGS': (),\n                    b'INTERNALDATE': datetime.datetime(2011, 2, 24, 19, 30, 36),\n                    b'SEQ': 110}}\n\n        \"\"\"\n        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            seq_to_parenstr_upper(data),\n            seq_to_parenstr_upper(modifiers) if modifiers else None,\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n\n    def append(self, folder, msg, flags=(), msg_time=None):\n        \"\"\"Append a message to *folder*.\n\n        *msg* should be a string contains the full message including\n        headers.\n\n        *flags* should be a sequence of message flags to set. If not\n        specified no flags will be set.\n\n        *msg_time* is an optional datetime instance specifying the\n        date and time to set on the message. The server will set a\n        time if it isn't specified. If *msg_time* contains timezone\n        information (tzinfo), this will be honoured. Otherwise the\n        local machine's time zone sent to the server.\n\n        Returns the APPEND response as returned by the server.\n        \"\"\"\n        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n\n    @require_capability(\"MULTIAPPEND\")\n    def multiappend(self, folder, msgs):\n        \"\"\"Append messages to *folder* using the MULTIAPPEND feature from :rfc:`3502`.\n\n        *msgs* must be an iterable. Each item must be either a string containing the\n        full message including headers, or a dict containing the keys \"msg\" with the\n        full message as before, \"flags\" with a sequence of message flags to set, and\n        \"date\" with a datetime instance specifying the internal date to set.\n        The keys \"flags\" and \"date\" are optional.\n\n        Returns the APPEND response from the server.\n        \"\"\"\n\n        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n\n    def copy(self, messages, folder):\n        \"\"\"Copy one or more messages from the current folder to\n        *folder*. Returns the COPY response string returned by the\n        server.\n        \"\"\"\n        return self._command_and_check(\n            \"copy\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    @require_capability(\"MOVE\")\n    def move(self, messages, folder):\n        \"\"\"Atomically move messages to another folder.\n\n        Requires the MOVE capability, see :rfc:`6851`.\n\n        :param messages: List of message UIDs to move.\n        :param folder: The destination folder name.\n        \"\"\"\n        return self._command_and_check(\n            \"move\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    def expunge(self, messages=None):\n        \"\"\"Use of the *messages* argument is discouraged.\n        Please see the ``uid_expunge`` method instead.\n\n        When, no *messages* are specified, remove all messages\n        from the currently selected folder that have the\n        ``\\\\Deleted`` flag set.\n\n        The return value is the server response message\n        followed by a list of expunge responses. For example::\n\n            ('Expunge completed.',\n             [(2, 'EXPUNGE'),\n              (1, 'EXPUNGE'),\n              (0, 'RECENT')])\n\n        In this case, the responses indicate that the message with\n        sequence numbers 2 and 1 where deleted, leaving no recent\n        messages in the folder.\n\n        See :rfc:`3501#section-6.4.3` section 6.4.3 and\n        :rfc:`3501#section-7.4.1` section 7.4.1 for more details.\n\n        When *messages* are specified, remove the specified messages\n        from the selected folder, provided those messages also have\n        the ``\\\\Deleted`` flag set. The return value is ``None`` in\n        this case.\n\n        Expunging messages by id(s) requires that *use_uid* is\n        ``True`` for the client.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        if messages:\n            if not self.use_uid:\n                raise ValueError(\"cannot EXPUNGE by ID when not using uids\")\n            return self._command_and_check(\n                \"EXPUNGE\", join_message_ids(messages), uid=True\n            )\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")\n\n    @require_capability(\"UIDPLUS\")\n    def uid_expunge(self, messages):\n        \"\"\"Expunge deleted messages with the specified message ids from the\n        folder.\n\n        This requires the UIDPLUS capability.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=True)\n\n    @require_capability(\"ACL\")\n    def getacl(self, folder):\n        \"\"\"Returns a list of ``(who, acl)`` tuples describing the\n        access controls for *folder*.\n        \"\"\"\n        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n\n    @require_capability(\"ACL\")\n    def setacl(self, folder, who, what):\n        \"\"\"Set an ACL (*what*) for user (*who*) for a folder.\n\n        Set *what* to an empty string to remove an ACL. Returns the\n        server response string.\n        \"\"\"\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n\n    @require_capability(\"QUOTA\")\n    def get_quota(self, mailbox=\"INBOX\"):\n        \"\"\"Get the quotas associated with a mailbox.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return self.get_quota_root(mailbox)[1]\n\n    @require_capability(\"QUOTA\")\n    def _get_quota(self, quota_root=\"\"):\n        \"\"\"Get the quotas associated with a quota root.\n\n        This method is not private but put behind an underscore to show that\n        it is a low-level function. Users probably want to use `get_quota`\n        instead.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return _parse_quota(self._command_and_check(\"getquota\", _quote(quota_root)))\n\n    @require_capability(\"QUOTA\")\n    def get_quota_root(self, mailbox):\n        \"\"\"Get the quota roots for a mailbox.\n\n        The IMAP server responds with the quota root and the quotas associated\n        so there is usually no need to call `get_quota` after.\n\n        See :rfc:`2087` for more details.\n\n        Return a tuple of MailboxQuotaRoots and list of Quota associated\n        \"\"\"\n        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n\n    @require_capability(\"QUOTA\")\n    def set_quota(self, quotas):\n        \"\"\"Set one or more quotas on resources.\n\n        :param quotas: list of Quota objects\n        \"\"\"\n        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n\n    def _check_resp(self, expected, command, typ, data):\n        \"\"\"Check command responses for errors.\n\n        Raises IMAPClient.Error if the command fails.\n        \"\"\"\n        if typ != expected:\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(data[0]))\n            )\n\n    def _consume_until_tagged_response(self, tag, command):\n        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n\n    def _raw_command_untagged(\n        self, command, args, response_name=None, unpack=False, uid=True\n    ):\n        # TODO: eventually this should replace _command_and_check (call it _command)\n        typ, data = self._raw_command(command, args, uid=uid)\n        if response_name is None:\n            response_name = command\n        typ, data = self._imap._untagged_response(typ, data, to_unicode(response_name))\n        self._checkok(to_unicode(command), typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _raw_command(self, command, args, uid=True):\n        \"\"\"Run the specific command with the arguments given. 8-bit arguments\n        are sent as literals. The return value is (typ, data).\n\n        This sidesteps much of imaplib's command sending\n        infrastructure because imaplib can't send more than one\n        literal.\n\n        *command* should be specified as bytes.\n        *args* should be specified as a list of bytes.\n        \"\"\"\n        command = command.upper()\n\n        if isinstance(args, tuple):\n            args = list(args)\n        if not isinstance(args, list):\n            args = [args]\n\n        tag = self._imap._new_tag()\n        prefix = [to_bytes(tag)]\n        if uid and self.use_uid:\n            prefix.append(b\"UID\")\n        prefix.append(command)\n\n        line = []\n        for item, is_last in _iter_with_last(prefix + args):\n            if not isinstance(item, bytes):\n                raise ValueError(\"command args must be passed as bytes\")\n\n            if _is8bit(item):\n                # If a line was already started send it\n                if line:\n                    out = b\" \".join(line)\n                    logger.debug(\"> %s\", out)\n                    self._imap.send(out)\n                    line = []\n\n                # Now send the (unquoted) literal\n                if isinstance(item, _quoted):\n                    item = item.original\n                self._send_literal(tag, item)\n                if not is_last:\n                    self._imap.send(b\" \")\n            else:\n                line.append(item)\n\n        if line:\n            out = b\" \".join(line)\n            logger.debug(\"> %s\", out)\n            self._imap.send(out)\n\n        self._imap.send(b\"\\r\\n\")\n\n        return self._imap._command_complete(to_unicode(command), tag)\n\n    def _send_literal(self, tag, item):\n        \"\"\"Send a single literal for the command with *tag*.\"\"\"\n        if b\"LITERAL+\" in self._cached_capabilities:\n            out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"+}\\r\\n\" + item\n            logger.debug(\"> %s\", debug_trunc(out, 64))\n            self._imap.send(out)\n            return\n\n        out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"}\\r\\n\"\n        logger.debug(\"> %s\", out)\n        self._imap.send(out)\n\n        # Wait for continuation response\n        while self._imap._get_response():\n            tagged_resp = self._imap.tagged_commands.get(tag)\n            if tagged_resp:\n                raise exceptions.IMAPClientAbortError(\n                    \"unexpected response while waiting for continuation response: \"\n                    + repr(tagged_resp)\n                )\n\n        logger.debug(\"   (literal) > %s\", debug_trunc(item, 256))\n        self._imap.send(item)\n\n    def _command_and_check(\n        self, command, *args, unpack: bool = False, uid: bool = False\n    ):\n        if uid and self.use_uid:\n            command = to_unicode(command)  # imaplib must die\n            typ, data = self._imap.uid(command, *args)\n        else:\n            meth = getattr(self._imap, to_unicode(command))\n            typ, data = meth(*args)\n        self._checkok(command, typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _checkok(self, command, typ, data):\n        self._check_resp(\"OK\", command, typ, data)\n\n    def _gm_label_store(self, cmd, messages, labels, silent):\n        response = self._store(\n            cmd, messages, self._normalise_labels(labels), b\"X-GM-LABELS\", silent=silent\n        )\n        return (\n            {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n            if response\n            else None\n        )\n\n    def _store(self, cmd, messages, flags, fetch_key, silent):\n        \"\"\"Worker function for the various flag manipulation methods.\n\n        *cmd* is the STORE command to use (eg. '+FLAGS').\n        \"\"\"\n        if not messages:\n            return {}\n        if silent:\n            cmd += b\".SILENT\"\n\n        data = self._command_and_check(\n            \"store\", join_message_ids(messages), cmd, seq_to_parenstr(flags), uid=True\n        )\n        if silent:\n            return None\n        return self._filter_fetch_dict(parse_fetch_response(data), fetch_key)\n\n    def _filter_fetch_dict(self, fetch_dict, key):\n        return dict((msgid, data[key]) for msgid, data in fetch_dict.items())\n\n    def _normalise_folder(self, folder_name):\n        if isinstance(folder_name, bytes):\n            folder_name = folder_name.decode(\"ascii\")\n        if self.folder_encode:\n            folder_name = encode_utf7(folder_name)\n        return _quote(folder_name)\n\n    def _normalise_labels(self, labels):\n        if isinstance(labels, (str, bytes)):\n            labels = (labels,)\n        return [_quote(encode_utf7(label)) for label in labels]\n\n    @property\n    def welcome(self):\n        \"\"\"access the server greeting message\"\"\"\n        try:\n            return self._imap.welcome\n        except AttributeError:\n            pass\n\n\ndef _quote(arg):\n    if isinstance(arg, str):\n        arg = arg.replace(\"\\\\\", \"\\\\\\\\\")\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b\"\\\\\", b\"\\\\\\\\\")\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q\n\n\ndef _normalise_search_criteria(criteria, charset=None):\n    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n\n\ndef _normalise_sort_criteria(criteria, charset=None):\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    return b\"(\" + b\" \".join(to_bytes(item).upper() for item in criteria) + b\")\"\n\n\nclass _literal(bytes):\n    \"\"\"Hold message data that should always be sent as a literal.\"\"\"\n\n\nclass _quoted(bytes):\n    \"\"\"\n    This class holds a quoted bytes value which provides access to the\n    unquoted value via the *original* attribute.\n\n    They should be created via the *maybe* classmethod.\n    \"\"\"\n\n    @classmethod\n    def maybe(cls, original):\n        \"\"\"Maybe quote a bytes value.\n\n        If the input requires no quoting it is returned unchanged.\n\n        If quoting is required a *_quoted* instance is returned. This\n        holds the quoted version of the input while also providing\n        access to the original unquoted source.\n        \"\"\"\n        quoted = original.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted = quoted.replace(b'\"', b'\\\\\"')\n        if quoted != original or b\" \" in quoted or not quoted:\n            out = cls(b'\"' + quoted + b'\"')\n            out.original = original\n            return out\n        return original\n\n\n# normalise_text_list, seq_to_parentstr etc have to return unicode\n# because imaplib handles flags and sort criteria assuming these are\n# passed as unicode\ndef normalise_text_list(items):\n    return list(_normalise_text_list(items))\n\n\ndef seq_to_parenstr(items):\n    return _join_and_paren(_normalise_text_list(items))\n\n\ndef seq_to_parenstr_upper(items):\n    return _join_and_paren(item.upper() for item in _normalise_text_list(items))\n\n\ndef _join_and_paren(items):\n    return \"(\" + \" \".join(items) + \")\"\n\n\ndef _normalise_text_list(items):\n    if isinstance(items, (str, bytes)):\n        items = (items,)\n    return (to_unicode(c) for c in items)\n\n\ndef join_message_ids(messages):\n    \"\"\"Convert a sequence of messages ids or a single integer message id\n    into an id byte string for use with IMAP commands\n    \"\"\"\n    if isinstance(messages, (str, bytes, int)):\n        messages = (to_bytes(messages),)\n    return b\",\".join(_maybe_int_to_bytes(m) for m in messages)\n\n\ndef _maybe_int_to_bytes(val):\n    if isinstance(val, int):\n        return str(val).encode(\"us-ascii\")\n    return to_bytes(val)\n\n\ndef _parse_untagged_response(text):\n    assert_imap_protocol(text.startswith(b\"* \"))\n    text = text[2:]\n    if text.startswith((b\"OK \", b\"NO \")):\n        return tuple(text.split(b\" \", 1))\n    return parse_response([text])\n\n\ndef as_pairs(items):\n    i = 0\n    last_item = None\n    for item in items:\n        if i % 2:\n            yield last_item, item\n        else:\n            last_item = item\n        i += 1\n\n\ndef as_triplets(items):\n    a = iter(items)\n    return zip(a, a, a)\n\n\ndef _is8bit(data):\n    return isinstance(data, _literal) or any(b > 127 for b in data)\n\n\ndef _iter_with_last(items):\n    last_i = len(items) - 1\n    for i, item in enumerate(items):\n        yield item, i == last_i\n\n\n_not_present = object()\n\n\nclass _dict_bytes_normaliser:\n    \"\"\"Wrap a dict with unicode/bytes keys and normalise the keys to\n    bytes.\n    \"\"\"\n\n    def __init__(self, d):\n        self._d = d\n\n    def iteritems(self):\n        for key, value in self._d.items():\n            yield to_bytes(key), value\n\n    # For Python 3 compatibility.\n    items = iteritems\n\n    def __contains__(self, ink):\n        for k in self._gen_keys(ink):\n            if k in self._d:\n                return True\n        return False\n\n    def get(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d[k]\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def pop(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d.pop(k)\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def _gen_keys(self, k):\n        yield k\n        if isinstance(k, bytes):\n            yield to_unicode(k)\n        else:\n            yield to_bytes(k)\n\n\ndef debug_trunc(v, maxlen):\n    if len(v) < maxlen:\n        return repr(v)\n    hl = maxlen // 2\n    return repr(v[:hl]) + \"...\" + repr(v[-hl:])\n\n\ndef utf7_decode_sequence(seq):\n    return [decode_utf7(s) for s in seq]\n\n\ndef _parse_quota(quota_rep):\n    quota_rep = parse_response(quota_rep)\n    rv = []\n    for quota_root, quota_resource_infos in as_pairs(quota_rep):\n        for quota_resource_info in as_triplets(quota_resource_infos):\n            rv.append(\n                Quota(\n                    quota_root=to_unicode(quota_root),\n                    resource=to_unicode(quota_resource_info[0]),\n                    usage=quota_resource_info[1],\n                    limit=quota_resource_info[2],\n                )\n            )\n    return rv\n\n\nclass IMAPlibLoggerAdapter(LoggerAdapter):\n    \"\"\"Adapter preventing IMAP secrets from going to the logging facility.\"\"\"\n\n    def process(self, msg, kwargs):\n        # msg is usually unicode but see #367. Convert bytes to\n        # unicode if required.\n        if isinstance(msg, bytes):\n            msg = msg.decode(\"ascii\", \"ignore\")\n\n        for command in (\"LOGIN\", \"AUTHENTICATE\"):\n            if msg.startswith(\">\") and command in msg:\n                msg_start = msg.split(command)[0]\n                msg = \"{}{} **REDACTED**\".format(msg_start, command)\n                break\n        return super().process(msg, kwargs)\n", "cross_context": [{"imapclient.exceptions": "import imaplib\n\n# Base class allowing to catch any IMAPClient related exceptions\n# To ensure backward compatibility, we \"rename\" the imaplib general\n# exception class, so we can catch its exceptions without having to\n# deal with it in IMAPClient codebase\n\nIMAPClientError = imaplib.IMAP4.error\nIMAPClientAbortError = imaplib.IMAP4.abort\nIMAPClientReadOnlyError = imaplib.IMAP4.readonly\n\n\nclass CapabilityError(IMAPClientError):\n    \"\"\"\n    The command tried by the user needs a capability not installed\n    on the IMAP server\n    \"\"\"\n\n\nclass LoginError(IMAPClientError):\n    \"\"\"\n    A connection has been established with the server but an error\n    occurred during the authentication.\n    \"\"\"\n\n\nclass IllegalStateError(IMAPClientError):\n    \"\"\"\n    The command tried needs a different state to be executed. This\n    means the user is not logged in or the command needs a folder to\n    be selected.\n    \"\"\"\n\n\nclass InvalidCriteriaError(IMAPClientError):\n    \"\"\"\n    A command using a search criteria failed, probably due to a syntax\n    error in the criteria string.\n    \"\"\"\n\n\nclass ProtocolError(IMAPClientError):\n    \"\"\"The server replied with a response that violates the IMAP protocol.\"\"\"\n"}, {"imapclient.exceptions.CapabilityError": "import imaplib\n\n# Base class allowing to catch any IMAPClient related exceptions\n# To ensure backward compatibility, we \"rename\" the imaplib general\n# exception class, so we can catch its exceptions without having to\n# deal with it in IMAPClient codebase\n\nIMAPClientError = imaplib.IMAP4.error\nIMAPClientAbortError = imaplib.IMAP4.abort\nIMAPClientReadOnlyError = imaplib.IMAP4.readonly\n\n\nclass CapabilityError(IMAPClientError):\n    \"\"\"\n    The command tried by the user needs a capability not installed\n    on the IMAP server\n    \"\"\"\n\n\nclass LoginError(IMAPClientError):\n    \"\"\"\n    A connection has been established with the server but an error\n    occurred during the authentication.\n    \"\"\"\n\n\nclass IllegalStateError(IMAPClientError):\n    \"\"\"\n    The command tried needs a different state to be executed. This\n    means the user is not logged in or the command needs a folder to\n    be selected.\n    \"\"\"\n\n\nclass InvalidCriteriaError(IMAPClientError):\n    \"\"\"\n    A command using a search criteria failed, probably due to a syntax\n    error in the criteria string.\n    \"\"\"\n\n\nclass ProtocolError(IMAPClientError):\n    \"\"\"The server replied with a response that violates the IMAP protocol.\"\"\"\n"}, {"imapclient.response_parser.parse_response": "# Copyright (c) 2014, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\n\"\"\"\nParsing for IMAP command responses with focus on FETCH responses as\nreturned by imaplib.\n\nInitially inspired by http://effbot.org/zone/simple-iterator-parser.htm\n\"\"\"\n\n# TODO more exact error reporting\n\nimport datetime\nimport re\nimport sys\nfrom collections import defaultdict\nfrom typing import cast, Dict, Iterator, List, Optional, Tuple, TYPE_CHECKING, Union\n\nfrom .datetime_util import parse_to_datetime\nfrom .exceptions import ProtocolError\nfrom .response_lexer import TokenSource\nfrom .response_types import Address, BodyData, Envelope, SearchIds\nfrom .typing_imapclient import _Atom\n\n__all__ = [\"parse_response\", \"parse_message_list\"]\n\n\ndef parse_response(data: List[bytes]) -> Tuple[_Atom, ...]:\n    \"\"\"Pull apart IMAP command responses.\n\n    Returns nested tuples of appropriately typed objects.\n    \"\"\"\n    if data == [None]:\n        return tuple()\n    return tuple(gen_parsed_response(data))\n\n\n_msg_id_pattern = re.compile(r\"(\\d+(?: +\\d+)*)\")\n\n\ndef parse_message_list(data: List[Union[bytes, str]]) -> SearchIds:\n    \"\"\"Parse a list of message ids and return them as a list.\n\n    parse_response is also capable of doing this but this is\n    faster. This also has special handling of the optional MODSEQ part\n    of a SEARCH response.\n\n    The returned list is a SearchIds instance which has a *modseq*\n    attribute which contains the MODSEQ response (if returned by the\n    server).\n    \"\"\"\n    if len(data) != 1:\n        raise ValueError(\"unexpected message list data\")\n\n    message_data = data[0]\n    if not message_data:\n        return SearchIds()\n\n    if isinstance(message_data, bytes):\n        message_data = message_data.decode(\"ascii\")\n\n    m = _msg_id_pattern.match(message_data)\n    if not m:\n        raise ValueError(\"unexpected message list format\")\n\n    ids = SearchIds(int(n) for n in m.group(1).split())\n\n    # Parse any non-numeric part on the end using parse_response (this\n    # is likely to be the MODSEQ section).\n    extra = message_data[m.end(1) :]\n    if extra:\n        for item in parse_response([extra.encode(\"ascii\")]):\n            if (\n                isinstance(item, tuple)\n                and len(item) == 2\n                and cast(bytes, item[0]).lower() == b\"modseq\"\n            ):\n                if TYPE_CHECKING:\n                    assert isinstance(item[1], int)\n                ids.modseq = item[1]\n            elif isinstance(item, int):\n                ids.append(item)\n    return ids\n\n\ndef gen_parsed_response(text: List[bytes]) -> Iterator[_Atom]:\n    if not text:\n        return\n    src = TokenSource(text)\n\n    token = None\n    try:\n        for token in src:\n            yield atom(src, token)\n    except ProtocolError:\n        raise\n    except ValueError:\n        _, err, _ = sys.exc_info()\n        raise ProtocolError(\"%s: %r\" % (str(err), token))\n\n\n_ParseFetchResponseInnerDict = Dict[\n    bytes, Optional[Union[datetime.datetime, int, BodyData, Envelope, _Atom]]\n]\n\n\ndef parse_fetch_response(\n    text: List[bytes], normalise_times: bool = True, uid_is_key: bool = True\n) -> \"defaultdict[int, _ParseFetchResponseInnerDict]\":\n    \"\"\"Pull apart IMAP FETCH responses as returned by imaplib.\n\n    Returns a dictionary, keyed by message ID. Each value a dictionary\n    keyed by FETCH field type (eg.\"RFC822\").\n    \"\"\"\n    if text == [None]:\n        return defaultdict()\n    response = gen_parsed_response(text)\n\n    parsed_response: \"defaultdict[int, _ParseFetchResponseInnerDict]\" = defaultdict(\n        dict\n    )\n    while True:\n        try:\n            msg_id = seq = _int_or_error(next(response), \"invalid message ID\")\n        except StopIteration:\n            break\n\n        try:\n            msg_response = next(response)\n        except StopIteration:\n            raise ProtocolError(\"unexpected EOF\")\n\n        if not isinstance(msg_response, tuple):\n            raise ProtocolError(\"bad response type: %s\" % repr(msg_response))\n        if len(msg_response) % 2:\n            raise ProtocolError(\n                \"uneven number of response items: %s\" % repr(msg_response)\n            )\n\n        # always return the sequence of the message, so it is available\n        # even if we return keyed by UID.\n        msg_data: _ParseFetchResponseInnerDict = {b\"SEQ\": seq}\n        for i in range(0, len(msg_response), 2):\n            msg_attribute = msg_response[i]\n            if TYPE_CHECKING:\n                assert isinstance(msg_attribute, bytes)\n            word = msg_attribute.upper()\n            value = msg_response[i + 1]\n\n            if word == b\"UID\":\n                uid = _int_or_error(value, \"invalid UID\")\n                if uid_is_key:\n                    msg_id = uid\n                else:\n                    msg_data[word] = uid\n            elif word == b\"INTERNALDATE\":\n                msg_data[word] = _convert_INTERNALDATE(value, normalise_times)\n            elif word == b\"ENVELOPE\":\n                msg_data[word] = _convert_ENVELOPE(value, normalise_times)\n            elif word in (b\"BODY\", b\"BODYSTRUCTURE\"):\n                if TYPE_CHECKING:\n                    assert isinstance(value, tuple)\n                msg_data[word] = BodyData.create(value)\n            else:\n                msg_data[word] = value\n\n        parsed_response[msg_id].update(msg_data)\n\n    return parsed_response\n\n\ndef _int_or_error(value: _Atom, error_text: str) -> int:\n    try:\n        return int(value)  # type: ignore[arg-type]\n    except (TypeError, ValueError):\n        raise ProtocolError(\"%s: %s\" % (error_text, repr(value)))\n\n\ndef _convert_INTERNALDATE(\n    date_string: _Atom, normalise_times: bool = True\n) -> Optional[datetime.datetime]:\n    if date_string is None:\n        return None\n\n    try:\n        if TYPE_CHECKING:\n            assert isinstance(date_string, bytes)\n        return parse_to_datetime(date_string, normalise=normalise_times)\n    except ValueError:\n        return None\n\n\ndef _convert_ENVELOPE(\n    envelope_response: _Atom, normalise_times: bool = True\n) -> Envelope:\n    if TYPE_CHECKING:\n        assert isinstance(envelope_response, tuple)\n    dt = None\n    if envelope_response[0]:\n        try:\n            if TYPE_CHECKING:\n                assert isinstance(envelope_response[0], bytes)\n            dt = parse_to_datetime(\n                envelope_response[0],\n                normalise=normalise_times,\n            )\n        except ValueError:\n            pass\n\n    subject = envelope_response[1]\n    in_reply_to = envelope_response[8]\n    message_id = envelope_response[9]\n    if TYPE_CHECKING:\n        assert isinstance(subject, bytes)\n        assert isinstance(in_reply_to, bytes)\n        assert isinstance(message_id, bytes)\n\n    # addresses contains a tuple of addresses\n    # from, sender, reply_to, to, cc, bcc headers\n    addresses: List[Optional[Tuple[Address, ...]]] = []\n    for addr_list in envelope_response[2:8]:\n        addrs = []\n        if addr_list:\n            if TYPE_CHECKING:\n                assert isinstance(addr_list, tuple)\n            for addr_tuple in addr_list:\n                if TYPE_CHECKING:\n                    assert isinstance(addr_tuple, tuple)\n                if addr_tuple:\n                    if TYPE_CHECKING:\n                        addr_tuple = cast(Tuple[bytes, bytes, bytes, bytes], addr_tuple)\n                    addrs.append(Address(*addr_tuple))\n            addresses.append(tuple(addrs))\n        else:\n            addresses.append(None)\n\n    return Envelope(\n        date=dt,\n        subject=subject,\n        from_=addresses[0],\n        sender=addresses[1],\n        reply_to=addresses[2],\n        to=addresses[3],\n        cc=addresses[4],\n        bcc=addresses[5],\n        in_reply_to=in_reply_to,\n        message_id=message_id,\n    )\n\n\ndef atom(src: TokenSource, token: bytes) -> _Atom:\n    if token == b\"(\":\n        return parse_tuple(src)\n    if token == b\"NIL\":\n        return None\n    if token[:1] == b\"{\":\n        literal_len = int(token[1:-1])\n        literal_text = src.current_literal\n        if literal_text is None:\n            raise ProtocolError(\"No literal corresponds to %r\" % token)\n        if len(literal_text) != literal_len:\n            raise ProtocolError(\n                \"Expecting literal of size %d, got %d\"\n                % (literal_len, len(literal_text))\n            )\n        return literal_text\n    if len(token) >= 2 and (token[:1] == token[-1:] == b'\"'):\n        return token[1:-1]\n    if token.isdigit() and (token[:1] != b\"0\" or len(token) == 1):\n        # this prevents converting items like 0123 to 123\n        return int(token)\n    return token\n\n\ndef parse_tuple(src: TokenSource) -> _Atom:\n    out: List[_Atom] = []\n    for token in src:\n        if token == b\")\":\n            return tuple(out)\n        out.append(atom(src, token))\n    # no terminator\n    raise ProtocolError('Tuple incomplete before \"(%s\"' % _fmt_tuple(out))\n\n\ndef _fmt_tuple(t: List[_Atom]) -> str:\n    return \" \".join(str(item) for item in t)\n"}, {"imapclient.util.to_bytes": "# Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport logging\nfrom typing import Iterator, Optional, Tuple, Union\n\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef to_unicode(s: Union[bytes, str]) -> str:\n    if isinstance(s, bytes):\n        try:\n            return s.decode(\"ascii\")\n        except UnicodeDecodeError:\n            logger.warning(\n                \"An error occurred while decoding %s in ASCII 'strict' mode. Fallback to \"\n                \"'ignore' errors handling, some characters might have been stripped\",\n                s,\n            )\n            return s.decode(\"ascii\", \"ignore\")\n    return s\n\n\ndef to_bytes(s: Union[bytes, str], charset: str = \"ascii\") -> bytes:\n    if isinstance(s, str):\n        return s.encode(charset)\n    return s\n\n\ndef assert_imap_protocol(condition: bool, message: Optional[bytes] = None) -> None:\n    from . import exceptions\n    if not condition:\n        msg = \"Server replied with a response that violates the IMAP protocol\"\n        if message:\n            # FIXME(jlvillal): This looks wrong as it repeats `msg` twice\n            msg += \"{}: {}\".format(\n                msg, message.decode(encoding=\"ascii\", errors=\"ignore\")\n            )\n        raise exceptions.ProtocolError(msg)\n\n\n_TupleAtomPart = Union[None, int, bytes]\n_TupleAtom = Tuple[Union[_TupleAtomPart, \"_TupleAtom\"], ...]\n\n\ndef chunk(lst: _TupleAtom, size: int) -> Iterator[_TupleAtom]:\n    for i in range(0, len(lst), size):\n        yield lst[i : i + size]\n"}], "prompt": "Please write a python function called 'thread' base the context. Return a list of message threads from the currently selected folder that match the specified criteria. Each returned thread is a list of message IDs.\n:param algorithm: String, the threading algorithm to use. It defaults to \"REFERENCES\" if not specified.\n:param criteria: String, the search criteria to match the messages. It defaults to \"ALL\" if not specified.\n:param charset: String, the character set to be used. It defaults to \"UTF-8\" if not specified.\n:return: List[Tuple], each tuple represents a message thread, where each element of the tuple is a message ID. For example, \"((1, 2), (3,), (4, 5, 6))\".\n.\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # Copyright (c) 2015, Menno Smits\n# Released subject to the New BSD License\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\n\nimport dataclasses\nimport functools\nimport imaplib\nimport itertools\nimport re\nimport select\nimport socket\nimport ssl as ssl_lib\nimport sys\nimport warnings\nfrom datetime import date, datetime\nfrom logging import getLogger, LoggerAdapter\nfrom operator import itemgetter\nfrom typing import List, Optional\n\nfrom . import exceptions, imap4, tls\nfrom .datetime_util import datetime_to_INTERNALDATE\nfrom .imap_utf7 import decode as decode_utf7\nfrom .imap_utf7 import encode as encode_utf7\nfrom .response_parser import parse_fetch_response, parse_message_list, parse_response\nfrom .util import assert_imap_protocol, to_bytes, to_unicode\n\nif hasattr(select, \"poll\"):\n    POLL_SUPPORT = True\nelse:\n    # Fallback to select() on systems that don't support poll()\n    POLL_SUPPORT = False\n\n\nlogger = getLogger(__name__)\n\n__all__ = [\n    \"IMAPClient\",\n    \"SocketTimeout\",\n    \"DELETED\",\n    \"SEEN\",\n    \"ANSWERED\",\n    \"FLAGGED\",\n    \"DRAFT\",\n    \"RECENT\",\n]\n\n\n# We also offer the gmail-specific XLIST command...\nif \"XLIST\" not in imaplib.Commands:\n    imaplib.Commands[\"XLIST\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ...and IDLE\nif \"IDLE\" not in imaplib.Commands:\n    imaplib.Commands[\"IDLE\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ..and STARTTLS\nif \"STARTTLS\" not in imaplib.Commands:\n    imaplib.Commands[\"STARTTLS\"] = (\"NONAUTH\",)\n\n# ...and ID. RFC2971 says that this command is valid in all states,\n# but not that some servers (*cough* FastMail *cough*) don't seem to\n# accept it in state NONAUTH.\nif \"ID\" not in imaplib.Commands:\n    imaplib.Commands[\"ID\"] = (\"NONAUTH\", \"AUTH\", \"SELECTED\")\n\n# ... and UNSELECT. RFC3691 does not specify the state but there is no\n# reason to use the command without AUTH state and a mailbox selected.\nif \"UNSELECT\" not in imaplib.Commands:\n    imaplib.Commands[\"UNSELECT\"] = (\"AUTH\", \"SELECTED\")\n\n# .. and ENABLE.\nif \"ENABLE\" not in imaplib.Commands:\n    imaplib.Commands[\"ENABLE\"] = (\"AUTH\",)\n\n# .. and MOVE for RFC6851.\nif \"MOVE\" not in imaplib.Commands:\n    imaplib.Commands[\"MOVE\"] = (\"AUTH\", \"SELECTED\")\n\n# System flags\nDELETED = rb\"\\Deleted\"\nSEEN = rb\"\\Seen\"\nANSWERED = rb\"\\Answered\"\nFLAGGED = rb\"\\Flagged\"\nDRAFT = rb\"\\Draft\"\nRECENT = rb\"\\Recent\"  # This flag is read-only\n\n# Special folders, see RFC6154\n# \\Flagged is omitted because it is the same as the flag defined above\nALL = rb\"\\All\"\nARCHIVE = rb\"\\Archive\"\nDRAFTS = rb\"\\Drafts\"\nJUNK = rb\"\\Junk\"\nSENT = rb\"\\Sent\"\nTRASH = rb\"\\Trash\"\n\n# Personal namespaces that are common among providers\n# used as a fallback when the server does not support the NAMESPACE capability\n_POPULAR_PERSONAL_NAMESPACES = ((\"\", \"\"), (\"INBOX.\", \".\"))\n\n# Names of special folders that are common among providers\n_POPULAR_SPECIAL_FOLDERS = {\n    SENT: (\"Sent\", \"Sent Items\", \"Sent items\"),\n    DRAFTS: (\"Drafts\",),\n    ARCHIVE: (\"Archive\",),\n    TRASH: (\"Trash\", \"Deleted Items\", \"Deleted Messages\", \"Deleted\"),\n    JUNK: (\"Junk\", \"Spam\"),\n}\n\n_RE_SELECT_RESPONSE = re.compile(rb\"\\[(?P<key>[A-Z-]+)( \\((?P<data>.*)\\))?\\]\")\n\n\nclass Namespace(tuple):\n    def __new__(cls, personal, other, shared):\n        return tuple.__new__(cls, (personal, other, shared))\n\n    personal = property(itemgetter(0))\n    other = property(itemgetter(1))\n    shared = property(itemgetter(2))\n\n\n@dataclasses.dataclass\nclass SocketTimeout:\n    \"\"\"Represents timeout configuration for an IMAP connection.\n\n    :ivar connect: maximum time to wait for a connection attempt to remote server\n    :ivar read: maximum time to wait for performing a read/write operation\n\n    As an example, ``SocketTimeout(connect=15, read=60)`` will make the socket\n    timeout if the connection takes more than 15 seconds to establish but\n    read/write operations can take up to 60 seconds once the connection is done.\n    \"\"\"\n\n    connect: float\n    read: float\n\n\n@dataclasses.dataclass\nclass MailboxQuotaRoots:\n    \"\"\"Quota roots associated with a mailbox.\n\n    Represents the response of a GETQUOTAROOT command.\n\n    :ivar mailbox: the mailbox\n    :ivar quota_roots: list of quota roots associated with the mailbox\n    \"\"\"\n\n    mailbox: str\n    quota_roots: List[str]\n\n\n@dataclasses.dataclass\nclass Quota:\n    \"\"\"Resource quota.\n\n    Represents the response of a GETQUOTA command.\n\n    :ivar quota_roots: the quota roots for which the limit apply\n    :ivar resource: the resource being limited (STORAGE, MESSAGES...)\n    :ivar usage: the current usage of the resource\n    :ivar limit: the maximum allowed usage of the resource\n    \"\"\"\n\n    quota_root: str\n    resource: str\n    usage: bytes\n    limit: bytes\n\n\ndef require_capability(capability):\n    \"\"\"Decorator raising CapabilityError when a capability is not available.\"\"\"\n\n    def actual_decorator(func):\n        @functools.wraps(func)\n        def wrapper(client, *args, **kwargs):\n            if not client.has_capability(capability):\n                raise exceptions.CapabilityError(\n                    \"Server does not support {} capability\".format(capability)\n                )\n            return func(client, *args, **kwargs)\n\n        return wrapper\n\n    return actual_decorator\n\n\nclass IMAPClient:\n    \"\"\"A connection to the IMAP server specified by *host* is made when\n    this class is instantiated.\n\n    *port* defaults to 993, or 143 if *ssl* is ``False``.\n\n    If *use_uid* is ``True`` unique message UIDs be used for all calls\n    that accept message ids (defaults to ``True``).\n\n    If *ssl* is ``True`` (the default) a secure connection will be made.\n    Otherwise an insecure connection over plain text will be\n    established.\n\n    If *ssl* is ``True`` the optional *ssl_context* argument can be\n    used to provide an ``ssl.SSLContext`` instance used to\n    control SSL/TLS connection parameters. If this is not provided a\n    sensible default context will be used.\n\n    If *stream* is ``True`` then *host* is used as the command to run\n    to establish a connection to the IMAP server (defaults to\n    ``False``). This is useful for exotic connection or authentication\n    setups.\n\n    Use *timeout* to specify a timeout for the socket connected to the\n    IMAP server. The timeout can be either a float number, or an instance\n    of :py:class:`imapclient.SocketTimeout`.\n\n    * If a single float number is passed, the same timeout delay applies\n      during the  initial connection to the server and for all future socket\n      reads and writes.\n\n    * In case of a ``SocketTimeout``, connection timeout and\n      read/write operations can have distinct timeouts.\n\n    * The default is ``None``, where no timeout is used.\n\n    The *normalise_times* attribute specifies whether datetimes\n    returned by ``fetch()`` are normalised to the local system time\n    and include no timezone information (native), or are datetimes\n    that include timezone information (aware). By default\n    *normalise_times* is True (times are normalised to the local\n    system time). This attribute can be changed between ``fetch()``\n    calls if required.\n\n    Can be used as a context manager to automatically close opened connections:\n\n    >>> with IMAPClient(host=\"imap.foo.org\") as client:\n    ...     client.login(\"bar@foo.org\", \"passwd\")\n\n    \"\"\"\n\n    # Those exceptions are kept for backward-compatibility, since\n    # previous versions included these attributes as references to\n    # imaplib original exceptions\n    Error = exceptions.IMAPClientError\n    AbortError = exceptions.IMAPClientAbortError\n    ReadOnlyError = exceptions.IMAPClientReadOnlyError\n\n    def __init__(\n        self,\n        host: str,\n        port: int = None,\n        use_uid: bool = True,\n        ssl: bool = True,\n        stream: bool = False,\n        ssl_context: Optional[ssl_lib.SSLContext] = None,\n        timeout: Optional[float] = None,\n    ):\n        if stream:\n            if port is not None:\n                raise ValueError(\"can't set 'port' when 'stream' True\")\n            if ssl:\n                raise ValueError(\"can't use 'ssl' when 'stream' is True\")\n        elif port is None:\n            port = ssl and 993 or 143\n\n        if ssl and port == 143:\n            logger.warning(\n                \"Attempting to establish an encrypted connection \"\n                \"to a port (143) often used for unencrypted \"\n                \"connections\"\n            )\n\n        self.host = host\n        self.port = port\n        self.ssl = ssl\n        self.ssl_context = ssl_context\n        self.stream = stream\n        self.use_uid = use_uid\n        self.folder_encode = True\n        self.normalise_times = True\n\n        # If the user gives a single timeout value, assume it is the same for\n        # connection and read/write operations\n        if not isinstance(timeout, SocketTimeout):\n            timeout = SocketTimeout(timeout, timeout)\n\n        self._timeout = timeout\n        self._starttls_done = False\n        self._cached_capabilities = None\n        self._idle_tag = None\n\n        self._imap = self._create_IMAP4()\n        logger.debug(\n            \"Connected to host %s over %s\",\n            self.host,\n            \"SSL/TLS\" if ssl else \"plain text\",\n        )\n\n        self._set_read_timeout()\n        # Small hack to make imaplib log everything to its own logger\n        imaplib_logger = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._imap.debug = 5\n        self._imap._mesg = imaplib_logger.debug\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Logout and closes the connection when exiting the context manager.\n\n        All exceptions during logout and connection shutdown are caught because\n        an error here usually means the connection was already closed.\n        \"\"\"\n        try:\n            self.logout()\n        except Exception:\n            try:\n                self.shutdown()\n            except Exception as e:\n                logger.info(\"Could not close the connection cleanly: %s\", e)\n\n    def _create_IMAP4(self):\n        if self.stream:\n            return imaplib.IMAP4_stream(self.host)\n\n        connect_timeout = getattr(self._timeout, \"connect\", None)\n\n        if self.ssl:\n            return tls.IMAP4_TLS(\n                self.host,\n                self.port,\n                self.ssl_context,\n                connect_timeout,\n            )\n\n        return imap4.IMAP4WithTimeout(self.host, self.port, connect_timeout)\n\n    def _set_read_timeout(self):\n        if self._timeout is not None:\n            self.socket().settimeout(self._timeout.read)\n\n    @property\n    def _sock(self):\n        warnings.warn(\"_sock is deprecated. Use socket().\", DeprecationWarning)\n        return self.socket()\n\n    def socket(self):\n        \"\"\"Returns socket used to connect to server.\n\n        The socket is provided for polling purposes only.\n        It can be used in,\n        for example, :py:meth:`selectors.BaseSelector.register`\n        and :py:meth:`asyncio.loop.add_reader` to wait for data.\n\n        .. WARNING::\n           All other uses of the returned socket are unsupported.\n           This includes reading from and writing to the socket,\n           as they are likely to break internal bookkeeping of messages.\n        \"\"\"\n        # In py2, imaplib has sslobj (for SSL connections), and sock for non-SSL.\n        # In the py3 version it's just sock.\n        return getattr(self._imap, \"sslobj\", self._imap.sock)\n\n    @require_capability(\"STARTTLS\")\n    def starttls(self, ssl_context=None):\n        \"\"\"Switch to an SSL encrypted connection by sending a STARTTLS command.\n\n        The *ssl_context* argument is optional and should be a\n        :py:class:`ssl.SSLContext` object. If no SSL context is given, a SSL\n        context with reasonable default settings will be used.\n\n        You can enable checking of the hostname in the certificate presented\n        by the server  against the hostname which was used for connecting, by\n        setting the *check_hostname* attribute of the SSL context to ``True``.\n        The default SSL context has this setting enabled.\n\n        Raises :py:exc:`Error` if the SSL connection could not be established.\n\n        Raises :py:exc:`AbortError` if the server does not support STARTTLS\n        or an SSL connection is already established.\n        \"\"\"\n        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n\n    def login(self, username: str, password: str):\n        \"\"\"Login using *username* and *password*, returning the\n        server response.\n        \"\"\"\n        try:\n            rv = self._command_and_check(\n                \"login\",\n                to_unicode(username),\n                to_unicode(password),\n                unpack=True,\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n        logger.debug(\"Logged in as %s\", username)\n        return rv\n\n    def oauth2_login(\n        self,\n        user: str,\n        access_token: str,\n        mech: str = \"XOAUTH2\",\n        vendor: Optional[str] = None,\n    ):\n        \"\"\"Authenticate using the OAUTH2 or XOAUTH2 methods.\n\n        Gmail and Yahoo both support the 'XOAUTH2' mechanism, but Yahoo requires\n        the 'vendor' portion in the payload.\n        \"\"\"\n        auth_string = \"user=%s\\1auth=Bearer %s\\1\" % (user, access_token)\n        if vendor:\n            auth_string += \"vendor=%s\\1\" % vendor\n        auth_string += \"\\1\"\n        try:\n            return self._command_and_check(\"authenticate\", mech, lambda x: auth_string)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def oauthbearer_login(self, identity, access_token):\n        \"\"\"Authenticate using the OAUTHBEARER method.\n\n        This is supported by Gmail and is meant to supersede the non-standard\n        'OAUTH2' and 'XOAUTH2' mechanisms.\n        \"\"\"\n        # https://tools.ietf.org/html/rfc5801#section-4\n        # Technically this is the authorization_identity, but at least for Gmail it's\n        # mandatory and practically behaves like the regular username/identity.\n        if identity:\n            gs2_header = \"n,a=%s,\" % identity.replace(\"=\", \"=3D\").replace(\",\", \"=2C\")\n        else:\n            gs2_header = \"n,,\"\n        # https://tools.ietf.org/html/rfc6750#section-2.1\n        http_authz = \"Bearer %s\" % access_token\n        # https://tools.ietf.org/html/rfc7628#section-3.1\n        auth_string = \"%s\\1auth=%s\\1\\1\" % (gs2_header, http_authz)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"OAUTHBEARER\", lambda x: auth_string\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def plain_login(self, identity, password, authorization_identity=None):\n        \"\"\"Authenticate using the PLAIN method (requires server support).\"\"\"\n        if not authorization_identity:\n            authorization_identity = \"\"\n        auth_string = \"%s\\0%s\\0%s\" % (authorization_identity, identity, password)\n        try:\n            return self._command_and_check(\n                \"authenticate\", \"PLAIN\", lambda _: auth_string, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def sasl_login(self, mech_name, mech_callable):\n        \"\"\"Authenticate using a provided SASL mechanism (requires server support).\n\n        The *mech_callable* will be called with one parameter (the server\n        challenge as bytes) and must return the corresponding client response\n        (as bytes, or as string which will be automatically encoded).\n\n        It will be called as many times as the server produces challenges,\n        which will depend on the specific SASL mechanism. (If the mechanism is\n        defined as \"client-first\", the server will nevertheless produce a\n        zero-length challenge.)\n\n        For example, PLAIN has just one step with empty challenge, so a handler\n        might look like this::\n\n            plain_mech = lambda _: \"\\\\0%s\\\\0%s\" % (username, password)\n\n            imap.sasl_login(\"PLAIN\", plain_mech)\n\n        A more complex but still stateless handler might look like this::\n\n            def example_mech(challenge):\n                if challenge == b\"Username:\"\n                    return username.encode(\"utf-8\")\n                elif challenge == b\"Password:\"\n                    return password.encode(\"utf-8\")\n                else:\n                    return b\"\"\n\n            imap.sasl_login(\"EXAMPLE\", example_mech)\n\n        A stateful handler might look like this::\n\n            class ScramSha256SaslMechanism():\n                def __init__(self, username, password):\n                    ...\n\n                def __call__(self, challenge):\n                    self.step += 1\n                    if self.step == 1:\n                        response = ...\n                    elif self.step == 2:\n                        response = ...\n                    return response\n\n            scram_mech = ScramSha256SaslMechanism(username, password)\n\n            imap.sasl_login(\"SCRAM-SHA-256\", scram_mech)\n        \"\"\"\n        try:\n            return self._command_and_check(\n                \"authenticate\", mech_name, mech_callable, unpack=True\n            )\n        except exceptions.IMAPClientError as e:\n            raise exceptions.LoginError(str(e))\n\n    def logout(self):\n        \"\"\"Logout, returning the server response.\"\"\"\n        typ, data = self._imap.logout()\n        self._check_resp(\"BYE\", \"logout\", typ, data)\n        logger.debug(\"Logged out, connection closed\")\n        return data[0]\n\n    def shutdown(self) -> None:\n        \"\"\"Close the connection to the IMAP server (without logging out)\n\n        In most cases, :py:meth:`.logout` should be used instead of\n        this. The logout method also shutdown down the connection.\n        \"\"\"\n        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n\n    @require_capability(\"ENABLE\")\n    def enable(self, *capabilities):\n        \"\"\"Activate one or more server side capability extensions.\n\n        Most capabilities do not need to be enabled. This is only\n        required for extensions which introduce backwards incompatible\n        behaviour. Two capabilities which may require enable are\n        ``CONDSTORE`` and ``UTF8=ACCEPT``.\n\n        A list of the requested extensions that were successfully\n        enabled on the server is returned.\n\n        Once enabled each extension remains active until the IMAP\n        connection is closed.\n\n        See :rfc:`5161` for more details.\n        \"\"\"\n        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()\n\n    @require_capability(\"ID\")\n    def id_(self, parameters=None):\n        \"\"\"Issue the ID command, returning a dict of server implementation\n        fields.\n\n        *parameters* should be specified as a dictionary of field/value pairs,\n        for example: ``{\"name\": \"IMAPClient\", \"version\": \"0.12\"}``\n        \"\"\"\n        if parameters is None:\n            args = \"NIL\"\n        else:\n            if not isinstance(parameters, dict):\n                raise TypeError(\"'parameters' should be a dictionary\")\n            args = seq_to_parenstr(\n                _quote(v) for v in itertools.chain.from_iterable(parameters.items())\n            )\n\n        typ, data = self._imap._simple_command(\"ID\", args)\n        self._checkok(\"id\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"ID\")\n        return parse_response(data)\n\n    def capabilities(self):\n        \"\"\"Returns the server capability list.\n\n        If the session is authenticated and the server has returned an\n        untagged CAPABILITY response at authentication time, this\n        response will be returned. Otherwise, the CAPABILITY command\n        will be issued to the server, with the results cached for\n        future calls.\n\n        If the session is not yet authenticated, the capabilities\n        requested at connection time will be returned.\n        \"\"\"\n        # Ensure cached capabilities aren't used post-STARTTLS. As per\n        # https://tools.ietf.org/html/rfc2595#section-3.1\n        if self._starttls_done and self._imap.state == \"NONAUTH\":\n            self._cached_capabilities = None\n            return self._do_capabilites()\n\n        # If a capability response has been cached, use that.\n        if self._cached_capabilities:\n            return self._cached_capabilities\n\n        # If the server returned an untagged CAPABILITY response\n        # (during authentication), cache it and return that.\n        untagged = _dict_bytes_normaliser(self._imap.untagged_responses)\n        response = untagged.pop(\"CAPABILITY\", None)\n        if response:\n            self._cached_capabilities = self._normalise_capabilites(response[0])\n            return self._cached_capabilities\n\n        # If authenticated, but don't have a capability response, ask for one\n        if self._imap.state in (\"SELECTED\", \"AUTH\"):\n            self._cached_capabilities = self._do_capabilites()\n            return self._cached_capabilities\n\n        # Return capabilities that imaplib requested at connection\n        # time (pre-auth)\n        return tuple(to_bytes(c) for c in self._imap.capabilities)\n\n    def _do_capabilites(self):\n        raw_response = self._command_and_check(\"capability\", unpack=True)\n        return self._normalise_capabilites(raw_response)\n\n    def _normalise_capabilites(self, raw_response):\n        raw_response = to_bytes(raw_response)\n        return tuple(raw_response.upper().split())\n\n    def has_capability(self, capability):\n        \"\"\"Return ``True`` if the IMAP server has the given *capability*.\"\"\"\n        # FIXME: this will not detect capabilities that are backwards\n        # compatible with the current level. For instance the SORT\n        # capabilities may in the future be named SORT2 which is\n        # still compatible with the current standard and will not\n        # be detected by this method.\n        return to_bytes(capability).upper() in self.capabilities()\n\n    @require_capability(\"NAMESPACE\")\n    def namespace(self):\n        \"\"\"Return the namespace for the account as a (personal, other,\n        shared) tuple.\n\n        Each element may be None if no namespace of that type exists,\n        or a sequence of (prefix, separator) pairs.\n\n        For convenience the tuple elements may be accessed\n        positionally or using attributes named *personal*, *other* and\n        *shared*.\n\n        See :rfc:`2342` for more details.\n        \"\"\"\n        data = self._command_and_check(\"namespace\")\n        parts = []\n        for item in parse_response(data):\n            if item is None:\n                parts.append(item)\n            else:\n                converted = []\n                for prefix, separator in item:\n                    if self.folder_encode:\n                        prefix = decode_utf7(prefix)\n                    converted.append((prefix, to_unicode(separator)))\n                parts.append(tuple(converted))\n        return Namespace(*parts)\n\n    def list_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Get a listing of folders on the server as a list of\n        ``(flags, delimiter, name)`` tuples.\n\n        Specifying *directory* will limit returned folders to the\n        given base directory. The directory and any child directories\n        will returned.\n\n        Specifying *pattern* will limit returned folders to those with\n        matching names. The wildcards are supported in\n        *pattern*. ``*`` matches zero or more of any character and\n        ``%`` matches 0 or more characters except the folder\n        delimiter.\n\n        Calling list_folders with no arguments will recursively list\n        all folders available for the logged in user.\n\n        Folder names are always returned as unicode strings, and\n        decoded from modified UTF-7, except if folder_decode is not\n        set.\n        \"\"\"\n        return self._do_list(\"LIST\", directory, pattern)\n\n    @require_capability(\"XLIST\")\n    def xlist_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Execute the XLIST command, returning ``(flags, delimiter,\n        name)`` tuples.\n\n        This method returns special flags for each folder and a\n        localized name for certain folders (e.g. the name of the\n        inbox may be localized and the flags can be used to\n        determine the actual inbox, even if the name has been\n        localized.\n\n        A ``XLIST`` response could look something like::\n\n            [((b'\\\\HasNoChildren', b'\\\\Inbox'), b'/', u'Inbox'),\n             ((b'\\\\Noselect', b'\\\\HasChildren'), b'/', u'[Gmail]'),\n             ((b'\\\\HasNoChildren', b'\\\\AllMail'), b'/', u'[Gmail]/All Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Drafts'), b'/', u'[Gmail]/Drafts'),\n             ((b'\\\\HasNoChildren', b'\\\\Important'), b'/', u'[Gmail]/Important'),\n             ((b'\\\\HasNoChildren', b'\\\\Sent'), b'/', u'[Gmail]/Sent Mail'),\n             ((b'\\\\HasNoChildren', b'\\\\Spam'), b'/', u'[Gmail]/Spam'),\n             ((b'\\\\HasNoChildren', b'\\\\Starred'), b'/', u'[Gmail]/Starred'),\n             ((b'\\\\HasNoChildren', b'\\\\Trash'), b'/', u'[Gmail]/Trash')]\n\n        This is a *deprecated* Gmail-specific IMAP extension (See\n        https://developers.google.com/gmail/imap_extensions#xlist_is_deprecated\n        for more information).\n\n        The *directory* and *pattern* arguments are as per\n        list_folders().\n        \"\"\"\n        return self._do_list(\"XLIST\", directory, pattern)\n\n    def list_sub_folders(self, directory=\"\", pattern=\"*\"):\n        \"\"\"Return a list of subscribed folders on the server as\n        ``(flags, delimiter, name)`` tuples.\n\n        The default behaviour will list all subscribed folders. The\n        *directory* and *pattern* arguments are as per list_folders().\n        \"\"\"\n        return self._do_list(\"LSUB\", directory, pattern)\n\n    def _do_list(self, cmd, directory, pattern):\n        directory = self._normalise_folder(directory)\n        pattern = self._normalise_folder(pattern)\n        typ, dat = self._imap._simple_command(cmd, directory, pattern)\n        self._checkok(cmd, typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, cmd)\n        return self._proc_folder_list(dat)\n\n    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret\n\n    def find_special_folder(self, folder_flag):\n        \"\"\"Try to locate a special folder, like the Sent or Trash folder.\n\n        >>> server.find_special_folder(imapclient.SENT)\n        'INBOX.Sent'\n\n        This function tries its best to find the correct folder (if any) but\n        uses heuristics when the server is unable to precisely tell where\n        special folders are located.\n\n        Returns the name of the folder if found, or None otherwise.\n        \"\"\"\n        # Detect folder by looking for known attributes\n        # TODO: avoid listing all folders by using extended LIST (RFC6154)\n        for folder in self.list_folders():\n            if folder and len(folder[0]) > 0 and folder_flag in folder[0]:\n                return folder[2]\n\n        # Detect folder by looking for common names\n        # We only look for folders in the \"personal\" namespace of the user\n        if self.has_capability(\"NAMESPACE\"):\n            personal_namespaces = self.namespace().personal\n        else:\n            personal_namespaces = _POPULAR_PERSONAL_NAMESPACES\n\n        for personal_namespace in personal_namespaces:\n            for pattern in _POPULAR_SPECIAL_FOLDERS.get(folder_flag, tuple()):\n                pattern = personal_namespace[0] + pattern\n                sent_folders = self.list_folders(pattern=pattern)\n                if sent_folders:\n                    return sent_folders[0][2]\n\n        return None\n\n    def select_folder(self, folder, readonly=False):\n        \"\"\"Set the current folder on the server.\n\n        Future calls to methods such as search and fetch will act on\n        the selected folder.\n\n        Returns a dictionary containing the ``SELECT`` response. At least\n        the ``b'EXISTS'``, ``b'FLAGS'`` and ``b'RECENT'`` keys are guaranteed\n        to exist. An example::\n\n            {b'EXISTS': 3,\n             b'FLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'RECENT': 0,\n             b'PERMANENTFLAGS': (b'\\\\Answered', b'\\\\Flagged', b'\\\\Deleted', ... ),\n             b'READ-WRITE': True,\n             b'UIDNEXT': 11,\n             b'UIDVALIDITY': 1239278212}\n        \"\"\"\n        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n\n    @require_capability(\"UNSELECT\")\n    def unselect_folder(self):\n        r\"\"\"Unselect the current folder and release associated resources.\n\n        Unlike ``close_folder``, the ``UNSELECT`` command does not expunge\n        the mailbox, keeping messages with \\Deleted flag set for example.\n\n        Returns the UNSELECT response string returned by the server.\n        \"\"\"\n        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]\n\n    def _process_select_response(self, resp):\n        untagged = _dict_bytes_normaliser(resp)\n        out = {}\n\n        # imaplib doesn't parse these correctly (broken regex) so replace\n        # with the raw values out of the OK section\n        for line in untagged.get(\"OK\", []):\n            match = _RE_SELECT_RESPONSE.match(line)\n            if match:\n                key = match.group(\"key\")\n                if key == b\"PERMANENTFLAGS\":\n                    out[key] = tuple(match.group(\"data\").split())\n\n        for key, value in untagged.items():\n            key = key.upper()\n            if key in (b\"OK\", b\"PERMANENTFLAGS\"):\n                continue  # already handled above\n            if key in (\n                b\"EXISTS\",\n                b\"RECENT\",\n                b\"UIDNEXT\",\n                b\"UIDVALIDITY\",\n                b\"HIGHESTMODSEQ\",\n            ):\n                value = int(value[0])\n            elif key == b\"READ-WRITE\":\n                value = True\n            elif key == b\"FLAGS\":\n                value = tuple(value[0][1:-1].split())\n            out[key] = value\n        return out\n\n    def noop(self):\n        \"\"\"Execute the NOOP command.\n\n        This command returns immediately, returning any server side\n        status updates. It can also be used to reset any auto-logout\n        timers.\n\n        The return value is the server command response message\n        followed by a list of status responses. For example::\n\n            (b'NOOP completed.',\n             [(4, b'EXISTS'),\n              (3, b'FETCH', (b'FLAGS', (b'bar', b'sne'))),\n              (6, b'FETCH', (b'FLAGS', (b'sne',)))])\n\n        \"\"\"\n        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")\n\n    @require_capability(\"IDLE\")\n    def idle(self):\n        \"\"\"Put the server into IDLE mode.\n\n        In this mode the server will return unsolicited responses\n        about changes to the selected mailbox. This method returns\n        immediately. Use ``idle_check()`` to look for IDLE responses\n        and ``idle_done()`` to stop IDLE mode.\n\n        .. note::\n\n            Any other commands issued while the server is in IDLE\n            mode will fail.\n\n        See :rfc:`2177` for more information about the IDLE extension.\n        \"\"\"\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n    def _poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is more scalable because it ALLOWS your process\n        to have more than 1024 file descriptors.\n        \"\"\"\n        poller = select.poll()\n        poller.register(sock.fileno(), select.POLLIN)\n        timeout = timeout * 1000 if timeout is not None else None\n        return poller.poll(timeout)\n\n    def _select_poll_socket(self, sock, timeout=None):\n        \"\"\"\n        Polls the socket for events telling us it's available to read.\n        This implementation is a fallback because it FAILS if your process\n        has more than 1024 file descriptors.\n        We still need this for Windows and some other niche systems.\n        \"\"\"\n        return select.select([sock], [], [], timeout)[0]\n\n    @require_capability(\"IDLE\")\n    def idle_check(self, timeout=None):\n        \"\"\"Check for any IDLE responses sent by the server.\n\n        This method should only be called if the server is in IDLE\n        mode (see ``idle()``).\n\n        By default, this method will block until an IDLE response is\n        received. If *timeout* is provided, the call will block for at\n        most this many seconds while waiting for an IDLE response.\n\n        The return value is a list of received IDLE responses. These\n        will be parsed with values converted to appropriate types. For\n        example::\n\n            [(b'OK', b'Still here'),\n             (1, b'EXISTS'),\n             (1, b'FETCH', (b'FLAGS', (b'\\\\NotJunk',)))]\n        \"\"\"\n        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n\n    @require_capability(\"IDLE\")\n    def idle_done(self):\n        \"\"\"Take the server out of IDLE mode.\n\n        This method should only be called if the server is already in\n        IDLE mode.\n\n        The return value is of the form ``(command_text,\n        idle_responses)`` where *command_text* is the text sent by the\n        server when the IDLE command finished (eg. ``b'Idle\n        terminated'``) and *idle_responses* is a list of parsed idle\n        responses received since the last call to ``idle_check()`` (if\n        any). These are returned in parsed form as per\n        ``idle_check()``.\n        \"\"\"\n        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n\n    def folder_status(self, folder, what=None):\n        \"\"\"Return the status of *folder*.\n\n        *what* should be a sequence of status items to query. This\n        defaults to ``('MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY',\n        'UNSEEN')``.\n\n        Returns a dictionary of the status items for the folder with\n        keys matching *what*.\n        \"\"\"\n        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))\n\n    def close_folder(self):\n        \"\"\"Close the currently selected folder, returning the server\n        response string.\n        \"\"\"\n        return self._command_and_check(\"close\", unpack=True)\n\n    def create_folder(self, folder):\n        \"\"\"Create *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"create\", self._normalise_folder(folder), unpack=True\n        )\n\n    def rename_folder(self, old_name, new_name):\n        \"\"\"Change the name of a folder on the server.\"\"\"\n        return self._command_and_check(\n            \"rename\",\n            self._normalise_folder(old_name),\n            self._normalise_folder(new_name),\n            unpack=True,\n        )\n\n    def delete_folder(self, folder):\n        \"\"\"Delete *folder* on the server returning the server response string.\"\"\"\n        return self._command_and_check(\n            \"delete\", self._normalise_folder(folder), unpack=True\n        )\n\n    def folder_exists(self, folder):\n        \"\"\"Return ``True`` if *folder* exists on the server.\"\"\"\n        return len(self.list_folders(\"\", folder)) > 0\n\n    def subscribe_folder(self, folder):\n        \"\"\"Subscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"subscribe\", self._normalise_folder(folder))\n\n    def unsubscribe_folder(self, folder):\n        \"\"\"Unsubscribe to *folder*, returning the server response string.\"\"\"\n        return self._command_and_check(\"unsubscribe\", self._normalise_folder(folder))\n\n    def search(self, criteria=\"ALL\", charset=None):\n        \"\"\"Return a list of messages ids from the currently selected\n        folder matching *criteria*.\n\n        *criteria* should be a sequence of one or more criteria\n        items. Each criteria item may be either unicode or\n        bytes. Example values::\n\n            [u'UNSEEN']\n            [u'SMALLER', 500]\n            [b'NOT', b'DELETED']\n            [u'TEXT', u'foo bar', u'FLAGGED', u'SUBJECT', u'baz']\n            [u'SINCE', date(2005, 4, 3)]\n\n        IMAPClient will perform conversion and quoting as\n        required. The caller shouldn't do this.\n\n        It is also possible (but not recommended) to pass the combined\n        criteria as a single string. In this case IMAPClient won't\n        perform quoting, allowing lower-level specification of\n        criteria. Examples of this style::\n\n            u'UNSEEN'\n            u'SMALLER 500'\n            b'NOT DELETED'\n            u'TEXT \"foo bar\" FLAGGED SUBJECT \"baz\"'\n            b'SINCE 03-Apr-2005'\n\n        To support complex search expressions, criteria lists can be\n        nested. IMAPClient will insert parentheses in the right\n        places. The following will match messages that are both not\n        flagged and do not have \"foo\" in the subject::\n\n            ['NOT', ['SUBJECT', 'foo', 'FLAGGED']]\n\n        *charset* specifies the character set of the criteria. It\n        defaults to US-ASCII as this is the only charset that a server\n        is required to support by the RFC. UTF-8 is commonly supported\n        however.\n\n        Any criteria specified using unicode will be encoded as per\n        *charset*. Specifying a unicode criteria that can not be\n        encoded using *charset* will result in an error.\n\n        Any criteria specified using bytes will be sent as-is but\n        should use an encoding that matches *charset* (the character\n        set given is still passed on to the server).\n\n        See :rfc:`3501#section-6.4.4` for more details.\n\n        Note that criteria arguments that are 8-bit will be\n        transparently sent by IMAPClient as IMAP literals to ensure\n        adherence to IMAP standards.\n\n        The returned list of message ids will have a special *modseq*\n        attribute. This is set if the server included a MODSEQ value\n        to the search response (i.e. if a MODSEQ criteria was included\n        in the search).\n\n        \"\"\"\n        return self._search(criteria, charset)\n\n    @require_capability(\"X-GM-EXT-1\")\n    def gmail_search(self, query, charset=\"UTF-8\"):\n        \"\"\"Search using Gmail's X-GM-RAW attribute.\n\n        *query* should be a valid Gmail search query string. For\n        example: ``has:attachment in:unread``. The search string may\n        be unicode and will be encoded using the specified *charset*\n        (defaulting to UTF-8).\n\n        This method only works for IMAP servers that support X-GM-RAW,\n        which is only likely to be Gmail.\n\n        See https://developers.google.com/gmail/imap_extensions#extension_of_the_search_command_x-gm-raw\n        for more info.\n        \"\"\"\n        return self._search([b\"X-GM-RAW\", query], charset)\n\n    def _search(self, criteria, charset):\n        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n\n    @require_capability(\"SORT\")\n    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):\n        \"\"\"Return a list of message ids from the currently selected\n        folder, sorted by *sort_criteria* and optionally filtered by\n        *criteria*.\n\n        *sort_criteria* may be specified as a sequence of strings or a\n        single string. IMAPClient will take care any required\n        conversions. Valid *sort_criteria* values::\n\n            ['ARRIVAL']\n            ['SUBJECT', 'ARRIVAL']\n            'ARRIVAL'\n            'REVERSE SIZE'\n\n        The *criteria* and *charset* arguments are as per\n        :py:meth:`.search`.\n\n        See :rfc:`5256` for full details.\n\n        Note that SORT is an extension to the IMAP4 standard so it may\n        not be supported by all IMAP servers.\n        \"\"\"\n        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]\n\n###The function: thread###\n    def get_flags(self, messages):\n        \"\"\"Return the flags set for each message in *messages* from\n        the currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (flag1, flag2, ... ), }``.\n        \"\"\"\n        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")\n\n    def add_flags(self, messages, flags, silent=False):\n        \"\"\"Add *flags* to *messages* in the currently selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"+FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def remove_flags(self, messages, flags, silent=False):\n        \"\"\"Remove one or more *flags* from *messages* in the currently\n        selected folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"-FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def set_flags(self, messages, flags, silent=False):\n        \"\"\"Set the *flags* for *messages* in the currently selected\n        folder.\n\n        *flags* should be a sequence of strings.\n\n        Returns the flags set for each modified message (see\n        *get_flags*), or None if *silent* is true.\n        \"\"\"\n        return self._store(b\"FLAGS\", messages, flags, b\"FLAGS\", silent=silent)\n\n    def get_gmail_labels(self, messages):\n        \"\"\"Return the label set for each message in *messages* in the\n        currently selected folder.\n\n        The return value is a dictionary structured like this: ``{\n        msgid1: (label1, label2, ... ), }``.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n\n    def add_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Add *labels* to *messages* in the currently selected folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"+X-GM-LABELS\", messages, labels, silent=silent)\n\n    def remove_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Remove one or more *labels* from *messages* in the\n        currently selected folder, or None if *silent* is true.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*).\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"-X-GM-LABELS\", messages, labels, silent=silent)\n\n    def set_gmail_labels(self, messages, labels, silent=False):\n        \"\"\"Set the *labels* for *messages* in the currently selected\n        folder.\n\n        *labels* should be a sequence of strings.\n\n        Returns the label set for each modified message (see\n        *get_gmail_labels*), or None if *silent* is true.\n\n        This only works with IMAP servers that support the X-GM-LABELS\n        attribute (eg. Gmail).\n        \"\"\"\n        return self._gm_label_store(b\"X-GM-LABELS\", messages, labels, silent=silent)\n\n    def delete_messages(self, messages, silent=False):\n        \"\"\"Delete one or more *messages* from the currently selected\n        folder.\n\n        Returns the flags set for each modified message (see\n        *get_flags*).\n        \"\"\"\n        return self.add_flags(messages, DELETED, silent=silent)\n\n    def fetch(self, messages, data, modifiers=None):\n        \"\"\"Retrieve selected *data* associated with one or more\n        *messages* in the currently selected folder.\n\n        *data* should be specified as a sequence of strings, one item\n        per data selector, for example ``['INTERNALDATE',\n        'RFC822']``.\n\n        *modifiers* are required for some extensions to the IMAP\n        protocol (eg. :rfc:`4551`). These should be a sequence of strings\n        if specified, for example ``['CHANGEDSINCE 123']``.\n\n        A dictionary is returned, indexed by message number. Each item\n        in this dictionary is also a dictionary, with an entry\n        corresponding to each item in *data*. Returned values will be\n        appropriately typed. For example, integer values will be returned as\n        Python integers, timestamps will be returned as datetime\n        instances and ENVELOPE responses will be returned as\n        :py:class:`Envelope <imapclient.response_types.Envelope>` instances.\n\n        String data will generally be returned as bytes (Python 3) or\n        str (Python 2).\n\n        In addition to an element for each *data* item, the dict\n        returned for each message also contains a *SEQ* key containing\n        the sequence number for the message. This allows for mapping\n        between the UID and sequence number (when the *use_uid*\n        property is ``True``).\n\n        Example::\n\n            >> c.fetch([3293, 3230], ['INTERNALDATE', 'FLAGS'])\n            {3230: {b'FLAGS': (b'\\\\Seen',),\n                    b'INTERNALDATE': datetime.datetime(2011, 1, 30, 13, 32, 9),\n                    b'SEQ': 84},\n             3293: {b'FLAGS': (),\n                    b'INTERNALDATE': datetime.datetime(2011, 2, 24, 19, 30, 36),\n                    b'SEQ': 110}}\n\n        \"\"\"\n        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            seq_to_parenstr_upper(data),\n            seq_to_parenstr_upper(modifiers) if modifiers else None,\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)\n\n    def append(self, folder, msg, flags=(), msg_time=None):\n        \"\"\"Append a message to *folder*.\n\n        *msg* should be a string contains the full message including\n        headers.\n\n        *flags* should be a sequence of message flags to set. If not\n        specified no flags will be set.\n\n        *msg_time* is an optional datetime instance specifying the\n        date and time to set on the message. The server will set a\n        time if it isn't specified. If *msg_time* contains timezone\n        information (tzinfo), this will be honoured. Otherwise the\n        local machine's time zone sent to the server.\n\n        Returns the APPEND response as returned by the server.\n        \"\"\"\n        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )\n\n    @require_capability(\"MULTIAPPEND\")\n    def multiappend(self, folder, msgs):\n        \"\"\"Append messages to *folder* using the MULTIAPPEND feature from :rfc:`3502`.\n\n        *msgs* must be an iterable. Each item must be either a string containing the\n        full message including headers, or a dict containing the keys \"msg\" with the\n        full message as before, \"flags\" with a sequence of message flags to set, and\n        \"date\" with a datetime instance specifying the internal date to set.\n        The keys \"flags\" and \"date\" are optional.\n\n        Returns the APPEND response from the server.\n        \"\"\"\n\n        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )\n\n    def copy(self, messages, folder):\n        \"\"\"Copy one or more messages from the current folder to\n        *folder*. Returns the COPY response string returned by the\n        server.\n        \"\"\"\n        return self._command_and_check(\n            \"copy\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    @require_capability(\"MOVE\")\n    def move(self, messages, folder):\n        \"\"\"Atomically move messages to another folder.\n\n        Requires the MOVE capability, see :rfc:`6851`.\n\n        :param messages: List of message UIDs to move.\n        :param folder: The destination folder name.\n        \"\"\"\n        return self._command_and_check(\n            \"move\",\n            join_message_ids(messages),\n            self._normalise_folder(folder),\n            uid=True,\n            unpack=True,\n        )\n\n    def expunge(self, messages=None):\n        \"\"\"Use of the *messages* argument is discouraged.\n        Please see the ``uid_expunge`` method instead.\n\n        When, no *messages* are specified, remove all messages\n        from the currently selected folder that have the\n        ``\\\\Deleted`` flag set.\n\n        The return value is the server response message\n        followed by a list of expunge responses. For example::\n\n            ('Expunge completed.',\n             [(2, 'EXPUNGE'),\n              (1, 'EXPUNGE'),\n              (0, 'RECENT')])\n\n        In this case, the responses indicate that the message with\n        sequence numbers 2 and 1 where deleted, leaving no recent\n        messages in the folder.\n\n        See :rfc:`3501#section-6.4.3` section 6.4.3 and\n        :rfc:`3501#section-7.4.1` section 7.4.1 for more details.\n\n        When *messages* are specified, remove the specified messages\n        from the selected folder, provided those messages also have\n        the ``\\\\Deleted`` flag set. The return value is ``None`` in\n        this case.\n\n        Expunging messages by id(s) requires that *use_uid* is\n        ``True`` for the client.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        if messages:\n            if not self.use_uid:\n                raise ValueError(\"cannot EXPUNGE by ID when not using uids\")\n            return self._command_and_check(\n                \"EXPUNGE\", join_message_ids(messages), uid=True\n            )\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")\n\n    @require_capability(\"UIDPLUS\")\n    def uid_expunge(self, messages):\n        \"\"\"Expunge deleted messages with the specified message ids from the\n        folder.\n\n        This requires the UIDPLUS capability.\n\n        See :rfc:`4315#section-2.1` section 2.1 for more details.\n        \"\"\"\n        return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=True)\n\n    @require_capability(\"ACL\")\n    def getacl(self, folder):\n        \"\"\"Returns a list of ``(who, acl)`` tuples describing the\n        access controls for *folder*.\n        \"\"\"\n        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]\n\n    @require_capability(\"ACL\")\n    def setacl(self, folder, who, what):\n        \"\"\"Set an ACL (*what*) for user (*who*) for a folder.\n\n        Set *what* to an empty string to remove an ACL. Returns the\n        server response string.\n        \"\"\"\n        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )\n\n    @require_capability(\"QUOTA\")\n    def get_quota(self, mailbox=\"INBOX\"):\n        \"\"\"Get the quotas associated with a mailbox.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return self.get_quota_root(mailbox)[1]\n\n    @require_capability(\"QUOTA\")\n    def _get_quota(self, quota_root=\"\"):\n        \"\"\"Get the quotas associated with a quota root.\n\n        This method is not private but put behind an underscore to show that\n        it is a low-level function. Users probably want to use `get_quota`\n        instead.\n\n        Returns a list of Quota objects.\n        \"\"\"\n        return _parse_quota(self._command_and_check(\"getquota\", _quote(quota_root)))\n\n    @require_capability(\"QUOTA\")\n    def get_quota_root(self, mailbox):\n        \"\"\"Get the quota roots for a mailbox.\n\n        The IMAP server responds with the quota root and the quotas associated\n        so there is usually no need to call `get_quota` after.\n\n        See :rfc:`2087` for more details.\n\n        Return a tuple of MailboxQuotaRoots and list of Quota associated\n        \"\"\"\n        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)\n\n    @require_capability(\"QUOTA\")\n    def set_quota(self, quotas):\n        \"\"\"Set one or more quotas on resources.\n\n        :param quotas: list of Quota objects\n        \"\"\"\n        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)\n\n    def _check_resp(self, expected, command, typ, data):\n        \"\"\"Check command responses for errors.\n\n        Raises IMAPClient.Error if the command fails.\n        \"\"\"\n        if typ != expected:\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(data[0]))\n            )\n\n    def _consume_until_tagged_response(self, tag, command):\n        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps\n\n    def _raw_command_untagged(\n        self, command, args, response_name=None, unpack=False, uid=True\n    ):\n        # TODO: eventually this should replace _command_and_check (call it _command)\n        typ, data = self._raw_command(command, args, uid=uid)\n        if response_name is None:\n            response_name = command\n        typ, data = self._imap._untagged_response(typ, data, to_unicode(response_name))\n        self._checkok(to_unicode(command), typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _raw_command(self, command, args, uid=True):\n        \"\"\"Run the specific command with the arguments given. 8-bit arguments\n        are sent as literals. The return value is (typ, data).\n\n        This sidesteps much of imaplib's command sending\n        infrastructure because imaplib can't send more than one\n        literal.\n\n        *command* should be specified as bytes.\n        *args* should be specified as a list of bytes.\n        \"\"\"\n        command = command.upper()\n\n        if isinstance(args, tuple):\n            args = list(args)\n        if not isinstance(args, list):\n            args = [args]\n\n        tag = self._imap._new_tag()\n        prefix = [to_bytes(tag)]\n        if uid and self.use_uid:\n            prefix.append(b\"UID\")\n        prefix.append(command)\n\n        line = []\n        for item, is_last in _iter_with_last(prefix + args):\n            if not isinstance(item, bytes):\n                raise ValueError(\"command args must be passed as bytes\")\n\n            if _is8bit(item):\n                # If a line was already started send it\n                if line:\n                    out = b\" \".join(line)\n                    logger.debug(\"> %s\", out)\n                    self._imap.send(out)\n                    line = []\n\n                # Now send the (unquoted) literal\n                if isinstance(item, _quoted):\n                    item = item.original\n                self._send_literal(tag, item)\n                if not is_last:\n                    self._imap.send(b\" \")\n            else:\n                line.append(item)\n\n        if line:\n            out = b\" \".join(line)\n            logger.debug(\"> %s\", out)\n            self._imap.send(out)\n\n        self._imap.send(b\"\\r\\n\")\n\n        return self._imap._command_complete(to_unicode(command), tag)\n\n    def _send_literal(self, tag, item):\n        \"\"\"Send a single literal for the command with *tag*.\"\"\"\n        if b\"LITERAL+\" in self._cached_capabilities:\n            out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"+}\\r\\n\" + item\n            logger.debug(\"> %s\", debug_trunc(out, 64))\n            self._imap.send(out)\n            return\n\n        out = b\" {\" + str(len(item)).encode(\"ascii\") + b\"}\\r\\n\"\n        logger.debug(\"> %s\", out)\n        self._imap.send(out)\n\n        # Wait for continuation response\n        while self._imap._get_response():\n            tagged_resp = self._imap.tagged_commands.get(tag)\n            if tagged_resp:\n                raise exceptions.IMAPClientAbortError(\n                    \"unexpected response while waiting for continuation response: \"\n                    + repr(tagged_resp)\n                )\n\n        logger.debug(\"   (literal) > %s\", debug_trunc(item, 256))\n        self._imap.send(item)\n\n    def _command_and_check(\n        self, command, *args, unpack: bool = False, uid: bool = False\n    ):\n        if uid and self.use_uid:\n            command = to_unicode(command)  # imaplib must die\n            typ, data = self._imap.uid(command, *args)\n        else:\n            meth = getattr(self._imap, to_unicode(command))\n            typ, data = meth(*args)\n        self._checkok(command, typ, data)\n        if unpack:\n            return data[0]\n        return data\n\n    def _checkok(self, command, typ, data):\n        self._check_resp(\"OK\", command, typ, data)\n\n    def _gm_label_store(self, cmd, messages, labels, silent):\n        response = self._store(\n            cmd, messages, self._normalise_labels(labels), b\"X-GM-LABELS\", silent=silent\n        )\n        return (\n            {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}\n            if response\n            else None\n        )\n\n    def _store(self, cmd, messages, flags, fetch_key, silent):\n        \"\"\"Worker function for the various flag manipulation methods.\n\n        *cmd* is the STORE command to use (eg. '+FLAGS').\n        \"\"\"\n        if not messages:\n            return {}\n        if silent:\n            cmd += b\".SILENT\"\n\n        data = self._command_and_check(\n            \"store\", join_message_ids(messages), cmd, seq_to_parenstr(flags), uid=True\n        )\n        if silent:\n            return None\n        return self._filter_fetch_dict(parse_fetch_response(data), fetch_key)\n\n    def _filter_fetch_dict(self, fetch_dict, key):\n        return dict((msgid, data[key]) for msgid, data in fetch_dict.items())\n\n    def _normalise_folder(self, folder_name):\n        if isinstance(folder_name, bytes):\n            folder_name = folder_name.decode(\"ascii\")\n        if self.folder_encode:\n            folder_name = encode_utf7(folder_name)\n        return _quote(folder_name)\n\n    def _normalise_labels(self, labels):\n        if isinstance(labels, (str, bytes)):\n            labels = (labels,)\n        return [_quote(encode_utf7(label)) for label in labels]\n\n    @property\n    def welcome(self):\n        \"\"\"access the server greeting message\"\"\"\n        try:\n            return self._imap.welcome\n        except AttributeError:\n            pass\n\n\ndef _quote(arg):\n    if isinstance(arg, str):\n        arg = arg.replace(\"\\\\\", \"\\\\\\\\\")\n        arg = arg.replace('\"', '\\\\\"')\n        q = '\"'\n    else:\n        arg = arg.replace(b\"\\\\\", b\"\\\\\\\\\")\n        arg = arg.replace(b'\"', b'\\\\\"')\n        q = b'\"'\n    return q + arg + q\n\n\ndef _normalise_search_criteria(criteria, charset=None):\n    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out\n\n\ndef _normalise_sort_criteria(criteria, charset=None):\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    return b\"(\" + b\" \".join(to_bytes(item).upper() for item in criteria) + b\")\"\n\n\nclass _literal(bytes):\n    \"\"\"Hold message data that should always be sent as a literal.\"\"\"\n\n\nclass _quoted(bytes):\n    \"\"\"\n    This class holds a quoted bytes value which provides access to the\n    unquoted value via the *original* attribute.\n\n    They should be created via the *maybe* classmethod.\n    \"\"\"\n\n    @classmethod\n    def maybe(cls, original):\n        \"\"\"Maybe quote a bytes value.\n\n        If the input requires no quoting it is returned unchanged.\n\n        If quoting is required a *_quoted* instance is returned. This\n        holds the quoted version of the input while also providing\n        access to the original unquoted source.\n        \"\"\"\n        quoted = original.replace(b\"\\\\\", b\"\\\\\\\\\")\n        quoted = quoted.replace(b'\"', b'\\\\\"')\n        if quoted != original or b\" \" in quoted or not quoted:\n            out = cls(b'\"' + quoted + b'\"')\n            out.original = original\n            return out\n        return original\n\n\n# normalise_text_list, seq_to_parentstr etc have to return unicode\n# because imaplib handles flags and sort criteria assuming these are\n# passed as unicode\ndef normalise_text_list(items):\n    return list(_normalise_text_list(items))\n\n\ndef seq_to_parenstr(items):\n    return _join_and_paren(_normalise_text_list(items))\n\n\ndef seq_to_parenstr_upper(items):\n    return _join_and_paren(item.upper() for item in _normalise_text_list(items))\n\n\ndef _join_and_paren(items):\n    return \"(\" + \" \".join(items) + \")\"\n\n\ndef _normalise_text_list(items):\n    if isinstance(items, (str, bytes)):\n        items = (items,)\n    return (to_unicode(c) for c in items)\n\n\ndef join_message_ids(messages):\n    \"\"\"Convert a sequence of messages ids or a single integer message id\n    into an id byte string for use with IMAP commands\n    \"\"\"\n    if isinstance(messages, (str, bytes, int)):\n        messages = (to_bytes(messages),)\n    return b\",\".join(_maybe_int_to_bytes(m) for m in messages)\n\n\ndef _maybe_int_to_bytes(val):\n    if isinstance(val, int):\n        return str(val).encode(\"us-ascii\")\n    return to_bytes(val)\n\n\ndef _parse_untagged_response(text):\n    assert_imap_protocol(text.startswith(b\"* \"))\n    text = text[2:]\n    if text.startswith((b\"OK \", b\"NO \")):\n        return tuple(text.split(b\" \", 1))\n    return parse_response([text])\n\n\ndef as_pairs(items):\n    i = 0\n    last_item = None\n    for item in items:\n        if i % 2:\n            yield last_item, item\n        else:\n            last_item = item\n        i += 1\n\n\ndef as_triplets(items):\n    a = iter(items)\n    return zip(a, a, a)\n\n\ndef _is8bit(data):\n    return isinstance(data, _literal) or any(b > 127 for b in data)\n\n\ndef _iter_with_last(items):\n    last_i = len(items) - 1\n    for i, item in enumerate(items):\n        yield item, i == last_i\n\n\n_not_present = object()\n\n\nclass _dict_bytes_normaliser:\n    \"\"\"Wrap a dict with unicode/bytes keys and normalise the keys to\n    bytes.\n    \"\"\"\n\n    def __init__(self, d):\n        self._d = d\n\n    def iteritems(self):\n        for key, value in self._d.items():\n            yield to_bytes(key), value\n\n    # For Python 3 compatibility.\n    items = iteritems\n\n    def __contains__(self, ink):\n        for k in self._gen_keys(ink):\n            if k in self._d:\n                return True\n        return False\n\n    def get(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d[k]\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def pop(self, ink, default=_not_present):\n        for k in self._gen_keys(ink):\n            try:\n                return self._d.pop(k)\n            except KeyError:\n                pass\n        if default == _not_present:\n            raise KeyError(ink)\n        return default\n\n    def _gen_keys(self, k):\n        yield k\n        if isinstance(k, bytes):\n            yield to_unicode(k)\n        else:\n            yield to_bytes(k)\n\n\ndef debug_trunc(v, maxlen):\n    if len(v) < maxlen:\n        return repr(v)\n    hl = maxlen // 2\n    return repr(v[:hl]) + \"...\" + repr(v[-hl:])\n\n\ndef utf7_decode_sequence(seq):\n    return [decode_utf7(s) for s in seq]\n\n\ndef _parse_quota(quota_rep):\n    quota_rep = parse_response(quota_rep)\n    rv = []\n    for quota_root, quota_resource_infos in as_pairs(quota_rep):\n        for quota_resource_info in as_triplets(quota_resource_infos):\n            rv.append(\n                Quota(\n                    quota_root=to_unicode(quota_root),\n                    resource=to_unicode(quota_resource_info[0]),\n                    usage=quota_resource_info[1],\n                    limit=quota_resource_info[2],\n                )\n            )\n    return rv\n\n\nclass IMAPlibLoggerAdapter(LoggerAdapter):\n    \"\"\"Adapter preventing IMAP secrets from going to the logging facility.\"\"\"\n\n    def process(self, msg, kwargs):\n        # msg is usually unicode but see #367. Convert bytes to\n        # unicode if required.\n        if isinstance(msg, bytes):\n            msg = msg.decode(\"ascii\", \"ignore\")\n\n        for command in (\"LOGIN\", \"AUTHENTICATE\"):\n            if msg.startswith(\">\") and command in msg:\n                msg_start = msg.split(command)[0]\n                msg = \"{}{} **REDACTED**\".format(msg_start, command)\n                break\n        return super().process(msg, kwargs)\n\n        ####cross_file_context:\n        [{'imapclient.exceptions': 'import imaplib\\n\\n# Base class allowing to catch any IMAPClient related exceptions\\n# To ensure backward compatibility, we \"rename\" the imaplib general\\n# exception class, so we can catch its exceptions without having to\\n# deal with it in IMAPClient codebase\\n\\nIMAPClientError = imaplib.IMAP4.error\\nIMAPClientAbortError = imaplib.IMAP4.abort\\nIMAPClientReadOnlyError = imaplib.IMAP4.readonly\\n\\n\\nclass CapabilityError(IMAPClientError):\\n    \"\"\"\\n    The command tried by the user needs a capability not installed\\n    on the IMAP server\\n    \"\"\"\\n\\n\\nclass LoginError(IMAPClientError):\\n    \"\"\"\\n    A connection has been established with the server but an error\\n    occurred during the authentication.\\n    \"\"\"\\n\\n\\nclass IllegalStateError(IMAPClientError):\\n    \"\"\"\\n    The command tried needs a different state to be executed. This\\n    means the user is not logged in or the command needs a folder to\\n    be selected.\\n    \"\"\"\\n\\n\\nclass InvalidCriteriaError(IMAPClientError):\\n    \"\"\"\\n    A command using a search criteria failed, probably due to a syntax\\n    error in the criteria string.\\n    \"\"\"\\n\\n\\nclass ProtocolError(IMAPClientError):\\n    \"\"\"The server replied with a response that violates the IMAP protocol.\"\"\"\\n'}, {'imapclient.exceptions.CapabilityError': 'import imaplib\\n\\n# Base class allowing to catch any IMAPClient related exceptions\\n# To ensure backward compatibility, we \"rename\" the imaplib general\\n# exception class, so we can catch its exceptions without having to\\n# deal with it in IMAPClient codebase\\n\\nIMAPClientError = imaplib.IMAP4.error\\nIMAPClientAbortError = imaplib.IMAP4.abort\\nIMAPClientReadOnlyError = imaplib.IMAP4.readonly\\n\\n\\nclass CapabilityError(IMAPClientError):\\n    \"\"\"\\n    The command tried by the user needs a capability not installed\\n    on the IMAP server\\n    \"\"\"\\n\\n\\nclass LoginError(IMAPClientError):\\n    \"\"\"\\n    A connection has been established with the server but an error\\n    occurred during the authentication.\\n    \"\"\"\\n\\n\\nclass IllegalStateError(IMAPClientError):\\n    \"\"\"\\n    The command tried needs a different state to be executed. This\\n    means the user is not logged in or the command needs a folder to\\n    be selected.\\n    \"\"\"\\n\\n\\nclass InvalidCriteriaError(IMAPClientError):\\n    \"\"\"\\n    A command using a search criteria failed, probably due to a syntax\\n    error in the criteria string.\\n    \"\"\"\\n\\n\\nclass ProtocolError(IMAPClientError):\\n    \"\"\"The server replied with a response that violates the IMAP protocol.\"\"\"\\n'}, {'imapclient.response_parser.parse_response': '# Copyright (c) 2014, Menno Smits\\n# Released subject to the New BSD License\\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\\n\\n\"\"\"\\nParsing for IMAP command responses with focus on FETCH responses as\\nreturned by imaplib.\\n\\nInitially inspired by http://effbot.org/zone/simple-iterator-parser.htm\\n\"\"\"\\n\\n# TODO more exact error reporting\\n\\nimport datetime\\nimport re\\nimport sys\\nfrom collections import defaultdict\\nfrom typing import cast, Dict, Iterator, List, Optional, Tuple, TYPE_CHECKING, Union\\n\\nfrom .datetime_util import parse_to_datetime\\nfrom .exceptions import ProtocolError\\nfrom .response_lexer import TokenSource\\nfrom .response_types import Address, BodyData, Envelope, SearchIds\\nfrom .typing_imapclient import _Atom\\n\\n__all__ = [\"parse_response\", \"parse_message_list\"]\\n\\n\\ndef parse_response(data: List[bytes]) -> Tuple[_Atom, ...]:\\n    \"\"\"Pull apart IMAP command responses.\\n\\n    Returns nested tuples of appropriately typed objects.\\n    \"\"\"\\n    if data == [None]:\\n        return tuple()\\n    return tuple(gen_parsed_response(data))\\n\\n\\n_msg_id_pattern = re.compile(r\"(\\\\d+(?: +\\\\d+)*)\")\\n\\n\\ndef parse_message_list(data: List[Union[bytes, str]]) -> SearchIds:\\n    \"\"\"Parse a list of message ids and return them as a list.\\n\\n    parse_response is also capable of doing this but this is\\n    faster. This also has special handling of the optional MODSEQ part\\n    of a SEARCH response.\\n\\n    The returned list is a SearchIds instance which has a *modseq*\\n    attribute which contains the MODSEQ response (if returned by the\\n    server).\\n    \"\"\"\\n    if len(data) != 1:\\n        raise ValueError(\"unexpected message list data\")\\n\\n    message_data = data[0]\\n    if not message_data:\\n        return SearchIds()\\n\\n    if isinstance(message_data, bytes):\\n        message_data = message_data.decode(\"ascii\")\\n\\n    m = _msg_id_pattern.match(message_data)\\n    if not m:\\n        raise ValueError(\"unexpected message list format\")\\n\\n    ids = SearchIds(int(n) for n in m.group(1).split())\\n\\n    # Parse any non-numeric part on the end using parse_response (this\\n    # is likely to be the MODSEQ section).\\n    extra = message_data[m.end(1) :]\\n    if extra:\\n        for item in parse_response([extra.encode(\"ascii\")]):\\n            if (\\n                isinstance(item, tuple)\\n                and len(item) == 2\\n                and cast(bytes, item[0]).lower() == b\"modseq\"\\n            ):\\n                if TYPE_CHECKING:\\n                    assert isinstance(item[1], int)\\n                ids.modseq = item[1]\\n            elif isinstance(item, int):\\n                ids.append(item)\\n    return ids\\n\\n\\ndef gen_parsed_response(text: List[bytes]) -> Iterator[_Atom]:\\n    if not text:\\n        return\\n    src = TokenSource(text)\\n\\n    token = None\\n    try:\\n        for token in src:\\n            yield atom(src, token)\\n    except ProtocolError:\\n        raise\\n    except ValueError:\\n        _, err, _ = sys.exc_info()\\n        raise ProtocolError(\"%s: %r\" % (str(err), token))\\n\\n\\n_ParseFetchResponseInnerDict = Dict[\\n    bytes, Optional[Union[datetime.datetime, int, BodyData, Envelope, _Atom]]\\n]\\n\\n\\ndef parse_fetch_response(\\n    text: List[bytes], normalise_times: bool = True, uid_is_key: bool = True\\n) -> \"defaultdict[int, _ParseFetchResponseInnerDict]\":\\n    \"\"\"Pull apart IMAP FETCH responses as returned by imaplib.\\n\\n    Returns a dictionary, keyed by message ID. Each value a dictionary\\n    keyed by FETCH field type (eg.\"RFC822\").\\n    \"\"\"\\n    if text == [None]:\\n        return defaultdict()\\n    response = gen_parsed_response(text)\\n\\n    parsed_response: \"defaultdict[int, _ParseFetchResponseInnerDict]\" = defaultdict(\\n        dict\\n    )\\n    while True:\\n        try:\\n            msg_id = seq = _int_or_error(next(response), \"invalid message ID\")\\n        except StopIteration:\\n            break\\n\\n        try:\\n            msg_response = next(response)\\n        except StopIteration:\\n            raise ProtocolError(\"unexpected EOF\")\\n\\n        if not isinstance(msg_response, tuple):\\n            raise ProtocolError(\"bad response type: %s\" % repr(msg_response))\\n        if len(msg_response) % 2:\\n            raise ProtocolError(\\n                \"uneven number of response items: %s\" % repr(msg_response)\\n            )\\n\\n        # always return the sequence of the message, so it is available\\n        # even if we return keyed by UID.\\n        msg_data: _ParseFetchResponseInnerDict = {b\"SEQ\": seq}\\n        for i in range(0, len(msg_response), 2):\\n            msg_attribute = msg_response[i]\\n            if TYPE_CHECKING:\\n                assert isinstance(msg_attribute, bytes)\\n            word = msg_attribute.upper()\\n            value = msg_response[i + 1]\\n\\n            if word == b\"UID\":\\n                uid = _int_or_error(value, \"invalid UID\")\\n                if uid_is_key:\\n                    msg_id = uid\\n                else:\\n                    msg_data[word] = uid\\n            elif word == b\"INTERNALDATE\":\\n                msg_data[word] = _convert_INTERNALDATE(value, normalise_times)\\n            elif word == b\"ENVELOPE\":\\n                msg_data[word] = _convert_ENVELOPE(value, normalise_times)\\n            elif word in (b\"BODY\", b\"BODYSTRUCTURE\"):\\n                if TYPE_CHECKING:\\n                    assert isinstance(value, tuple)\\n                msg_data[word] = BodyData.create(value)\\n            else:\\n                msg_data[word] = value\\n\\n        parsed_response[msg_id].update(msg_data)\\n\\n    return parsed_response\\n\\n\\ndef _int_or_error(value: _Atom, error_text: str) -> int:\\n    try:\\n        return int(value)  # type: ignore[arg-type]\\n    except (TypeError, ValueError):\\n        raise ProtocolError(\"%s: %s\" % (error_text, repr(value)))\\n\\n\\ndef _convert_INTERNALDATE(\\n    date_string: _Atom, normalise_times: bool = True\\n) -> Optional[datetime.datetime]:\\n    if date_string is None:\\n        return None\\n\\n    try:\\n        if TYPE_CHECKING:\\n            assert isinstance(date_string, bytes)\\n        return parse_to_datetime(date_string, normalise=normalise_times)\\n    except ValueError:\\n        return None\\n\\n\\ndef _convert_ENVELOPE(\\n    envelope_response: _Atom, normalise_times: bool = True\\n) -> Envelope:\\n    if TYPE_CHECKING:\\n        assert isinstance(envelope_response, tuple)\\n    dt = None\\n    if envelope_response[0]:\\n        try:\\n            if TYPE_CHECKING:\\n                assert isinstance(envelope_response[0], bytes)\\n            dt = parse_to_datetime(\\n                envelope_response[0],\\n                normalise=normalise_times,\\n            )\\n        except ValueError:\\n            pass\\n\\n    subject = envelope_response[1]\\n    in_reply_to = envelope_response[8]\\n    message_id = envelope_response[9]\\n    if TYPE_CHECKING:\\n        assert isinstance(subject, bytes)\\n        assert isinstance(in_reply_to, bytes)\\n        assert isinstance(message_id, bytes)\\n\\n    # addresses contains a tuple of addresses\\n    # from, sender, reply_to, to, cc, bcc headers\\n    addresses: List[Optional[Tuple[Address, ...]]] = []\\n    for addr_list in envelope_response[2:8]:\\n        addrs = []\\n        if addr_list:\\n            if TYPE_CHECKING:\\n                assert isinstance(addr_list, tuple)\\n            for addr_tuple in addr_list:\\n                if TYPE_CHECKING:\\n                    assert isinstance(addr_tuple, tuple)\\n                if addr_tuple:\\n                    if TYPE_CHECKING:\\n                        addr_tuple = cast(Tuple[bytes, bytes, bytes, bytes], addr_tuple)\\n                    addrs.append(Address(*addr_tuple))\\n            addresses.append(tuple(addrs))\\n        else:\\n            addresses.append(None)\\n\\n    return Envelope(\\n        date=dt,\\n        subject=subject,\\n        from_=addresses[0],\\n        sender=addresses[1],\\n        reply_to=addresses[2],\\n        to=addresses[3],\\n        cc=addresses[4],\\n        bcc=addresses[5],\\n        in_reply_to=in_reply_to,\\n        message_id=message_id,\\n    )\\n\\n\\ndef atom(src: TokenSource, token: bytes) -> _Atom:\\n    if token == b\"(\":\\n        return parse_tuple(src)\\n    if token == b\"NIL\":\\n        return None\\n    if token[:1] == b\"{\":\\n        literal_len = int(token[1:-1])\\n        literal_text = src.current_literal\\n        if literal_text is None:\\n            raise ProtocolError(\"No literal corresponds to %r\" % token)\\n        if len(literal_text) != literal_len:\\n            raise ProtocolError(\\n                \"Expecting literal of size %d, got %d\"\\n                % (literal_len, len(literal_text))\\n            )\\n        return literal_text\\n    if len(token) >= 2 and (token[:1] == token[-1:] == b\\'\"\\'):\\n        return token[1:-1]\\n    if token.isdigit() and (token[:1] != b\"0\" or len(token) == 1):\\n        # this prevents converting items like 0123 to 123\\n        return int(token)\\n    return token\\n\\n\\ndef parse_tuple(src: TokenSource) -> _Atom:\\n    out: List[_Atom] = []\\n    for token in src:\\n        if token == b\")\":\\n            return tuple(out)\\n        out.append(atom(src, token))\\n    # no terminator\\n    raise ProtocolError(\\'Tuple incomplete before \"(%s\"\\' % _fmt_tuple(out))\\n\\n\\ndef _fmt_tuple(t: List[_Atom]) -> str:\\n    return \" \".join(str(item) for item in t)\\n'}, {'imapclient.util.to_bytes': '# Copyright (c) 2015, Menno Smits\\n# Released subject to the New BSD License\\n# Please see http://en.wikipedia.org/wiki/BSD_licenses\\n\\nimport logging\\nfrom typing import Iterator, Optional, Tuple, Union\\n\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\ndef to_unicode(s: Union[bytes, str]) -> str:\\n    if isinstance(s, bytes):\\n        try:\\n            return s.decode(\"ascii\")\\n        except UnicodeDecodeError:\\n            logger.warning(\\n                \"An error occurred while decoding %s in ASCII \\'strict\\' mode. Fallback to \"\\n                \"\\'ignore\\' errors handling, some characters might have been stripped\",\\n                s,\\n            )\\n            return s.decode(\"ascii\", \"ignore\")\\n    return s\\n\\n\\ndef to_bytes(s: Union[bytes, str], charset: str = \"ascii\") -> bytes:\\n    if isinstance(s, str):\\n        return s.encode(charset)\\n    return s\\n\\n\\ndef assert_imap_protocol(condition: bool, message: Optional[bytes] = None) -> None:\\n    from . import exceptions\\n    if not condition:\\n        msg = \"Server replied with a response that violates the IMAP protocol\"\\n        if message:\\n            # FIXME(jlvillal): This looks wrong as it repeats `msg` twice\\n            msg += \"{}: {}\".format(\\n                msg, message.decode(encoding=\"ascii\", errors=\"ignore\")\\n            )\\n        raise exceptions.ProtocolError(msg)\\n\\n\\n_TupleAtomPart = Union[None, int, bytes]\\n_TupleAtom = Tuple[Union[_TupleAtomPart, \"_TupleAtom\"], ...]\\n\\n\\ndef chunk(lst: _TupleAtom, size: int) -> Iterator[_TupleAtom]:\\n    for i in range(0, len(lst), size):\\n        yield lst[i : i + size]\\n'}]", "test_list": ["def test_unsupported_algorithm(self):\n    self.client._cached_capabilities = (b'THREAD=FOO',)\n    self.assertRaises(CapabilityError, self.client.thread)", "def test_no_thread_support(self):\n    self.client._cached_capabilities = (b'NOT-THREAD',)\n    self.assertRaises(CapabilityError, self.client.thread)", "def test_defaults(self):\n    threads = self.client.thread()\n    self.check_call([b'REFERENCES', b'UTF-8', b'ALL'])\n    self.assertSequenceEqual(threads, ((1, 2), (3,), (4, 5, 6)))", "def test_all_args(self):\n    self.client._cached_capabilities = (b'THREAD=COTTON',)\n    self.client.thread('COTTON', ['TEXT', '\u261e'], 'UTF-7')\n    self.check_call([b'COTTON', b'UTF-7', b'TEXT', b'+Jh4-'])"], "requirements": {"Input-Output Conditions": {"requirement": "The 'thread' function should accept only valid threading algorithms and character sets. If the input does not meet the requirements, report ValueError. It should return a tuple of message threads, where each is a message IDs.", "unit_test": ["def test_invalid_algorithm(self):\n    with self.assertRaises(ValueError):\n        self.client.thread('INVALID_ALGO')\n\ndef test_invalid_charset(self):\n    with self.assertRaises(ValueError):\n        self.client.thread(charset='INVALID_CHARSET')"], "test": "tests/test_thread.py::TestThread::test_invalid_algorithm"}, "Exception Handling": {"requirement": "The 'thread' function should raise a CapabilityError if the server does not support the specified threading algorithm.", "unit_test": ["def test_thread_capability_error(self):\n    self.client._cached_capabilities = (b'NOT-THREAD',)\n    with self.assertRaises(CapabilityError):\n        self.client.thread('REFERENCES')"], "test": "tests/test_thread.py::TestThread::test_thread_capability_error"}, "Edge Case Handling": {"requirement": "The 'thread' function should handle the edge case where no messages match the criteria by returning an empty tuple.", "unit_test": ["def test_no_matching_messages(self):\n    self.client._raw_command_untagged = lambda *args, **kwargs: []\n    threads = self.client.thread()\n    self.assertEqual(threads, [])"], "test": "tests/test_thread.py::TestThread::test_no_matching_messages"}, "Functionality Extension": {"requirement": "Extend the 'thread' function to support an optional 'max_results' parameter that limits the number of threads returned.", "unit_test": ["def test_thread_max_results(self):\n    self.client._raw_command_untagged = lambda *args, **kwargs: [(1, 2), (3,), (4, 5, 6)]\n    threads = self.client.thread(max_results=2)\n    self.assertEqual(threads, [(1, 2), (3,)])"], "test": "tests/test_thread.py::TestThread::test_thread_max_results"}, "Annotation Coverage": {"requirement": "Ensure that the 'thread' function has complete type annotations for all parameters and return types.", "unit_test": ["def test_thread_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(self.client.thread)\n    self.assertEqual(hints['algorithm'], str)\n    self.assertEqual(hints['criteria'], str)\n    self.assertEqual(hints['charset'], str)\n    self.assertEqual(hints['return'], tuple)"], "test": "tests/test_thread.py::TestThread::test_thread_annotations"}, "Code Complexity": {"requirement": "The 'thread' function should maintain a cyclomatic complexity of 10 or less to ensure maintainability.", "unit_test": ["def test_thread_cyclomatic_complexity(self):\n    import radon.complexity as rc\n    complexity = rc.cc_visit(self.client.thread.__code__)\n    self.assertLessEqual(complexity[0].complexity, 10)"], "test": "tests/test_thread.py::TestThread::test_thread_cyclomatic_complexity"}, "Code Standard": {"requirement": "The 'thread' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_check_code_style(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['imapclient/imapclient.py'])\n    self.assertEqual(result.total_errors, 0)"], "test": "tests/test_thread.py::TestThread::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'thread' function should utilize the necessary methods such as '_raw_command_untagged', 'has_capability', etc. from the contexts.", "unit_test": ["def test_thread_context_usage(self):\n    self.client._raw_command_untagged = lambda *args, **kwargs: [(1, 2), (3,)]\n    self.client.has_capability = lambda x: True\n    threads = self.client.thread()\n    self.assertEqual(threads, [(1, 2), (3,)])"], "test": "tests/test_thread.py::TestThread::test_thread_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'thread' function should correctly utilize the necessary methods from the contexts.", "unit_test": ["def test_thread_correct_capability_check(self):\n    self.client.has_capability = lambda x: x == 'THREAD=REFERENCES'\n    self.client._raw_command_untagged = lambda *args, **kwargs: [(1, 2), (3,)]\n    threads = self.client.thread('REFERENCES')\n    self.assertEqual(threads, [(1, 2), (3,)])"], "test": "tests/test_thread.py::TestThread::test_thread_correct_capability_check"}}}
{"namespace": "bplustree.memory.FileMemory.get_node", "type": "method", "project_path": "Database/bplustree", "completion_path": "Database/bplustree/bplustree/memory.py", "signature_position": [125, 125], "body_position": [137, 147], "dependency": {"intra_class": ["bplustree.memory.FileMemory._cache", "bplustree.memory.FileMemory._read_page", "bplustree.memory.FileMemory._tree_conf", "bplustree.memory.FileMemory._wal"], "intra_file": ["bplustree.memory.WAL.get_page"], "cross_file": ["bplustree.node.Node", "bplustree.node.Node.from_page_data", "bplustree.node.Node.page"]}, "requirement": {"Functionality": "This function retrieves a node from storage. It first checks if the node is present in the cache. If not, it retrieves the data from the storage and creates a Node object using the data. The created node is then added to the cache for future use.", "Arguments": ":param self: FileMemory. An instance of the FileMemory class.\n:param page: int. The page number of the node to retrieve.\n:return: Node. The retrieved node."}, "tests": ["tests/test_memory.py::test_file_memory_write_transaction", "tests/test_memory.py::test_file_memory_node"], "indent": 8, "domain": "Database", "code": "    def get_node(self, page: int):\n        \"\"\"Get a node from storage.\n\n        The cache is not there to prevent hitting the disk, the OS is already\n        very good at it. It is there to avoid paying the price of deserializing\n        the data to create the Node object and its entry. This is a very\n        expensive operation in Python.\n\n        Since we have at most a single writer we can write to cache on\n        `set_node` if we invalidate the cache when a transaction is rolled\n        back.\n        \"\"\"\n        node = self._cache.get(page)\n        if node is not None:\n            return node\n\n        data = self._wal.get_page(page)\n        if not data:\n            data = self._read_page(page)\n\n        node = Node.from_page_data(self._tree_conf, data=data, page=page)\n        self._cache[node.page] = node\n        return node\n", "intra_context": "import enum\nimport io\nfrom logging import getLogger\nimport os\nimport platform\nfrom typing import Union, Tuple, Optional\n\nimport cachetools\nimport rwlock\n\nfrom .node import Node\nfrom .const import (\n    ENDIAN, PAGE_REFERENCE_BYTES, OTHERS_BYTES, TreeConf, FRAME_TYPE_BYTES\n)\n\nlogger = getLogger(__name__)\n\n\nclass ReachedEndOfFile(Exception):\n    \"\"\"Read a file until its end.\"\"\"\n\n\ndef open_file_in_dir(path: str) -> Tuple[io.FileIO, Optional[int]]:\n    \"\"\"Open a file and its directory.\n\n    The file is opened in binary mode and created if it does not exist.\n    Both file descriptors must be closed after use to prevent them from\n    leaking.\n\n    On Windows, the directory is not opened, as it is useless.\n    \"\"\"\n    directory = os.path.dirname(path)\n    if not os.path.isdir(directory):\n        raise ValueError('No directory {}'.format(directory))\n\n    if not os.path.exists(path):\n        file_fd = open(path, mode='x+b', buffering=0)\n    else:\n        file_fd = open(path, mode='r+b', buffering=0)\n\n    if platform.system() == 'Windows':\n        # Opening a directory is not possible on Windows, but that is not\n        # a problem since Windows does not need to fsync the directory in\n        # order to persist metadata\n        dir_fd = None\n    else:\n        dir_fd = os.open(directory, os.O_RDONLY)\n\n    return file_fd, dir_fd\n\n\ndef write_to_file(file_fd: io.FileIO, dir_fileno: Optional[int],\n                  data: bytes, fsync: bool=True):\n    length_to_write = len(data)\n    written = 0\n    while written < length_to_write:\n        written = file_fd.write(data[written:])\n    if fsync:\n        fsync_file_and_dir(file_fd.fileno(), dir_fileno)\n\n\ndef fsync_file_and_dir(file_fileno: int, dir_fileno: Optional[int]):\n    os.fsync(file_fileno)\n    if dir_fileno is not None:\n        os.fsync(dir_fileno)\n\n\ndef read_from_file(file_fd: io.FileIO, start: int, stop: int) -> bytes:\n    length = stop - start\n    assert length >= 0\n    file_fd.seek(start)\n    data = bytes()\n    while file_fd.tell() < stop:\n        read_data = file_fd.read(stop - file_fd.tell())\n        if read_data == b'':\n            raise ReachedEndOfFile('Read until the end of file')\n        data += read_data\n    assert len(data) == length\n    return data\n\n\nclass FakeCache:\n    \"\"\"A cache that doesn't cache anything.\n\n    Because cachetools does not work with maxsize=0.\n    \"\"\"\n\n    def get(self, k):\n        pass\n\n    def __setitem__(self, key, value):\n        pass\n\n    def clear(self):\n        pass\n\n\nclass FileMemory:\n\n    __slots__ = ['_filename', '_tree_conf', '_lock', '_cache', '_fd',\n                 '_dir_fd', '_wal', 'last_page']\n\n    def __init__(self, filename: str, tree_conf: TreeConf,\n                 cache_size: int=512):\n        self._filename = filename\n        self._tree_conf = tree_conf\n        self._lock = rwlock.RWLock()\n\n        if cache_size == 0:\n            self._cache = FakeCache()\n        else:\n            self._cache = cachetools.LRUCache(maxsize=cache_size)\n\n        self._fd, self._dir_fd = open_file_in_dir(filename)\n\n        self._wal = WAL(filename, tree_conf.page_size)\n        if self._wal.needs_recovery:\n            self.perform_checkpoint(reopen_wal=True)\n\n        # Get the next available page\n        self._fd.seek(0, io.SEEK_END)\n        last_byte = self._fd.tell()\n        self.last_page = int(last_byte / self._tree_conf.page_size)\n\n###The function: get_node###\n    def set_node(self, node: Node):\n        self._wal.set_page(node.page, node.dump())\n        self._cache[node.page] = node\n\n    def set_page(self, page: int, data: bytes):\n        \"\"\"Set a raw page of data.\n\n        Used currently only for overflow pages.\n        \"\"\"\n        self._wal.set_page(page, data)\n\n    def get_page(self, page: int) -> bytes:\n        data = self._wal.get_page(page)\n        if not data:\n            data = self._read_page(page)\n        return data\n\n    @property\n    def read_transaction(self):\n\n        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()\n\n    @property\n    def write_transaction(self):\n\n        class WriteTransaction:\n\n            def __enter__(self2):\n                self._lock.writer_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                if exc_type:\n                    # When an error happens in the middle of a write\n                    # transaction we must roll it back and clear the cache\n                    # because the writer may have partially modified the Nodes\n                    self._wal.rollback()\n                    self._cache.clear()\n                else:\n                    self._wal.commit()\n                self._lock.writer_lock.release()\n\n        return WriteTransaction()\n\n    @property\n    def next_available_page(self) -> int:\n        self.last_page += 1\n        return self.last_page\n\n    def get_metadata(self) -> tuple:\n        try:\n            data = self._read_page(0)\n        except ReachedEndOfFile:\n            raise ValueError('Metadata not set yet')\n        end_root_node_page = PAGE_REFERENCE_BYTES\n        root_node_page = int.from_bytes(\n            data[0:end_root_node_page], ENDIAN\n        )\n        end_page_size = end_root_node_page + OTHERS_BYTES\n        page_size = int.from_bytes(\n            data[end_root_node_page:end_page_size], ENDIAN\n        )\n        end_order = end_page_size + OTHERS_BYTES\n        order = int.from_bytes(\n            data[end_page_size:end_order], ENDIAN\n        )\n        end_key_size = end_order + OTHERS_BYTES\n        key_size = int.from_bytes(\n            data[end_order:end_key_size], ENDIAN\n        )\n        end_value_size = end_key_size + OTHERS_BYTES\n        value_size = int.from_bytes(\n            data[end_key_size:end_value_size], ENDIAN\n        )\n        self._tree_conf = TreeConf(\n            page_size, order, key_size, value_size, self._tree_conf.serializer\n        )\n        return root_node_page, self._tree_conf\n\n    def set_metadata(self, root_node_page: int, tree_conf: TreeConf):\n        self._tree_conf = tree_conf\n        length = PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES\n        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            self._tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            bytes(self._tree_conf.page_size - length)\n        )\n        self._write_page_in_tree(0, data, fsync=True)\n\n    def close(self):\n        self.perform_checkpoint()\n        self._fd.close()\n        if self._dir_fd is not None:\n            os.close(self._dir_fd)\n\n    def perform_checkpoint(self, reopen_wal=False):\n        logger.info('Performing checkpoint of %s', self._filename)\n        for page, page_data in self._wal.checkpoint():\n            self._write_page_in_tree(page, page_data, fsync=False)\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n        if reopen_wal:\n            self._wal = WAL(self._filename, self._tree_conf.page_size)\n\n    def _read_page(self, page: int) -> bytes:\n        start = page * self._tree_conf.page_size\n        stop = start + self._tree_conf.page_size\n        assert stop - start == self._tree_conf.page_size\n        return read_from_file(self._fd, start, stop)\n\n    def _write_page_in_tree(self, page: int, data: Union[bytes, bytearray],\n                            fsync: bool=True):\n        \"\"\"Write a page of data in the tree file itself.\n\n        To be used during checkpoints and other non-standard uses.\n        \"\"\"\n        assert len(data) == self._tree_conf.page_size\n        self._fd.seek(page * self._tree_conf.page_size)\n        write_to_file(self._fd, self._dir_fd, data, fsync=fsync)\n\n    def __repr__(self):\n        return '<FileMemory: {}>'.format(self._filename)\n\n\nclass FrameType(enum.Enum):\n    PAGE = 1\n    COMMIT = 2\n    ROLLBACK = 3\n\n\nclass WAL:\n\n    __slots__ = ['filename', '_fd', '_dir_fd', '_page_size',\n                 '_committed_pages', '_not_committed_pages', 'needs_recovery']\n\n    FRAME_HEADER_LENGTH = (\n        FRAME_TYPE_BYTES + PAGE_REFERENCE_BYTES\n    )\n\n    def __init__(self, filename: str, page_size: int):\n        self.filename = filename + '-wal'\n        self._fd, self._dir_fd = open_file_in_dir(self.filename)\n        self._page_size = page_size\n        self._committed_pages = dict()\n        self._not_committed_pages = dict()\n\n        self._fd.seek(0, io.SEEK_END)\n        if self._fd.tell() == 0:\n            self._create_header()\n            self.needs_recovery = False\n        else:\n            logger.warning('Found an existing WAL file, '\n                           'the B+Tree was not closed properly')\n            self.needs_recovery = True\n            self._load_wal()\n\n    def checkpoint(self):\n        \"\"\"Transfer the modified data back to the tree and close the WAL.\"\"\"\n        if self._not_committed_pages:\n            logger.warning('Closing WAL with uncommitted data, discarding it')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_start in self._committed_pages.items():\n            page_data = read_from_file(\n                self._fd,\n                page_start,\n                page_start + self._page_size\n            )\n            yield page, page_data\n\n        self._fd.close()\n        os.unlink(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)\n            os.close(self._dir_fd)\n\n    def _create_header(self):\n        data = self._page_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        self._fd.seek(0)\n        write_to_file(self._fd, self._dir_fd, data, True)\n\n    def _load_wal(self):\n        self._fd.seek(0)\n        header_data = read_from_file(self._fd, 0, OTHERS_BYTES)\n        assert int.from_bytes(header_data, ENDIAN) == self._page_size\n\n        while True:\n            try:\n                self._load_next_frame()\n            except ReachedEndOfFile:\n                break\n        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted data, discarding it')\n            self._not_committed_pages = dict()\n\n    def _load_next_frame(self):\n        start = self._fd.tell()\n        stop = start + self.FRAME_HEADER_LENGTH\n        data = read_from_file(self._fd, start, stop)\n\n        frame_type = int.from_bytes(data[0:FRAME_TYPE_BYTES], ENDIAN)\n        page = int.from_bytes(\n            data[FRAME_TYPE_BYTES:FRAME_TYPE_BYTES+PAGE_REFERENCE_BYTES],\n            ENDIAN\n        )\n\n        frame_type = FrameType(frame_type)\n        if frame_type is FrameType.PAGE:\n            self._fd.seek(stop + self._page_size)\n\n        self._index_frame(frame_type, page, stop)\n\n    def _index_frame(self, frame_type: FrameType, page: int, page_start: int):\n        if frame_type is FrameType.PAGE:\n            self._not_committed_pages[page] = page_start\n        elif frame_type is FrameType.COMMIT:\n            self._committed_pages.update(self._not_committed_pages)\n            self._not_committed_pages = dict()\n        elif frame_type is FrameType.ROLLBACK:\n            self._not_committed_pages = dict()\n        else:\n            assert False\n\n    def _add_frame(self, frame_type: FrameType, page: Optional[int]=None,\n                   page_data: Optional[bytes]=None):\n        if frame_type is FrameType.PAGE and (not page or not page_data):\n            raise ValueError('PAGE frame without page data')\n        if page_data and len(page_data) != self._page_size:\n            raise ValueError('Page data is different from page size')\n        if not page:\n            page = 0\n        if frame_type is not FrameType.PAGE:\n            page_data = b''\n        data = (\n            frame_type.value.to_bytes(FRAME_TYPE_BYTES, ENDIAN) +\n            page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            page_data\n        )\n        self._fd.seek(0, io.SEEK_END)\n        write_to_file(self._fd, self._dir_fd, data,\n                      fsync=frame_type != FrameType.PAGE)\n        self._index_frame(frame_type, page, self._fd.tell() - self._page_size)\n\n    def get_page(self, page: int) -> Optional[bytes]:\n        page_start = None\n        for store in (self._not_committed_pages, self._committed_pages):\n            page_start = store.get(page)\n            if page_start:\n                break\n\n        if not page_start:\n            return None\n\n        return read_from_file(self._fd, page_start,\n                              page_start + self._page_size)\n\n    def set_page(self, page: int, page_data: bytes):\n        self._add_frame(FrameType.PAGE, page, page_data)\n\n    def commit(self):\n        # Commit is a no-op when there is no uncommitted pages\n        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n\n    def rollback(self):\n        # Rollback is a no-op when there is no uncommitted pages\n        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n\n    def __repr__(self):\n        return '<WAL: {}>'.format(self.filename)\n", "cross_context": [{"bplustree.node.Node": "import abc\nimport bisect\nimport math\nfrom typing import Optional\n\nfrom .const import (ENDIAN, NODE_TYPE_BYTES, USED_PAGE_LENGTH_BYTES,\n                    PAGE_REFERENCE_BYTES, TreeConf)\nfrom .entry import Entry, Record, Reference\n\n\nclass Node(metaclass=abc.ABCMeta):\n\n    __slots__ = ['_tree_conf', 'entries', 'page', 'parent', 'next_page']\n\n    # Attributes to redefine in inherited classes\n    _node_type_int = 0\n    max_children = 0\n    min_children = 0\n    _entry_class = None\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._tree_conf = tree_conf\n        self.entries = list()\n        self.page = page\n        self.parent = parent\n        self.next_page = next_page\n        if data:\n            self.load(data)\n\n    def load(self, data: bytes):\n        assert len(data) == self._tree_conf.page_size\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\n        used_page_length = int.from_bytes(\n            data[NODE_TYPE_BYTES:end_used_page_length], ENDIAN\n        )\n        end_header = end_used_page_length + PAGE_REFERENCE_BYTES\n        self.next_page = int.from_bytes(\n            data[end_used_page_length:end_header], ENDIAN\n        )\n        if self.next_page == 0:\n            self.next_page = None\n\n        entry_length = self._entry_class(self._tree_conf).length\n        for start_offset in range(end_header, used_page_length, entry_length):\n            entry_data = data[start_offset:start_offset+entry_length]\n            entry = self._entry_class(self._tree_conf, data=entry_data)\n            self.entries.append(entry)\n\n    def dump(self) -> bytearray:\n        data = bytearray()\n        for record in self.entries:\n            data.extend(record.dump())\n\n        used_page_length = len(data) + 4\n        assert 0 <= used_page_length < self._tree_conf.page_size\n        next_page = 0 if self.next_page is None else self.next_page\n        header = (\n            self._node_type_int.to_bytes(1, ENDIAN) +\n            used_page_length.to_bytes(3, ENDIAN) +\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n\n        data = bytearray(header) + data\n\n        padding = self._tree_conf.page_size - len(data)\n        assert padding >= 0\n        data.extend(bytearray(padding))\n        assert len(data) == self._tree_conf.page_size\n\n        return data\n\n    @property\n    def can_add_entry(self) -> bool:\n        return self.num_children < self.max_children\n\n    @property\n    def can_delete_entry(self) -> bool:\n        return self.num_children > self.min_children\n\n    @property\n    def smallest_key(self):\n        return self.smallest_entry.key\n\n    @property\n    def smallest_entry(self):\n        return self.entries[0]\n\n    @property\n    def biggest_key(self):\n        return self.biggest_entry.key\n\n    @property\n    def biggest_entry(self):\n        return self.entries[-1]\n\n    @property\n    @abc.abstractmethod\n    def num_children(self) -> int:\n        \"\"\"Number of entries or other nodes connected to the node.\"\"\"\n\n    def pop_smallest(self) -> Entry:\n        \"\"\"Remove and return the smallest entry.\"\"\"\n        return self.entries.pop(0)\n\n    def insert_entry(self, entry: Entry):\n        bisect.insort(self.entries, entry)\n\n    def insert_entry_at_the_end(self, entry: Entry):\n        \"\"\"Insert an entry at the end of the entry list.\n\n        This is an optimized version of `insert_entry` when it is known that\n        the key to insert is bigger than any other entries.\n        \"\"\"\n        self.entries.append(entry)\n\n    def remove_entry(self, key):\n        self.entries.pop(self._find_entry_index(key))\n\n    def get_entry(self, key) -> Entry:\n        return self.entries[self._find_entry_index(key)]\n\n    def _find_entry_index(self, key) -> int:\n        entry = self._entry_class(\n            self._tree_conf,\n            key=key  # Hack to compare and order\n        )\n        i = bisect.bisect_left(self.entries, entry)\n        if i != len(self.entries) and self.entries[i] == entry:\n            return i\n        raise ValueError('No entry for key {}'.format(key))\n\n    def split_entries(self) -> list:\n        \"\"\"Split the entries in half.\n\n        Keep the lower part in the node and return the upper one.\n        \"\"\"\n        len_entries = len(self.entries)\n        rv = self.entries[len_entries//2:]\n        self.entries = self.entries[:len_entries//2]\n        assert len(self.entries) + len(rv) == len_entries\n        return rv\n\n    @classmethod\n    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\n                       page: int=None) -> 'Node':\n        node_type_byte = data[0:NODE_TYPE_BYTES]\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\n        if node_type_int == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type_int == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type_int == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type_int == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            assert False, 'No Node with type {} exists'.format(node_type_int)\n\n    def __repr__(self):\n        return '<{}: page={} entries={}>'.format(\n            self.__class__.__name__, self.page, len(self.entries)\n        )\n\n    def __eq__(self, other):\n        return (\n            self.__class__ is other.__class__ and\n            self.page == other.page and\n            self.entries == other.entries\n        )\n\n\nclass RecordNode(Node):\n\n    __slots__ = ['_entry_class']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._entry_class = Record\n        super().__init__(tree_conf, data, page, parent, next_page)\n\n    @property\n    def num_children(self) -> int:\n        return len(self.entries)\n\n\nclass LonelyRootNode(RecordNode):\n    \"\"\"A Root node that holds records.\n\n    It is an exception for when there is only a single node in the tree.\n    \"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 1\n        self.min_children = 0\n        self.max_children = tree_conf.order - 1\n        super().__init__(tree_conf, data, page, parent)\n\n    def convert_to_leaf(self):\n        leaf = LeafNode(self._tree_conf, page=self.page)\n        leaf.entries = self.entries\n        return leaf\n\n\nclass LeafNode(RecordNode):\n    \"\"\"Node that holds the actual records within the tree.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._node_type_int = 4\n        self.min_children = math.ceil(tree_conf.order / 2) - 1\n        self.max_children = tree_conf.order - 1\n        super().__init__(tree_conf, data, page, parent, next_page)\n\n\nclass ReferenceNode(Node):\n\n    __slots__ = ['_entry_class']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._entry_class = Reference\n        super().__init__(tree_conf, data, page, parent)\n\n    @property\n    def num_children(self) -> int:\n        return len(self.entries) + 1 if self.entries else 0\n\n    def insert_entry(self, entry: 'Reference'):\n        \"\"\"Make sure that after of a reference matches before of the next one.\n\n        Probably very inefficient approach.\n        \"\"\"\n        super().insert_entry(entry)\n        i = self.entries.index(entry)\n        if i > 0:\n            previous_entry = self.entries[i-1]\n            previous_entry.after = entry.before\n        try:\n            next_entry = self.entries[i+1]\n        except IndexError:\n            pass\n        else:\n            next_entry.before = entry.after\n\n\nclass RootNode(ReferenceNode):\n    \"\"\"The first node at the top of the tree.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 2\n        self.min_children = 2\n        self.max_children = tree_conf.order\n        super().__init__(tree_conf, data, page, parent)\n\n    def convert_to_internal(self):\n        internal = InternalNode(self._tree_conf, page=self.page)\n        internal.entries = self.entries\n        return internal\n\n\nclass InternalNode(ReferenceNode):\n    \"\"\"Node that only holds references to other Internal nodes or Leaves.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 3\n        self.min_children = math.ceil(tree_conf.order / 2)\n        self.max_children = tree_conf.order\n        super().__init__(tree_conf, data, page, parent)\n"}, {"bplustree.node.Node.from_page_data": "import abc\nimport bisect\nimport math\nfrom typing import Optional\n\nfrom .const import (ENDIAN, NODE_TYPE_BYTES, USED_PAGE_LENGTH_BYTES,\n                    PAGE_REFERENCE_BYTES, TreeConf)\nfrom .entry import Entry, Record, Reference\n\n\nclass Node(metaclass=abc.ABCMeta):\n\n    __slots__ = ['_tree_conf', 'entries', 'page', 'parent', 'next_page']\n\n    # Attributes to redefine in inherited classes\n    _node_type_int = 0\n    max_children = 0\n    min_children = 0\n    _entry_class = None\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._tree_conf = tree_conf\n        self.entries = list()\n        self.page = page\n        self.parent = parent\n        self.next_page = next_page\n        if data:\n            self.load(data)\n\n    def load(self, data: bytes):\n        assert len(data) == self._tree_conf.page_size\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\n        used_page_length = int.from_bytes(\n            data[NODE_TYPE_BYTES:end_used_page_length], ENDIAN\n        )\n        end_header = end_used_page_length + PAGE_REFERENCE_BYTES\n        self.next_page = int.from_bytes(\n            data[end_used_page_length:end_header], ENDIAN\n        )\n        if self.next_page == 0:\n            self.next_page = None\n\n        entry_length = self._entry_class(self._tree_conf).length\n        for start_offset in range(end_header, used_page_length, entry_length):\n            entry_data = data[start_offset:start_offset+entry_length]\n            entry = self._entry_class(self._tree_conf, data=entry_data)\n            self.entries.append(entry)\n\n    def dump(self) -> bytearray:\n        data = bytearray()\n        for record in self.entries:\n            data.extend(record.dump())\n\n        used_page_length = len(data) + 4\n        assert 0 <= used_page_length < self._tree_conf.page_size\n        next_page = 0 if self.next_page is None else self.next_page\n        header = (\n            self._node_type_int.to_bytes(1, ENDIAN) +\n            used_page_length.to_bytes(3, ENDIAN) +\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n\n        data = bytearray(header) + data\n\n        padding = self._tree_conf.page_size - len(data)\n        assert padding >= 0\n        data.extend(bytearray(padding))\n        assert len(data) == self._tree_conf.page_size\n\n        return data\n\n    @property\n    def can_add_entry(self) -> bool:\n        return self.num_children < self.max_children\n\n    @property\n    def can_delete_entry(self) -> bool:\n        return self.num_children > self.min_children\n\n    @property\n    def smallest_key(self):\n        return self.smallest_entry.key\n\n    @property\n    def smallest_entry(self):\n        return self.entries[0]\n\n    @property\n    def biggest_key(self):\n        return self.biggest_entry.key\n\n    @property\n    def biggest_entry(self):\n        return self.entries[-1]\n\n    @property\n    @abc.abstractmethod\n    def num_children(self) -> int:\n        \"\"\"Number of entries or other nodes connected to the node.\"\"\"\n\n    def pop_smallest(self) -> Entry:\n        \"\"\"Remove and return the smallest entry.\"\"\"\n        return self.entries.pop(0)\n\n    def insert_entry(self, entry: Entry):\n        bisect.insort(self.entries, entry)\n\n    def insert_entry_at_the_end(self, entry: Entry):\n        \"\"\"Insert an entry at the end of the entry list.\n\n        This is an optimized version of `insert_entry` when it is known that\n        the key to insert is bigger than any other entries.\n        \"\"\"\n        self.entries.append(entry)\n\n    def remove_entry(self, key):\n        self.entries.pop(self._find_entry_index(key))\n\n    def get_entry(self, key) -> Entry:\n        return self.entries[self._find_entry_index(key)]\n\n    def _find_entry_index(self, key) -> int:\n        entry = self._entry_class(\n            self._tree_conf,\n            key=key  # Hack to compare and order\n        )\n        i = bisect.bisect_left(self.entries, entry)\n        if i != len(self.entries) and self.entries[i] == entry:\n            return i\n        raise ValueError('No entry for key {}'.format(key))\n\n    def split_entries(self) -> list:\n        \"\"\"Split the entries in half.\n\n        Keep the lower part in the node and return the upper one.\n        \"\"\"\n        len_entries = len(self.entries)\n        rv = self.entries[len_entries//2:]\n        self.entries = self.entries[:len_entries//2]\n        assert len(self.entries) + len(rv) == len_entries\n        return rv\n\n    @classmethod\n    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\n                       page: int=None) -> 'Node':\n        node_type_byte = data[0:NODE_TYPE_BYTES]\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\n        if node_type_int == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type_int == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type_int == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type_int == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            assert False, 'No Node with type {} exists'.format(node_type_int)\n\n    def __repr__(self):\n        return '<{}: page={} entries={}>'.format(\n            self.__class__.__name__, self.page, len(self.entries)\n        )\n\n    def __eq__(self, other):\n        return (\n            self.__class__ is other.__class__ and\n            self.page == other.page and\n            self.entries == other.entries\n        )\n\n\nclass RecordNode(Node):\n\n    __slots__ = ['_entry_class']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._entry_class = Record\n        super().__init__(tree_conf, data, page, parent, next_page)\n\n    @property\n    def num_children(self) -> int:\n        return len(self.entries)\n\n\nclass LonelyRootNode(RecordNode):\n    \"\"\"A Root node that holds records.\n\n    It is an exception for when there is only a single node in the tree.\n    \"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 1\n        self.min_children = 0\n        self.max_children = tree_conf.order - 1\n        super().__init__(tree_conf, data, page, parent)\n\n    def convert_to_leaf(self):\n        leaf = LeafNode(self._tree_conf, page=self.page)\n        leaf.entries = self.entries\n        return leaf\n\n\nclass LeafNode(RecordNode):\n    \"\"\"Node that holds the actual records within the tree.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._node_type_int = 4\n        self.min_children = math.ceil(tree_conf.order / 2) - 1\n        self.max_children = tree_conf.order - 1\n        super().__init__(tree_conf, data, page, parent, next_page)\n\n\nclass ReferenceNode(Node):\n\n    __slots__ = ['_entry_class']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._entry_class = Reference\n        super().__init__(tree_conf, data, page, parent)\n\n    @property\n    def num_children(self) -> int:\n        return len(self.entries) + 1 if self.entries else 0\n\n    def insert_entry(self, entry: 'Reference'):\n        \"\"\"Make sure that after of a reference matches before of the next one.\n\n        Probably very inefficient approach.\n        \"\"\"\n        super().insert_entry(entry)\n        i = self.entries.index(entry)\n        if i > 0:\n            previous_entry = self.entries[i-1]\n            previous_entry.after = entry.before\n        try:\n            next_entry = self.entries[i+1]\n        except IndexError:\n            pass\n        else:\n            next_entry.before = entry.after\n\n\nclass RootNode(ReferenceNode):\n    \"\"\"The first node at the top of the tree.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 2\n        self.min_children = 2\n        self.max_children = tree_conf.order\n        super().__init__(tree_conf, data, page, parent)\n\n    def convert_to_internal(self):\n        internal = InternalNode(self._tree_conf, page=self.page)\n        internal.entries = self.entries\n        return internal\n\n\nclass InternalNode(ReferenceNode):\n    \"\"\"Node that only holds references to other Internal nodes or Leaves.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 3\n        self.min_children = math.ceil(tree_conf.order / 2)\n        self.max_children = tree_conf.order\n        super().__init__(tree_conf, data, page, parent)\n"}, {"bplustree.node.Node.page": "import abc\nimport bisect\nimport math\nfrom typing import Optional\n\nfrom .const import (ENDIAN, NODE_TYPE_BYTES, USED_PAGE_LENGTH_BYTES,\n                    PAGE_REFERENCE_BYTES, TreeConf)\nfrom .entry import Entry, Record, Reference\n\n\nclass Node(metaclass=abc.ABCMeta):\n\n    __slots__ = ['_tree_conf', 'entries', 'page', 'parent', 'next_page']\n\n    # Attributes to redefine in inherited classes\n    _node_type_int = 0\n    max_children = 0\n    min_children = 0\n    _entry_class = None\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._tree_conf = tree_conf\n        self.entries = list()\n        self.page = page\n        self.parent = parent\n        self.next_page = next_page\n        if data:\n            self.load(data)\n\n    def load(self, data: bytes):\n        assert len(data) == self._tree_conf.page_size\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\n        used_page_length = int.from_bytes(\n            data[NODE_TYPE_BYTES:end_used_page_length], ENDIAN\n        )\n        end_header = end_used_page_length + PAGE_REFERENCE_BYTES\n        self.next_page = int.from_bytes(\n            data[end_used_page_length:end_header], ENDIAN\n        )\n        if self.next_page == 0:\n            self.next_page = None\n\n        entry_length = self._entry_class(self._tree_conf).length\n        for start_offset in range(end_header, used_page_length, entry_length):\n            entry_data = data[start_offset:start_offset+entry_length]\n            entry = self._entry_class(self._tree_conf, data=entry_data)\n            self.entries.append(entry)\n\n    def dump(self) -> bytearray:\n        data = bytearray()\n        for record in self.entries:\n            data.extend(record.dump())\n\n        used_page_length = len(data) + 4\n        assert 0 <= used_page_length < self._tree_conf.page_size\n        next_page = 0 if self.next_page is None else self.next_page\n        header = (\n            self._node_type_int.to_bytes(1, ENDIAN) +\n            used_page_length.to_bytes(3, ENDIAN) +\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n\n        data = bytearray(header) + data\n\n        padding = self._tree_conf.page_size - len(data)\n        assert padding >= 0\n        data.extend(bytearray(padding))\n        assert len(data) == self._tree_conf.page_size\n\n        return data\n\n    @property\n    def can_add_entry(self) -> bool:\n        return self.num_children < self.max_children\n\n    @property\n    def can_delete_entry(self) -> bool:\n        return self.num_children > self.min_children\n\n    @property\n    def smallest_key(self):\n        return self.smallest_entry.key\n\n    @property\n    def smallest_entry(self):\n        return self.entries[0]\n\n    @property\n    def biggest_key(self):\n        return self.biggest_entry.key\n\n    @property\n    def biggest_entry(self):\n        return self.entries[-1]\n\n    @property\n    @abc.abstractmethod\n    def num_children(self) -> int:\n        \"\"\"Number of entries or other nodes connected to the node.\"\"\"\n\n    def pop_smallest(self) -> Entry:\n        \"\"\"Remove and return the smallest entry.\"\"\"\n        return self.entries.pop(0)\n\n    def insert_entry(self, entry: Entry):\n        bisect.insort(self.entries, entry)\n\n    def insert_entry_at_the_end(self, entry: Entry):\n        \"\"\"Insert an entry at the end of the entry list.\n\n        This is an optimized version of `insert_entry` when it is known that\n        the key to insert is bigger than any other entries.\n        \"\"\"\n        self.entries.append(entry)\n\n    def remove_entry(self, key):\n        self.entries.pop(self._find_entry_index(key))\n\n    def get_entry(self, key) -> Entry:\n        return self.entries[self._find_entry_index(key)]\n\n    def _find_entry_index(self, key) -> int:\n        entry = self._entry_class(\n            self._tree_conf,\n            key=key  # Hack to compare and order\n        )\n        i = bisect.bisect_left(self.entries, entry)\n        if i != len(self.entries) and self.entries[i] == entry:\n            return i\n        raise ValueError('No entry for key {}'.format(key))\n\n    def split_entries(self) -> list:\n        \"\"\"Split the entries in half.\n\n        Keep the lower part in the node and return the upper one.\n        \"\"\"\n        len_entries = len(self.entries)\n        rv = self.entries[len_entries//2:]\n        self.entries = self.entries[:len_entries//2]\n        assert len(self.entries) + len(rv) == len_entries\n        return rv\n\n    @classmethod\n    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\n                       page: int=None) -> 'Node':\n        node_type_byte = data[0:NODE_TYPE_BYTES]\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\n        if node_type_int == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type_int == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type_int == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type_int == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            assert False, 'No Node with type {} exists'.format(node_type_int)\n\n    def __repr__(self):\n        return '<{}: page={} entries={}>'.format(\n            self.__class__.__name__, self.page, len(self.entries)\n        )\n\n    def __eq__(self, other):\n        return (\n            self.__class__ is other.__class__ and\n            self.page == other.page and\n            self.entries == other.entries\n        )\n\n\nclass RecordNode(Node):\n\n    __slots__ = ['_entry_class']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._entry_class = Record\n        super().__init__(tree_conf, data, page, parent, next_page)\n\n    @property\n    def num_children(self) -> int:\n        return len(self.entries)\n\n\nclass LonelyRootNode(RecordNode):\n    \"\"\"A Root node that holds records.\n\n    It is an exception for when there is only a single node in the tree.\n    \"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 1\n        self.min_children = 0\n        self.max_children = tree_conf.order - 1\n        super().__init__(tree_conf, data, page, parent)\n\n    def convert_to_leaf(self):\n        leaf = LeafNode(self._tree_conf, page=self.page)\n        leaf.entries = self.entries\n        return leaf\n\n\nclass LeafNode(RecordNode):\n    \"\"\"Node that holds the actual records within the tree.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None, next_page: int=None):\n        self._node_type_int = 4\n        self.min_children = math.ceil(tree_conf.order / 2) - 1\n        self.max_children = tree_conf.order - 1\n        super().__init__(tree_conf, data, page, parent, next_page)\n\n\nclass ReferenceNode(Node):\n\n    __slots__ = ['_entry_class']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._entry_class = Reference\n        super().__init__(tree_conf, data, page, parent)\n\n    @property\n    def num_children(self) -> int:\n        return len(self.entries) + 1 if self.entries else 0\n\n    def insert_entry(self, entry: 'Reference'):\n        \"\"\"Make sure that after of a reference matches before of the next one.\n\n        Probably very inefficient approach.\n        \"\"\"\n        super().insert_entry(entry)\n        i = self.entries.index(entry)\n        if i > 0:\n            previous_entry = self.entries[i-1]\n            previous_entry.after = entry.before\n        try:\n            next_entry = self.entries[i+1]\n        except IndexError:\n            pass\n        else:\n            next_entry.before = entry.after\n\n\nclass RootNode(ReferenceNode):\n    \"\"\"The first node at the top of the tree.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 2\n        self.min_children = 2\n        self.max_children = tree_conf.order\n        super().__init__(tree_conf, data, page, parent)\n\n    def convert_to_internal(self):\n        internal = InternalNode(self._tree_conf, page=self.page)\n        internal.entries = self.entries\n        return internal\n\n\nclass InternalNode(ReferenceNode):\n    \"\"\"Node that only holds references to other Internal nodes or Leaves.\"\"\"\n\n    __slots__ = ['_node_type_int', 'min_children', 'max_children']\n\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\n                 page: int=None, parent: 'Node'=None):\n        self._node_type_int = 3\n        self.min_children = math.ceil(tree_conf.order / 2)\n        self.max_children = tree_conf.order\n        super().__init__(tree_conf, data, page, parent)\n"}], "prompt": "Please write a python function called 'get_node' base the context. This function retrieves a node from storage. It first checks if the node is present in the cache. If not, it retrieves the data from the storage and creates a Node object using the data. The created node is then added to the cache for future use.:param self: FileMemory. An instance of the FileMemory class.\n:param page: int. The page number of the node to retrieve.\n:return: Node. The retrieved node..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        import enum\nimport io\nfrom logging import getLogger\nimport os\nimport platform\nfrom typing import Union, Tuple, Optional\n\nimport cachetools\nimport rwlock\n\nfrom .node import Node\nfrom .const import (\n    ENDIAN, PAGE_REFERENCE_BYTES, OTHERS_BYTES, TreeConf, FRAME_TYPE_BYTES\n)\n\nlogger = getLogger(__name__)\n\n\nclass ReachedEndOfFile(Exception):\n    \"\"\"Read a file until its end.\"\"\"\n\n\ndef open_file_in_dir(path: str) -> Tuple[io.FileIO, Optional[int]]:\n    \"\"\"Open a file and its directory.\n\n    The file is opened in binary mode and created if it does not exist.\n    Both file descriptors must be closed after use to prevent them from\n    leaking.\n\n    On Windows, the directory is not opened, as it is useless.\n    \"\"\"\n    directory = os.path.dirname(path)\n    if not os.path.isdir(directory):\n        raise ValueError('No directory {}'.format(directory))\n\n    if not os.path.exists(path):\n        file_fd = open(path, mode='x+b', buffering=0)\n    else:\n        file_fd = open(path, mode='r+b', buffering=0)\n\n    if platform.system() == 'Windows':\n        # Opening a directory is not possible on Windows, but that is not\n        # a problem since Windows does not need to fsync the directory in\n        # order to persist metadata\n        dir_fd = None\n    else:\n        dir_fd = os.open(directory, os.O_RDONLY)\n\n    return file_fd, dir_fd\n\n\ndef write_to_file(file_fd: io.FileIO, dir_fileno: Optional[int],\n                  data: bytes, fsync: bool=True):\n    length_to_write = len(data)\n    written = 0\n    while written < length_to_write:\n        written = file_fd.write(data[written:])\n    if fsync:\n        fsync_file_and_dir(file_fd.fileno(), dir_fileno)\n\n\ndef fsync_file_and_dir(file_fileno: int, dir_fileno: Optional[int]):\n    os.fsync(file_fileno)\n    if dir_fileno is not None:\n        os.fsync(dir_fileno)\n\n\ndef read_from_file(file_fd: io.FileIO, start: int, stop: int) -> bytes:\n    length = stop - start\n    assert length >= 0\n    file_fd.seek(start)\n    data = bytes()\n    while file_fd.tell() < stop:\n        read_data = file_fd.read(stop - file_fd.tell())\n        if read_data == b'':\n            raise ReachedEndOfFile('Read until the end of file')\n        data += read_data\n    assert len(data) == length\n    return data\n\n\nclass FakeCache:\n    \"\"\"A cache that doesn't cache anything.\n\n    Because cachetools does not work with maxsize=0.\n    \"\"\"\n\n    def get(self, k):\n        pass\n\n    def __setitem__(self, key, value):\n        pass\n\n    def clear(self):\n        pass\n\n\nclass FileMemory:\n\n    __slots__ = ['_filename', '_tree_conf', '_lock', '_cache', '_fd',\n                 '_dir_fd', '_wal', 'last_page']\n\n    def __init__(self, filename: str, tree_conf: TreeConf,\n                 cache_size: int=512):\n        self._filename = filename\n        self._tree_conf = tree_conf\n        self._lock = rwlock.RWLock()\n\n        if cache_size == 0:\n            self._cache = FakeCache()\n        else:\n            self._cache = cachetools.LRUCache(maxsize=cache_size)\n\n        self._fd, self._dir_fd = open_file_in_dir(filename)\n\n        self._wal = WAL(filename, tree_conf.page_size)\n        if self._wal.needs_recovery:\n            self.perform_checkpoint(reopen_wal=True)\n\n        # Get the next available page\n        self._fd.seek(0, io.SEEK_END)\n        last_byte = self._fd.tell()\n        self.last_page = int(last_byte / self._tree_conf.page_size)\n\n###The function: get_node###\n    def set_node(self, node: Node):\n        self._wal.set_page(node.page, node.dump())\n        self._cache[node.page] = node\n\n    def set_page(self, page: int, data: bytes):\n        \"\"\"Set a raw page of data.\n\n        Used currently only for overflow pages.\n        \"\"\"\n        self._wal.set_page(page, data)\n\n    def get_page(self, page: int) -> bytes:\n        data = self._wal.get_page(page)\n        if not data:\n            data = self._read_page(page)\n        return data\n\n    @property\n    def read_transaction(self):\n\n        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()\n\n    @property\n    def write_transaction(self):\n\n        class WriteTransaction:\n\n            def __enter__(self2):\n                self._lock.writer_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                if exc_type:\n                    # When an error happens in the middle of a write\n                    # transaction we must roll it back and clear the cache\n                    # because the writer may have partially modified the Nodes\n                    self._wal.rollback()\n                    self._cache.clear()\n                else:\n                    self._wal.commit()\n                self._lock.writer_lock.release()\n\n        return WriteTransaction()\n\n    @property\n    def next_available_page(self) -> int:\n        self.last_page += 1\n        return self.last_page\n\n    def get_metadata(self) -> tuple:\n        try:\n            data = self._read_page(0)\n        except ReachedEndOfFile:\n            raise ValueError('Metadata not set yet')\n        end_root_node_page = PAGE_REFERENCE_BYTES\n        root_node_page = int.from_bytes(\n            data[0:end_root_node_page], ENDIAN\n        )\n        end_page_size = end_root_node_page + OTHERS_BYTES\n        page_size = int.from_bytes(\n            data[end_root_node_page:end_page_size], ENDIAN\n        )\n        end_order = end_page_size + OTHERS_BYTES\n        order = int.from_bytes(\n            data[end_page_size:end_order], ENDIAN\n        )\n        end_key_size = end_order + OTHERS_BYTES\n        key_size = int.from_bytes(\n            data[end_order:end_key_size], ENDIAN\n        )\n        end_value_size = end_key_size + OTHERS_BYTES\n        value_size = int.from_bytes(\n            data[end_key_size:end_value_size], ENDIAN\n        )\n        self._tree_conf = TreeConf(\n            page_size, order, key_size, value_size, self._tree_conf.serializer\n        )\n        return root_node_page, self._tree_conf\n\n    def set_metadata(self, root_node_page: int, tree_conf: TreeConf):\n        self._tree_conf = tree_conf\n        length = PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES\n        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            self._tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            bytes(self._tree_conf.page_size - length)\n        )\n        self._write_page_in_tree(0, data, fsync=True)\n\n    def close(self):\n        self.perform_checkpoint()\n        self._fd.close()\n        if self._dir_fd is not None:\n            os.close(self._dir_fd)\n\n    def perform_checkpoint(self, reopen_wal=False):\n        logger.info('Performing checkpoint of %s', self._filename)\n        for page, page_data in self._wal.checkpoint():\n            self._write_page_in_tree(page, page_data, fsync=False)\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n        if reopen_wal:\n            self._wal = WAL(self._filename, self._tree_conf.page_size)\n\n    def _read_page(self, page: int) -> bytes:\n        start = page * self._tree_conf.page_size\n        stop = start + self._tree_conf.page_size\n        assert stop - start == self._tree_conf.page_size\n        return read_from_file(self._fd, start, stop)\n\n    def _write_page_in_tree(self, page: int, data: Union[bytes, bytearray],\n                            fsync: bool=True):\n        \"\"\"Write a page of data in the tree file itself.\n\n        To be used during checkpoints and other non-standard uses.\n        \"\"\"\n        assert len(data) == self._tree_conf.page_size\n        self._fd.seek(page * self._tree_conf.page_size)\n        write_to_file(self._fd, self._dir_fd, data, fsync=fsync)\n\n    def __repr__(self):\n        return '<FileMemory: {}>'.format(self._filename)\n\n\nclass FrameType(enum.Enum):\n    PAGE = 1\n    COMMIT = 2\n    ROLLBACK = 3\n\n\nclass WAL:\n\n    __slots__ = ['filename', '_fd', '_dir_fd', '_page_size',\n                 '_committed_pages', '_not_committed_pages', 'needs_recovery']\n\n    FRAME_HEADER_LENGTH = (\n        FRAME_TYPE_BYTES + PAGE_REFERENCE_BYTES\n    )\n\n    def __init__(self, filename: str, page_size: int):\n        self.filename = filename + '-wal'\n        self._fd, self._dir_fd = open_file_in_dir(self.filename)\n        self._page_size = page_size\n        self._committed_pages = dict()\n        self._not_committed_pages = dict()\n\n        self._fd.seek(0, io.SEEK_END)\n        if self._fd.tell() == 0:\n            self._create_header()\n            self.needs_recovery = False\n        else:\n            logger.warning('Found an existing WAL file, '\n                           'the B+Tree was not closed properly')\n            self.needs_recovery = True\n            self._load_wal()\n\n    def checkpoint(self):\n        \"\"\"Transfer the modified data back to the tree and close the WAL.\"\"\"\n        if self._not_committed_pages:\n            logger.warning('Closing WAL with uncommitted data, discarding it')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_start in self._committed_pages.items():\n            page_data = read_from_file(\n                self._fd,\n                page_start,\n                page_start + self._page_size\n            )\n            yield page, page_data\n\n        self._fd.close()\n        os.unlink(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)\n            os.close(self._dir_fd)\n\n    def _create_header(self):\n        data = self._page_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        self._fd.seek(0)\n        write_to_file(self._fd, self._dir_fd, data, True)\n\n    def _load_wal(self):\n        self._fd.seek(0)\n        header_data = read_from_file(self._fd, 0, OTHERS_BYTES)\n        assert int.from_bytes(header_data, ENDIAN) == self._page_size\n\n        while True:\n            try:\n                self._load_next_frame()\n            except ReachedEndOfFile:\n                break\n        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted data, discarding it')\n            self._not_committed_pages = dict()\n\n    def _load_next_frame(self):\n        start = self._fd.tell()\n        stop = start + self.FRAME_HEADER_LENGTH\n        data = read_from_file(self._fd, start, stop)\n\n        frame_type = int.from_bytes(data[0:FRAME_TYPE_BYTES], ENDIAN)\n        page = int.from_bytes(\n            data[FRAME_TYPE_BYTES:FRAME_TYPE_BYTES+PAGE_REFERENCE_BYTES],\n            ENDIAN\n        )\n\n        frame_type = FrameType(frame_type)\n        if frame_type is FrameType.PAGE:\n            self._fd.seek(stop + self._page_size)\n\n        self._index_frame(frame_type, page, stop)\n\n    def _index_frame(self, frame_type: FrameType, page: int, page_start: int):\n        if frame_type is FrameType.PAGE:\n            self._not_committed_pages[page] = page_start\n        elif frame_type is FrameType.COMMIT:\n            self._committed_pages.update(self._not_committed_pages)\n            self._not_committed_pages = dict()\n        elif frame_type is FrameType.ROLLBACK:\n            self._not_committed_pages = dict()\n        else:\n            assert False\n\n    def _add_frame(self, frame_type: FrameType, page: Optional[int]=None,\n                   page_data: Optional[bytes]=None):\n        if frame_type is FrameType.PAGE and (not page or not page_data):\n            raise ValueError('PAGE frame without page data')\n        if page_data and len(page_data) != self._page_size:\n            raise ValueError('Page data is different from page size')\n        if not page:\n            page = 0\n        if frame_type is not FrameType.PAGE:\n            page_data = b''\n        data = (\n            frame_type.value.to_bytes(FRAME_TYPE_BYTES, ENDIAN) +\n            page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            page_data\n        )\n        self._fd.seek(0, io.SEEK_END)\n        write_to_file(self._fd, self._dir_fd, data,\n                      fsync=frame_type != FrameType.PAGE)\n        self._index_frame(frame_type, page, self._fd.tell() - self._page_size)\n\n    def get_page(self, page: int) -> Optional[bytes]:\n        page_start = None\n        for store in (self._not_committed_pages, self._committed_pages):\n            page_start = store.get(page)\n            if page_start:\n                break\n\n        if not page_start:\n            return None\n\n        return read_from_file(self._fd, page_start,\n                              page_start + self._page_size)\n\n    def set_page(self, page: int, page_data: bytes):\n        self._add_frame(FrameType.PAGE, page, page_data)\n\n    def commit(self):\n        # Commit is a no-op when there is no uncommitted pages\n        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n\n    def rollback(self):\n        # Rollback is a no-op when there is no uncommitted pages\n        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n\n    def __repr__(self):\n        return '<WAL: {}>'.format(self.filename)\n\n        ####cross_file_context:\n        [{'bplustree.node.Node': 'import abc\\nimport bisect\\nimport math\\nfrom typing import Optional\\n\\nfrom .const import (ENDIAN, NODE_TYPE_BYTES, USED_PAGE_LENGTH_BYTES,\\n                    PAGE_REFERENCE_BYTES, TreeConf)\\nfrom .entry import Entry, Record, Reference\\n\\n\\nclass Node(metaclass=abc.ABCMeta):\\n\\n    __slots__ = [\\'_tree_conf\\', \\'entries\\', \\'page\\', \\'parent\\', \\'next_page\\']\\n\\n    # Attributes to redefine in inherited classes\\n    _node_type_int = 0\\n    max_children = 0\\n    min_children = 0\\n    _entry_class = None\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._tree_conf = tree_conf\\n        self.entries = list()\\n        self.page = page\\n        self.parent = parent\\n        self.next_page = next_page\\n        if data:\\n            self.load(data)\\n\\n    def load(self, data: bytes):\\n        assert len(data) == self._tree_conf.page_size\\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\\n        used_page_length = int.from_bytes(\\n            data[NODE_TYPE_BYTES:end_used_page_length], ENDIAN\\n        )\\n        end_header = end_used_page_length + PAGE_REFERENCE_BYTES\\n        self.next_page = int.from_bytes(\\n            data[end_used_page_length:end_header], ENDIAN\\n        )\\n        if self.next_page == 0:\\n            self.next_page = None\\n\\n        entry_length = self._entry_class(self._tree_conf).length\\n        for start_offset in range(end_header, used_page_length, entry_length):\\n            entry_data = data[start_offset:start_offset+entry_length]\\n            entry = self._entry_class(self._tree_conf, data=entry_data)\\n            self.entries.append(entry)\\n\\n    def dump(self) -> bytearray:\\n        data = bytearray()\\n        for record in self.entries:\\n            data.extend(record.dump())\\n\\n        used_page_length = len(data) + 4\\n        assert 0 <= used_page_length < self._tree_conf.page_size\\n        next_page = 0 if self.next_page is None else self.next_page\\n        header = (\\n            self._node_type_int.to_bytes(1, ENDIAN) +\\n            used_page_length.to_bytes(3, ENDIAN) +\\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\\n        )\\n\\n        data = bytearray(header) + data\\n\\n        padding = self._tree_conf.page_size - len(data)\\n        assert padding >= 0\\n        data.extend(bytearray(padding))\\n        assert len(data) == self._tree_conf.page_size\\n\\n        return data\\n\\n    @property\\n    def can_add_entry(self) -> bool:\\n        return self.num_children < self.max_children\\n\\n    @property\\n    def can_delete_entry(self) -> bool:\\n        return self.num_children > self.min_children\\n\\n    @property\\n    def smallest_key(self):\\n        return self.smallest_entry.key\\n\\n    @property\\n    def smallest_entry(self):\\n        return self.entries[0]\\n\\n    @property\\n    def biggest_key(self):\\n        return self.biggest_entry.key\\n\\n    @property\\n    def biggest_entry(self):\\n        return self.entries[-1]\\n\\n    @property\\n    @abc.abstractmethod\\n    def num_children(self) -> int:\\n        \"\"\"Number of entries or other nodes connected to the node.\"\"\"\\n\\n    def pop_smallest(self) -> Entry:\\n        \"\"\"Remove and return the smallest entry.\"\"\"\\n        return self.entries.pop(0)\\n\\n    def insert_entry(self, entry: Entry):\\n        bisect.insort(self.entries, entry)\\n\\n    def insert_entry_at_the_end(self, entry: Entry):\\n        \"\"\"Insert an entry at the end of the entry list.\\n\\n        This is an optimized version of `insert_entry` when it is known that\\n        the key to insert is bigger than any other entries.\\n        \"\"\"\\n        self.entries.append(entry)\\n\\n    def remove_entry(self, key):\\n        self.entries.pop(self._find_entry_index(key))\\n\\n    def get_entry(self, key) -> Entry:\\n        return self.entries[self._find_entry_index(key)]\\n\\n    def _find_entry_index(self, key) -> int:\\n        entry = self._entry_class(\\n            self._tree_conf,\\n            key=key  # Hack to compare and order\\n        )\\n        i = bisect.bisect_left(self.entries, entry)\\n        if i != len(self.entries) and self.entries[i] == entry:\\n            return i\\n        raise ValueError(\\'No entry for key {}\\'.format(key))\\n\\n    def split_entries(self) -> list:\\n        \"\"\"Split the entries in half.\\n\\n        Keep the lower part in the node and return the upper one.\\n        \"\"\"\\n        len_entries = len(self.entries)\\n        rv = self.entries[len_entries//2:]\\n        self.entries = self.entries[:len_entries//2]\\n        assert len(self.entries) + len(rv) == len_entries\\n        return rv\\n\\n    @classmethod\\n    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\\n                       page: int=None) -> \\'Node\\':\\n        node_type_byte = data[0:NODE_TYPE_BYTES]\\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\\n        if node_type_int == 1:\\n            return LonelyRootNode(tree_conf, data, page)\\n        elif node_type_int == 2:\\n            return RootNode(tree_conf, data, page)\\n        elif node_type_int == 3:\\n            return InternalNode(tree_conf, data, page)\\n        elif node_type_int == 4:\\n            return LeafNode(tree_conf, data, page)\\n        else:\\n            assert False, \\'No Node with type {} exists\\'.format(node_type_int)\\n\\n    def __repr__(self):\\n        return \\'<{}: page={} entries={}>\\'.format(\\n            self.__class__.__name__, self.page, len(self.entries)\\n        )\\n\\n    def __eq__(self, other):\\n        return (\\n            self.__class__ is other.__class__ and\\n            self.page == other.page and\\n            self.entries == other.entries\\n        )\\n\\n\\nclass RecordNode(Node):\\n\\n    __slots__ = [\\'_entry_class\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._entry_class = Record\\n        super().__init__(tree_conf, data, page, parent, next_page)\\n\\n    @property\\n    def num_children(self) -> int:\\n        return len(self.entries)\\n\\n\\nclass LonelyRootNode(RecordNode):\\n    \"\"\"A Root node that holds records.\\n\\n    It is an exception for when there is only a single node in the tree.\\n    \"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 1\\n        self.min_children = 0\\n        self.max_children = tree_conf.order - 1\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    def convert_to_leaf(self):\\n        leaf = LeafNode(self._tree_conf, page=self.page)\\n        leaf.entries = self.entries\\n        return leaf\\n\\n\\nclass LeafNode(RecordNode):\\n    \"\"\"Node that holds the actual records within the tree.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._node_type_int = 4\\n        self.min_children = math.ceil(tree_conf.order / 2) - 1\\n        self.max_children = tree_conf.order - 1\\n        super().__init__(tree_conf, data, page, parent, next_page)\\n\\n\\nclass ReferenceNode(Node):\\n\\n    __slots__ = [\\'_entry_class\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._entry_class = Reference\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    @property\\n    def num_children(self) -> int:\\n        return len(self.entries) + 1 if self.entries else 0\\n\\n    def insert_entry(self, entry: \\'Reference\\'):\\n        \"\"\"Make sure that after of a reference matches before of the next one.\\n\\n        Probably very inefficient approach.\\n        \"\"\"\\n        super().insert_entry(entry)\\n        i = self.entries.index(entry)\\n        if i > 0:\\n            previous_entry = self.entries[i-1]\\n            previous_entry.after = entry.before\\n        try:\\n            next_entry = self.entries[i+1]\\n        except IndexError:\\n            pass\\n        else:\\n            next_entry.before = entry.after\\n\\n\\nclass RootNode(ReferenceNode):\\n    \"\"\"The first node at the top of the tree.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 2\\n        self.min_children = 2\\n        self.max_children = tree_conf.order\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    def convert_to_internal(self):\\n        internal = InternalNode(self._tree_conf, page=self.page)\\n        internal.entries = self.entries\\n        return internal\\n\\n\\nclass InternalNode(ReferenceNode):\\n    \"\"\"Node that only holds references to other Internal nodes or Leaves.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 3\\n        self.min_children = math.ceil(tree_conf.order / 2)\\n        self.max_children = tree_conf.order\\n        super().__init__(tree_conf, data, page, parent)\\n'}, {'bplustree.node.Node.from_page_data': 'import abc\\nimport bisect\\nimport math\\nfrom typing import Optional\\n\\nfrom .const import (ENDIAN, NODE_TYPE_BYTES, USED_PAGE_LENGTH_BYTES,\\n                    PAGE_REFERENCE_BYTES, TreeConf)\\nfrom .entry import Entry, Record, Reference\\n\\n\\nclass Node(metaclass=abc.ABCMeta):\\n\\n    __slots__ = [\\'_tree_conf\\', \\'entries\\', \\'page\\', \\'parent\\', \\'next_page\\']\\n\\n    # Attributes to redefine in inherited classes\\n    _node_type_int = 0\\n    max_children = 0\\n    min_children = 0\\n    _entry_class = None\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._tree_conf = tree_conf\\n        self.entries = list()\\n        self.page = page\\n        self.parent = parent\\n        self.next_page = next_page\\n        if data:\\n            self.load(data)\\n\\n    def load(self, data: bytes):\\n        assert len(data) == self._tree_conf.page_size\\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\\n        used_page_length = int.from_bytes(\\n            data[NODE_TYPE_BYTES:end_used_page_length], ENDIAN\\n        )\\n        end_header = end_used_page_length + PAGE_REFERENCE_BYTES\\n        self.next_page = int.from_bytes(\\n            data[end_used_page_length:end_header], ENDIAN\\n        )\\n        if self.next_page == 0:\\n            self.next_page = None\\n\\n        entry_length = self._entry_class(self._tree_conf).length\\n        for start_offset in range(end_header, used_page_length, entry_length):\\n            entry_data = data[start_offset:start_offset+entry_length]\\n            entry = self._entry_class(self._tree_conf, data=entry_data)\\n            self.entries.append(entry)\\n\\n    def dump(self) -> bytearray:\\n        data = bytearray()\\n        for record in self.entries:\\n            data.extend(record.dump())\\n\\n        used_page_length = len(data) + 4\\n        assert 0 <= used_page_length < self._tree_conf.page_size\\n        next_page = 0 if self.next_page is None else self.next_page\\n        header = (\\n            self._node_type_int.to_bytes(1, ENDIAN) +\\n            used_page_length.to_bytes(3, ENDIAN) +\\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\\n        )\\n\\n        data = bytearray(header) + data\\n\\n        padding = self._tree_conf.page_size - len(data)\\n        assert padding >= 0\\n        data.extend(bytearray(padding))\\n        assert len(data) == self._tree_conf.page_size\\n\\n        return data\\n\\n    @property\\n    def can_add_entry(self) -> bool:\\n        return self.num_children < self.max_children\\n\\n    @property\\n    def can_delete_entry(self) -> bool:\\n        return self.num_children > self.min_children\\n\\n    @property\\n    def smallest_key(self):\\n        return self.smallest_entry.key\\n\\n    @property\\n    def smallest_entry(self):\\n        return self.entries[0]\\n\\n    @property\\n    def biggest_key(self):\\n        return self.biggest_entry.key\\n\\n    @property\\n    def biggest_entry(self):\\n        return self.entries[-1]\\n\\n    @property\\n    @abc.abstractmethod\\n    def num_children(self) -> int:\\n        \"\"\"Number of entries or other nodes connected to the node.\"\"\"\\n\\n    def pop_smallest(self) -> Entry:\\n        \"\"\"Remove and return the smallest entry.\"\"\"\\n        return self.entries.pop(0)\\n\\n    def insert_entry(self, entry: Entry):\\n        bisect.insort(self.entries, entry)\\n\\n    def insert_entry_at_the_end(self, entry: Entry):\\n        \"\"\"Insert an entry at the end of the entry list.\\n\\n        This is an optimized version of `insert_entry` when it is known that\\n        the key to insert is bigger than any other entries.\\n        \"\"\"\\n        self.entries.append(entry)\\n\\n    def remove_entry(self, key):\\n        self.entries.pop(self._find_entry_index(key))\\n\\n    def get_entry(self, key) -> Entry:\\n        return self.entries[self._find_entry_index(key)]\\n\\n    def _find_entry_index(self, key) -> int:\\n        entry = self._entry_class(\\n            self._tree_conf,\\n            key=key  # Hack to compare and order\\n        )\\n        i = bisect.bisect_left(self.entries, entry)\\n        if i != len(self.entries) and self.entries[i] == entry:\\n            return i\\n        raise ValueError(\\'No entry for key {}\\'.format(key))\\n\\n    def split_entries(self) -> list:\\n        \"\"\"Split the entries in half.\\n\\n        Keep the lower part in the node and return the upper one.\\n        \"\"\"\\n        len_entries = len(self.entries)\\n        rv = self.entries[len_entries//2:]\\n        self.entries = self.entries[:len_entries//2]\\n        assert len(self.entries) + len(rv) == len_entries\\n        return rv\\n\\n    @classmethod\\n    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\\n                       page: int=None) -> \\'Node\\':\\n        node_type_byte = data[0:NODE_TYPE_BYTES]\\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\\n        if node_type_int == 1:\\n            return LonelyRootNode(tree_conf, data, page)\\n        elif node_type_int == 2:\\n            return RootNode(tree_conf, data, page)\\n        elif node_type_int == 3:\\n            return InternalNode(tree_conf, data, page)\\n        elif node_type_int == 4:\\n            return LeafNode(tree_conf, data, page)\\n        else:\\n            assert False, \\'No Node with type {} exists\\'.format(node_type_int)\\n\\n    def __repr__(self):\\n        return \\'<{}: page={} entries={}>\\'.format(\\n            self.__class__.__name__, self.page, len(self.entries)\\n        )\\n\\n    def __eq__(self, other):\\n        return (\\n            self.__class__ is other.__class__ and\\n            self.page == other.page and\\n            self.entries == other.entries\\n        )\\n\\n\\nclass RecordNode(Node):\\n\\n    __slots__ = [\\'_entry_class\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._entry_class = Record\\n        super().__init__(tree_conf, data, page, parent, next_page)\\n\\n    @property\\n    def num_children(self) -> int:\\n        return len(self.entries)\\n\\n\\nclass LonelyRootNode(RecordNode):\\n    \"\"\"A Root node that holds records.\\n\\n    It is an exception for when there is only a single node in the tree.\\n    \"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 1\\n        self.min_children = 0\\n        self.max_children = tree_conf.order - 1\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    def convert_to_leaf(self):\\n        leaf = LeafNode(self._tree_conf, page=self.page)\\n        leaf.entries = self.entries\\n        return leaf\\n\\n\\nclass LeafNode(RecordNode):\\n    \"\"\"Node that holds the actual records within the tree.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._node_type_int = 4\\n        self.min_children = math.ceil(tree_conf.order / 2) - 1\\n        self.max_children = tree_conf.order - 1\\n        super().__init__(tree_conf, data, page, parent, next_page)\\n\\n\\nclass ReferenceNode(Node):\\n\\n    __slots__ = [\\'_entry_class\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._entry_class = Reference\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    @property\\n    def num_children(self) -> int:\\n        return len(self.entries) + 1 if self.entries else 0\\n\\n    def insert_entry(self, entry: \\'Reference\\'):\\n        \"\"\"Make sure that after of a reference matches before of the next one.\\n\\n        Probably very inefficient approach.\\n        \"\"\"\\n        super().insert_entry(entry)\\n        i = self.entries.index(entry)\\n        if i > 0:\\n            previous_entry = self.entries[i-1]\\n            previous_entry.after = entry.before\\n        try:\\n            next_entry = self.entries[i+1]\\n        except IndexError:\\n            pass\\n        else:\\n            next_entry.before = entry.after\\n\\n\\nclass RootNode(ReferenceNode):\\n    \"\"\"The first node at the top of the tree.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 2\\n        self.min_children = 2\\n        self.max_children = tree_conf.order\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    def convert_to_internal(self):\\n        internal = InternalNode(self._tree_conf, page=self.page)\\n        internal.entries = self.entries\\n        return internal\\n\\n\\nclass InternalNode(ReferenceNode):\\n    \"\"\"Node that only holds references to other Internal nodes or Leaves.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 3\\n        self.min_children = math.ceil(tree_conf.order / 2)\\n        self.max_children = tree_conf.order\\n        super().__init__(tree_conf, data, page, parent)\\n'}, {'bplustree.node.Node.page': 'import abc\\nimport bisect\\nimport math\\nfrom typing import Optional\\n\\nfrom .const import (ENDIAN, NODE_TYPE_BYTES, USED_PAGE_LENGTH_BYTES,\\n                    PAGE_REFERENCE_BYTES, TreeConf)\\nfrom .entry import Entry, Record, Reference\\n\\n\\nclass Node(metaclass=abc.ABCMeta):\\n\\n    __slots__ = [\\'_tree_conf\\', \\'entries\\', \\'page\\', \\'parent\\', \\'next_page\\']\\n\\n    # Attributes to redefine in inherited classes\\n    _node_type_int = 0\\n    max_children = 0\\n    min_children = 0\\n    _entry_class = None\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._tree_conf = tree_conf\\n        self.entries = list()\\n        self.page = page\\n        self.parent = parent\\n        self.next_page = next_page\\n        if data:\\n            self.load(data)\\n\\n    def load(self, data: bytes):\\n        assert len(data) == self._tree_conf.page_size\\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\\n        used_page_length = int.from_bytes(\\n            data[NODE_TYPE_BYTES:end_used_page_length], ENDIAN\\n        )\\n        end_header = end_used_page_length + PAGE_REFERENCE_BYTES\\n        self.next_page = int.from_bytes(\\n            data[end_used_page_length:end_header], ENDIAN\\n        )\\n        if self.next_page == 0:\\n            self.next_page = None\\n\\n        entry_length = self._entry_class(self._tree_conf).length\\n        for start_offset in range(end_header, used_page_length, entry_length):\\n            entry_data = data[start_offset:start_offset+entry_length]\\n            entry = self._entry_class(self._tree_conf, data=entry_data)\\n            self.entries.append(entry)\\n\\n    def dump(self) -> bytearray:\\n        data = bytearray()\\n        for record in self.entries:\\n            data.extend(record.dump())\\n\\n        used_page_length = len(data) + 4\\n        assert 0 <= used_page_length < self._tree_conf.page_size\\n        next_page = 0 if self.next_page is None else self.next_page\\n        header = (\\n            self._node_type_int.to_bytes(1, ENDIAN) +\\n            used_page_length.to_bytes(3, ENDIAN) +\\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\\n        )\\n\\n        data = bytearray(header) + data\\n\\n        padding = self._tree_conf.page_size - len(data)\\n        assert padding >= 0\\n        data.extend(bytearray(padding))\\n        assert len(data) == self._tree_conf.page_size\\n\\n        return data\\n\\n    @property\\n    def can_add_entry(self) -> bool:\\n        return self.num_children < self.max_children\\n\\n    @property\\n    def can_delete_entry(self) -> bool:\\n        return self.num_children > self.min_children\\n\\n    @property\\n    def smallest_key(self):\\n        return self.smallest_entry.key\\n\\n    @property\\n    def smallest_entry(self):\\n        return self.entries[0]\\n\\n    @property\\n    def biggest_key(self):\\n        return self.biggest_entry.key\\n\\n    @property\\n    def biggest_entry(self):\\n        return self.entries[-1]\\n\\n    @property\\n    @abc.abstractmethod\\n    def num_children(self) -> int:\\n        \"\"\"Number of entries or other nodes connected to the node.\"\"\"\\n\\n    def pop_smallest(self) -> Entry:\\n        \"\"\"Remove and return the smallest entry.\"\"\"\\n        return self.entries.pop(0)\\n\\n    def insert_entry(self, entry: Entry):\\n        bisect.insort(self.entries, entry)\\n\\n    def insert_entry_at_the_end(self, entry: Entry):\\n        \"\"\"Insert an entry at the end of the entry list.\\n\\n        This is an optimized version of `insert_entry` when it is known that\\n        the key to insert is bigger than any other entries.\\n        \"\"\"\\n        self.entries.append(entry)\\n\\n    def remove_entry(self, key):\\n        self.entries.pop(self._find_entry_index(key))\\n\\n    def get_entry(self, key) -> Entry:\\n        return self.entries[self._find_entry_index(key)]\\n\\n    def _find_entry_index(self, key) -> int:\\n        entry = self._entry_class(\\n            self._tree_conf,\\n            key=key  # Hack to compare and order\\n        )\\n        i = bisect.bisect_left(self.entries, entry)\\n        if i != len(self.entries) and self.entries[i] == entry:\\n            return i\\n        raise ValueError(\\'No entry for key {}\\'.format(key))\\n\\n    def split_entries(self) -> list:\\n        \"\"\"Split the entries in half.\\n\\n        Keep the lower part in the node and return the upper one.\\n        \"\"\"\\n        len_entries = len(self.entries)\\n        rv = self.entries[len_entries//2:]\\n        self.entries = self.entries[:len_entries//2]\\n        assert len(self.entries) + len(rv) == len_entries\\n        return rv\\n\\n    @classmethod\\n    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\\n                       page: int=None) -> \\'Node\\':\\n        node_type_byte = data[0:NODE_TYPE_BYTES]\\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\\n        if node_type_int == 1:\\n            return LonelyRootNode(tree_conf, data, page)\\n        elif node_type_int == 2:\\n            return RootNode(tree_conf, data, page)\\n        elif node_type_int == 3:\\n            return InternalNode(tree_conf, data, page)\\n        elif node_type_int == 4:\\n            return LeafNode(tree_conf, data, page)\\n        else:\\n            assert False, \\'No Node with type {} exists\\'.format(node_type_int)\\n\\n    def __repr__(self):\\n        return \\'<{}: page={} entries={}>\\'.format(\\n            self.__class__.__name__, self.page, len(self.entries)\\n        )\\n\\n    def __eq__(self, other):\\n        return (\\n            self.__class__ is other.__class__ and\\n            self.page == other.page and\\n            self.entries == other.entries\\n        )\\n\\n\\nclass RecordNode(Node):\\n\\n    __slots__ = [\\'_entry_class\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._entry_class = Record\\n        super().__init__(tree_conf, data, page, parent, next_page)\\n\\n    @property\\n    def num_children(self) -> int:\\n        return len(self.entries)\\n\\n\\nclass LonelyRootNode(RecordNode):\\n    \"\"\"A Root node that holds records.\\n\\n    It is an exception for when there is only a single node in the tree.\\n    \"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 1\\n        self.min_children = 0\\n        self.max_children = tree_conf.order - 1\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    def convert_to_leaf(self):\\n        leaf = LeafNode(self._tree_conf, page=self.page)\\n        leaf.entries = self.entries\\n        return leaf\\n\\n\\nclass LeafNode(RecordNode):\\n    \"\"\"Node that holds the actual records within the tree.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None, next_page: int=None):\\n        self._node_type_int = 4\\n        self.min_children = math.ceil(tree_conf.order / 2) - 1\\n        self.max_children = tree_conf.order - 1\\n        super().__init__(tree_conf, data, page, parent, next_page)\\n\\n\\nclass ReferenceNode(Node):\\n\\n    __slots__ = [\\'_entry_class\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._entry_class = Reference\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    @property\\n    def num_children(self) -> int:\\n        return len(self.entries) + 1 if self.entries else 0\\n\\n    def insert_entry(self, entry: \\'Reference\\'):\\n        \"\"\"Make sure that after of a reference matches before of the next one.\\n\\n        Probably very inefficient approach.\\n        \"\"\"\\n        super().insert_entry(entry)\\n        i = self.entries.index(entry)\\n        if i > 0:\\n            previous_entry = self.entries[i-1]\\n            previous_entry.after = entry.before\\n        try:\\n            next_entry = self.entries[i+1]\\n        except IndexError:\\n            pass\\n        else:\\n            next_entry.before = entry.after\\n\\n\\nclass RootNode(ReferenceNode):\\n    \"\"\"The first node at the top of the tree.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 2\\n        self.min_children = 2\\n        self.max_children = tree_conf.order\\n        super().__init__(tree_conf, data, page, parent)\\n\\n    def convert_to_internal(self):\\n        internal = InternalNode(self._tree_conf, page=self.page)\\n        internal.entries = self.entries\\n        return internal\\n\\n\\nclass InternalNode(ReferenceNode):\\n    \"\"\"Node that only holds references to other Internal nodes or Leaves.\"\"\"\\n\\n    __slots__ = [\\'_node_type_int\\', \\'min_children\\', \\'max_children\\']\\n\\n    def __init__(self, tree_conf: TreeConf, data: Optional[bytes]=None,\\n                 page: int=None, parent: \\'Node\\'=None):\\n        self._node_type_int = 3\\n        self.min_children = math.ceil(tree_conf.order / 2)\\n        self.max_children = tree_conf.order\\n        super().__init__(tree_conf, data, page, parent)\\n'}]", "test_list": ["def test_file_memory_write_transaction():\n    mem = FileMemory(filename, tree_conf)\n    mem._lock = mock.Mock()\n    assert mem._wal._not_committed_pages == {}\n    assert mem._wal._committed_pages == {}\n    with mem.write_transaction:\n        mem.set_node(node)\n        assert mem._wal._not_committed_pages == {3: 9}\n        assert mem._wal._committed_pages == {}\n        assert mem._lock.writer_lock.acquire.call_count == 1\n    assert mem._wal._not_committed_pages == {}\n    assert mem._wal._committed_pages == {3: 9}\n    assert mem._lock.writer_lock.release.call_count == 1\n    assert mem._lock.reader_lock.acquire.call_count == 0\n    with mem.read_transaction:\n        assert mem._lock.reader_lock.acquire.call_count == 1\n        assert node == mem.get_node(3)\n    assert mem._lock.reader_lock.release.call_count == 1\n    mem.close()", "def test_file_memory_node():\n    mem = FileMemory(filename, tree_conf)\n    with pytest.raises(ReachedEndOfFile):\n        mem.get_node(3)\n    mem.set_node(node)\n    assert node == mem.get_node(3)\n    mem.close()"], "requirements": {"Input-Output Conditions": {"requirement": "The 'get_node' function should return a Node object when provided with a valid page number, and it should return None if the page number is invalid or out of range.", "unit_test": ["def test_get_node_valid_page():\n    mem = FileMemory(filename, tree_conf)\n    mem.set_node(node)\n    retrieved_node = mem.get_node(3)\n    assert isinstance(retrieved_node, Node)\n    assert retrieved_node == node\n    mem.close()\n\ndef test_get_node_invalid_page():\n    mem = FileMemory(filename, tree_conf)\n    retrieved_node = mem.get_node(999)\n    assert retrieved_node is None\n    mem.close()"], "test": "tests/test_memory.py::test_get_node_valid_page"}, "Exception Handling": {"requirement": "The 'get_node' function should raise a ValueError with a descriptive message if the page number is negative.", "unit_test": ["def test_get_node_negative_page():\n    mem = FileMemory(filename, tree_conf)\n    with pytest.raises(ValueError, match='Page number cannot be negative'):\n        mem.get_node(-1)\n    mem.close()"], "test": "tests/test_memory.py::test_get_node_negative_page"}, "Edge Case Handling": {"requirement": "The 'get_node' function should handle the edge case where the cache is empty and the node is not present in storage, returning None.", "unit_test": ["def test_get_node_empty_cache_and_storage():\n    mem = FileMemory(filename, tree_conf)\n    retrieved_node = mem.get_node(1)\n    assert retrieved_node is None\n    mem.close()"], "test": "tests/test_memory.py::test_get_node_empty_cache_and_storage"}, "Functionality Extension": {"requirement": "Extend the 'get_node' function to accept an optional parameter 'use_cache' which, when set to False, forces retrieval from storage even if the node is present in the cache.", "unit_test": ["def test_get_node_use_cache_false():\n    mem = FileMemory(filename, tree_conf)\n    mem.set_node(node)\n    with mock.patch.object(mem, '_cache', create=True) as mock_cache:\n        mock_cache.get.return_value = node\n        retrieved_node = mem.get_node(3, use_cache=False)\n        assert retrieved_node == node\n        mock_cache.get.assert_not_called()\n    mem.close()"], "test": "tests/test_memory.py::test_get_node_use_cache_false"}, "Annotation Coverage": {"requirement": "Ensure that the 'get_node' function has complete type annotations for all parameters and the return type.", "unit_test": ["def test_get_node_annotations():\n    from bplustree.memory import Node\n    from typing import Optional\n    annotations = FileMemory.get_node.__annotations__\n    assert annotations['page'] == int\n    assert annotations['return'] == Node"], "test": "tests/test_memory.py::test_get_node_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'get_node' function should not exceed 5.", "unit_test": ["def test_get_node_cyclomatic_complexity():\n    complexity = calculate_cyclomatic_complexity(FileMemory.get_node)\n    assert complexity <= 5"], "test": "tests/test_memory.py::test_get_node_cyclomatic_complexity"}, "Code Standard": {"requirement": "The 'get_node' function should adhere to PEP 8 standards, including proper indentation, spacing, and line length.", "unit_test": ["def test_get_node_pep8_compliance():\n    import pep8\n    style_guide = pep8.StyleGuide(quiet=True)\n    result = style_guide.check_files(['path_to_file_containing_get_node.py'])\n    assert result.total_errors == 0"], "test": "tests/test_memory.py::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'get_node' function should utilize the '_cache' attributes of the FileMemory class.", "unit_test": ["def test_get_node_context_usage():\n    mem = FileMemory(filename, tree_conf)\n    with mock.patch.object(mem, '_cache', create=True) as mock_cache,\n         mock.patch.object(mem, '_read_page', create=True) as mock_read_page,\n         mock.patch.object(mem, '_wal', create=True) as mock_wal:\n        mem.get_node(3)\n        mock_cache.get.assert_called()\n        mock_read_page.assert_called()\n        mock_wal.get_page.assert_called()\n    mem.close()"], "test": "tests/test_memory.py::test_get_node_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'get_node' function should correctly use the '_cache' to check for a node before accessing '_wal'", "unit_test": ["def test_get_node_cache_usage_correctness():\n    mem = FileMemory(filename, tree_conf)\n    with mock.patch.object(mem, '_cache', create=True) as mock_cache,\n         mock.patch.object(mem, '_wal', create=True) as mock_wal:\n        mock_cache.get.return_value = node\n        mem.get_node(3)\n        mock_cache.get.assert_called_once_with(3)\n        mock_wal.get_page.assert_not_called()\n    mem.close()"], "test": "tests/test_memory.py::test_get_node_cache_usage_correctness"}}}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "type": "method", "project_path": "Internet/boto", "completion_path": "Internet/boto/boto/s3/connection.py", "signature_position": [357, 360], "body_position": [361, 381], "dependency": {"intra_class": ["boto.s3.connection.S3Connection.calling_format"], "intra_file": ["boto.s3.connection._CallingFormat.build_auth_path", "boto.s3.connection._CallingFormat.build_host", "boto.s3.connection._CallingFormat.build_path_base"], "cross_file": ["boto.connection.AWSAuthConnection._auth_handler", "boto.connection.AWSAuthConnection.build_base_http_request", "boto.connection.AWSAuthConnection.server_name", "boto.auth.S3HmacAuthV4Handler.presign"]}, "requirement": {"Functionality": "Generate a presigned URL with Signature Version 4 for accessing an S3 object. It constructs the necessary parameters and builds an HTTP request. Then, it uses the authentication handler to generate the presigned URL. For presigned URLs we should ignore the port if it's HTTPS", "Arguments": ":param self: S3Connection. An instance of S3Connection class\n:param expires_in: Integer. The number of seconds until the presigned URL expires.\n:param method: String. The HTTP method to be used for the request.\n:param bucket: String. The name of the S3 bucket.\n:param key: String. The key of the S3 object.\n:param headers: Dictionary. Additional headers to include in the request.\n:param force_http: Bool. Whether to force the use of HTTP instead of HTTPS.\n:param response_headers: Dictionary. Additional response headers to include in the presigned URL.\n:param version_id: String. The version ID of the S3 object.\n:param iso_date: String. The ISO-formatted date to be used for signing the request.\n:return: String. The generated presigned URL."}, "tests": ["tests/unit/s3/test_connection.py::TestSigV4Presigned::test_sigv4_presign_headers", "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_sigv4_presign_respects_is_secure", "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_sigv4_presign_optional_params", "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_sigv4_presign_response_headers", "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_sigv4_presign"], "indent": 8, "domain": "Internet", "code": "    def generate_url_sigv4(self, expires_in, method, bucket='', key='',\n                            headers=None, force_http=False,\n                            response_headers=None, version_id=None,\n                            iso_date=None):\n        path = self.calling_format.build_path_base(bucket, key)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        host = self.calling_format.build_host(self.server_name(), bucket)\n\n        # For presigned URLs we should ignore the port if it's HTTPS\n        if host.endswith(':443'):\n            host = host[:-4]\n\n        params = {}\n        if version_id is not None:\n            params['VersionId'] = version_id\n\n        if response_headers is not None:\n            params.update(response_headers)\n\n        http_request = self.build_base_http_request(method, path, auth_path,\n                                                    headers=headers, host=host,\n                                                    params=params)\n\n        return self._auth_handler.presign(http_request, expires_in,\n                                          iso_date=iso_date)\n", "intra_context": "# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n# All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\nimport xml.sax\nimport base64\nfrom boto.compat import six, urllib\nimport time\n\nfrom boto.auth import detect_potential_s3sigv4\nimport boto.utils\nfrom boto.connection import AWSAuthConnection\nfrom boto import handler\nfrom boto.s3.bucket import Bucket\nfrom boto.s3.key import Key\nfrom boto.resultset import ResultSet\nfrom boto.exception import BotoClientError, S3ResponseError\n\n\ndef check_lowercase_bucketname(n):\n    \"\"\"\n    Bucket names must not contain uppercase characters. We check for\n    this by appending a lowercase character and testing with islower().\n    Note this also covers cases like numeric bucket names with dashes.\n\n    >>> check_lowercase_bucketname(\"Aaaa\")\n    Traceback (most recent call last):\n    ...\n    BotoClientError: S3Error: Bucket names cannot contain upper-case\n    characters when using either the sub-domain or virtual hosting calling\n    format.\n\n    >>> check_lowercase_bucketname(\"1234-5678-9123\")\n    True\n    >>> check_lowercase_bucketname(\"abcdefg1234\")\n    True\n    \"\"\"\n    if not (n + 'a').islower():\n        raise BotoClientError(\"Bucket names cannot contain upper-case \" \\\n            \"characters when using either the sub-domain or virtual \" \\\n            \"hosting calling format.\")\n    return True\n\n\ndef assert_case_insensitive(f):\n    def wrapper(*args, **kwargs):\n        if len(args) == 3 and check_lowercase_bucketname(args[2]):\n            pass\n        return f(*args, **kwargs)\n    return wrapper\n\n\nclass _CallingFormat(object):\n\n    def get_bucket_server(self, server, bucket):\n        return ''\n\n    def build_url_base(self, connection, protocol, server, bucket, key=''):\n        url_base = '%s://' % protocol\n        url_base += self.build_host(server, bucket)\n        url_base += connection.get_path(self.build_path_base(bucket, key))\n        return url_base\n\n    def build_host(self, server, bucket):\n        if bucket == '':\n            return server\n        else:\n            return self.get_bucket_server(server, bucket)\n\n    def build_auth_path(self, bucket, key=''):\n        key = boto.utils.get_utf8_value(key)\n        path = ''\n        if bucket != '':\n            path = '/' + bucket\n        return path + '/%s' % urllib.parse.quote(key)\n\n    def build_path_base(self, bucket, key=''):\n        key = boto.utils.get_utf8_value(key)\n        return '/%s' % urllib.parse.quote(key)\n\n\nclass SubdomainCallingFormat(_CallingFormat):\n\n    @assert_case_insensitive\n    def get_bucket_server(self, server, bucket):\n        return '%s.%s' % (bucket, server)\n\n\nclass VHostCallingFormat(_CallingFormat):\n\n    @assert_case_insensitive\n    def get_bucket_server(self, server, bucket):\n        return bucket\n\n\nclass OrdinaryCallingFormat(_CallingFormat):\n\n    def get_bucket_server(self, server, bucket):\n        return server\n\n    def build_path_base(self, bucket, key=''):\n        key = boto.utils.get_utf8_value(key)\n        path_base = '/'\n        if bucket:\n            path_base += \"%s/\" % bucket\n        return path_base + urllib.parse.quote(key)\n\n\nclass ProtocolIndependentOrdinaryCallingFormat(OrdinaryCallingFormat):\n\n    def build_url_base(self, connection, protocol, server, bucket, key=''):\n        url_base = '//'\n        url_base += self.build_host(server, bucket)\n        url_base += connection.get_path(self.build_path_base(bucket, key))\n        return url_base\n\n\nclass Location(object):\n\n    DEFAULT = ''  # US Classic Region\n    EU = 'EU'  # Ireland\n    EUCentral1 = 'eu-central-1'  # Frankfurt\n    USWest = 'us-west-1'\n    USWest2 = 'us-west-2'\n    SAEast = 'sa-east-1'\n    APNortheast = 'ap-northeast-1'\n    APSoutheast = 'ap-southeast-1'\n    APSoutheast2 = 'ap-southeast-2'\n    CNNorth1 = 'cn-north-1'\n\n\nclass NoHostProvided(object):\n    # An identifying object to help determine whether the user provided a\n    # ``host`` or not. Never instantiated.\n    pass\n\n\nclass HostRequiredError(BotoClientError):\n    pass\n\n\nclass S3Connection(AWSAuthConnection):\n\n    DefaultHost = 's3.amazonaws.com'\n    DefaultCallingFormat = boto.config.get('s3', 'calling_format', 'boto.s3.connection.SubdomainCallingFormat')\n    QueryString = 'Signature=%s&Expires=%d&AWSAccessKeyId=%s'\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None,\n                 host=NoHostProvided, debug=0, https_connection_factory=None,\n                 calling_format=DefaultCallingFormat, path='/',\n                 provider='aws', bucket_class=Bucket, security_token=None,\n                 suppress_consec_slashes=True, anon=False,\n                 validate_certs=None, profile_name=None):\n        no_host_provided = False\n        # Try falling back to the boto config file's value, if present.\n        if host is NoHostProvided:\n            host = boto.config.get('s3', 'host')\n            if host is None:\n                host = self.DefaultHost\n                no_host_provided = True\n        if isinstance(calling_format, six.string_types):\n            calling_format=boto.utils.find_class(calling_format)()\n        self.calling_format = calling_format\n        self.bucket_class = bucket_class\n        self.anon = anon\n        super(S3Connection, self).__init__(host,\n                aws_access_key_id, aws_secret_access_key,\n                is_secure, port, proxy, proxy_port, proxy_user, proxy_pass,\n                debug=debug, https_connection_factory=https_connection_factory,\n                path=path, provider=provider, security_token=security_token,\n                suppress_consec_slashes=suppress_consec_slashes,\n                validate_certs=validate_certs, profile_name=profile_name)\n        # We need to delay until after the call to ``super`` before checking\n        # to see if SigV4 is in use.\n        if no_host_provided:\n            if 'hmac-v4-s3' in self._required_auth_capability():\n                raise HostRequiredError(\n                    \"When using SigV4, you must specify a 'host' parameter.\"\n                )\n\n    @detect_potential_s3sigv4\n    def _required_auth_capability(self):\n        if self.anon:\n            return ['anon']\n        else:\n            return ['s3']\n\n    def __iter__(self):\n        for bucket in self.get_all_buckets():\n            yield bucket\n\n    def __contains__(self, bucket_name):\n        return not (self.lookup(bucket_name) is None)\n\n    def set_bucket_class(self, bucket_class):\n        \"\"\"\n        Set the Bucket class associated with this bucket.  By default, this\n        would be the boto.s3.key.Bucket class but if you want to subclass that\n        for some reason this allows you to associate your new class.\n\n        :type bucket_class: class\n        :param bucket_class: A subclass of Bucket that can be more specific\n        \"\"\"\n        self.bucket_class = bucket_class\n\n    def build_post_policy(self, expiration_time, conditions):\n        \"\"\"\n        Taken from the AWS book Python examples and modified for use with boto\n        \"\"\"\n        assert isinstance(expiration_time, time.struct_time), \\\n            'Policy document must include a valid expiration Time object'\n\n        # Convert conditions object mappings to condition statements\n\n        return '{\"expiration\": \"%s\",\\n\"conditions\": [%s]}' % \\\n            (time.strftime(boto.utils.ISO8601, expiration_time), \",\".join(conditions))\n\n    def build_post_form_args(self, bucket_name, key, expires_in=6000,\n                             acl=None, success_action_redirect=None,\n                             max_content_length=None,\n                             http_method='http', fields=None,\n                             conditions=None, storage_class='STANDARD',\n                             server_side_encryption=None):\n        \"\"\"\n        Taken from the AWS book Python examples and modified for use with boto\n        This only returns the arguments required for the post form, not the\n        actual form.  This does not return the file input field which also\n        needs to be added\n\n        :type bucket_name: string\n        :param bucket_name: Bucket to submit to\n\n        :type key: string\n        :param key:  Key name, optionally add ${filename} to the end to\n            attach the submitted filename\n\n        :type expires_in: integer\n        :param expires_in: Time (in seconds) before this expires, defaults\n            to 6000\n\n        :type acl: string\n        :param acl: A canned ACL.  One of:\n            * private\n            * public-read\n            * public-read-write\n            * authenticated-read\n            * bucket-owner-read\n            * bucket-owner-full-control\n\n        :type success_action_redirect: string\n        :param success_action_redirect: URL to redirect to on success\n\n        :type max_content_length: integer\n        :param max_content_length: Maximum size for this file\n\n        :type http_method: string\n        :param http_method:  HTTP Method to use, \"http\" or \"https\"\n\n        :type storage_class: string\n        :param storage_class: Storage class to use for storing the object.\n            Valid values: STANDARD | REDUCED_REDUNDANCY\n\n        :type server_side_encryption: string\n        :param server_side_encryption: Specifies server-side encryption\n            algorithm to use when Amazon S3 creates an object.\n            Valid values: None | AES256\n\n        :rtype: dict\n        :return: A dictionary containing field names/values as well as\n            a url to POST to\n\n            .. code-block:: python\n\n\n        \"\"\"\n        if fields is None:\n            fields = []\n        if conditions is None:\n            conditions = []\n        expiration = time.gmtime(int(time.time() + expires_in))\n\n        # Generate policy document\n        conditions.append('{\"bucket\": \"%s\"}' % bucket_name)\n        if key.endswith(\"${filename}\"):\n            conditions.append('[\"starts-with\", \"$key\", \"%s\"]' % key[:-len(\"${filename}\")])\n        else:\n            conditions.append('{\"key\": \"%s\"}' % key)\n        if acl:\n            conditions.append('{\"acl\": \"%s\"}' % acl)\n            fields.append({\"name\": \"acl\", \"value\": acl})\n        if success_action_redirect:\n            conditions.append('{\"success_action_redirect\": \"%s\"}' % success_action_redirect)\n            fields.append({\"name\": \"success_action_redirect\", \"value\": success_action_redirect})\n        if max_content_length:\n            conditions.append('[\"content-length-range\", 0, %i]' % max_content_length)\n\n        if self.provider.security_token:\n            fields.append({'name': 'x-amz-security-token',\n                           'value': self.provider.security_token})\n            conditions.append('{\"x-amz-security-token\": \"%s\"}' % self.provider.security_token)\n\n        if storage_class:\n            fields.append({'name': 'x-amz-storage-class',\n                           'value': storage_class})\n            conditions.append('{\"x-amz-storage-class\": \"%s\"}' % storage_class)\n\n        if server_side_encryption:\n            fields.append({'name': 'x-amz-server-side-encryption',\n                           'value': server_side_encryption})\n            conditions.append('{\"x-amz-server-side-encryption\": \"%s\"}' % server_side_encryption)\n\n        policy = self.build_post_policy(expiration, conditions)\n\n        # Add the base64-encoded policy document as the 'policy' field\n        policy_b64 = base64.b64encode(policy)\n        fields.append({\"name\": \"policy\", \"value\": policy_b64})\n\n        # Add the AWS access key as the 'AWSAccessKeyId' field\n        fields.append({\"name\": \"AWSAccessKeyId\",\n                       \"value\": self.aws_access_key_id})\n\n        # Add signature for encoded policy document as the\n        # 'signature' field\n        signature = self._auth_handler.sign_string(policy_b64)\n        fields.append({\"name\": \"signature\", \"value\": signature})\n        fields.append({\"name\": \"key\", \"value\": key})\n\n        # HTTPS protocol will be used if the secure HTTP option is enabled.\n        url = '%s://%s/' % (http_method,\n                            self.calling_format.build_host(self.server_name(),\n                                                           bucket_name))\n\n        return {\"action\": url, \"fields\": fields}\n\n###The function: generate_url_sigv4###\n    def generate_url(self, expires_in, method, bucket='', key='', headers=None,\n                     query_auth=True, force_http=False, response_headers=None,\n                     expires_in_absolute=False, version_id=None):\n        if self._auth_handler.capability[0] == 'hmac-v4-s3' and query_auth:\n            # Handle the special sigv4 case\n            return self.generate_url_sigv4(expires_in, method, bucket=bucket,\n                key=key, headers=headers, force_http=force_http,\n                response_headers=response_headers, version_id=version_id)\n\n        headers = headers or {}\n        if expires_in_absolute:\n            expires = int(expires_in)\n        else:\n            expires = int(time.time() + expires_in)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        auth_path = self.get_path(auth_path)\n        # optional version_id and response_headers need to be added to\n        # the query param list.\n        extra_qp = []\n        if version_id is not None:\n            extra_qp.append(\"versionId=%s\" % version_id)\n        if response_headers:\n            for k, v in response_headers.items():\n                extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n        if extra_qp:\n            delimiter = '?' if '?' not in auth_path else '&'\n            auth_path += delimiter + '&'.join(extra_qp)\n        self.calling_format.build_path_base(bucket, key)\n        if query_auth and not self.anon:\n            c_string = boto.utils.canonical_string(method, auth_path, headers,\n                                                   expires, self.provider)\n            b64_hmac = self._auth_handler.sign_string(c_string)\n            encoded_canonical = urllib.parse.quote(b64_hmac, safe='')\n            query_part = '?' + self.QueryString % (encoded_canonical, expires,\n                                                   self.aws_access_key_id)\n        else:\n            query_part = ''\n        if headers:\n            hdr_prefix = self.provider.header_prefix\n            for k, v in headers.items():\n                if k.startswith(hdr_prefix):\n                    # headers used for sig generation must be\n                    # included in the url also.\n                    extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if extra_qp:\n            delimiter = '?' if not query_part else '&'\n            query_part += delimiter + '&'.join(extra_qp)\n        if force_http:\n            protocol = 'http'\n            port = 80\n        else:\n            protocol = self.protocol\n            port = self.port\n        return self.calling_format.build_url_base(self, protocol,\n                                                  self.server_name(port),\n                                                  bucket, key) + query_part\n\n    def get_all_buckets(self, headers=None):\n        response = self.make_request('GET', headers=headers)\n        body = response.read()\n        if response.status > 300:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n        rs = ResultSet([('Bucket', self.bucket_class)])\n        h = handler.XmlHandler(rs, self)\n        if not isinstance(body, bytes):\n            body = body.encode('utf-8')\n        xml.sax.parseString(body, h)\n        return rs\n\n    def get_canonical_user_id(self, headers=None):\n        \"\"\"\n        Convenience method that returns the \"CanonicalUserID\" of the\n        user who's credentials are associated with the connection.\n        The only way to get this value is to do a GET request on the\n        service which returns all buckets associated with the account.\n        As part of that response, the canonical userid is returned.\n        This method simply does all of that and then returns just the\n        user id.\n\n        :rtype: string\n        :return: A string containing the canonical user id.\n        \"\"\"\n        rs = self.get_all_buckets(headers=headers)\n        return rs.owner.id\n\n    def get_bucket(self, bucket_name, validate=True, headers=None):\n        \"\"\"\n        Retrieves a bucket by name.\n\n        If the bucket does not exist, an ``S3ResponseError`` will be raised. If\n        you are unsure if the bucket exists or not, you can use the\n        ``S3Connection.lookup`` method, which will either return a valid bucket\n        or ``None``.\n\n        If ``validate=False`` is passed, no request is made to the service (no\n        charge/communication delay). This is only safe to do if you are **sure**\n        the bucket exists.\n\n        If the default ``validate=True`` is passed, a request is made to the\n        service to ensure the bucket exists. Prior to Boto v2.25.0, this fetched\n        a list of keys (but with a max limit set to ``0``, always returning an empty\n        list) in the bucket (& included better error messages), at an\n        increased expense. As of Boto v2.25.0, this now performs a HEAD request\n        (less expensive but worse error messages).\n\n        If you were relying on parsing the error message before, you should call\n        something like::\n\n            bucket = conn.get_bucket('<bucket_name>', validate=False)\n            bucket.get_all_keys(maxkeys=0)\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n\n        :type validate: boolean\n        :param validate: If ``True``, it will try to verify the bucket exists\n            on the service-side. (Default: ``True``)\n        \"\"\"\n        if validate:\n            return self.head_bucket(bucket_name, headers=headers)\n        else:\n            return self.bucket_class(self, bucket_name)\n\n    def head_bucket(self, bucket_name, headers=None):\n        \"\"\"\n        Determines if a bucket exists by name.\n\n        If the bucket does not exist, an ``S3ResponseError`` will be raised.\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n\n        :returns: A <Bucket> object\n        \"\"\"\n        response = self.make_request('HEAD', bucket_name, headers=headers)\n        body = response.read()\n        if response.status == 200:\n            return self.bucket_class(self, bucket_name)\n        elif response.status == 403:\n            # For backward-compatibility, we'll populate part of the exception\n            # with the most-common default.\n            err = self.provider.storage_response_error(\n                response.status,\n                response.reason,\n                body\n            )\n            err.error_code = 'AccessDenied'\n            err.error_message = 'Access Denied'\n            raise err\n        elif response.status == 404:\n            # For backward-compatibility, we'll populate part of the exception\n            # with the most-common default.\n            err = self.provider.storage_response_error(\n                response.status,\n                response.reason,\n                body\n            )\n            err.error_code = 'NoSuchBucket'\n            err.error_message = 'The specified bucket does not exist'\n            raise err\n        else:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n\n    def lookup(self, bucket_name, validate=True, headers=None):\n        \"\"\"\n        Attempts to get a bucket from S3.\n\n        Works identically to ``S3Connection.get_bucket``, save for that it\n        will return ``None`` if the bucket does not exist instead of throwing\n        an exception.\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n\n        :type validate: boolean\n        :param validate: If ``True``, it will try to fetch all keys within the\n            given bucket. (Default: ``True``)\n        \"\"\"\n        try:\n            bucket = self.get_bucket(bucket_name, validate, headers=headers)\n        except:\n            bucket = None\n        return bucket\n\n    def create_bucket(self, bucket_name, headers=None,\n                      location=Location.DEFAULT, policy=None):\n        \"\"\"\n        Creates a new located bucket. By default it's in the USA. You can pass\n        Location.EU to create a European bucket (S3) or European Union bucket\n        (GCS).\n\n        :type bucket_name: string\n        :param bucket_name: The name of the new bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to AWS.\n\n        :type location: str\n        :param location: The location of the new bucket.  You can use one of the\n            constants in :class:`boto.s3.connection.Location` (e.g. Location.EU,\n            Location.USWest, etc.).\n\n        :type policy: :class:`boto.s3.acl.CannedACLStrings`\n        :param policy: A canned ACL policy that will be applied to the\n            new key in S3.\n\n        \"\"\"\n        check_lowercase_bucketname(bucket_name)\n\n        if policy:\n            if headers:\n                headers[self.provider.acl_header] = policy\n            else:\n                headers = {self.provider.acl_header: policy}\n        if location == Location.DEFAULT:\n            data = ''\n        else:\n            data = '<CreateBucketConfiguration><LocationConstraint>' + \\\n                    location + '</LocationConstraint></CreateBucketConfiguration>'\n        response = self.make_request('PUT', bucket_name, headers=headers,\n                data=data)\n        body = response.read()\n        if response.status == 409:\n            raise self.provider.storage_create_error(\n                response.status, response.reason, body)\n        if response.status == 200:\n            return self.bucket_class(self, bucket_name)\n        else:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n\n    def delete_bucket(self, bucket, headers=None):\n        \"\"\"\n        Removes an S3 bucket.\n\n        In order to remove the bucket, it must first be empty. If the bucket is\n        not empty, an ``S3ResponseError`` will be raised.\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n        \"\"\"\n        response = self.make_request('DELETE', bucket, headers=headers)\n        body = response.read()\n        if response.status != 204:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n\n    def make_request(self, method, bucket='', key='', headers=None, data='',\n                     query_args=None, sender=None, override_num_retries=None,\n                     retry_handler=None):\n        if isinstance(bucket, self.bucket_class):\n            bucket = bucket.name\n        if isinstance(key, Key):\n            key = key.name\n        path = self.calling_format.build_path_base(bucket, key)\n        boto.log.debug('path=%s' % path)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        boto.log.debug('auth_path=%s' % auth_path)\n        host = self.calling_format.build_host(self.server_name(), bucket)\n        if query_args:\n            path += '?' + query_args\n            boto.log.debug('path=%s' % path)\n            auth_path += '?' + query_args\n            boto.log.debug('auth_path=%s' % auth_path)\n        return super(S3Connection, self).make_request(\n            method, path, headers,\n            data, host, auth_path, sender,\n            override_num_retries=override_num_retries,\n            retry_handler=retry_handler\n        )\n", "cross_context": [{"boto.connection.AWSAuthConnection._auth_handler": "# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\n# Copyright (c) 2010 Google\n# Copyright (c) 2008 rPath, Inc.\n# Copyright (c) 2009 The Echo Nest Corporation\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n# Copyright (c) 2011, Nexenta Systems Inc.\n# All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n#\n# Parts of this code were copied or derived from sample code supplied by AWS.\n# The following notice applies to that code.\n#\n#  This software code is made available \"AS IS\" without warranties of any\n#  kind.  You may copy, display, modify and redistribute the software\n#  code either by itself or as incorporated into your code; provided that\n#  you do not remove any proprietary notices.  Your use of this software\n#  code is at your own risk and you waive any claim against Amazon\n#  Digital Services, Inc. or its affiliates with respect to your use of\n#  this software code. (c) 2006 Amazon Digital Services, Inc. or its\n#  affiliates.\n\n\"\"\"\nHandles basic connections to AWS\n\"\"\"\nfrom datetime import datetime\nimport errno\nimport os\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport xml.sax\nimport copy\n\nfrom boto import auth\nfrom boto import auth_handler\nimport boto\nimport boto.utils\nimport boto.handler\nimport boto.cacerts\n\nfrom boto import config, UserAgent\nfrom boto.compat import six, http_client, urlparse, quote, encodebytes\nfrom boto.exception import AWSConnectionError\nfrom boto.exception import BotoClientError\nfrom boto.exception import BotoServerError\nfrom boto.exception import PleaseRetryException\nfrom boto.provider import Provider\nfrom boto.resultset import ResultSet\n\nHAVE_HTTPS_CONNECTION = False\ntry:\n    import ssl\n    from boto import https_connection\n    # Google App Engine runs on Python 2.5 so doesn't have ssl.SSLError.\n    if hasattr(ssl, 'SSLError'):\n        HAVE_HTTPS_CONNECTION = True\nexcept ImportError:\n    pass\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading\n\nON_APP_ENGINE = all(key in os.environ for key in (\n    'USER_IS_ADMIN', 'CURRENT_VERSION_ID', 'APPLICATION_ID'))\n\nPORTS_BY_SECURITY = {True: 443,\n                     False: 80}\n\nDEFAULT_CA_CERTS_FILE = os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)), \"cacerts.txt\")\n\n\nclass HostConnectionPool(object):\n\n    \"\"\"\n    A pool of connections for one remote (host,port,is_secure).\n\n    When connections are added to the pool, they are put into a\n    pending queue.  The _mexe method returns connections to the pool\n    before the response body has been read, so they connections aren't\n    ready to send another request yet.  They stay in the pending queue\n    until they are ready for another request, at which point they are\n    returned to the pool of ready connections.\n\n    The pool of ready connections is an ordered list of\n    (connection,time) pairs, where the time is the time the connection\n    was returned from _mexe.  After a certain period of time,\n    connections are considered stale, and discarded rather than being\n    reused.  This saves having to wait for the connection to time out\n    if AWS has decided to close it on the other end because of\n    inactivity.\n\n    Thread Safety:\n\n        This class is used only from ConnectionPool while it's mutex\n        is held.\n    \"\"\"\n\n    def __init__(self):\n        self.queue = []\n\n    def size(self):\n        \"\"\"\n        Returns the number of connections in the pool for this host.\n        Some of the connections may still be in use, and may not be\n        ready to be returned by get().\n        \"\"\"\n        return len(self.queue)\n\n    def put(self, conn):\n        \"\"\"\n        Adds a connection to the pool, along with the time it was\n        added.\n        \"\"\"\n        self.queue.append((conn, time.time()))\n\n    def get(self):\n        \"\"\"\n        Returns the next connection in this pool that is ready to be\n        reused.  Returns None if there aren't any.\n        \"\"\"\n        # Discard ready connections that are too old.\n        self.clean()\n\n        # Return the first connection that is ready, and remove it\n        # from the queue.  Connections that aren't ready are returned\n        # to the end of the queue with an updated time, on the\n        # assumption that somebody is actively reading the response.\n        for _ in range(len(self.queue)):\n            (conn, _) = self.queue.pop(0)\n            if self._conn_ready(conn):\n                return conn\n            else:\n                self.put(conn)\n        return None\n\n    def _conn_ready(self, conn):\n        \"\"\"\n        There is a nice state diagram at the top of http_client.py.  It\n        indicates that once the response headers have been read (which\n        _mexe does before adding the connection to the pool), a\n        response is attached to the connection, and it stays there\n        until it's done reading.  This isn't entirely true: even after\n        the client is done reading, the response may be closed, but\n        not removed from the connection yet.\n\n        This is ugly, reading a private instance variable, but the\n        state we care about isn't available in any public methods.\n        \"\"\"\n        if ON_APP_ENGINE:\n            # Google AppEngine implementation of HTTPConnection doesn't contain\n            # _HTTPConnection__response attribute. Moreover, it's not possible\n            # to determine if given connection is ready. Reusing connections\n            # simply doesn't make sense with App Engine urlfetch service.\n            return False\n        else:\n            response = getattr(conn, '_HTTPConnection__response', None)\n            return (response is None) or response.isclosed()\n\n    def clean(self):\n        \"\"\"\n        Get rid of stale connections.\n        \"\"\"\n        # Note that we do not close the connection here -- somebody\n        # may still be reading from it.\n        while len(self.queue) > 0 and self._pair_stale(self.queue[0]):\n            self.queue.pop(0)\n\n    def _pair_stale(self, pair):\n        \"\"\"\n        Returns true of the (connection,time) pair is too old to be\n        used.\n        \"\"\"\n        (_conn, return_time) = pair\n        now = time.time()\n        return return_time + ConnectionPool.STALE_DURATION < now\n\n\nclass ConnectionPool(object):\n\n    \"\"\"\n    A connection pool that expires connections after a fixed period of\n    time.  This saves time spent waiting for a connection that AWS has\n    timed out on the other end.\n\n    This class is thread-safe.\n    \"\"\"\n\n    #\n    # The amout of time between calls to clean.\n    #\n\n    CLEAN_INTERVAL = 5.0\n\n    #\n    # How long before a connection becomes \"stale\" and won't be reused\n    # again.  The intention is that this time is less that the timeout\n    # period that AWS uses, so we'll never try to reuse a connection\n    # and find that AWS is timing it out.\n    #\n    # Experimentation in July 2011 shows that AWS starts timing things\n    # out after three minutes.  The 60 seconds here is conservative so\n    # we should never hit that 3-minute timout.\n    #\n\n    STALE_DURATION = 60.0\n\n    def __init__(self):\n        # Mapping from (host,port,is_secure) to HostConnectionPool.\n        # If a pool becomes empty, it is removed.\n        self.host_to_pool = {}\n        # The last time the pool was cleaned.\n        self.last_clean_time = 0.0\n        self.mutex = threading.Lock()\n        ConnectionPool.STALE_DURATION = \\\n            config.getfloat('Boto', 'connection_stale_duration',\n                            ConnectionPool.STALE_DURATION)\n\n    def __getstate__(self):\n        pickled_dict = copy.copy(self.__dict__)\n        pickled_dict['host_to_pool'] = {}\n        del pickled_dict['mutex']\n        return pickled_dict\n\n    def __setstate__(self, dct):\n        self.__init__()\n\n    def size(self):\n        \"\"\"\n        Returns the number of connections in the pool.\n        \"\"\"\n        return sum(pool.size() for pool in self.host_to_pool.values())\n\n    def get_http_connection(self, host, port, is_secure):\n        \"\"\"\n        Gets a connection from the pool for the named host.  Returns\n        None if there is no connection that can be reused. It's the caller's\n        responsibility to call close() on the connection when it's no longer\n        needed.\n        \"\"\"\n        self.clean()\n        with self.mutex:\n            key = (host, port, is_secure)\n            if key not in self.host_to_pool:\n                return None\n            return self.host_to_pool[key].get()\n\n    def put_http_connection(self, host, port, is_secure, conn):\n        \"\"\"\n        Adds a connection to the pool of connections that can be\n        reused for the named host.\n        \"\"\"\n        with self.mutex:\n            key = (host, port, is_secure)\n            if key not in self.host_to_pool:\n                self.host_to_pool[key] = HostConnectionPool()\n            self.host_to_pool[key].put(conn)\n\n    def clean(self):\n        \"\"\"\n        Clean up the stale connections in all of the pools, and then\n        get rid of empty pools.  Pools clean themselves every time a\n        connection is fetched; this cleaning takes care of pools that\n        aren't being used any more, so nothing is being gotten from\n        them.\n        \"\"\"\n        with self.mutex:\n            now = time.time()\n            if self.last_clean_time + self.CLEAN_INTERVAL < now:\n                to_remove = []\n                for (host, pool) in self.host_to_pool.items():\n                    pool.clean()\n                    if pool.size() == 0:\n                        to_remove.append(host)\n                for host in to_remove:\n                    del self.host_to_pool[host]\n                self.last_clean_time = now\n\n\nclass HTTPRequest(object):\n\n    def __init__(self, method, protocol, host, port, path, auth_path,\n                 params, headers, body):\n        \"\"\"Represents an HTTP request.\n\n        :type method: string\n        :param method: The HTTP method name, 'GET', 'POST', 'PUT' etc.\n\n        :type protocol: string\n        :param protocol: The http protocol used, 'http' or 'https'.\n\n        :type host: string\n        :param host: Host to which the request is addressed. eg. abc.com\n\n        :type port: int\n        :param port: port on which the request is being sent. Zero means unset,\n            in which case default port will be chosen.\n\n        :type path: string\n        :param path: URL path that is being accessed.\n\n        :type auth_path: string\n        :param path: The part of the URL path used when creating the\n            authentication string.\n\n        :type params: dict\n        :param params: HTTP url query parameters, with key as name of\n            the param, and value as value of param.\n\n        :type headers: dict\n        :param headers: HTTP headers, with key as name of the header and value\n            as value of header.\n\n        :type body: string\n        :param body: Body of the HTTP request. If not present, will be None or\n            empty string ('').\n        \"\"\"\n        self.method = method\n        self.protocol = protocol\n        self.host = host\n        self.port = port\n        self.path = path\n        if auth_path is None:\n            auth_path = path\n        self.auth_path = auth_path\n        self.params = params\n        # chunked Transfer-Encoding should act only on PUT request.\n        if headers and 'Transfer-Encoding' in headers and \\\n                headers['Transfer-Encoding'] == 'chunked' and \\\n                self.method != 'PUT':\n            self.headers = headers.copy()\n            del self.headers['Transfer-Encoding']\n        else:\n            self.headers = headers\n        self.body = body\n\n    def __str__(self):\n        return (('method:(%s) protocol:(%s) host(%s) port(%s) path(%s) '\n                 'params(%s) headers(%s) body(%s)') % (self.method,\n                 self.protocol, self.host, self.port, self.path, self.params,\n                 self.headers, self.body))\n\n    def authorize(self, connection, **kwargs):\n        if not getattr(self, '_headers_quoted', False):\n            for key in self.headers:\n                val = self.headers[key]\n                if isinstance(val, six.text_type):\n                    safe = '!\"#$%&\\'()*+,/:;<=>?@[\\\\]^`{|}~ '\n                    self.headers[key] = quote(val.encode('utf-8'), safe)\n            setattr(self, '_headers_quoted', True)\n\n        self.headers['User-Agent'] = UserAgent\n\n        connection._auth_handler.add_auth(self, **kwargs)\n\n        # I'm not sure if this is still needed, now that add_auth is\n        # setting the content-length for POST requests.\n        if 'Content-Length' not in self.headers:\n            if 'Transfer-Encoding' not in self.headers or \\\n                    self.headers['Transfer-Encoding'] != 'chunked':\n                self.headers['Content-Length'] = str(len(self.body))\n\n\nclass HTTPResponse(http_client.HTTPResponse):\n\n    def __init__(self, *args, **kwargs):\n        http_client.HTTPResponse.__init__(self, *args, **kwargs)\n        self._cached_response = ''\n\n    def read(self, amt=None):\n        \"\"\"Read the response.\n\n        This method does not have the same behavior as\n        http_client.HTTPResponse.read.  Instead, if this method is called with\n        no ``amt`` arg, then the response body will be cached.  Subsequent\n        calls to ``read()`` with no args **will return the cached response**.\n\n        \"\"\"\n        if amt is None:\n            # The reason for doing this is that many places in boto call\n            # response.read() and except to get the response body that they\n            # can then process.  To make sure this always works as they expect\n            # we're caching the response so that multiple calls to read()\n            # will return the full body.  Note that this behavior only\n            # happens if the amt arg is not specified.\n            if not self._cached_response:\n                self._cached_response = http_client.HTTPResponse.read(self)\n            return self._cached_response\n        else:\n            return http_client.HTTPResponse.read(self, amt)\n\n\nclass AWSAuthConnection(object):\n    def __init__(self, host, aws_access_key_id=None,\n                 aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, debug=0,\n                 https_connection_factory=None, path='/',\n                 provider='aws', security_token=None,\n                 suppress_consec_slashes=True,\n                 validate_certs=True, profile_name=None):\n        \"\"\"\n        :type host: str\n        :param host: The host to make the connection to\n\n        :keyword str aws_access_key_id: Your AWS Access Key ID (provided by\n            Amazon). If none is specified, the value in your\n            ``AWS_ACCESS_KEY_ID`` environmental variable is used.\n        :keyword str aws_secret_access_key: Your AWS Secret Access Key\n            (provided by Amazon). If none is specified, the value in your\n            ``AWS_SECRET_ACCESS_KEY`` environmental variable is used.\n        :keyword str security_token: The security token associated with\n            temporary credentials issued by STS.  Optional unless using\n            temporary credentials.  If none is specified, the environment\n            variable ``AWS_SECURITY_TOKEN`` is used if defined.\n\n        :type is_secure: boolean\n        :param is_secure: Whether the connection is over SSL\n\n        :type https_connection_factory: list or tuple\n        :param https_connection_factory: A pair of an HTTP connection\n            factory and the exceptions to catch.  The factory should have\n            a similar interface to L{http_client.HTTPSConnection}.\n\n        :param str proxy: Address/hostname for a proxy server\n\n        :type proxy_port: int\n        :param proxy_port: The port to use when connecting over a proxy\n\n        :type proxy_user: str\n        :param proxy_user: The username to connect with on the proxy\n\n        :type proxy_pass: str\n        :param proxy_pass: The password to use when connection over a proxy.\n\n        :type port: int\n        :param port: The port to use to connect\n\n        :type suppress_consec_slashes: bool\n        :param suppress_consec_slashes: If provided, controls whether\n            consecutive slashes will be suppressed in key paths.\n\n        :type validate_certs: bool\n        :param validate_certs: Controls whether SSL certificates\n            will be validated or not.  Defaults to True.\n\n        :type profile_name: str\n        :param profile_name: Override usual Credentials section in config\n            file to use a named set of keys instead.\n        \"\"\"\n        self.suppress_consec_slashes = suppress_consec_slashes\n        self.num_retries = 6\n        # Override passed-in is_secure setting if value was defined in config.\n        if config.has_option('Boto', 'is_secure'):\n            is_secure = config.getboolean('Boto', 'is_secure')\n        self.is_secure = is_secure\n        # Whether or not to validate server certificates.\n        # The default is now to validate certificates.  This can be\n        # overridden in the boto config file are by passing an\n        # explicit validate_certs parameter to the class constructor.\n        self.https_validate_certificates = config.getbool(\n            'Boto', 'https_validate_certificates',\n            validate_certs)\n        if self.https_validate_certificates and not HAVE_HTTPS_CONNECTION:\n            raise BotoClientError(\n                \"SSL server certificate validation is enabled in boto \"\n                \"configuration, but Python dependencies required to \"\n                \"support this feature are not available. Certificate \"\n                \"validation is only supported when running under Python \"\n                \"2.6 or later.\")\n        certs_file = config.get_value(\n            'Boto', 'ca_certificates_file', DEFAULT_CA_CERTS_FILE)\n        if certs_file == 'system':\n            certs_file = None\n        self.ca_certificates_file = certs_file\n        if port:\n            self.port = port\n        else:\n            self.port = PORTS_BY_SECURITY[is_secure]\n\n        self.handle_proxy(proxy, proxy_port, proxy_user, proxy_pass)\n        # define exceptions from http_client that we want to catch and retry\n        self.http_exceptions = (http_client.HTTPException, socket.error,\n                                socket.gaierror, http_client.BadStatusLine)\n        # define subclasses of the above that are not retryable.\n        self.http_unretryable_exceptions = []\n        if HAVE_HTTPS_CONNECTION:\n            self.http_unretryable_exceptions.append(\n                https_connection.InvalidCertificateException)\n\n        # define values in socket exceptions we don't want to catch\n        self.socket_exception_values = (errno.EINTR,)\n        if https_connection_factory is not None:\n            self.https_connection_factory = https_connection_factory[0]\n            self.http_exceptions += https_connection_factory[1]\n        else:\n            self.https_connection_factory = None\n        if (is_secure):\n            self.protocol = 'https'\n        else:\n            self.protocol = 'http'\n        self.host = host\n        self.path = path\n        # if the value passed in for debug\n        if not isinstance(debug, six.integer_types):\n            debug = 0\n        self.debug = config.getint('Boto', 'debug', debug)\n        self.host_header = None\n\n        # Timeout used to tell http_client how long to wait for socket timeouts.\n        # Default is to leave timeout unchanged, which will in turn result in\n        # the socket's default global timeout being used. To specify a\n        # timeout, set http_socket_timeout in Boto config. Regardless,\n        # timeouts will only be applied if Python is 2.6 or greater.\n        self.http_connection_kwargs = {}\n        if (sys.version_info[0], sys.version_info[1]) >= (2, 6):\n            # If timeout isn't defined in boto config file, use 70 second\n            # default as recommended by\n            # http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForActivityTask.html\n            self.http_connection_kwargs['timeout'] = config.getint(\n                'Boto', 'http_socket_timeout', 70)\n\n        if isinstance(provider, Provider):\n            # Allow overriding Provider\n            self.provider = provider\n        else:\n            self._provider_type = provider\n            self.provider = Provider(self._provider_type,\n                                     aws_access_key_id,\n                                     aws_secret_access_key,\n                                     security_token,\n                                     profile_name)\n\n        # Allow config file to override default host, port, and host header.\n        if self.provider.host:\n            self.host = self.provider.host\n        if self.provider.port:\n            self.port = self.provider.port\n        if self.provider.host_header:\n            self.host_header = self.provider.host_header\n\n        self._pool = ConnectionPool()\n        self._connection = (self.host, self.port, self.is_secure)\n        self._last_rs = None\n        self._auth_handler = auth.get_auth_handler(\n            host, config, self.provider, self._required_auth_capability())\n        if getattr(self, 'AuthServiceName', None) is not None:\n            self.auth_service_name = self.AuthServiceName\n        self.request_hook = None\n\n    def __repr__(self):\n        return '%s:%s' % (self.__class__.__name__, self.host)\n\n    def _required_auth_capability(self):\n        return []\n\n    def _get_auth_service_name(self):\n        return getattr(self._auth_handler, 'service_name')\n\n    # For Sigv4, the auth_service_name/auth_region_name properties allow\n    # the service_name/region_name to be explicitly set instead of being\n    # derived from the endpoint url.\n    def _set_auth_service_name(self, value):\n        self._auth_handler.service_name = value\n    auth_service_name = property(_get_auth_service_name, _set_auth_service_name)\n\n    def _get_auth_region_name(self):\n        return getattr(self._auth_handler, 'region_name')\n\n    def _set_auth_region_name(self, value):\n        self._auth_handler.region_name = value\n    auth_region_name = property(_get_auth_region_name, _set_auth_region_name)\n\n    def connection(self):\n        return self.get_http_connection(*self._connection)\n    connection = property(connection)\n\n    def aws_access_key_id(self):\n        return self.provider.access_key\n    aws_access_key_id = property(aws_access_key_id)\n    gs_access_key_id = aws_access_key_id\n    access_key = aws_access_key_id\n\n    def aws_secret_access_key(self):\n        return self.provider.secret_key\n    aws_secret_access_key = property(aws_secret_access_key)\n    gs_secret_access_key = aws_secret_access_key\n    secret_key = aws_secret_access_key\n\n    def profile_name(self):\n        return self.provider.profile_name\n    profile_name = property(profile_name)\n\n    def get_path(self, path='/'):\n        # The default behavior is to suppress consecutive slashes for reasons\n        # discussed at\n        # https://groups.google.com/forum/#!topic/boto-dev/-ft0XPUy0y8\n        # You can override that behavior with the suppress_consec_slashes param.\n        if not self.suppress_consec_slashes:\n            return self.path + re.sub('^(/*)/', \"\\\\1\", path)\n        pos = path.find('?')\n        if pos >= 0:\n            params = path[pos:]\n            path = path[:pos]\n        else:\n            params = None\n        if path[-1] == '/':\n            need_trailing = True\n        else:\n            need_trailing = False\n        path_elements = self.path.split('/')\n        path_elements.extend(path.split('/'))\n        path_elements = [p for p in path_elements if p]\n        path = '/' + '/'.join(path_elements)\n        if path[-1] != '/' and need_trailing:\n            path += '/'\n        if params:\n            path = path + params\n        return path\n\n    def server_name(self, port=None):\n        if not port:\n            port = self.port\n        if port == 80:\n            signature_host = self.host\n        else:\n            # This unfortunate little hack can be attributed to\n            # a difference in the 2.6 version of http_client.  In old\n            # versions, it would append \":443\" to the hostname sent\n            # in the Host header and so we needed to make sure we\n            # did the same when calculating the V2 signature.  In 2.6\n            # (and higher!)\n            # it no longer does that.  Hence, this kludge.\n            if ((ON_APP_ENGINE and sys.version[:3] == '2.5') or\n                    sys.version[:3] in ('2.6', '2.7')) and port == 443:\n                signature_host = self.host\n            else:\n                signature_host = '%s:%d' % (self.host, port)\n        return signature_host\n\n    def handle_proxy(self, proxy, proxy_port, proxy_user, proxy_pass):\n        self.proxy = proxy\n        self.proxy_port = proxy_port\n        self.proxy_user = proxy_user\n        self.proxy_pass = proxy_pass\n        if 'http_proxy' in os.environ and not self.proxy:\n            pattern = re.compile(\n                '(?:http://)?'\n                '(?:(?P<user>[\\w\\-\\.]+):(?P<pass>.*)@)?'\n                '(?P<host>[\\w\\-\\.]+)'\n                '(?::(?P<port>\\d+))?'\n            )\n            match = pattern.match(os.environ['http_proxy'])\n            if match:\n                self.proxy = match.group('host')\n                self.proxy_port = match.group('port')\n                self.proxy_user = match.group('user')\n                self.proxy_pass = match.group('pass')\n        else:\n            if not self.proxy:\n                self.proxy = config.get_value('Boto', 'proxy', None)\n            if not self.proxy_port:\n                self.proxy_port = config.get_value('Boto', 'proxy_port', None)\n            if not self.proxy_user:\n                self.proxy_user = config.get_value('Boto', 'proxy_user', None)\n            if not self.proxy_pass:\n                self.proxy_pass = config.get_value('Boto', 'proxy_pass', None)\n\n        if not self.proxy_port and self.proxy:\n            print(\"http_proxy environment variable does not specify \"\n                  \"a port, using default\")\n            self.proxy_port = self.port\n\n        self.no_proxy = os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', '')\n        self.use_proxy = (self.proxy is not None)\n\n    def get_http_connection(self, host, port, is_secure):\n        conn = self._pool.get_http_connection(host, port, is_secure)\n        if conn is not None:\n            return conn\n        else:\n            return self.new_http_connection(host, port, is_secure)\n\n    def skip_proxy(self, host):\n        if not self.no_proxy:\n            return False\n\n        if self.no_proxy == \"*\":\n            return True\n\n        hostonly = host\n        hostonly = host.split(':')[0]\n\n        for name in self.no_proxy.split(','):\n            if name and (hostonly.endswith(name) or host.endswith(name)):\n                return True\n\n        return False\n\n    def new_http_connection(self, host, port, is_secure):\n        if host is None:\n            host = self.server_name()\n\n        # Make sure the host is really just the host, not including\n        # the port number\n        host = boto.utils.parse_host(host)\n\n        http_connection_kwargs = self.http_connection_kwargs.copy()\n\n        # Connection factories below expect a port keyword argument\n        http_connection_kwargs['port'] = port\n\n        # Override host with proxy settings if needed\n        if self.use_proxy and not is_secure and \\\n                not self.skip_proxy(host):\n            host = self.proxy\n            http_connection_kwargs['port'] = int(self.proxy_port)\n\n        if is_secure:\n            boto.log.debug(\n                'establishing HTTPS connection: host=%s, kwargs=%s',\n                host, http_connection_kwargs)\n            if self.use_proxy and not self.skip_proxy(host):\n                connection = self.proxy_ssl(host, is_secure and 443 or 80)\n            elif self.https_connection_factory:\n                connection = self.https_connection_factory(host)\n            elif self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\n                connection = https_connection.CertValidatingHTTPSConnection(\n                    host, ca_certs=self.ca_certificates_file,\n                    **http_connection_kwargs)\n            else:\n                connection = http_client.HTTPSConnection(\n                    host, **http_connection_kwargs)\n        else:\n            boto.log.debug('establishing HTTP connection: kwargs=%s' %\n                           http_connection_kwargs)\n            if self.https_connection_factory:\n                # even though the factory says https, this is too handy\n                # to not be able to allow overriding for http also.\n                connection = self.https_connection_factory(\n                    host, **http_connection_kwargs)\n            else:\n                connection = http_client.HTTPConnection(\n                    host, **http_connection_kwargs)\n        if self.debug > 1:\n            connection.set_debuglevel(self.debug)\n        # self.connection must be maintained for backwards-compatibility\n        # however, it must be dynamically pulled from the connection pool\n        # set a private variable which will enable that\n        if host.split(':')[0] == self.host and is_secure == self.is_secure:\n            self._connection = (host, port, is_secure)\n        # Set the response class of the http connection to use our custom\n        # class.\n        connection.response_class = HTTPResponse\n        return connection\n\n    def put_http_connection(self, host, port, is_secure, connection):\n        self._pool.put_http_connection(host, port, is_secure, connection)\n\n    def proxy_ssl(self, host=None, port=None):\n        if host and port:\n            host = '%s:%d' % (host, port)\n        else:\n            host = '%s:%d' % (self.host, self.port)\n        # Seems properly to use timeout for connect too\n        timeout = self.http_connection_kwargs.get(\"timeout\")\n        if timeout is not None:\n            sock = socket.create_connection((self.proxy,\n                                             int(self.proxy_port)), timeout)\n        else:\n            sock = socket.create_connection((self.proxy, int(self.proxy_port)))\n        boto.log.debug(\"Proxy connection: CONNECT %s HTTP/1.0\\r\\n\", host)\n        sock.sendall(\"CONNECT %s HTTP/1.0\\r\\n\" % host)\n        sock.sendall(\"User-Agent: %s\\r\\n\" % UserAgent)\n        if self.proxy_user and self.proxy_pass:\n            for k, v in self.get_proxy_auth_header().items():\n                sock.sendall(\"%s: %s\\r\\n\" % (k, v))\n            # See discussion about this config option at\n            # https://groups.google.com/forum/?fromgroups#!topic/boto-dev/teenFvOq2Cc\n            if config.getbool('Boto', 'send_crlf_after_proxy_auth_headers', False):\n                sock.sendall(\"\\r\\n\")\n        else:\n            sock.sendall(\"\\r\\n\")\n        resp = http_client.HTTPResponse(sock, strict=True, debuglevel=self.debug)\n        resp.begin()\n\n        if resp.status != 200:\n            # Fake a socket error, use a code that make it obvious it hasn't\n            # been generated by the socket library\n            raise socket.error(-71,\n                               \"Error talking to HTTP proxy %s:%s: %s (%s)\" %\n                               (self.proxy, self.proxy_port,\n                                resp.status, resp.reason))\n\n        # We can safely close the response, it duped the original socket\n        resp.close()\n\n        h = http_client.HTTPConnection(host)\n\n        if self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\n            msg = \"wrapping ssl socket for proxied connection; \"\n            if self.ca_certificates_file:\n                msg += \"CA certificate file=%s\" % self.ca_certificates_file\n            else:\n                msg += \"using system provided SSL certs\"\n            boto.log.debug(msg)\n            key_file = self.http_connection_kwargs.get('key_file', None)\n            cert_file = self.http_connection_kwargs.get('cert_file', None)\n            sslSock = ssl.wrap_socket(sock, keyfile=key_file,\n                                      certfile=cert_file,\n                                      cert_reqs=ssl.CERT_REQUIRED,\n                                      ca_certs=self.ca_certificates_file)\n            cert = sslSock.getpeercert()\n            hostname = self.host.split(':', 0)[0]\n            if not https_connection.ValidateCertificateHostname(cert, hostname):\n                raise https_connection.InvalidCertificateException(\n                    hostname, cert, 'hostname mismatch')\n        else:\n            # Fallback for old Python without ssl.wrap_socket\n            if hasattr(http_client, 'ssl'):\n                sslSock = http_client.ssl.SSLSocket(sock)\n            else:\n                sslSock = socket.ssl(sock, None, None)\n                sslSock = http_client.FakeSocket(sock, sslSock)\n\n        # This is a bit unclean\n        h.sock = sslSock\n        return h\n\n    def prefix_proxy_to_path(self, path, host=None):\n        path = self.protocol + '://' + (host or self.server_name()) + path\n        return path\n\n    def get_proxy_auth_header(self):\n        auth = encodebytes(self.proxy_user + ':' + self.proxy_pass)\n        return {'Proxy-Authorization': 'Basic %s' % auth}\n\n    # For passing proxy information to other connection libraries, e.g. cloudsearch2\n    def get_proxy_url_with_auth(self):\n        if not self.use_proxy:\n            return None\n\n        if self.proxy_user or self.proxy_pass:\n            if self.proxy_pass:\n                login_info = '%s:%s@' % (self.proxy_user, self.proxy_pass)\n            else:\n                login_info = '%s@' % self.proxy_user\n        else:\n            login_info = ''\n\n        return 'http://%s%s:%s' % (login_info, self.proxy, str(self.proxy_port or self.port))\n\n    def set_host_header(self, request):\n        try:\n            request.headers['Host'] = \\\n                self._auth_handler.host_header(self.host, request)\n        except AttributeError:\n            request.headers['Host'] = self.host.split(':', 1)[0]\n\n    def set_request_hook(self, hook):\n        self.request_hook = hook\n\n    def _mexe(self, request, sender=None, override_num_retries=None,\n              retry_handler=None):\n        \"\"\"\n        mexe - Multi-execute inside a loop, retrying multiple times to handle\n               transient Internet errors by simply trying again.\n               Also handles redirects.\n\n        This code was inspired by the S3Utils classes posted to the boto-users\n        Google group by Larry Bates.  Thanks!\n\n        \"\"\"\n        boto.log.debug('Method: %s' % request.method)\n        boto.log.debug('Path: %s' % request.path)\n        boto.log.debug('Data: %s' % request.body)\n        boto.log.debug('Headers: %s' % request.headers)\n        boto.log.debug('Host: %s' % request.host)\n        boto.log.debug('Port: %s' % request.port)\n        boto.log.debug('Params: %s' % request.params)\n        response = None\n        body = None\n        ex = None\n        if override_num_retries is None:\n            num_retries = config.getint('Boto', 'num_retries', self.num_retries)\n        else:\n            num_retries = override_num_retries\n        i = 0\n        connection = self.get_http_connection(request.host, request.port,\n                                              self.is_secure)\n\n        # Convert body to bytes if needed\n        if not isinstance(request.body, bytes) and hasattr(request.body,\n                                                           'encode'):\n            request.body = request.body.encode('utf-8')\n\n        while i <= num_retries:\n            # Use binary exponential backoff to desynchronize client requests.\n            next_sleep = min(random.random() * (2 ** i),\n                             boto.config.get('Boto', 'max_retry_delay', 60))\n            try:\n                # we now re-sign each request before it is retried\n                boto.log.debug('Token: %s' % self.provider.security_token)\n                request.authorize(connection=self)\n                # Only force header for non-s3 connections, because s3 uses\n                # an older signing method + bucket resource URLs that include\n                # the port info. All others should be now be up to date and\n                # not include the port.\n                if 's3' not in self._required_auth_capability():\n                    if not getattr(self, 'anon', False):\n                        if not request.headers.get('Host'):\n                            self.set_host_header(request)\n                boto.log.debug('Final headers: %s' % request.headers)\n                request.start_time = datetime.now()\n                if callable(sender):\n                    response = sender(connection, request.method, request.path,\n                                      request.body, request.headers)\n                else:\n                    connection.request(request.method, request.path,\n                                       request.body, request.headers)\n                    response = connection.getresponse()\n                boto.log.debug('Response headers: %s' % response.getheaders())\n                location = response.getheader('location')\n                # -- gross hack --\n                # http_client gets confused with chunked responses to HEAD requests\n                # so I have to fake it out\n                if request.method == 'HEAD' and getattr(response,\n                                                        'chunked', False):\n                    response.chunked = 0\n                if callable(retry_handler):\n                    status = retry_handler(response, i, next_sleep)\n                    if status:\n                        msg, i, next_sleep = status\n                        if msg:\n                            boto.log.debug(msg)\n                        time.sleep(next_sleep)\n                        continue\n                if response.status in [500, 502, 503, 504]:\n                    msg = 'Received %d response.  ' % response.status\n                    msg += 'Retrying in %3.1f seconds' % next_sleep\n                    boto.log.debug(msg)\n                    body = response.read()\n                    if isinstance(body, bytes):\n                        body = body.decode('utf-8')\n                elif response.status < 300 or response.status >= 400 or \\\n                        not location:\n                    # don't return connection to the pool if response contains\n                    # Connection:close header, because the connection has been\n                    # closed and default reconnect behavior may do something\n                    # different than new_http_connection. Also, it's probably\n                    # less efficient to try to reuse a closed connection.\n                    conn_header_value = response.getheader('connection')\n                    if conn_header_value == 'close':\n                        connection.close()\n                    else:\n                        self.put_http_connection(request.host, request.port,\n                                                 self.is_secure, connection)\n                    if self.request_hook is not None:\n                        self.request_hook.handle_request_data(request, response)\n                    return response\n                else:\n                    scheme, request.host, request.path, \\\n                        params, query, fragment = urlparse(location)\n                    if query:\n                        request.path += '?' + query\n                    # urlparse can return both host and port in netloc, so if\n                    # that's the case we need to split them up properly\n                    if ':' in request.host:\n                        request.host, request.port = request.host.split(':', 1)\n                    msg = 'Redirecting: %s' % scheme + '://'\n                    msg += request.host + request.path\n                    boto.log.debug(msg)\n                    connection = self.get_http_connection(request.host,\n                                                          request.port,\n                                                          scheme == 'https')\n                    response = None\n                    continue\n            except PleaseRetryException as e:\n                boto.log.debug('encountered a retry exception: %s' % e)\n                connection = self.new_http_connection(request.host, request.port,\n                                                      self.is_secure)\n                response = e.response\n                ex = e\n            except self.http_exceptions as e:\n                for unretryable in self.http_unretryable_exceptions:\n                    if isinstance(e, unretryable):\n                        boto.log.debug(\n                            'encountered unretryable %s exception, re-raising' %\n                            e.__class__.__name__)\n                        raise\n                boto.log.debug('encountered %s exception, reconnecting' %\n                               e.__class__.__name__)\n                connection = self.new_http_connection(request.host, request.port,\n                                                      self.is_secure)\n                ex = e\n            time.sleep(next_sleep)\n            i += 1\n        # If we made it here, it's because we have exhausted our retries\n        # and stil haven't succeeded.  So, if we have a response object,\n        # use it to raise an exception.\n        # Otherwise, raise the exception that must have already happened.\n        if self.request_hook is not None:\n            self.request_hook.handle_request_data(request, response, error=True)\n        if response:\n            raise BotoServerError(response.status, response.reason, body)\n        elif ex:\n            raise ex\n        else:\n            msg = 'Please report this exception as a Boto Issue!'\n            raise BotoClientError(msg)\n\n    def build_base_http_request(self, method, path, auth_path,\n                                params=None, headers=None, data='', host=None):\n        path = self.get_path(path)\n        if auth_path is not None:\n            auth_path = self.get_path(auth_path)\n        if params is None:\n            params = {}\n        else:\n            params = params.copy()\n        if headers is None:\n            headers = {}\n        else:\n            headers = headers.copy()\n        if self.host_header and not boto.utils.find_matching_headers('host', headers):\n            headers['host'] = self.host_header\n        host = host or self.host\n        if self.use_proxy and not self.skip_proxy(host):\n            if not auth_path:\n                auth_path = path\n            path = self.prefix_proxy_to_path(path, host)\n            if self.proxy_user and self.proxy_pass and not self.is_secure:\n                # If is_secure, we don't have to set the proxy authentication\n                # header here, we did that in the CONNECT to the proxy.\n                headers.update(self.get_proxy_auth_header())\n        return HTTPRequest(method, self.protocol, host, self.port,\n                           path, auth_path, params, headers, data)\n\n    def make_request(self, method, path, headers=None, data='', host=None,\n                     auth_path=None, sender=None, override_num_retries=None,\n                     params=None, retry_handler=None):\n        \"\"\"Makes a request to the server, with stock multiple-retry logic.\"\"\"\n        if params is None:\n            params = {}\n        http_request = self.build_base_http_request(method, path, auth_path,\n                                                    params, headers, data, host)\n        return self._mexe(http_request, sender, override_num_retries,\n                          retry_handler=retry_handler)\n\n    def close(self):\n        \"\"\"(Optional) Close any open HTTP connections.  This is non-destructive,\n        and making a new request will open a connection again.\"\"\"\n\n        boto.log.debug('closing all HTTP connections')\n        self._connection = None  # compat field\n\n\nclass AWSQueryConnection(AWSAuthConnection):\n\n    APIVersion = ''\n    ResponseError = BotoServerError\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, host=None, debug=0,\n                 https_connection_factory=None, path='/', security_token=None,\n                 validate_certs=True, profile_name=None, provider='aws'):\n        super(AWSQueryConnection, self).__init__(\n            host, aws_access_key_id,\n            aws_secret_access_key,\n            is_secure, port, proxy,\n            proxy_port, proxy_user, proxy_pass,\n            debug, https_connection_factory, path,\n            security_token=security_token,\n            validate_certs=validate_certs,\n            profile_name=profile_name,\n            provider=provider)\n\n    def _required_auth_capability(self):\n        return []\n\n    def get_utf8_value(self, value):\n        return boto.utils.get_utf8_value(value)\n\n    def make_request(self, action, params=None, path='/', verb='GET'):\n        http_request = self.build_base_http_request(verb, path, None,\n                                                    params, {}, '',\n                                                    self.host)\n        if action:\n            http_request.params['Action'] = action\n        if self.APIVersion:\n            http_request.params['Version'] = self.APIVersion\n        return self._mexe(http_request)\n\n    def build_list_params(self, params, items, label):\n        if isinstance(items, six.string_types):\n            items = [items]\n        for i in range(1, len(items) + 1):\n            params['%s.%d' % (label, i)] = items[i - 1]\n\n    def build_complex_list_params(self, params, items, label, names):\n        \"\"\"Serialize a list of structures.\n\n        For example::\n\n            items = [('foo', 'bar', 'baz'), ('foo2', 'bar2', 'baz2')]\n            label = 'ParamName.member'\n            names = ('One', 'Two', 'Three')\n            self.build_complex_list_params(params, items, label, names)\n\n        would result in the params dict being updated with these params::\n\n            ParamName.member.1.One = foo\n            ParamName.member.1.Two = bar\n            ParamName.member.1.Three = baz\n\n            ParamName.member.2.One = foo2\n            ParamName.member.2.Two = bar2\n            ParamName.member.2.Three = baz2\n\n        :type params: dict\n        :param params: The params dict.  The complex list params\n            will be added to this dict.\n\n        :type items: list of tuples\n        :param items: The list to serialize.\n\n        :type label: string\n        :param label: The prefix to apply to the parameter.\n\n        :type names: tuple of strings\n        :param names: The names associated with each tuple element.\n\n        \"\"\"\n        for i, item in enumerate(items, 1):\n            current_prefix = '%s.%s' % (label, i)\n            for key, value in zip(names, item):\n                full_key = '%s.%s' % (current_prefix, key)\n                params[full_key] = value\n\n    # generics\n\n    def get_list(self, action, params, markers, path='/',\n                 parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            rs = ResultSet(markers)\n            h = boto.handler.XmlHandler(rs, parent)\n            if isinstance(body, six.text_type):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return rs\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n\n    def get_object(self, action, params, cls, path='/',\n                   parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            obj = cls(parent)\n            h = boto.handler.XmlHandler(obj, parent)\n            if isinstance(body, six.text_type):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return obj\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n\n    def get_status(self, action, params, path='/', parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            rs = ResultSet()\n            h = boto.handler.XmlHandler(rs, parent)\n            xml.sax.parseString(body, h)\n            return rs.status\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n"}, {"boto.connection.AWSAuthConnection.build_base_http_request": "# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\n# Copyright (c) 2010 Google\n# Copyright (c) 2008 rPath, Inc.\n# Copyright (c) 2009 The Echo Nest Corporation\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n# Copyright (c) 2011, Nexenta Systems Inc.\n# All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n#\n# Parts of this code were copied or derived from sample code supplied by AWS.\n# The following notice applies to that code.\n#\n#  This software code is made available \"AS IS\" without warranties of any\n#  kind.  You may copy, display, modify and redistribute the software\n#  code either by itself or as incorporated into your code; provided that\n#  you do not remove any proprietary notices.  Your use of this software\n#  code is at your own risk and you waive any claim against Amazon\n#  Digital Services, Inc. or its affiliates with respect to your use of\n#  this software code. (c) 2006 Amazon Digital Services, Inc. or its\n#  affiliates.\n\n\"\"\"\nHandles basic connections to AWS\n\"\"\"\nfrom datetime import datetime\nimport errno\nimport os\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport xml.sax\nimport copy\n\nfrom boto import auth\nfrom boto import auth_handler\nimport boto\nimport boto.utils\nimport boto.handler\nimport boto.cacerts\n\nfrom boto import config, UserAgent\nfrom boto.compat import six, http_client, urlparse, quote, encodebytes\nfrom boto.exception import AWSConnectionError\nfrom boto.exception import BotoClientError\nfrom boto.exception import BotoServerError\nfrom boto.exception import PleaseRetryException\nfrom boto.provider import Provider\nfrom boto.resultset import ResultSet\n\nHAVE_HTTPS_CONNECTION = False\ntry:\n    import ssl\n    from boto import https_connection\n    # Google App Engine runs on Python 2.5 so doesn't have ssl.SSLError.\n    if hasattr(ssl, 'SSLError'):\n        HAVE_HTTPS_CONNECTION = True\nexcept ImportError:\n    pass\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading\n\nON_APP_ENGINE = all(key in os.environ for key in (\n    'USER_IS_ADMIN', 'CURRENT_VERSION_ID', 'APPLICATION_ID'))\n\nPORTS_BY_SECURITY = {True: 443,\n                     False: 80}\n\nDEFAULT_CA_CERTS_FILE = os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)), \"cacerts.txt\")\n\n\nclass HostConnectionPool(object):\n\n    \"\"\"\n    A pool of connections for one remote (host,port,is_secure).\n\n    When connections are added to the pool, they are put into a\n    pending queue.  The _mexe method returns connections to the pool\n    before the response body has been read, so they connections aren't\n    ready to send another request yet.  They stay in the pending queue\n    until they are ready for another request, at which point they are\n    returned to the pool of ready connections.\n\n    The pool of ready connections is an ordered list of\n    (connection,time) pairs, where the time is the time the connection\n    was returned from _mexe.  After a certain period of time,\n    connections are considered stale, and discarded rather than being\n    reused.  This saves having to wait for the connection to time out\n    if AWS has decided to close it on the other end because of\n    inactivity.\n\n    Thread Safety:\n\n        This class is used only from ConnectionPool while it's mutex\n        is held.\n    \"\"\"\n\n    def __init__(self):\n        self.queue = []\n\n    def size(self):\n        \"\"\"\n        Returns the number of connections in the pool for this host.\n        Some of the connections may still be in use, and may not be\n        ready to be returned by get().\n        \"\"\"\n        return len(self.queue)\n\n    def put(self, conn):\n        \"\"\"\n        Adds a connection to the pool, along with the time it was\n        added.\n        \"\"\"\n        self.queue.append((conn, time.time()))\n\n    def get(self):\n        \"\"\"\n        Returns the next connection in this pool that is ready to be\n        reused.  Returns None if there aren't any.\n        \"\"\"\n        # Discard ready connections that are too old.\n        self.clean()\n\n        # Return the first connection that is ready, and remove it\n        # from the queue.  Connections that aren't ready are returned\n        # to the end of the queue with an updated time, on the\n        # assumption that somebody is actively reading the response.\n        for _ in range(len(self.queue)):\n            (conn, _) = self.queue.pop(0)\n            if self._conn_ready(conn):\n                return conn\n            else:\n                self.put(conn)\n        return None\n\n    def _conn_ready(self, conn):\n        \"\"\"\n        There is a nice state diagram at the top of http_client.py.  It\n        indicates that once the response headers have been read (which\n        _mexe does before adding the connection to the pool), a\n        response is attached to the connection, and it stays there\n        until it's done reading.  This isn't entirely true: even after\n        the client is done reading, the response may be closed, but\n        not removed from the connection yet.\n\n        This is ugly, reading a private instance variable, but the\n        state we care about isn't available in any public methods.\n        \"\"\"\n        if ON_APP_ENGINE:\n            # Google AppEngine implementation of HTTPConnection doesn't contain\n            # _HTTPConnection__response attribute. Moreover, it's not possible\n            # to determine if given connection is ready. Reusing connections\n            # simply doesn't make sense with App Engine urlfetch service.\n            return False\n        else:\n            response = getattr(conn, '_HTTPConnection__response', None)\n            return (response is None) or response.isclosed()\n\n    def clean(self):\n        \"\"\"\n        Get rid of stale connections.\n        \"\"\"\n        # Note that we do not close the connection here -- somebody\n        # may still be reading from it.\n        while len(self.queue) > 0 and self._pair_stale(self.queue[0]):\n            self.queue.pop(0)\n\n    def _pair_stale(self, pair):\n        \"\"\"\n        Returns true of the (connection,time) pair is too old to be\n        used.\n        \"\"\"\n        (_conn, return_time) = pair\n        now = time.time()\n        return return_time + ConnectionPool.STALE_DURATION < now\n\n\nclass ConnectionPool(object):\n\n    \"\"\"\n    A connection pool that expires connections after a fixed period of\n    time.  This saves time spent waiting for a connection that AWS has\n    timed out on the other end.\n\n    This class is thread-safe.\n    \"\"\"\n\n    #\n    # The amout of time between calls to clean.\n    #\n\n    CLEAN_INTERVAL = 5.0\n\n    #\n    # How long before a connection becomes \"stale\" and won't be reused\n    # again.  The intention is that this time is less that the timeout\n    # period that AWS uses, so we'll never try to reuse a connection\n    # and find that AWS is timing it out.\n    #\n    # Experimentation in July 2011 shows that AWS starts timing things\n    # out after three minutes.  The 60 seconds here is conservative so\n    # we should never hit that 3-minute timout.\n    #\n\n    STALE_DURATION = 60.0\n\n    def __init__(self):\n        # Mapping from (host,port,is_secure) to HostConnectionPool.\n        # If a pool becomes empty, it is removed.\n        self.host_to_pool = {}\n        # The last time the pool was cleaned.\n        self.last_clean_time = 0.0\n        self.mutex = threading.Lock()\n        ConnectionPool.STALE_DURATION = \\\n            config.getfloat('Boto', 'connection_stale_duration',\n                            ConnectionPool.STALE_DURATION)\n\n    def __getstate__(self):\n        pickled_dict = copy.copy(self.__dict__)\n        pickled_dict['host_to_pool'] = {}\n        del pickled_dict['mutex']\n        return pickled_dict\n\n    def __setstate__(self, dct):\n        self.__init__()\n\n    def size(self):\n        \"\"\"\n        Returns the number of connections in the pool.\n        \"\"\"\n        return sum(pool.size() for pool in self.host_to_pool.values())\n\n    def get_http_connection(self, host, port, is_secure):\n        \"\"\"\n        Gets a connection from the pool for the named host.  Returns\n        None if there is no connection that can be reused. It's the caller's\n        responsibility to call close() on the connection when it's no longer\n        needed.\n        \"\"\"\n        self.clean()\n        with self.mutex:\n            key = (host, port, is_secure)\n            if key not in self.host_to_pool:\n                return None\n            return self.host_to_pool[key].get()\n\n    def put_http_connection(self, host, port, is_secure, conn):\n        \"\"\"\n        Adds a connection to the pool of connections that can be\n        reused for the named host.\n        \"\"\"\n        with self.mutex:\n            key = (host, port, is_secure)\n            if key not in self.host_to_pool:\n                self.host_to_pool[key] = HostConnectionPool()\n            self.host_to_pool[key].put(conn)\n\n    def clean(self):\n        \"\"\"\n        Clean up the stale connections in all of the pools, and then\n        get rid of empty pools.  Pools clean themselves every time a\n        connection is fetched; this cleaning takes care of pools that\n        aren't being used any more, so nothing is being gotten from\n        them.\n        \"\"\"\n        with self.mutex:\n            now = time.time()\n            if self.last_clean_time + self.CLEAN_INTERVAL < now:\n                to_remove = []\n                for (host, pool) in self.host_to_pool.items():\n                    pool.clean()\n                    if pool.size() == 0:\n                        to_remove.append(host)\n                for host in to_remove:\n                    del self.host_to_pool[host]\n                self.last_clean_time = now\n\n\nclass HTTPRequest(object):\n\n    def __init__(self, method, protocol, host, port, path, auth_path,\n                 params, headers, body):\n        \"\"\"Represents an HTTP request.\n\n        :type method: string\n        :param method: The HTTP method name, 'GET', 'POST', 'PUT' etc.\n\n        :type protocol: string\n        :param protocol: The http protocol used, 'http' or 'https'.\n\n        :type host: string\n        :param host: Host to which the request is addressed. eg. abc.com\n\n        :type port: int\n        :param port: port on which the request is being sent. Zero means unset,\n            in which case default port will be chosen.\n\n        :type path: string\n        :param path: URL path that is being accessed.\n\n        :type auth_path: string\n        :param path: The part of the URL path used when creating the\n            authentication string.\n\n        :type params: dict\n        :param params: HTTP url query parameters, with key as name of\n            the param, and value as value of param.\n\n        :type headers: dict\n        :param headers: HTTP headers, with key as name of the header and value\n            as value of header.\n\n        :type body: string\n        :param body: Body of the HTTP request. If not present, will be None or\n            empty string ('').\n        \"\"\"\n        self.method = method\n        self.protocol = protocol\n        self.host = host\n        self.port = port\n        self.path = path\n        if auth_path is None:\n            auth_path = path\n        self.auth_path = auth_path\n        self.params = params\n        # chunked Transfer-Encoding should act only on PUT request.\n        if headers and 'Transfer-Encoding' in headers and \\\n                headers['Transfer-Encoding'] == 'chunked' and \\\n                self.method != 'PUT':\n            self.headers = headers.copy()\n            del self.headers['Transfer-Encoding']\n        else:\n            self.headers = headers\n        self.body = body\n\n    def __str__(self):\n        return (('method:(%s) protocol:(%s) host(%s) port(%s) path(%s) '\n                 'params(%s) headers(%s) body(%s)') % (self.method,\n                 self.protocol, self.host, self.port, self.path, self.params,\n                 self.headers, self.body))\n\n    def authorize(self, connection, **kwargs):\n        if not getattr(self, '_headers_quoted', False):\n            for key in self.headers:\n                val = self.headers[key]\n                if isinstance(val, six.text_type):\n                    safe = '!\"#$%&\\'()*+,/:;<=>?@[\\\\]^`{|}~ '\n                    self.headers[key] = quote(val.encode('utf-8'), safe)\n            setattr(self, '_headers_quoted', True)\n\n        self.headers['User-Agent'] = UserAgent\n\n        connection._auth_handler.add_auth(self, **kwargs)\n\n        # I'm not sure if this is still needed, now that add_auth is\n        # setting the content-length for POST requests.\n        if 'Content-Length' not in self.headers:\n            if 'Transfer-Encoding' not in self.headers or \\\n                    self.headers['Transfer-Encoding'] != 'chunked':\n                self.headers['Content-Length'] = str(len(self.body))\n\n\nclass HTTPResponse(http_client.HTTPResponse):\n\n    def __init__(self, *args, **kwargs):\n        http_client.HTTPResponse.__init__(self, *args, **kwargs)\n        self._cached_response = ''\n\n    def read(self, amt=None):\n        \"\"\"Read the response.\n\n        This method does not have the same behavior as\n        http_client.HTTPResponse.read.  Instead, if this method is called with\n        no ``amt`` arg, then the response body will be cached.  Subsequent\n        calls to ``read()`` with no args **will return the cached response**.\n\n        \"\"\"\n        if amt is None:\n            # The reason for doing this is that many places in boto call\n            # response.read() and except to get the response body that they\n            # can then process.  To make sure this always works as they expect\n            # we're caching the response so that multiple calls to read()\n            # will return the full body.  Note that this behavior only\n            # happens if the amt arg is not specified.\n            if not self._cached_response:\n                self._cached_response = http_client.HTTPResponse.read(self)\n            return self._cached_response\n        else:\n            return http_client.HTTPResponse.read(self, amt)\n\n\nclass AWSAuthConnection(object):\n    def __init__(self, host, aws_access_key_id=None,\n                 aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, debug=0,\n                 https_connection_factory=None, path='/',\n                 provider='aws', security_token=None,\n                 suppress_consec_slashes=True,\n                 validate_certs=True, profile_name=None):\n        \"\"\"\n        :type host: str\n        :param host: The host to make the connection to\n\n        :keyword str aws_access_key_id: Your AWS Access Key ID (provided by\n            Amazon). If none is specified, the value in your\n            ``AWS_ACCESS_KEY_ID`` environmental variable is used.\n        :keyword str aws_secret_access_key: Your AWS Secret Access Key\n            (provided by Amazon). If none is specified, the value in your\n            ``AWS_SECRET_ACCESS_KEY`` environmental variable is used.\n        :keyword str security_token: The security token associated with\n            temporary credentials issued by STS.  Optional unless using\n            temporary credentials.  If none is specified, the environment\n            variable ``AWS_SECURITY_TOKEN`` is used if defined.\n\n        :type is_secure: boolean\n        :param is_secure: Whether the connection is over SSL\n\n        :type https_connection_factory: list or tuple\n        :param https_connection_factory: A pair of an HTTP connection\n            factory and the exceptions to catch.  The factory should have\n            a similar interface to L{http_client.HTTPSConnection}.\n\n        :param str proxy: Address/hostname for a proxy server\n\n        :type proxy_port: int\n        :param proxy_port: The port to use when connecting over a proxy\n\n        :type proxy_user: str\n        :param proxy_user: The username to connect with on the proxy\n\n        :type proxy_pass: str\n        :param proxy_pass: The password to use when connection over a proxy.\n\n        :type port: int\n        :param port: The port to use to connect\n\n        :type suppress_consec_slashes: bool\n        :param suppress_consec_slashes: If provided, controls whether\n            consecutive slashes will be suppressed in key paths.\n\n        :type validate_certs: bool\n        :param validate_certs: Controls whether SSL certificates\n            will be validated or not.  Defaults to True.\n\n        :type profile_name: str\n        :param profile_name: Override usual Credentials section in config\n            file to use a named set of keys instead.\n        \"\"\"\n        self.suppress_consec_slashes = suppress_consec_slashes\n        self.num_retries = 6\n        # Override passed-in is_secure setting if value was defined in config.\n        if config.has_option('Boto', 'is_secure'):\n            is_secure = config.getboolean('Boto', 'is_secure')\n        self.is_secure = is_secure\n        # Whether or not to validate server certificates.\n        # The default is now to validate certificates.  This can be\n        # overridden in the boto config file are by passing an\n        # explicit validate_certs parameter to the class constructor.\n        self.https_validate_certificates = config.getbool(\n            'Boto', 'https_validate_certificates',\n            validate_certs)\n        if self.https_validate_certificates and not HAVE_HTTPS_CONNECTION:\n            raise BotoClientError(\n                \"SSL server certificate validation is enabled in boto \"\n                \"configuration, but Python dependencies required to \"\n                \"support this feature are not available. Certificate \"\n                \"validation is only supported when running under Python \"\n                \"2.6 or later.\")\n        certs_file = config.get_value(\n            'Boto', 'ca_certificates_file', DEFAULT_CA_CERTS_FILE)\n        if certs_file == 'system':\n            certs_file = None\n        self.ca_certificates_file = certs_file\n        if port:\n            self.port = port\n        else:\n            self.port = PORTS_BY_SECURITY[is_secure]\n\n        self.handle_proxy(proxy, proxy_port, proxy_user, proxy_pass)\n        # define exceptions from http_client that we want to catch and retry\n        self.http_exceptions = (http_client.HTTPException, socket.error,\n                                socket.gaierror, http_client.BadStatusLine)\n        # define subclasses of the above that are not retryable.\n        self.http_unretryable_exceptions = []\n        if HAVE_HTTPS_CONNECTION:\n            self.http_unretryable_exceptions.append(\n                https_connection.InvalidCertificateException)\n\n        # define values in socket exceptions we don't want to catch\n        self.socket_exception_values = (errno.EINTR,)\n        if https_connection_factory is not None:\n            self.https_connection_factory = https_connection_factory[0]\n            self.http_exceptions += https_connection_factory[1]\n        else:\n            self.https_connection_factory = None\n        if (is_secure):\n            self.protocol = 'https'\n        else:\n            self.protocol = 'http'\n        self.host = host\n        self.path = path\n        # if the value passed in for debug\n        if not isinstance(debug, six.integer_types):\n            debug = 0\n        self.debug = config.getint('Boto', 'debug', debug)\n        self.host_header = None\n\n        # Timeout used to tell http_client how long to wait for socket timeouts.\n        # Default is to leave timeout unchanged, which will in turn result in\n        # the socket's default global timeout being used. To specify a\n        # timeout, set http_socket_timeout in Boto config. Regardless,\n        # timeouts will only be applied if Python is 2.6 or greater.\n        self.http_connection_kwargs = {}\n        if (sys.version_info[0], sys.version_info[1]) >= (2, 6):\n            # If timeout isn't defined in boto config file, use 70 second\n            # default as recommended by\n            # http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForActivityTask.html\n            self.http_connection_kwargs['timeout'] = config.getint(\n                'Boto', 'http_socket_timeout', 70)\n\n        if isinstance(provider, Provider):\n            # Allow overriding Provider\n            self.provider = provider\n        else:\n            self._provider_type = provider\n            self.provider = Provider(self._provider_type,\n                                     aws_access_key_id,\n                                     aws_secret_access_key,\n                                     security_token,\n                                     profile_name)\n\n        # Allow config file to override default host, port, and host header.\n        if self.provider.host:\n            self.host = self.provider.host\n        if self.provider.port:\n            self.port = self.provider.port\n        if self.provider.host_header:\n            self.host_header = self.provider.host_header\n\n        self._pool = ConnectionPool()\n        self._connection = (self.host, self.port, self.is_secure)\n        self._last_rs = None\n        self._auth_handler = auth.get_auth_handler(\n            host, config, self.provider, self._required_auth_capability())\n        if getattr(self, 'AuthServiceName', None) is not None:\n            self.auth_service_name = self.AuthServiceName\n        self.request_hook = None\n\n    def __repr__(self):\n        return '%s:%s' % (self.__class__.__name__, self.host)\n\n    def _required_auth_capability(self):\n        return []\n\n    def _get_auth_service_name(self):\n        return getattr(self._auth_handler, 'service_name')\n\n    # For Sigv4, the auth_service_name/auth_region_name properties allow\n    # the service_name/region_name to be explicitly set instead of being\n    # derived from the endpoint url.\n    def _set_auth_service_name(self, value):\n        self._auth_handler.service_name = value\n    auth_service_name = property(_get_auth_service_name, _set_auth_service_name)\n\n    def _get_auth_region_name(self):\n        return getattr(self._auth_handler, 'region_name')\n\n    def _set_auth_region_name(self, value):\n        self._auth_handler.region_name = value\n    auth_region_name = property(_get_auth_region_name, _set_auth_region_name)\n\n    def connection(self):\n        return self.get_http_connection(*self._connection)\n    connection = property(connection)\n\n    def aws_access_key_id(self):\n        return self.provider.access_key\n    aws_access_key_id = property(aws_access_key_id)\n    gs_access_key_id = aws_access_key_id\n    access_key = aws_access_key_id\n\n    def aws_secret_access_key(self):\n        return self.provider.secret_key\n    aws_secret_access_key = property(aws_secret_access_key)\n    gs_secret_access_key = aws_secret_access_key\n    secret_key = aws_secret_access_key\n\n    def profile_name(self):\n        return self.provider.profile_name\n    profile_name = property(profile_name)\n\n    def get_path(self, path='/'):\n        # The default behavior is to suppress consecutive slashes for reasons\n        # discussed at\n        # https://groups.google.com/forum/#!topic/boto-dev/-ft0XPUy0y8\n        # You can override that behavior with the suppress_consec_slashes param.\n        if not self.suppress_consec_slashes:\n            return self.path + re.sub('^(/*)/', \"\\\\1\", path)\n        pos = path.find('?')\n        if pos >= 0:\n            params = path[pos:]\n            path = path[:pos]\n        else:\n            params = None\n        if path[-1] == '/':\n            need_trailing = True\n        else:\n            need_trailing = False\n        path_elements = self.path.split('/')\n        path_elements.extend(path.split('/'))\n        path_elements = [p for p in path_elements if p]\n        path = '/' + '/'.join(path_elements)\n        if path[-1] != '/' and need_trailing:\n            path += '/'\n        if params:\n            path = path + params\n        return path\n\n    def server_name(self, port=None):\n        if not port:\n            port = self.port\n        if port == 80:\n            signature_host = self.host\n        else:\n            # This unfortunate little hack can be attributed to\n            # a difference in the 2.6 version of http_client.  In old\n            # versions, it would append \":443\" to the hostname sent\n            # in the Host header and so we needed to make sure we\n            # did the same when calculating the V2 signature.  In 2.6\n            # (and higher!)\n            # it no longer does that.  Hence, this kludge.\n            if ((ON_APP_ENGINE and sys.version[:3] == '2.5') or\n                    sys.version[:3] in ('2.6', '2.7')) and port == 443:\n                signature_host = self.host\n            else:\n                signature_host = '%s:%d' % (self.host, port)\n        return signature_host\n\n    def handle_proxy(self, proxy, proxy_port, proxy_user, proxy_pass):\n        self.proxy = proxy\n        self.proxy_port = proxy_port\n        self.proxy_user = proxy_user\n        self.proxy_pass = proxy_pass\n        if 'http_proxy' in os.environ and not self.proxy:\n            pattern = re.compile(\n                '(?:http://)?'\n                '(?:(?P<user>[\\w\\-\\.]+):(?P<pass>.*)@)?'\n                '(?P<host>[\\w\\-\\.]+)'\n                '(?::(?P<port>\\d+))?'\n            )\n            match = pattern.match(os.environ['http_proxy'])\n            if match:\n                self.proxy = match.group('host')\n                self.proxy_port = match.group('port')\n                self.proxy_user = match.group('user')\n                self.proxy_pass = match.group('pass')\n        else:\n            if not self.proxy:\n                self.proxy = config.get_value('Boto', 'proxy', None)\n            if not self.proxy_port:\n                self.proxy_port = config.get_value('Boto', 'proxy_port', None)\n            if not self.proxy_user:\n                self.proxy_user = config.get_value('Boto', 'proxy_user', None)\n            if not self.proxy_pass:\n                self.proxy_pass = config.get_value('Boto', 'proxy_pass', None)\n\n        if not self.proxy_port and self.proxy:\n            print(\"http_proxy environment variable does not specify \"\n                  \"a port, using default\")\n            self.proxy_port = self.port\n\n        self.no_proxy = os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', '')\n        self.use_proxy = (self.proxy is not None)\n\n    def get_http_connection(self, host, port, is_secure):\n        conn = self._pool.get_http_connection(host, port, is_secure)\n        if conn is not None:\n            return conn\n        else:\n            return self.new_http_connection(host, port, is_secure)\n\n    def skip_proxy(self, host):\n        if not self.no_proxy:\n            return False\n\n        if self.no_proxy == \"*\":\n            return True\n\n        hostonly = host\n        hostonly = host.split(':')[0]\n\n        for name in self.no_proxy.split(','):\n            if name and (hostonly.endswith(name) or host.endswith(name)):\n                return True\n\n        return False\n\n    def new_http_connection(self, host, port, is_secure):\n        if host is None:\n            host = self.server_name()\n\n        # Make sure the host is really just the host, not including\n        # the port number\n        host = boto.utils.parse_host(host)\n\n        http_connection_kwargs = self.http_connection_kwargs.copy()\n\n        # Connection factories below expect a port keyword argument\n        http_connection_kwargs['port'] = port\n\n        # Override host with proxy settings if needed\n        if self.use_proxy and not is_secure and \\\n                not self.skip_proxy(host):\n            host = self.proxy\n            http_connection_kwargs['port'] = int(self.proxy_port)\n\n        if is_secure:\n            boto.log.debug(\n                'establishing HTTPS connection: host=%s, kwargs=%s',\n                host, http_connection_kwargs)\n            if self.use_proxy and not self.skip_proxy(host):\n                connection = self.proxy_ssl(host, is_secure and 443 or 80)\n            elif self.https_connection_factory:\n                connection = self.https_connection_factory(host)\n            elif self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\n                connection = https_connection.CertValidatingHTTPSConnection(\n                    host, ca_certs=self.ca_certificates_file,\n                    **http_connection_kwargs)\n            else:\n                connection = http_client.HTTPSConnection(\n                    host, **http_connection_kwargs)\n        else:\n            boto.log.debug('establishing HTTP connection: kwargs=%s' %\n                           http_connection_kwargs)\n            if self.https_connection_factory:\n                # even though the factory says https, this is too handy\n                # to not be able to allow overriding for http also.\n                connection = self.https_connection_factory(\n                    host, **http_connection_kwargs)\n            else:\n                connection = http_client.HTTPConnection(\n                    host, **http_connection_kwargs)\n        if self.debug > 1:\n            connection.set_debuglevel(self.debug)\n        # self.connection must be maintained for backwards-compatibility\n        # however, it must be dynamically pulled from the connection pool\n        # set a private variable which will enable that\n        if host.split(':')[0] == self.host and is_secure == self.is_secure:\n            self._connection = (host, port, is_secure)\n        # Set the response class of the http connection to use our custom\n        # class.\n        connection.response_class = HTTPResponse\n        return connection\n\n    def put_http_connection(self, host, port, is_secure, connection):\n        self._pool.put_http_connection(host, port, is_secure, connection)\n\n    def proxy_ssl(self, host=None, port=None):\n        if host and port:\n            host = '%s:%d' % (host, port)\n        else:\n            host = '%s:%d' % (self.host, self.port)\n        # Seems properly to use timeout for connect too\n        timeout = self.http_connection_kwargs.get(\"timeout\")\n        if timeout is not None:\n            sock = socket.create_connection((self.proxy,\n                                             int(self.proxy_port)), timeout)\n        else:\n            sock = socket.create_connection((self.proxy, int(self.proxy_port)))\n        boto.log.debug(\"Proxy connection: CONNECT %s HTTP/1.0\\r\\n\", host)\n        sock.sendall(\"CONNECT %s HTTP/1.0\\r\\n\" % host)\n        sock.sendall(\"User-Agent: %s\\r\\n\" % UserAgent)\n        if self.proxy_user and self.proxy_pass:\n            for k, v in self.get_proxy_auth_header().items():\n                sock.sendall(\"%s: %s\\r\\n\" % (k, v))\n            # See discussion about this config option at\n            # https://groups.google.com/forum/?fromgroups#!topic/boto-dev/teenFvOq2Cc\n            if config.getbool('Boto', 'send_crlf_after_proxy_auth_headers', False):\n                sock.sendall(\"\\r\\n\")\n        else:\n            sock.sendall(\"\\r\\n\")\n        resp = http_client.HTTPResponse(sock, strict=True, debuglevel=self.debug)\n        resp.begin()\n\n        if resp.status != 200:\n            # Fake a socket error, use a code that make it obvious it hasn't\n            # been generated by the socket library\n            raise socket.error(-71,\n                               \"Error talking to HTTP proxy %s:%s: %s (%s)\" %\n                               (self.proxy, self.proxy_port,\n                                resp.status, resp.reason))\n\n        # We can safely close the response, it duped the original socket\n        resp.close()\n\n        h = http_client.HTTPConnection(host)\n\n        if self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\n            msg = \"wrapping ssl socket for proxied connection; \"\n            if self.ca_certificates_file:\n                msg += \"CA certificate file=%s\" % self.ca_certificates_file\n            else:\n                msg += \"using system provided SSL certs\"\n            boto.log.debug(msg)\n            key_file = self.http_connection_kwargs.get('key_file', None)\n            cert_file = self.http_connection_kwargs.get('cert_file', None)\n            sslSock = ssl.wrap_socket(sock, keyfile=key_file,\n                                      certfile=cert_file,\n                                      cert_reqs=ssl.CERT_REQUIRED,\n                                      ca_certs=self.ca_certificates_file)\n            cert = sslSock.getpeercert()\n            hostname = self.host.split(':', 0)[0]\n            if not https_connection.ValidateCertificateHostname(cert, hostname):\n                raise https_connection.InvalidCertificateException(\n                    hostname, cert, 'hostname mismatch')\n        else:\n            # Fallback for old Python without ssl.wrap_socket\n            if hasattr(http_client, 'ssl'):\n                sslSock = http_client.ssl.SSLSocket(sock)\n            else:\n                sslSock = socket.ssl(sock, None, None)\n                sslSock = http_client.FakeSocket(sock, sslSock)\n\n        # This is a bit unclean\n        h.sock = sslSock\n        return h\n\n    def prefix_proxy_to_path(self, path, host=None):\n        path = self.protocol + '://' + (host or self.server_name()) + path\n        return path\n\n    def get_proxy_auth_header(self):\n        auth = encodebytes(self.proxy_user + ':' + self.proxy_pass)\n        return {'Proxy-Authorization': 'Basic %s' % auth}\n\n    # For passing proxy information to other connection libraries, e.g. cloudsearch2\n    def get_proxy_url_with_auth(self):\n        if not self.use_proxy:\n            return None\n\n        if self.proxy_user or self.proxy_pass:\n            if self.proxy_pass:\n                login_info = '%s:%s@' % (self.proxy_user, self.proxy_pass)\n            else:\n                login_info = '%s@' % self.proxy_user\n        else:\n            login_info = ''\n\n        return 'http://%s%s:%s' % (login_info, self.proxy, str(self.proxy_port or self.port))\n\n    def set_host_header(self, request):\n        try:\n            request.headers['Host'] = \\\n                self._auth_handler.host_header(self.host, request)\n        except AttributeError:\n            request.headers['Host'] = self.host.split(':', 1)[0]\n\n    def set_request_hook(self, hook):\n        self.request_hook = hook\n\n    def _mexe(self, request, sender=None, override_num_retries=None,\n              retry_handler=None):\n        \"\"\"\n        mexe - Multi-execute inside a loop, retrying multiple times to handle\n               transient Internet errors by simply trying again.\n               Also handles redirects.\n\n        This code was inspired by the S3Utils classes posted to the boto-users\n        Google group by Larry Bates.  Thanks!\n\n        \"\"\"\n        boto.log.debug('Method: %s' % request.method)\n        boto.log.debug('Path: %s' % request.path)\n        boto.log.debug('Data: %s' % request.body)\n        boto.log.debug('Headers: %s' % request.headers)\n        boto.log.debug('Host: %s' % request.host)\n        boto.log.debug('Port: %s' % request.port)\n        boto.log.debug('Params: %s' % request.params)\n        response = None\n        body = None\n        ex = None\n        if override_num_retries is None:\n            num_retries = config.getint('Boto', 'num_retries', self.num_retries)\n        else:\n            num_retries = override_num_retries\n        i = 0\n        connection = self.get_http_connection(request.host, request.port,\n                                              self.is_secure)\n\n        # Convert body to bytes if needed\n        if not isinstance(request.body, bytes) and hasattr(request.body,\n                                                           'encode'):\n            request.body = request.body.encode('utf-8')\n\n        while i <= num_retries:\n            # Use binary exponential backoff to desynchronize client requests.\n            next_sleep = min(random.random() * (2 ** i),\n                             boto.config.get('Boto', 'max_retry_delay', 60))\n            try:\n                # we now re-sign each request before it is retried\n                boto.log.debug('Token: %s' % self.provider.security_token)\n                request.authorize(connection=self)\n                # Only force header for non-s3 connections, because s3 uses\n                # an older signing method + bucket resource URLs that include\n                # the port info. All others should be now be up to date and\n                # not include the port.\n                if 's3' not in self._required_auth_capability():\n                    if not getattr(self, 'anon', False):\n                        if not request.headers.get('Host'):\n                            self.set_host_header(request)\n                boto.log.debug('Final headers: %s' % request.headers)\n                request.start_time = datetime.now()\n                if callable(sender):\n                    response = sender(connection, request.method, request.path,\n                                      request.body, request.headers)\n                else:\n                    connection.request(request.method, request.path,\n                                       request.body, request.headers)\n                    response = connection.getresponse()\n                boto.log.debug('Response headers: %s' % response.getheaders())\n                location = response.getheader('location')\n                # -- gross hack --\n                # http_client gets confused with chunked responses to HEAD requests\n                # so I have to fake it out\n                if request.method == 'HEAD' and getattr(response,\n                                                        'chunked', False):\n                    response.chunked = 0\n                if callable(retry_handler):\n                    status = retry_handler(response, i, next_sleep)\n                    if status:\n                        msg, i, next_sleep = status\n                        if msg:\n                            boto.log.debug(msg)\n                        time.sleep(next_sleep)\n                        continue\n                if response.status in [500, 502, 503, 504]:\n                    msg = 'Received %d response.  ' % response.status\n                    msg += 'Retrying in %3.1f seconds' % next_sleep\n                    boto.log.debug(msg)\n                    body = response.read()\n                    if isinstance(body, bytes):\n                        body = body.decode('utf-8')\n                elif response.status < 300 or response.status >= 400 or \\\n                        not location:\n                    # don't return connection to the pool if response contains\n                    # Connection:close header, because the connection has been\n                    # closed and default reconnect behavior may do something\n                    # different than new_http_connection. Also, it's probably\n                    # less efficient to try to reuse a closed connection.\n                    conn_header_value = response.getheader('connection')\n                    if conn_header_value == 'close':\n                        connection.close()\n                    else:\n                        self.put_http_connection(request.host, request.port,\n                                                 self.is_secure, connection)\n                    if self.request_hook is not None:\n                        self.request_hook.handle_request_data(request, response)\n                    return response\n                else:\n                    scheme, request.host, request.path, \\\n                        params, query, fragment = urlparse(location)\n                    if query:\n                        request.path += '?' + query\n                    # urlparse can return both host and port in netloc, so if\n                    # that's the case we need to split them up properly\n                    if ':' in request.host:\n                        request.host, request.port = request.host.split(':', 1)\n                    msg = 'Redirecting: %s' % scheme + '://'\n                    msg += request.host + request.path\n                    boto.log.debug(msg)\n                    connection = self.get_http_connection(request.host,\n                                                          request.port,\n                                                          scheme == 'https')\n                    response = None\n                    continue\n            except PleaseRetryException as e:\n                boto.log.debug('encountered a retry exception: %s' % e)\n                connection = self.new_http_connection(request.host, request.port,\n                                                      self.is_secure)\n                response = e.response\n                ex = e\n            except self.http_exceptions as e:\n                for unretryable in self.http_unretryable_exceptions:\n                    if isinstance(e, unretryable):\n                        boto.log.debug(\n                            'encountered unretryable %s exception, re-raising' %\n                            e.__class__.__name__)\n                        raise\n                boto.log.debug('encountered %s exception, reconnecting' %\n                               e.__class__.__name__)\n                connection = self.new_http_connection(request.host, request.port,\n                                                      self.is_secure)\n                ex = e\n            time.sleep(next_sleep)\n            i += 1\n        # If we made it here, it's because we have exhausted our retries\n        # and stil haven't succeeded.  So, if we have a response object,\n        # use it to raise an exception.\n        # Otherwise, raise the exception that must have already happened.\n        if self.request_hook is not None:\n            self.request_hook.handle_request_data(request, response, error=True)\n        if response:\n            raise BotoServerError(response.status, response.reason, body)\n        elif ex:\n            raise ex\n        else:\n            msg = 'Please report this exception as a Boto Issue!'\n            raise BotoClientError(msg)\n\n    def build_base_http_request(self, method, path, auth_path,\n                                params=None, headers=None, data='', host=None):\n        path = self.get_path(path)\n        if auth_path is not None:\n            auth_path = self.get_path(auth_path)\n        if params is None:\n            params = {}\n        else:\n            params = params.copy()\n        if headers is None:\n            headers = {}\n        else:\n            headers = headers.copy()\n        if self.host_header and not boto.utils.find_matching_headers('host', headers):\n            headers['host'] = self.host_header\n        host = host or self.host\n        if self.use_proxy and not self.skip_proxy(host):\n            if not auth_path:\n                auth_path = path\n            path = self.prefix_proxy_to_path(path, host)\n            if self.proxy_user and self.proxy_pass and not self.is_secure:\n                # If is_secure, we don't have to set the proxy authentication\n                # header here, we did that in the CONNECT to the proxy.\n                headers.update(self.get_proxy_auth_header())\n        return HTTPRequest(method, self.protocol, host, self.port,\n                           path, auth_path, params, headers, data)\n\n    def make_request(self, method, path, headers=None, data='', host=None,\n                     auth_path=None, sender=None, override_num_retries=None,\n                     params=None, retry_handler=None):\n        \"\"\"Makes a request to the server, with stock multiple-retry logic.\"\"\"\n        if params is None:\n            params = {}\n        http_request = self.build_base_http_request(method, path, auth_path,\n                                                    params, headers, data, host)\n        return self._mexe(http_request, sender, override_num_retries,\n                          retry_handler=retry_handler)\n\n    def close(self):\n        \"\"\"(Optional) Close any open HTTP connections.  This is non-destructive,\n        and making a new request will open a connection again.\"\"\"\n\n        boto.log.debug('closing all HTTP connections')\n        self._connection = None  # compat field\n\n\nclass AWSQueryConnection(AWSAuthConnection):\n\n    APIVersion = ''\n    ResponseError = BotoServerError\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, host=None, debug=0,\n                 https_connection_factory=None, path='/', security_token=None,\n                 validate_certs=True, profile_name=None, provider='aws'):\n        super(AWSQueryConnection, self).__init__(\n            host, aws_access_key_id,\n            aws_secret_access_key,\n            is_secure, port, proxy,\n            proxy_port, proxy_user, proxy_pass,\n            debug, https_connection_factory, path,\n            security_token=security_token,\n            validate_certs=validate_certs,\n            profile_name=profile_name,\n            provider=provider)\n\n    def _required_auth_capability(self):\n        return []\n\n    def get_utf8_value(self, value):\n        return boto.utils.get_utf8_value(value)\n\n    def make_request(self, action, params=None, path='/', verb='GET'):\n        http_request = self.build_base_http_request(verb, path, None,\n                                                    params, {}, '',\n                                                    self.host)\n        if action:\n            http_request.params['Action'] = action\n        if self.APIVersion:\n            http_request.params['Version'] = self.APIVersion\n        return self._mexe(http_request)\n\n    def build_list_params(self, params, items, label):\n        if isinstance(items, six.string_types):\n            items = [items]\n        for i in range(1, len(items) + 1):\n            params['%s.%d' % (label, i)] = items[i - 1]\n\n    def build_complex_list_params(self, params, items, label, names):\n        \"\"\"Serialize a list of structures.\n\n        For example::\n\n            items = [('foo', 'bar', 'baz'), ('foo2', 'bar2', 'baz2')]\n            label = 'ParamName.member'\n            names = ('One', 'Two', 'Three')\n            self.build_complex_list_params(params, items, label, names)\n\n        would result in the params dict being updated with these params::\n\n            ParamName.member.1.One = foo\n            ParamName.member.1.Two = bar\n            ParamName.member.1.Three = baz\n\n            ParamName.member.2.One = foo2\n            ParamName.member.2.Two = bar2\n            ParamName.member.2.Three = baz2\n\n        :type params: dict\n        :param params: The params dict.  The complex list params\n            will be added to this dict.\n\n        :type items: list of tuples\n        :param items: The list to serialize.\n\n        :type label: string\n        :param label: The prefix to apply to the parameter.\n\n        :type names: tuple of strings\n        :param names: The names associated with each tuple element.\n\n        \"\"\"\n        for i, item in enumerate(items, 1):\n            current_prefix = '%s.%s' % (label, i)\n            for key, value in zip(names, item):\n                full_key = '%s.%s' % (current_prefix, key)\n                params[full_key] = value\n\n    # generics\n\n    def get_list(self, action, params, markers, path='/',\n                 parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            rs = ResultSet(markers)\n            h = boto.handler.XmlHandler(rs, parent)\n            if isinstance(body, six.text_type):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return rs\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n\n    def get_object(self, action, params, cls, path='/',\n                   parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            obj = cls(parent)\n            h = boto.handler.XmlHandler(obj, parent)\n            if isinstance(body, six.text_type):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return obj\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n\n    def get_status(self, action, params, path='/', parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            rs = ResultSet()\n            h = boto.handler.XmlHandler(rs, parent)\n            xml.sax.parseString(body, h)\n            return rs.status\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n"}, {"boto.connection.AWSAuthConnection.server_name": "# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\n# Copyright (c) 2010 Google\n# Copyright (c) 2008 rPath, Inc.\n# Copyright (c) 2009 The Echo Nest Corporation\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n# Copyright (c) 2011, Nexenta Systems Inc.\n# All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n#\n# Parts of this code were copied or derived from sample code supplied by AWS.\n# The following notice applies to that code.\n#\n#  This software code is made available \"AS IS\" without warranties of any\n#  kind.  You may copy, display, modify and redistribute the software\n#  code either by itself or as incorporated into your code; provided that\n#  you do not remove any proprietary notices.  Your use of this software\n#  code is at your own risk and you waive any claim against Amazon\n#  Digital Services, Inc. or its affiliates with respect to your use of\n#  this software code. (c) 2006 Amazon Digital Services, Inc. or its\n#  affiliates.\n\n\"\"\"\nHandles basic connections to AWS\n\"\"\"\nfrom datetime import datetime\nimport errno\nimport os\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport xml.sax\nimport copy\n\nfrom boto import auth\nfrom boto import auth_handler\nimport boto\nimport boto.utils\nimport boto.handler\nimport boto.cacerts\n\nfrom boto import config, UserAgent\nfrom boto.compat import six, http_client, urlparse, quote, encodebytes\nfrom boto.exception import AWSConnectionError\nfrom boto.exception import BotoClientError\nfrom boto.exception import BotoServerError\nfrom boto.exception import PleaseRetryException\nfrom boto.provider import Provider\nfrom boto.resultset import ResultSet\n\nHAVE_HTTPS_CONNECTION = False\ntry:\n    import ssl\n    from boto import https_connection\n    # Google App Engine runs on Python 2.5 so doesn't have ssl.SSLError.\n    if hasattr(ssl, 'SSLError'):\n        HAVE_HTTPS_CONNECTION = True\nexcept ImportError:\n    pass\n\ntry:\n    import threading\nexcept ImportError:\n    import dummy_threading as threading\n\nON_APP_ENGINE = all(key in os.environ for key in (\n    'USER_IS_ADMIN', 'CURRENT_VERSION_ID', 'APPLICATION_ID'))\n\nPORTS_BY_SECURITY = {True: 443,\n                     False: 80}\n\nDEFAULT_CA_CERTS_FILE = os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)), \"cacerts.txt\")\n\n\nclass HostConnectionPool(object):\n\n    \"\"\"\n    A pool of connections for one remote (host,port,is_secure).\n\n    When connections are added to the pool, they are put into a\n    pending queue.  The _mexe method returns connections to the pool\n    before the response body has been read, so they connections aren't\n    ready to send another request yet.  They stay in the pending queue\n    until they are ready for another request, at which point they are\n    returned to the pool of ready connections.\n\n    The pool of ready connections is an ordered list of\n    (connection,time) pairs, where the time is the time the connection\n    was returned from _mexe.  After a certain period of time,\n    connections are considered stale, and discarded rather than being\n    reused.  This saves having to wait for the connection to time out\n    if AWS has decided to close it on the other end because of\n    inactivity.\n\n    Thread Safety:\n\n        This class is used only from ConnectionPool while it's mutex\n        is held.\n    \"\"\"\n\n    def __init__(self):\n        self.queue = []\n\n    def size(self):\n        \"\"\"\n        Returns the number of connections in the pool for this host.\n        Some of the connections may still be in use, and may not be\n        ready to be returned by get().\n        \"\"\"\n        return len(self.queue)\n\n    def put(self, conn):\n        \"\"\"\n        Adds a connection to the pool, along with the time it was\n        added.\n        \"\"\"\n        self.queue.append((conn, time.time()))\n\n    def get(self):\n        \"\"\"\n        Returns the next connection in this pool that is ready to be\n        reused.  Returns None if there aren't any.\n        \"\"\"\n        # Discard ready connections that are too old.\n        self.clean()\n\n        # Return the first connection that is ready, and remove it\n        # from the queue.  Connections that aren't ready are returned\n        # to the end of the queue with an updated time, on the\n        # assumption that somebody is actively reading the response.\n        for _ in range(len(self.queue)):\n            (conn, _) = self.queue.pop(0)\n            if self._conn_ready(conn):\n                return conn\n            else:\n                self.put(conn)\n        return None\n\n    def _conn_ready(self, conn):\n        \"\"\"\n        There is a nice state diagram at the top of http_client.py.  It\n        indicates that once the response headers have been read (which\n        _mexe does before adding the connection to the pool), a\n        response is attached to the connection, and it stays there\n        until it's done reading.  This isn't entirely true: even after\n        the client is done reading, the response may be closed, but\n        not removed from the connection yet.\n\n        This is ugly, reading a private instance variable, but the\n        state we care about isn't available in any public methods.\n        \"\"\"\n        if ON_APP_ENGINE:\n            # Google AppEngine implementation of HTTPConnection doesn't contain\n            # _HTTPConnection__response attribute. Moreover, it's not possible\n            # to determine if given connection is ready. Reusing connections\n            # simply doesn't make sense with App Engine urlfetch service.\n            return False\n        else:\n            response = getattr(conn, '_HTTPConnection__response', None)\n            return (response is None) or response.isclosed()\n\n    def clean(self):\n        \"\"\"\n        Get rid of stale connections.\n        \"\"\"\n        # Note that we do not close the connection here -- somebody\n        # may still be reading from it.\n        while len(self.queue) > 0 and self._pair_stale(self.queue[0]):\n            self.queue.pop(0)\n\n    def _pair_stale(self, pair):\n        \"\"\"\n        Returns true of the (connection,time) pair is too old to be\n        used.\n        \"\"\"\n        (_conn, return_time) = pair\n        now = time.time()\n        return return_time + ConnectionPool.STALE_DURATION < now\n\n\nclass ConnectionPool(object):\n\n    \"\"\"\n    A connection pool that expires connections after a fixed period of\n    time.  This saves time spent waiting for a connection that AWS has\n    timed out on the other end.\n\n    This class is thread-safe.\n    \"\"\"\n\n    #\n    # The amout of time between calls to clean.\n    #\n\n    CLEAN_INTERVAL = 5.0\n\n    #\n    # How long before a connection becomes \"stale\" and won't be reused\n    # again.  The intention is that this time is less that the timeout\n    # period that AWS uses, so we'll never try to reuse a connection\n    # and find that AWS is timing it out.\n    #\n    # Experimentation in July 2011 shows that AWS starts timing things\n    # out after three minutes.  The 60 seconds here is conservative so\n    # we should never hit that 3-minute timout.\n    #\n\n    STALE_DURATION = 60.0\n\n    def __init__(self):\n        # Mapping from (host,port,is_secure) to HostConnectionPool.\n        # If a pool becomes empty, it is removed.\n        self.host_to_pool = {}\n        # The last time the pool was cleaned.\n        self.last_clean_time = 0.0\n        self.mutex = threading.Lock()\n        ConnectionPool.STALE_DURATION = \\\n            config.getfloat('Boto', 'connection_stale_duration',\n                            ConnectionPool.STALE_DURATION)\n\n    def __getstate__(self):\n        pickled_dict = copy.copy(self.__dict__)\n        pickled_dict['host_to_pool'] = {}\n        del pickled_dict['mutex']\n        return pickled_dict\n\n    def __setstate__(self, dct):\n        self.__init__()\n\n    def size(self):\n        \"\"\"\n        Returns the number of connections in the pool.\n        \"\"\"\n        return sum(pool.size() for pool in self.host_to_pool.values())\n\n    def get_http_connection(self, host, port, is_secure):\n        \"\"\"\n        Gets a connection from the pool for the named host.  Returns\n        None if there is no connection that can be reused. It's the caller's\n        responsibility to call close() on the connection when it's no longer\n        needed.\n        \"\"\"\n        self.clean()\n        with self.mutex:\n            key = (host, port, is_secure)\n            if key not in self.host_to_pool:\n                return None\n            return self.host_to_pool[key].get()\n\n    def put_http_connection(self, host, port, is_secure, conn):\n        \"\"\"\n        Adds a connection to the pool of connections that can be\n        reused for the named host.\n        \"\"\"\n        with self.mutex:\n            key = (host, port, is_secure)\n            if key not in self.host_to_pool:\n                self.host_to_pool[key] = HostConnectionPool()\n            self.host_to_pool[key].put(conn)\n\n    def clean(self):\n        \"\"\"\n        Clean up the stale connections in all of the pools, and then\n        get rid of empty pools.  Pools clean themselves every time a\n        connection is fetched; this cleaning takes care of pools that\n        aren't being used any more, so nothing is being gotten from\n        them.\n        \"\"\"\n        with self.mutex:\n            now = time.time()\n            if self.last_clean_time + self.CLEAN_INTERVAL < now:\n                to_remove = []\n                for (host, pool) in self.host_to_pool.items():\n                    pool.clean()\n                    if pool.size() == 0:\n                        to_remove.append(host)\n                for host in to_remove:\n                    del self.host_to_pool[host]\n                self.last_clean_time = now\n\n\nclass HTTPRequest(object):\n\n    def __init__(self, method, protocol, host, port, path, auth_path,\n                 params, headers, body):\n        \"\"\"Represents an HTTP request.\n\n        :type method: string\n        :param method: The HTTP method name, 'GET', 'POST', 'PUT' etc.\n\n        :type protocol: string\n        :param protocol: The http protocol used, 'http' or 'https'.\n\n        :type host: string\n        :param host: Host to which the request is addressed. eg. abc.com\n\n        :type port: int\n        :param port: port on which the request is being sent. Zero means unset,\n            in which case default port will be chosen.\n\n        :type path: string\n        :param path: URL path that is being accessed.\n\n        :type auth_path: string\n        :param path: The part of the URL path used when creating the\n            authentication string.\n\n        :type params: dict\n        :param params: HTTP url query parameters, with key as name of\n            the param, and value as value of param.\n\n        :type headers: dict\n        :param headers: HTTP headers, with key as name of the header and value\n            as value of header.\n\n        :type body: string\n        :param body: Body of the HTTP request. If not present, will be None or\n            empty string ('').\n        \"\"\"\n        self.method = method\n        self.protocol = protocol\n        self.host = host\n        self.port = port\n        self.path = path\n        if auth_path is None:\n            auth_path = path\n        self.auth_path = auth_path\n        self.params = params\n        # chunked Transfer-Encoding should act only on PUT request.\n        if headers and 'Transfer-Encoding' in headers and \\\n                headers['Transfer-Encoding'] == 'chunked' and \\\n                self.method != 'PUT':\n            self.headers = headers.copy()\n            del self.headers['Transfer-Encoding']\n        else:\n            self.headers = headers\n        self.body = body\n\n    def __str__(self):\n        return (('method:(%s) protocol:(%s) host(%s) port(%s) path(%s) '\n                 'params(%s) headers(%s) body(%s)') % (self.method,\n                 self.protocol, self.host, self.port, self.path, self.params,\n                 self.headers, self.body))\n\n    def authorize(self, connection, **kwargs):\n        if not getattr(self, '_headers_quoted', False):\n            for key in self.headers:\n                val = self.headers[key]\n                if isinstance(val, six.text_type):\n                    safe = '!\"#$%&\\'()*+,/:;<=>?@[\\\\]^`{|}~ '\n                    self.headers[key] = quote(val.encode('utf-8'), safe)\n            setattr(self, '_headers_quoted', True)\n\n        self.headers['User-Agent'] = UserAgent\n\n        connection._auth_handler.add_auth(self, **kwargs)\n\n        # I'm not sure if this is still needed, now that add_auth is\n        # setting the content-length for POST requests.\n        if 'Content-Length' not in self.headers:\n            if 'Transfer-Encoding' not in self.headers or \\\n                    self.headers['Transfer-Encoding'] != 'chunked':\n                self.headers['Content-Length'] = str(len(self.body))\n\n\nclass HTTPResponse(http_client.HTTPResponse):\n\n    def __init__(self, *args, **kwargs):\n        http_client.HTTPResponse.__init__(self, *args, **kwargs)\n        self._cached_response = ''\n\n    def read(self, amt=None):\n        \"\"\"Read the response.\n\n        This method does not have the same behavior as\n        http_client.HTTPResponse.read.  Instead, if this method is called with\n        no ``amt`` arg, then the response body will be cached.  Subsequent\n        calls to ``read()`` with no args **will return the cached response**.\n\n        \"\"\"\n        if amt is None:\n            # The reason for doing this is that many places in boto call\n            # response.read() and except to get the response body that they\n            # can then process.  To make sure this always works as they expect\n            # we're caching the response so that multiple calls to read()\n            # will return the full body.  Note that this behavior only\n            # happens if the amt arg is not specified.\n            if not self._cached_response:\n                self._cached_response = http_client.HTTPResponse.read(self)\n            return self._cached_response\n        else:\n            return http_client.HTTPResponse.read(self, amt)\n\n\nclass AWSAuthConnection(object):\n    def __init__(self, host, aws_access_key_id=None,\n                 aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, debug=0,\n                 https_connection_factory=None, path='/',\n                 provider='aws', security_token=None,\n                 suppress_consec_slashes=True,\n                 validate_certs=True, profile_name=None):\n        \"\"\"\n        :type host: str\n        :param host: The host to make the connection to\n\n        :keyword str aws_access_key_id: Your AWS Access Key ID (provided by\n            Amazon). If none is specified, the value in your\n            ``AWS_ACCESS_KEY_ID`` environmental variable is used.\n        :keyword str aws_secret_access_key: Your AWS Secret Access Key\n            (provided by Amazon). If none is specified, the value in your\n            ``AWS_SECRET_ACCESS_KEY`` environmental variable is used.\n        :keyword str security_token: The security token associated with\n            temporary credentials issued by STS.  Optional unless using\n            temporary credentials.  If none is specified, the environment\n            variable ``AWS_SECURITY_TOKEN`` is used if defined.\n\n        :type is_secure: boolean\n        :param is_secure: Whether the connection is over SSL\n\n        :type https_connection_factory: list or tuple\n        :param https_connection_factory: A pair of an HTTP connection\n            factory and the exceptions to catch.  The factory should have\n            a similar interface to L{http_client.HTTPSConnection}.\n\n        :param str proxy: Address/hostname for a proxy server\n\n        :type proxy_port: int\n        :param proxy_port: The port to use when connecting over a proxy\n\n        :type proxy_user: str\n        :param proxy_user: The username to connect with on the proxy\n\n        :type proxy_pass: str\n        :param proxy_pass: The password to use when connection over a proxy.\n\n        :type port: int\n        :param port: The port to use to connect\n\n        :type suppress_consec_slashes: bool\n        :param suppress_consec_slashes: If provided, controls whether\n            consecutive slashes will be suppressed in key paths.\n\n        :type validate_certs: bool\n        :param validate_certs: Controls whether SSL certificates\n            will be validated or not.  Defaults to True.\n\n        :type profile_name: str\n        :param profile_name: Override usual Credentials section in config\n            file to use a named set of keys instead.\n        \"\"\"\n        self.suppress_consec_slashes = suppress_consec_slashes\n        self.num_retries = 6\n        # Override passed-in is_secure setting if value was defined in config.\n        if config.has_option('Boto', 'is_secure'):\n            is_secure = config.getboolean('Boto', 'is_secure')\n        self.is_secure = is_secure\n        # Whether or not to validate server certificates.\n        # The default is now to validate certificates.  This can be\n        # overridden in the boto config file are by passing an\n        # explicit validate_certs parameter to the class constructor.\n        self.https_validate_certificates = config.getbool(\n            'Boto', 'https_validate_certificates',\n            validate_certs)\n        if self.https_validate_certificates and not HAVE_HTTPS_CONNECTION:\n            raise BotoClientError(\n                \"SSL server certificate validation is enabled in boto \"\n                \"configuration, but Python dependencies required to \"\n                \"support this feature are not available. Certificate \"\n                \"validation is only supported when running under Python \"\n                \"2.6 or later.\")\n        certs_file = config.get_value(\n            'Boto', 'ca_certificates_file', DEFAULT_CA_CERTS_FILE)\n        if certs_file == 'system':\n            certs_file = None\n        self.ca_certificates_file = certs_file\n        if port:\n            self.port = port\n        else:\n            self.port = PORTS_BY_SECURITY[is_secure]\n\n        self.handle_proxy(proxy, proxy_port, proxy_user, proxy_pass)\n        # define exceptions from http_client that we want to catch and retry\n        self.http_exceptions = (http_client.HTTPException, socket.error,\n                                socket.gaierror, http_client.BadStatusLine)\n        # define subclasses of the above that are not retryable.\n        self.http_unretryable_exceptions = []\n        if HAVE_HTTPS_CONNECTION:\n            self.http_unretryable_exceptions.append(\n                https_connection.InvalidCertificateException)\n\n        # define values in socket exceptions we don't want to catch\n        self.socket_exception_values = (errno.EINTR,)\n        if https_connection_factory is not None:\n            self.https_connection_factory = https_connection_factory[0]\n            self.http_exceptions += https_connection_factory[1]\n        else:\n            self.https_connection_factory = None\n        if (is_secure):\n            self.protocol = 'https'\n        else:\n            self.protocol = 'http'\n        self.host = host\n        self.path = path\n        # if the value passed in for debug\n        if not isinstance(debug, six.integer_types):\n            debug = 0\n        self.debug = config.getint('Boto', 'debug', debug)\n        self.host_header = None\n\n        # Timeout used to tell http_client how long to wait for socket timeouts.\n        # Default is to leave timeout unchanged, which will in turn result in\n        # the socket's default global timeout being used. To specify a\n        # timeout, set http_socket_timeout in Boto config. Regardless,\n        # timeouts will only be applied if Python is 2.6 or greater.\n        self.http_connection_kwargs = {}\n        if (sys.version_info[0], sys.version_info[1]) >= (2, 6):\n            # If timeout isn't defined in boto config file, use 70 second\n            # default as recommended by\n            # http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForActivityTask.html\n            self.http_connection_kwargs['timeout'] = config.getint(\n                'Boto', 'http_socket_timeout', 70)\n\n        if isinstance(provider, Provider):\n            # Allow overriding Provider\n            self.provider = provider\n        else:\n            self._provider_type = provider\n            self.provider = Provider(self._provider_type,\n                                     aws_access_key_id,\n                                     aws_secret_access_key,\n                                     security_token,\n                                     profile_name)\n\n        # Allow config file to override default host, port, and host header.\n        if self.provider.host:\n            self.host = self.provider.host\n        if self.provider.port:\n            self.port = self.provider.port\n        if self.provider.host_header:\n            self.host_header = self.provider.host_header\n\n        self._pool = ConnectionPool()\n        self._connection = (self.host, self.port, self.is_secure)\n        self._last_rs = None\n        self._auth_handler = auth.get_auth_handler(\n            host, config, self.provider, self._required_auth_capability())\n        if getattr(self, 'AuthServiceName', None) is not None:\n            self.auth_service_name = self.AuthServiceName\n        self.request_hook = None\n\n    def __repr__(self):\n        return '%s:%s' % (self.__class__.__name__, self.host)\n\n    def _required_auth_capability(self):\n        return []\n\n    def _get_auth_service_name(self):\n        return getattr(self._auth_handler, 'service_name')\n\n    # For Sigv4, the auth_service_name/auth_region_name properties allow\n    # the service_name/region_name to be explicitly set instead of being\n    # derived from the endpoint url.\n    def _set_auth_service_name(self, value):\n        self._auth_handler.service_name = value\n    auth_service_name = property(_get_auth_service_name, _set_auth_service_name)\n\n    def _get_auth_region_name(self):\n        return getattr(self._auth_handler, 'region_name')\n\n    def _set_auth_region_name(self, value):\n        self._auth_handler.region_name = value\n    auth_region_name = property(_get_auth_region_name, _set_auth_region_name)\n\n    def connection(self):\n        return self.get_http_connection(*self._connection)\n    connection = property(connection)\n\n    def aws_access_key_id(self):\n        return self.provider.access_key\n    aws_access_key_id = property(aws_access_key_id)\n    gs_access_key_id = aws_access_key_id\n    access_key = aws_access_key_id\n\n    def aws_secret_access_key(self):\n        return self.provider.secret_key\n    aws_secret_access_key = property(aws_secret_access_key)\n    gs_secret_access_key = aws_secret_access_key\n    secret_key = aws_secret_access_key\n\n    def profile_name(self):\n        return self.provider.profile_name\n    profile_name = property(profile_name)\n\n    def get_path(self, path='/'):\n        # The default behavior is to suppress consecutive slashes for reasons\n        # discussed at\n        # https://groups.google.com/forum/#!topic/boto-dev/-ft0XPUy0y8\n        # You can override that behavior with the suppress_consec_slashes param.\n        if not self.suppress_consec_slashes:\n            return self.path + re.sub('^(/*)/', \"\\\\1\", path)\n        pos = path.find('?')\n        if pos >= 0:\n            params = path[pos:]\n            path = path[:pos]\n        else:\n            params = None\n        if path[-1] == '/':\n            need_trailing = True\n        else:\n            need_trailing = False\n        path_elements = self.path.split('/')\n        path_elements.extend(path.split('/'))\n        path_elements = [p for p in path_elements if p]\n        path = '/' + '/'.join(path_elements)\n        if path[-1] != '/' and need_trailing:\n            path += '/'\n        if params:\n            path = path + params\n        return path\n\n    def server_name(self, port=None):\n        if not port:\n            port = self.port\n        if port == 80:\n            signature_host = self.host\n        else:\n            # This unfortunate little hack can be attributed to\n            # a difference in the 2.6 version of http_client.  In old\n            # versions, it would append \":443\" to the hostname sent\n            # in the Host header and so we needed to make sure we\n            # did the same when calculating the V2 signature.  In 2.6\n            # (and higher!)\n            # it no longer does that.  Hence, this kludge.\n            if ((ON_APP_ENGINE and sys.version[:3] == '2.5') or\n                    sys.version[:3] in ('2.6', '2.7')) and port == 443:\n                signature_host = self.host\n            else:\n                signature_host = '%s:%d' % (self.host, port)\n        return signature_host\n\n    def handle_proxy(self, proxy, proxy_port, proxy_user, proxy_pass):\n        self.proxy = proxy\n        self.proxy_port = proxy_port\n        self.proxy_user = proxy_user\n        self.proxy_pass = proxy_pass\n        if 'http_proxy' in os.environ and not self.proxy:\n            pattern = re.compile(\n                '(?:http://)?'\n                '(?:(?P<user>[\\w\\-\\.]+):(?P<pass>.*)@)?'\n                '(?P<host>[\\w\\-\\.]+)'\n                '(?::(?P<port>\\d+))?'\n            )\n            match = pattern.match(os.environ['http_proxy'])\n            if match:\n                self.proxy = match.group('host')\n                self.proxy_port = match.group('port')\n                self.proxy_user = match.group('user')\n                self.proxy_pass = match.group('pass')\n        else:\n            if not self.proxy:\n                self.proxy = config.get_value('Boto', 'proxy', None)\n            if not self.proxy_port:\n                self.proxy_port = config.get_value('Boto', 'proxy_port', None)\n            if not self.proxy_user:\n                self.proxy_user = config.get_value('Boto', 'proxy_user', None)\n            if not self.proxy_pass:\n                self.proxy_pass = config.get_value('Boto', 'proxy_pass', None)\n\n        if not self.proxy_port and self.proxy:\n            print(\"http_proxy environment variable does not specify \"\n                  \"a port, using default\")\n            self.proxy_port = self.port\n\n        self.no_proxy = os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', '')\n        self.use_proxy = (self.proxy is not None)\n\n    def get_http_connection(self, host, port, is_secure):\n        conn = self._pool.get_http_connection(host, port, is_secure)\n        if conn is not None:\n            return conn\n        else:\n            return self.new_http_connection(host, port, is_secure)\n\n    def skip_proxy(self, host):\n        if not self.no_proxy:\n            return False\n\n        if self.no_proxy == \"*\":\n            return True\n\n        hostonly = host\n        hostonly = host.split(':')[0]\n\n        for name in self.no_proxy.split(','):\n            if name and (hostonly.endswith(name) or host.endswith(name)):\n                return True\n\n        return False\n\n    def new_http_connection(self, host, port, is_secure):\n        if host is None:\n            host = self.server_name()\n\n        # Make sure the host is really just the host, not including\n        # the port number\n        host = boto.utils.parse_host(host)\n\n        http_connection_kwargs = self.http_connection_kwargs.copy()\n\n        # Connection factories below expect a port keyword argument\n        http_connection_kwargs['port'] = port\n\n        # Override host with proxy settings if needed\n        if self.use_proxy and not is_secure and \\\n                not self.skip_proxy(host):\n            host = self.proxy\n            http_connection_kwargs['port'] = int(self.proxy_port)\n\n        if is_secure:\n            boto.log.debug(\n                'establishing HTTPS connection: host=%s, kwargs=%s',\n                host, http_connection_kwargs)\n            if self.use_proxy and not self.skip_proxy(host):\n                connection = self.proxy_ssl(host, is_secure and 443 or 80)\n            elif self.https_connection_factory:\n                connection = self.https_connection_factory(host)\n            elif self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\n                connection = https_connection.CertValidatingHTTPSConnection(\n                    host, ca_certs=self.ca_certificates_file,\n                    **http_connection_kwargs)\n            else:\n                connection = http_client.HTTPSConnection(\n                    host, **http_connection_kwargs)\n        else:\n            boto.log.debug('establishing HTTP connection: kwargs=%s' %\n                           http_connection_kwargs)\n            if self.https_connection_factory:\n                # even though the factory says https, this is too handy\n                # to not be able to allow overriding for http also.\n                connection = self.https_connection_factory(\n                    host, **http_connection_kwargs)\n            else:\n                connection = http_client.HTTPConnection(\n                    host, **http_connection_kwargs)\n        if self.debug > 1:\n            connection.set_debuglevel(self.debug)\n        # self.connection must be maintained for backwards-compatibility\n        # however, it must be dynamically pulled from the connection pool\n        # set a private variable which will enable that\n        if host.split(':')[0] == self.host and is_secure == self.is_secure:\n            self._connection = (host, port, is_secure)\n        # Set the response class of the http connection to use our custom\n        # class.\n        connection.response_class = HTTPResponse\n        return connection\n\n    def put_http_connection(self, host, port, is_secure, connection):\n        self._pool.put_http_connection(host, port, is_secure, connection)\n\n    def proxy_ssl(self, host=None, port=None):\n        if host and port:\n            host = '%s:%d' % (host, port)\n        else:\n            host = '%s:%d' % (self.host, self.port)\n        # Seems properly to use timeout for connect too\n        timeout = self.http_connection_kwargs.get(\"timeout\")\n        if timeout is not None:\n            sock = socket.create_connection((self.proxy,\n                                             int(self.proxy_port)), timeout)\n        else:\n            sock = socket.create_connection((self.proxy, int(self.proxy_port)))\n        boto.log.debug(\"Proxy connection: CONNECT %s HTTP/1.0\\r\\n\", host)\n        sock.sendall(\"CONNECT %s HTTP/1.0\\r\\n\" % host)\n        sock.sendall(\"User-Agent: %s\\r\\n\" % UserAgent)\n        if self.proxy_user and self.proxy_pass:\n            for k, v in self.get_proxy_auth_header().items():\n                sock.sendall(\"%s: %s\\r\\n\" % (k, v))\n            # See discussion about this config option at\n            # https://groups.google.com/forum/?fromgroups#!topic/boto-dev/teenFvOq2Cc\n            if config.getbool('Boto', 'send_crlf_after_proxy_auth_headers', False):\n                sock.sendall(\"\\r\\n\")\n        else:\n            sock.sendall(\"\\r\\n\")\n        resp = http_client.HTTPResponse(sock, strict=True, debuglevel=self.debug)\n        resp.begin()\n\n        if resp.status != 200:\n            # Fake a socket error, use a code that make it obvious it hasn't\n            # been generated by the socket library\n            raise socket.error(-71,\n                               \"Error talking to HTTP proxy %s:%s: %s (%s)\" %\n                               (self.proxy, self.proxy_port,\n                                resp.status, resp.reason))\n\n        # We can safely close the response, it duped the original socket\n        resp.close()\n\n        h = http_client.HTTPConnection(host)\n\n        if self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\n            msg = \"wrapping ssl socket for proxied connection; \"\n            if self.ca_certificates_file:\n                msg += \"CA certificate file=%s\" % self.ca_certificates_file\n            else:\n                msg += \"using system provided SSL certs\"\n            boto.log.debug(msg)\n            key_file = self.http_connection_kwargs.get('key_file', None)\n            cert_file = self.http_connection_kwargs.get('cert_file', None)\n            sslSock = ssl.wrap_socket(sock, keyfile=key_file,\n                                      certfile=cert_file,\n                                      cert_reqs=ssl.CERT_REQUIRED,\n                                      ca_certs=self.ca_certificates_file)\n            cert = sslSock.getpeercert()\n            hostname = self.host.split(':', 0)[0]\n            if not https_connection.ValidateCertificateHostname(cert, hostname):\n                raise https_connection.InvalidCertificateException(\n                    hostname, cert, 'hostname mismatch')\n        else:\n            # Fallback for old Python without ssl.wrap_socket\n            if hasattr(http_client, 'ssl'):\n                sslSock = http_client.ssl.SSLSocket(sock)\n            else:\n                sslSock = socket.ssl(sock, None, None)\n                sslSock = http_client.FakeSocket(sock, sslSock)\n\n        # This is a bit unclean\n        h.sock = sslSock\n        return h\n\n    def prefix_proxy_to_path(self, path, host=None):\n        path = self.protocol + '://' + (host or self.server_name()) + path\n        return path\n\n    def get_proxy_auth_header(self):\n        auth = encodebytes(self.proxy_user + ':' + self.proxy_pass)\n        return {'Proxy-Authorization': 'Basic %s' % auth}\n\n    # For passing proxy information to other connection libraries, e.g. cloudsearch2\n    def get_proxy_url_with_auth(self):\n        if not self.use_proxy:\n            return None\n\n        if self.proxy_user or self.proxy_pass:\n            if self.proxy_pass:\n                login_info = '%s:%s@' % (self.proxy_user, self.proxy_pass)\n            else:\n                login_info = '%s@' % self.proxy_user\n        else:\n            login_info = ''\n\n        return 'http://%s%s:%s' % (login_info, self.proxy, str(self.proxy_port or self.port))\n\n    def set_host_header(self, request):\n        try:\n            request.headers['Host'] = \\\n                self._auth_handler.host_header(self.host, request)\n        except AttributeError:\n            request.headers['Host'] = self.host.split(':', 1)[0]\n\n    def set_request_hook(self, hook):\n        self.request_hook = hook\n\n    def _mexe(self, request, sender=None, override_num_retries=None,\n              retry_handler=None):\n        \"\"\"\n        mexe - Multi-execute inside a loop, retrying multiple times to handle\n               transient Internet errors by simply trying again.\n               Also handles redirects.\n\n        This code was inspired by the S3Utils classes posted to the boto-users\n        Google group by Larry Bates.  Thanks!\n\n        \"\"\"\n        boto.log.debug('Method: %s' % request.method)\n        boto.log.debug('Path: %s' % request.path)\n        boto.log.debug('Data: %s' % request.body)\n        boto.log.debug('Headers: %s' % request.headers)\n        boto.log.debug('Host: %s' % request.host)\n        boto.log.debug('Port: %s' % request.port)\n        boto.log.debug('Params: %s' % request.params)\n        response = None\n        body = None\n        ex = None\n        if override_num_retries is None:\n            num_retries = config.getint('Boto', 'num_retries', self.num_retries)\n        else:\n            num_retries = override_num_retries\n        i = 0\n        connection = self.get_http_connection(request.host, request.port,\n                                              self.is_secure)\n\n        # Convert body to bytes if needed\n        if not isinstance(request.body, bytes) and hasattr(request.body,\n                                                           'encode'):\n            request.body = request.body.encode('utf-8')\n\n        while i <= num_retries:\n            # Use binary exponential backoff to desynchronize client requests.\n            next_sleep = min(random.random() * (2 ** i),\n                             boto.config.get('Boto', 'max_retry_delay', 60))\n            try:\n                # we now re-sign each request before it is retried\n                boto.log.debug('Token: %s' % self.provider.security_token)\n                request.authorize(connection=self)\n                # Only force header for non-s3 connections, because s3 uses\n                # an older signing method + bucket resource URLs that include\n                # the port info. All others should be now be up to date and\n                # not include the port.\n                if 's3' not in self._required_auth_capability():\n                    if not getattr(self, 'anon', False):\n                        if not request.headers.get('Host'):\n                            self.set_host_header(request)\n                boto.log.debug('Final headers: %s' % request.headers)\n                request.start_time = datetime.now()\n                if callable(sender):\n                    response = sender(connection, request.method, request.path,\n                                      request.body, request.headers)\n                else:\n                    connection.request(request.method, request.path,\n                                       request.body, request.headers)\n                    response = connection.getresponse()\n                boto.log.debug('Response headers: %s' % response.getheaders())\n                location = response.getheader('location')\n                # -- gross hack --\n                # http_client gets confused with chunked responses to HEAD requests\n                # so I have to fake it out\n                if request.method == 'HEAD' and getattr(response,\n                                                        'chunked', False):\n                    response.chunked = 0\n                if callable(retry_handler):\n                    status = retry_handler(response, i, next_sleep)\n                    if status:\n                        msg, i, next_sleep = status\n                        if msg:\n                            boto.log.debug(msg)\n                        time.sleep(next_sleep)\n                        continue\n                if response.status in [500, 502, 503, 504]:\n                    msg = 'Received %d response.  ' % response.status\n                    msg += 'Retrying in %3.1f seconds' % next_sleep\n                    boto.log.debug(msg)\n                    body = response.read()\n                    if isinstance(body, bytes):\n                        body = body.decode('utf-8')\n                elif response.status < 300 or response.status >= 400 or \\\n                        not location:\n                    # don't return connection to the pool if response contains\n                    # Connection:close header, because the connection has been\n                    # closed and default reconnect behavior may do something\n                    # different than new_http_connection. Also, it's probably\n                    # less efficient to try to reuse a closed connection.\n                    conn_header_value = response.getheader('connection')\n                    if conn_header_value == 'close':\n                        connection.close()\n                    else:\n                        self.put_http_connection(request.host, request.port,\n                                                 self.is_secure, connection)\n                    if self.request_hook is not None:\n                        self.request_hook.handle_request_data(request, response)\n                    return response\n                else:\n                    scheme, request.host, request.path, \\\n                        params, query, fragment = urlparse(location)\n                    if query:\n                        request.path += '?' + query\n                    # urlparse can return both host and port in netloc, so if\n                    # that's the case we need to split them up properly\n                    if ':' in request.host:\n                        request.host, request.port = request.host.split(':', 1)\n                    msg = 'Redirecting: %s' % scheme + '://'\n                    msg += request.host + request.path\n                    boto.log.debug(msg)\n                    connection = self.get_http_connection(request.host,\n                                                          request.port,\n                                                          scheme == 'https')\n                    response = None\n                    continue\n            except PleaseRetryException as e:\n                boto.log.debug('encountered a retry exception: %s' % e)\n                connection = self.new_http_connection(request.host, request.port,\n                                                      self.is_secure)\n                response = e.response\n                ex = e\n            except self.http_exceptions as e:\n                for unretryable in self.http_unretryable_exceptions:\n                    if isinstance(e, unretryable):\n                        boto.log.debug(\n                            'encountered unretryable %s exception, re-raising' %\n                            e.__class__.__name__)\n                        raise\n                boto.log.debug('encountered %s exception, reconnecting' %\n                               e.__class__.__name__)\n                connection = self.new_http_connection(request.host, request.port,\n                                                      self.is_secure)\n                ex = e\n            time.sleep(next_sleep)\n            i += 1\n        # If we made it here, it's because we have exhausted our retries\n        # and stil haven't succeeded.  So, if we have a response object,\n        # use it to raise an exception.\n        # Otherwise, raise the exception that must have already happened.\n        if self.request_hook is not None:\n            self.request_hook.handle_request_data(request, response, error=True)\n        if response:\n            raise BotoServerError(response.status, response.reason, body)\n        elif ex:\n            raise ex\n        else:\n            msg = 'Please report this exception as a Boto Issue!'\n            raise BotoClientError(msg)\n\n    def build_base_http_request(self, method, path, auth_path,\n                                params=None, headers=None, data='', host=None):\n        path = self.get_path(path)\n        if auth_path is not None:\n            auth_path = self.get_path(auth_path)\n        if params is None:\n            params = {}\n        else:\n            params = params.copy()\n        if headers is None:\n            headers = {}\n        else:\n            headers = headers.copy()\n        if self.host_header and not boto.utils.find_matching_headers('host', headers):\n            headers['host'] = self.host_header\n        host = host or self.host\n        if self.use_proxy and not self.skip_proxy(host):\n            if not auth_path:\n                auth_path = path\n            path = self.prefix_proxy_to_path(path, host)\n            if self.proxy_user and self.proxy_pass and not self.is_secure:\n                # If is_secure, we don't have to set the proxy authentication\n                # header here, we did that in the CONNECT to the proxy.\n                headers.update(self.get_proxy_auth_header())\n        return HTTPRequest(method, self.protocol, host, self.port,\n                           path, auth_path, params, headers, data)\n\n    def make_request(self, method, path, headers=None, data='', host=None,\n                     auth_path=None, sender=None, override_num_retries=None,\n                     params=None, retry_handler=None):\n        \"\"\"Makes a request to the server, with stock multiple-retry logic.\"\"\"\n        if params is None:\n            params = {}\n        http_request = self.build_base_http_request(method, path, auth_path,\n                                                    params, headers, data, host)\n        return self._mexe(http_request, sender, override_num_retries,\n                          retry_handler=retry_handler)\n\n    def close(self):\n        \"\"\"(Optional) Close any open HTTP connections.  This is non-destructive,\n        and making a new request will open a connection again.\"\"\"\n\n        boto.log.debug('closing all HTTP connections')\n        self._connection = None  # compat field\n\n\nclass AWSQueryConnection(AWSAuthConnection):\n\n    APIVersion = ''\n    ResponseError = BotoServerError\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None, host=None, debug=0,\n                 https_connection_factory=None, path='/', security_token=None,\n                 validate_certs=True, profile_name=None, provider='aws'):\n        super(AWSQueryConnection, self).__init__(\n            host, aws_access_key_id,\n            aws_secret_access_key,\n            is_secure, port, proxy,\n            proxy_port, proxy_user, proxy_pass,\n            debug, https_connection_factory, path,\n            security_token=security_token,\n            validate_certs=validate_certs,\n            profile_name=profile_name,\n            provider=provider)\n\n    def _required_auth_capability(self):\n        return []\n\n    def get_utf8_value(self, value):\n        return boto.utils.get_utf8_value(value)\n\n    def make_request(self, action, params=None, path='/', verb='GET'):\n        http_request = self.build_base_http_request(verb, path, None,\n                                                    params, {}, '',\n                                                    self.host)\n        if action:\n            http_request.params['Action'] = action\n        if self.APIVersion:\n            http_request.params['Version'] = self.APIVersion\n        return self._mexe(http_request)\n\n    def build_list_params(self, params, items, label):\n        if isinstance(items, six.string_types):\n            items = [items]\n        for i in range(1, len(items) + 1):\n            params['%s.%d' % (label, i)] = items[i - 1]\n\n    def build_complex_list_params(self, params, items, label, names):\n        \"\"\"Serialize a list of structures.\n\n        For example::\n\n            items = [('foo', 'bar', 'baz'), ('foo2', 'bar2', 'baz2')]\n            label = 'ParamName.member'\n            names = ('One', 'Two', 'Three')\n            self.build_complex_list_params(params, items, label, names)\n\n        would result in the params dict being updated with these params::\n\n            ParamName.member.1.One = foo\n            ParamName.member.1.Two = bar\n            ParamName.member.1.Three = baz\n\n            ParamName.member.2.One = foo2\n            ParamName.member.2.Two = bar2\n            ParamName.member.2.Three = baz2\n\n        :type params: dict\n        :param params: The params dict.  The complex list params\n            will be added to this dict.\n\n        :type items: list of tuples\n        :param items: The list to serialize.\n\n        :type label: string\n        :param label: The prefix to apply to the parameter.\n\n        :type names: tuple of strings\n        :param names: The names associated with each tuple element.\n\n        \"\"\"\n        for i, item in enumerate(items, 1):\n            current_prefix = '%s.%s' % (label, i)\n            for key, value in zip(names, item):\n                full_key = '%s.%s' % (current_prefix, key)\n                params[full_key] = value\n\n    # generics\n\n    def get_list(self, action, params, markers, path='/',\n                 parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            rs = ResultSet(markers)\n            h = boto.handler.XmlHandler(rs, parent)\n            if isinstance(body, six.text_type):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return rs\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n\n    def get_object(self, action, params, cls, path='/',\n                   parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            obj = cls(parent)\n            h = boto.handler.XmlHandler(obj, parent)\n            if isinstance(body, six.text_type):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return obj\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n\n    def get_status(self, action, params, path='/', parent=None, verb='GET'):\n        if not parent:\n            parent = self\n        response = self.make_request(action, params, path, verb)\n        body = response.read()\n        boto.log.debug(body)\n        if not body:\n            boto.log.error('Null body %s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n        elif response.status == 200:\n            rs = ResultSet()\n            h = boto.handler.XmlHandler(rs, parent)\n            xml.sax.parseString(body, h)\n            return rs.status\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)\n"}, {"boto.auth.S3HmacAuthV4Handler.presign": "# Copyright 2010 Google Inc.\n# Copyright (c) 2011 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2011, Eucalyptus Systems, Inc.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\n\n\"\"\"\nHandles authentication required to AWS and GS\n\"\"\"\n\nimport base64\nimport boto\nimport boto.auth_handler\nimport boto.exception\nimport boto.plugin\nimport boto.utils\nimport copy\nimport datetime\nfrom email.utils import formatdate\nimport hmac\nimport os\nimport posixpath\n\nfrom boto.compat import urllib, encodebytes, parse_qs_safe, urlparse\nfrom boto.auth_handler import AuthHandler\nfrom boto.exception import BotoClientError\n\ntry:\n    from hashlib import sha1 as sha\n    from hashlib import sha256 as sha256\nexcept ImportError:\n    import sha\n    sha256 = None\n\n\n# Region detection strings to determine if SigV2 should be used\n# by default\nS3_AUTH_DETECT = [\n    '-ap-northeast-1',\n    '.ap-northeast-1',\n    '-ap-southeast-1',\n    '.ap-southeast-1',\n    '-ap-southeast-2',\n    '.ap-southeast-2',\n    '-eu-west-1',\n    '.eu-west-1',\n    '-external-1',\n    '.external-1',\n    '-sa-east-1',\n    '.sa-east-1',\n    '-us-east-1',\n    '.us-east-1',\n    '-us-gov-west-1',\n    '.us-gov-west-1',\n    '-us-west-1',\n    '.us-west-1',\n    '-us-west-2',\n    '.us-west-2'\n]\n\n\nSIGV4_DETECT = [\n    '.cn-',\n    # In eu-central and ap-northeast-2 we support both host styles for S3\n    '.eu-central',\n    '-eu-central',\n    '.ap-northeast-2',\n    '-ap-northeast-2',\n    '.ap-south-1',\n    '-ap-south-1',\n    '.us-east-2',\n    '-us-east-2',\n    '-ca-central',\n    '.ca-central',\n    '.eu-west-2',\n    '-eu-west-2',\n]\n\n\nclass HmacKeys(object):\n    \"\"\"Key based Auth handler helper.\"\"\"\n\n    def __init__(self, host, config, provider):\n        if provider.access_key is None or provider.secret_key is None:\n            raise boto.auth_handler.NotReadyToAuthenticate()\n        self.host = host\n        self.update_provider(provider)\n\n    def update_provider(self, provider):\n        self._provider = provider\n        self._hmac = hmac.new(self._provider.secret_key.encode('utf-8'),\n                              digestmod=sha)\n        if sha256:\n            self._hmac_256 = hmac.new(self._provider.secret_key.encode('utf-8'),\n                                      digestmod=sha256)\n        else:\n            self._hmac_256 = None\n\n    def algorithm(self):\n        if self._hmac_256:\n            return 'HmacSHA256'\n        else:\n            return 'HmacSHA1'\n\n    def _get_hmac(self):\n        if self._hmac_256:\n            digestmod = sha256\n        else:\n            digestmod = sha\n        return hmac.new(self._provider.secret_key.encode('utf-8'),\n                        digestmod=digestmod)\n\n    def sign_string(self, string_to_sign):\n        new_hmac = self._get_hmac()\n        new_hmac.update(string_to_sign.encode('utf-8'))\n        return encodebytes(new_hmac.digest()).decode('utf-8').strip()\n\n    def __getstate__(self):\n        pickled_dict = copy.copy(self.__dict__)\n        del pickled_dict['_hmac']\n        del pickled_dict['_hmac_256']\n        return pickled_dict\n\n    def __setstate__(self, dct):\n        self.__dict__ = dct\n        self.update_provider(self._provider)\n\n\nclass AnonAuthHandler(AuthHandler, HmacKeys):\n    \"\"\"\n    Implements Anonymous requests.\n    \"\"\"\n\n    capability = ['anon']\n\n    def __init__(self, host, config, provider):\n        super(AnonAuthHandler, self).__init__(host, config, provider)\n\n    def add_auth(self, http_request, **kwargs):\n        pass\n\n\nclass HmacAuthV1Handler(AuthHandler, HmacKeys):\n    \"\"\"    Implements the HMAC request signing used by S3 and GS.\"\"\"\n\n    capability = ['hmac-v1', 's3']\n\n    def __init__(self, host, config, provider):\n        AuthHandler.__init__(self, host, config, provider)\n        HmacKeys.__init__(self, host, config, provider)\n        self._hmac_256 = None\n\n    def update_provider(self, provider):\n        super(HmacAuthV1Handler, self).update_provider(provider)\n        self._hmac_256 = None\n\n    def add_auth(self, http_request, **kwargs):\n        headers = http_request.headers\n        method = http_request.method\n        auth_path = http_request.auth_path\n        if 'Date' not in headers:\n            headers['Date'] = formatdate(usegmt=True)\n\n        if self._provider.security_token:\n            key = self._provider.security_token_header\n            headers[key] = self._provider.security_token\n        string_to_sign = boto.utils.canonical_string(method, auth_path,\n                                                     headers, None,\n                                                     self._provider)\n        boto.log.debug('StringToSign:\\n%s' % string_to_sign)\n        b64_hmac = self.sign_string(string_to_sign)\n        auth_hdr = self._provider.auth_header\n        auth = (\"%s %s:%s\" % (auth_hdr, self._provider.access_key, b64_hmac))\n        boto.log.debug('Signature:\\n%s' % auth)\n        headers['Authorization'] = auth\n\n\nclass HmacAuthV2Handler(AuthHandler, HmacKeys):\n    \"\"\"\n    Implements the simplified HMAC authorization used by CloudFront.\n    \"\"\"\n    capability = ['hmac-v2', 'cloudfront']\n\n    def __init__(self, host, config, provider):\n        AuthHandler.__init__(self, host, config, provider)\n        HmacKeys.__init__(self, host, config, provider)\n        self._hmac_256 = None\n\n    def update_provider(self, provider):\n        super(HmacAuthV2Handler, self).update_provider(provider)\n        self._hmac_256 = None\n\n    def add_auth(self, http_request, **kwargs):\n        headers = http_request.headers\n        if 'Date' not in headers:\n            headers['Date'] = formatdate(usegmt=True)\n        if self._provider.security_token:\n            key = self._provider.security_token_header\n            headers[key] = self._provider.security_token\n\n        b64_hmac = self.sign_string(headers['Date'])\n        auth_hdr = self._provider.auth_header\n        headers['Authorization'] = (\"%s %s:%s\" %\n                                    (auth_hdr,\n                                     self._provider.access_key, b64_hmac))\n\n\nclass HmacAuthV3Handler(AuthHandler, HmacKeys):\n    \"\"\"Implements the new Version 3 HMAC authorization used by Route53.\"\"\"\n\n    capability = ['hmac-v3', 'route53', 'ses']\n\n    def __init__(self, host, config, provider):\n        AuthHandler.__init__(self, host, config, provider)\n        HmacKeys.__init__(self, host, config, provider)\n\n    def add_auth(self, http_request, **kwargs):\n        headers = http_request.headers\n        if 'Date' not in headers:\n            headers['Date'] = formatdate(usegmt=True)\n\n        if self._provider.security_token:\n            key = self._provider.security_token_header\n            headers[key] = self._provider.security_token\n\n        b64_hmac = self.sign_string(headers['Date'])\n        s = \"AWS3-HTTPS AWSAccessKeyId=%s,\" % self._provider.access_key\n        s += \"Algorithm=%s,Signature=%s\" % (self.algorithm(), b64_hmac)\n        headers['X-Amzn-Authorization'] = s\n\n\nclass HmacAuthV3HTTPHandler(AuthHandler, HmacKeys):\n    \"\"\"\n    Implements the new Version 3 HMAC authorization used by DynamoDB.\n    \"\"\"\n\n    capability = ['hmac-v3-http']\n\n    def __init__(self, host, config, provider):\n        AuthHandler.__init__(self, host, config, provider)\n        HmacKeys.__init__(self, host, config, provider)\n\n    def headers_to_sign(self, http_request):\n        \"\"\"\n        Select the headers from the request that need to be included\n        in the StringToSign.\n        \"\"\"\n        headers_to_sign = {'Host': self.host}\n        for name, value in http_request.headers.items():\n            lname = name.lower()\n            if lname.startswith('x-amz'):\n                headers_to_sign[name] = value\n        return headers_to_sign\n\n    def canonical_headers(self, headers_to_sign):\n        \"\"\"\n        Return the headers that need to be included in the StringToSign\n        in their canonical form by converting all header keys to lower\n        case, sorting them in alphabetical order and then joining\n        them into a string, separated by newlines.\n        \"\"\"\n        l = sorted(['%s:%s' % (n.lower().strip(),\n                    headers_to_sign[n].strip()) for n in headers_to_sign])\n        return '\\n'.join(l)\n\n    def string_to_sign(self, http_request):\n        \"\"\"\n        Return the canonical StringToSign as well as a dict\n        containing the original version of all headers that\n        were included in the StringToSign.\n        \"\"\"\n        headers_to_sign = self.headers_to_sign(http_request)\n        canonical_headers = self.canonical_headers(headers_to_sign)\n        string_to_sign = '\\n'.join([http_request.method,\n                                    http_request.auth_path,\n                                    '',\n                                    canonical_headers,\n                                    '',\n                                    http_request.body])\n        return string_to_sign, headers_to_sign\n\n    def add_auth(self, req, **kwargs):\n        \"\"\"\n        Add AWS3 authentication to a request.\n\n        :type req: :class`boto.connection.HTTPRequest`\n        :param req: The HTTPRequest object.\n        \"\"\"\n        # This could be a retry.  Make sure the previous\n        # authorization header is removed first.\n        if 'X-Amzn-Authorization' in req.headers:\n            del req.headers['X-Amzn-Authorization']\n        req.headers['X-Amz-Date'] = formatdate(usegmt=True)\n        if self._provider.security_token:\n            req.headers['X-Amz-Security-Token'] = self._provider.security_token\n        string_to_sign, headers_to_sign = self.string_to_sign(req)\n        boto.log.debug('StringToSign:\\n%s' % string_to_sign)\n        hash_value = sha256(string_to_sign.encode('utf-8')).digest()\n        b64_hmac = self.sign_string(hash_value)\n        s = \"AWS3 AWSAccessKeyId=%s,\" % self._provider.access_key\n        s += \"Algorithm=%s,\" % self.algorithm()\n        s += \"SignedHeaders=%s,\" % ';'.join(headers_to_sign)\n        s += \"Signature=%s\" % b64_hmac\n        req.headers['X-Amzn-Authorization'] = s\n\n\nclass HmacAuthV4Handler(AuthHandler, HmacKeys):\n    \"\"\"\n    Implements the new Version 4 HMAC authorization.\n    \"\"\"\n\n    capability = ['hmac-v4']\n\n    def __init__(self, host, config, provider,\n                 service_name=None, region_name=None):\n        AuthHandler.__init__(self, host, config, provider)\n        HmacKeys.__init__(self, host, config, provider)\n        # You can set the service_name and region_name to override the\n        # values which would otherwise come from the endpoint, e.g.\n        # <service>.<region>.amazonaws.com.\n        self.service_name = service_name\n        self.region_name = region_name\n\n    def _sign(self, key, msg, hex=False):\n        if not isinstance(key, bytes):\n            key = key.encode('utf-8')\n\n        if hex:\n            sig = hmac.new(key, msg.encode('utf-8'), sha256).hexdigest()\n        else:\n            sig = hmac.new(key, msg.encode('utf-8'), sha256).digest()\n        return sig\n\n    def headers_to_sign(self, http_request):\n        \"\"\"\n        Select the headers from the request that need to be included\n        in the StringToSign.\n        \"\"\"\n        host_header_value = self.host_header(self.host, http_request)\n        if http_request.headers.get('Host'):\n            host_header_value = http_request.headers['Host']\n        headers_to_sign = {'Host': host_header_value}\n        for name, value in http_request.headers.items():\n            lname = name.lower()\n            if lname.startswith('x-amz'):\n                if isinstance(value, bytes):\n                    value = value.decode('utf-8')\n                headers_to_sign[name] = value\n        return headers_to_sign\n\n    def host_header(self, host, http_request):\n        port = http_request.port\n        secure = http_request.protocol == 'https'\n        if ((port == 80 and not secure) or (port == 443 and secure)):\n            return host\n        return '%s:%s' % (host, port)\n\n    def query_string(self, http_request):\n        parameter_names = sorted(http_request.params.keys())\n        pairs = []\n        for pname in parameter_names:\n            pval = boto.utils.get_utf8_value(http_request.params[pname])\n            pairs.append(urllib.parse.quote(pname, safe='') + '=' +\n                         urllib.parse.quote(pval, safe='-_~'))\n        return '&'.join(pairs)\n\n    def canonical_query_string(self, http_request):\n        # POST requests pass parameters in through the\n        # http_request.body field.\n        if http_request.method == 'POST':\n            return \"\"\n        l = []\n        for param in sorted(http_request.params):\n            value = boto.utils.get_utf8_value(http_request.params[param])\n            l.append('%s=%s' % (urllib.parse.quote(param, safe='-_.~'),\n                                urllib.parse.quote(value, safe='-_.~')))\n        return '&'.join(l)\n\n    def canonical_headers(self, headers_to_sign):\n        \"\"\"\n        Return the headers that need to be included in the StringToSign\n        in their canonical form by converting all header keys to lower\n        case, sorting them in alphabetical order and then joining\n        them into a string, separated by newlines.\n        \"\"\"\n        canonical = []\n\n        for header in headers_to_sign:\n            c_name = header.lower().strip()\n            raw_value = str(headers_to_sign[header])\n            if '\"' in raw_value:\n                c_value = raw_value.strip()\n            else:\n                c_value = ' '.join(raw_value.strip().split())\n            canonical.append('%s:%s' % (c_name, c_value))\n        return '\\n'.join(sorted(canonical))\n\n    def signed_headers(self, headers_to_sign):\n        l = ['%s' % n.lower().strip() for n in headers_to_sign]\n        l = sorted(l)\n        return ';'.join(l)\n\n    def canonical_uri(self, http_request):\n        path = http_request.auth_path\n        # Normalize the path\n        # in windows normpath('/') will be '\\\\' so we chane it back to '/'\n        normalized = posixpath.normpath(path).replace('\\\\', '/')\n        # Then urlencode whatever's left.\n        encoded = urllib.parse.quote(normalized)\n        if len(path) > 1 and path.endswith('/'):\n            encoded += '/'\n        return encoded\n\n    def payload(self, http_request):\n        body = http_request.body\n        # If the body is a file like object, we can use\n        # boto.utils.compute_hash, which will avoid reading\n        # the entire body into memory.\n        if hasattr(body, 'seek') and hasattr(body, 'read'):\n            return boto.utils.compute_hash(body, hash_algorithm=sha256)[0]\n        elif not isinstance(body, bytes):\n            body = body.encode('utf-8')\n        return sha256(body).hexdigest()\n\n    def canonical_request(self, http_request):\n        cr = [http_request.method.upper()]\n        cr.append(self.canonical_uri(http_request))\n        cr.append(self.canonical_query_string(http_request))\n        headers_to_sign = self.headers_to_sign(http_request)\n        cr.append(self.canonical_headers(headers_to_sign) + '\\n')\n        cr.append(self.signed_headers(headers_to_sign))\n        cr.append(self.payload(http_request))\n        return '\\n'.join(cr)\n\n    def scope(self, http_request):\n        scope = [self._provider.access_key]\n        scope.append(http_request.timestamp)\n        scope.append(http_request.region_name)\n        scope.append(http_request.service_name)\n        scope.append('aws4_request')\n        return '/'.join(scope)\n\n    def split_host_parts(self, host):\n        return host.split('.')\n\n    def determine_region_name(self, host):\n        parts = self.split_host_parts(host)\n        if self.region_name is not None:\n            region_name = self.region_name\n        elif len(parts) > 1:\n            if parts[1] == 'us-gov':\n                region_name = 'us-gov-west-1'\n            else:\n                if len(parts) == 3:\n                    region_name = 'us-east-1'\n                else:\n                    region_name = parts[1]\n        else:\n            region_name = parts[0]\n\n        return region_name\n\n    def determine_service_name(self, host):\n        parts = self.split_host_parts(host)\n        if self.service_name is not None:\n            service_name = self.service_name\n        else:\n            service_name = parts[0]\n        return service_name\n\n    def credential_scope(self, http_request):\n        scope = []\n        http_request.timestamp = http_request.headers['X-Amz-Date'][0:8]\n        scope.append(http_request.timestamp)\n        # The service_name and region_name either come from:\n        # * The service_name/region_name attrs or (if these values are None)\n        # * parsed from the endpoint <service>.<region>.amazonaws.com.\n        region_name = self.determine_region_name(http_request.host)\n        service_name = self.determine_service_name(http_request.host)\n        http_request.service_name = service_name\n        http_request.region_name = region_name\n\n        scope.append(http_request.region_name)\n        scope.append(http_request.service_name)\n        scope.append('aws4_request')\n        return '/'.join(scope)\n\n    def string_to_sign(self, http_request, canonical_request):\n        \"\"\"\n        Return the canonical StringToSign as well as a dict\n        containing the original version of all headers that\n        were included in the StringToSign.\n        \"\"\"\n        sts = ['AWS4-HMAC-SHA256']\n        sts.append(http_request.headers['X-Amz-Date'])\n        sts.append(self.credential_scope(http_request))\n        sts.append(sha256(canonical_request.encode('utf-8')).hexdigest())\n        return '\\n'.join(sts)\n\n    def signature(self, http_request, string_to_sign):\n        key = self._provider.secret_key\n        k_date = self._sign(('AWS4' + key).encode('utf-8'),\n                            http_request.timestamp)\n        k_region = self._sign(k_date, http_request.region_name)\n        k_service = self._sign(k_region, http_request.service_name)\n        k_signing = self._sign(k_service, 'aws4_request')\n        return self._sign(k_signing, string_to_sign, hex=True)\n\n    def add_auth(self, req, **kwargs):\n        \"\"\"\n        Add AWS4 authentication to a request.\n\n        :type req: :class`boto.connection.HTTPRequest`\n        :param req: The HTTPRequest object.\n        \"\"\"\n        # This could be a retry.  Make sure the previous\n        # authorization header is removed first.\n        if 'X-Amzn-Authorization' in req.headers:\n            del req.headers['X-Amzn-Authorization']\n        now = datetime.datetime.utcnow()\n        req.headers['X-Amz-Date'] = now.strftime('%Y%m%dT%H%M%SZ')\n        if self._provider.security_token:\n            req.headers['X-Amz-Security-Token'] = self._provider.security_token\n        qs = self.query_string(req)\n\n        qs_to_post = qs\n\n        # We do not want to include any params that were mangled into\n        # the params if performing s3-sigv4 since it does not\n        # belong in the body of a post for some requests.  Mangled\n        # refers to items in the query string URL being added to the\n        # http response params. However, these params get added to\n        # the body of the request, but the query string URL does not\n        # belong in the body of the request. ``unmangled_resp`` is the\n        # response that happened prior to the mangling.  This ``unmangled_req``\n        # kwarg will only appear for s3-sigv4.\n        if 'unmangled_req' in kwargs:\n            qs_to_post = self.query_string(kwargs['unmangled_req'])\n\n        if qs_to_post and req.method == 'POST':\n            # Stash request parameters into post body\n            # before we generate the signature.\n            req.body = qs_to_post\n            req.headers['Content-Type'] = 'application/x-www-form-urlencoded; charset=UTF-8'\n            req.headers['Content-Length'] = str(len(req.body))\n        else:\n            # Safe to modify req.path here since\n            # the signature will use req.auth_path.\n            req.path = req.path.split('?')[0]\n\n            if qs:\n                # Don't insert the '?' unless there's actually a query string\n                req.path = req.path + '?' + qs\n        canonical_request = self.canonical_request(req)\n        boto.log.debug('CanonicalRequest:\\n%s' % canonical_request)\n        string_to_sign = self.string_to_sign(req, canonical_request)\n        boto.log.debug('StringToSign:\\n%s' % string_to_sign)\n        signature = self.signature(req, string_to_sign)\n        boto.log.debug('Signature:\\n%s' % signature)\n        headers_to_sign = self.headers_to_sign(req)\n        l = ['AWS4-HMAC-SHA256 Credential=%s' % self.scope(req)]\n        l.append('SignedHeaders=%s' % self.signed_headers(headers_to_sign))\n        l.append('Signature=%s' % signature)\n        req.headers['Authorization'] = ','.join(l)\n\n\nclass S3HmacAuthV4Handler(HmacAuthV4Handler, AuthHandler):\n    \"\"\"\n    Implements a variant of Version 4 HMAC authorization specific to S3.\n    \"\"\"\n    capability = ['hmac-v4-s3']\n\n    def __init__(self, *args, **kwargs):\n        super(S3HmacAuthV4Handler, self).__init__(*args, **kwargs)\n\n        if self.region_name:\n            self.region_name = self.clean_region_name(self.region_name)\n\n    def clean_region_name(self, region_name):\n        if region_name.startswith('s3-'):\n            return region_name[3:]\n\n        return region_name\n\n    def canonical_uri(self, http_request):\n        # S3 does **NOT** do path normalization that SigV4 typically does.\n        # Urlencode the path, **NOT** ``auth_path`` (because vhosting).\n        path = urllib.parse.urlparse(http_request.path)\n        # Because some quoting may have already been applied, let's back it out.\n        unquoted = urllib.parse.unquote(path.path)\n        # Requote, this time addressing all characters.\n        encoded = urllib.parse.quote(unquoted, safe='/~')\n        return encoded\n\n    def canonical_query_string(self, http_request):\n        # Note that we just do not return an empty string for\n        # POST request. Query strings in url are included in canonical\n        # query string.\n        l = []\n        for param in sorted(http_request.params):\n            value = boto.utils.get_utf8_value(http_request.params[param])\n            l.append('%s=%s' % (urllib.parse.quote(param, safe='-_.~'),\n                                urllib.parse.quote(value, safe='-_.~')))\n        return '&'.join(l)\n\n    def host_header(self, host, http_request):\n        port = http_request.port\n        secure = http_request.protocol == 'https'\n        if ((port == 80 and not secure) or (port == 443 and secure)):\n            return http_request.host\n        return '%s:%s' % (http_request.host, port)\n\n    def headers_to_sign(self, http_request):\n        \"\"\"\n        Select the headers from the request that need to be included\n        in the StringToSign.\n        \"\"\"\n        host_header_value = self.host_header(self.host, http_request)\n        headers_to_sign = {'Host': host_header_value}\n        for name, value in http_request.headers.items():\n            lname = name.lower()\n            # Hooray for the only difference! The main SigV4 signer only does\n            # ``Host`` + ``x-amz-*``. But S3 wants pretty much everything\n            # signed, except for authorization itself.\n            if lname not in ['authorization']:\n                headers_to_sign[name] = value\n        return headers_to_sign\n\n    def determine_region_name(self, host):\n        # S3's different format(s) of representing region/service from the\n        # rest of AWS makes this hurt too.\n        #\n        # Possible domain formats:\n        # - s3.amazonaws.com (Classic)\n        # - s3-us-west-2.amazonaws.com (Specific region)\n        # - bukkit.s3.amazonaws.com (Vhosted Classic)\n        # - bukkit.s3-ap-northeast-1.amazonaws.com (Vhosted specific region)\n        # - s3.cn-north-1.amazonaws.com.cn - (Beijing region)\n        # - bukkit.s3.cn-north-1.amazonaws.com.cn - (Vhosted Beijing region)\n        parts = self.split_host_parts(host)\n\n        if self.region_name is not None:\n            region_name = self.region_name\n        else:\n            # Classic URLs - s3-us-west-2.amazonaws.com\n            if len(parts) == 3:\n                region_name = self.clean_region_name(parts[0])\n\n                # Special-case for Classic.\n                if region_name == 's3':\n                    region_name = 'us-east-1'\n            else:\n                # Iterate over the parts in reverse order.\n                for offset, part in enumerate(reversed(parts)):\n                    part = part.lower()\n\n                    # Look for the first thing starting with 's3'.\n                    # Until there's a ``.s3`` TLD, we should be OK. :P\n                    if part == 's3':\n                        # If it's by itself, the region is the previous part.\n                        region_name = parts[-offset]\n\n                        # Unless it's Vhosted classic\n                        if region_name == 'amazonaws':\n                            region_name = 'us-east-1'\n\n                        break\n                    elif part.startswith('s3-'):\n                        region_name = self.clean_region_name(part)\n                        break\n\n        return region_name\n\n    def determine_service_name(self, host):\n        # Should this signing mechanism ever be used for anything else, this\n        # will fail. Consider utilizing the logic from the parent class should\n        # you find yourself here.\n        return 's3'\n\n    def mangle_path_and_params(self, req):\n        \"\"\"\n        Returns a copy of the request object with fixed ``auth_path/params``\n        attributes from the original.\n        \"\"\"\n        modified_req = copy.copy(req)\n\n        # Unlike the most other services, in S3, ``req.params`` isn't the only\n        # source of query string parameters.\n        # Because of the ``query_args``, we may already have a query string\n        # **ON** the ``path/auth_path``.\n        # Rip them apart, so the ``auth_path/params`` can be signed\n        # appropriately.\n        parsed_path = urllib.parse.urlparse(modified_req.auth_path)\n        modified_req.auth_path = parsed_path.path\n\n        if modified_req.params is None:\n            modified_req.params = {}\n        else:\n            # To keep the original request object untouched. We must make\n            # a copy of the params dictionary. Because the copy of the\n            # original request directly refers to the params dictionary\n            # of the original request.\n            copy_params = req.params.copy()\n            modified_req.params = copy_params\n\n        raw_qs = parsed_path.query\n        existing_qs = parse_qs_safe(\n            raw_qs,\n            keep_blank_values=True\n        )\n\n        # ``parse_qs`` will return lists. Don't do that unless there's a real,\n        # live list provided.\n        for key, value in existing_qs.items():\n            if isinstance(value, (list, tuple)):\n                if len(value) == 1:\n                    existing_qs[key] = value[0]\n\n        modified_req.params.update(existing_qs)\n        return modified_req\n\n    def payload(self, http_request):\n        if http_request.headers.get('x-amz-content-sha256'):\n            return http_request.headers['x-amz-content-sha256']\n\n        return super(S3HmacAuthV4Handler, self).payload(http_request)\n\n    def add_auth(self, req, **kwargs):\n        if 'x-amz-content-sha256' not in req.headers:\n            if '_sha256' in req.headers:\n                req.headers['x-amz-content-sha256'] = req.headers.pop('_sha256')\n            else:\n                req.headers['x-amz-content-sha256'] = self.payload(req)\n        updated_req = self.mangle_path_and_params(req)\n        return super(S3HmacAuthV4Handler, self).add_auth(updated_req,\n                                                         unmangled_req=req,\n                                                         **kwargs)\n\n    def presign(self, req, expires, iso_date=None):\n        \"\"\"\n        Presign a request using SigV4 query params. Takes in an HTTP request\n        and an expiration time in seconds and returns a URL.\n\n        http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\n        \"\"\"\n        if iso_date is None:\n            iso_date = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n\n        region = self.determine_region_name(req.host)\n        service = self.determine_service_name(req.host)\n\n        params = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': '%s/%s/%s/%s/aws4_request' % (\n                self._provider.access_key,\n                iso_date[:8],\n                region,\n                service\n            ),\n            'X-Amz-Date': iso_date,\n            'X-Amz-Expires': expires,\n            'X-Amz-SignedHeaders': 'host'\n        }\n\n        if self._provider.security_token:\n            params['X-Amz-Security-Token'] = self._provider.security_token\n\n        headers_to_sign = self.headers_to_sign(req)\n        l = sorted(['%s' % n.lower().strip() for n in headers_to_sign])\n        params['X-Amz-SignedHeaders'] = ';'.join(l)\n \n        req.params.update(params)\n\n        cr = self.canonical_request(req)\n\n        # We need to replace the payload SHA with a constant\n        cr = '\\n'.join(cr.split('\\n')[:-1]) + '\\nUNSIGNED-PAYLOAD'\n\n        # Date header is expected for string_to_sign, but unused otherwise\n        req.headers['X-Amz-Date'] = iso_date\n\n        sts = self.string_to_sign(req, cr)\n        signature = self.signature(req, sts)\n\n        # Add signature to params now that we have it\n        req.params['X-Amz-Signature'] = signature\n\n        return '%s://%s%s?%s' % (req.protocol, req.host, req.path,\n                                 urllib.parse.urlencode(req.params))\n\n\nclass STSAnonHandler(AuthHandler):\n    \"\"\"\n    Provides pure query construction (no actual signing).\n\n    Used for making anonymous STS request for operations like\n    ``assume_role_with_web_identity``.\n    \"\"\"\n\n    capability = ['sts-anon']\n\n    def _escape_value(self, value):\n        # This is changed from a previous version because this string is\n        # being passed to the query string and query strings must\n        # be url encoded. In particular STS requires the saml_response to\n        # be urlencoded when calling assume_role_with_saml.\n        return urllib.parse.quote(value)\n\n    def _build_query_string(self, params):\n        keys = list(params.keys())\n        keys.sort(key=lambda x: x.lower())\n        pairs = []\n        for key in keys:\n            val = boto.utils.get_utf8_value(params[key])\n            pairs.append(key + '=' + self._escape_value(val.decode('utf-8')))\n        return '&'.join(pairs)\n\n    def add_auth(self, http_request, **kwargs):\n        headers = http_request.headers\n        qs = self._build_query_string(\n            http_request.params\n        )\n        boto.log.debug('query_string in body: %s' % qs)\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n        # This will be  a POST so the query string should go into the body\n        # as opposed to being in the uri\n        http_request.body = qs\n\n\nclass QuerySignatureHelper(HmacKeys):\n    \"\"\"\n    Helper for Query signature based Auth handler.\n\n    Concrete sub class need to implement _calc_sigature method.\n    \"\"\"\n\n    def add_auth(self, http_request, **kwargs):\n        headers = http_request.headers\n        params = http_request.params\n        params['AWSAccessKeyId'] = self._provider.access_key\n        params['SignatureVersion'] = self.SignatureVersion\n        params['Timestamp'] = boto.utils.get_ts()\n        qs, signature = self._calc_signature(\n            http_request.params, http_request.method,\n            http_request.auth_path, http_request.host)\n        boto.log.debug('query_string: %s Signature: %s' % (qs, signature))\n        if http_request.method == 'POST':\n            headers['Content-Type'] = 'application/x-www-form-urlencoded; charset=UTF-8'\n            http_request.body = qs + '&Signature=' + urllib.parse.quote_plus(signature)\n            http_request.headers['Content-Length'] = str(len(http_request.body))\n        else:\n            http_request.body = ''\n            # if this is a retried request, the qs from the previous try will\n            # already be there, we need to get rid of that and rebuild it\n            http_request.path = http_request.path.split('?')[0]\n            http_request.path = (http_request.path + '?' + qs +\n                                 '&Signature=' + urllib.parse.quote_plus(signature))\n\n\nclass QuerySignatureV0AuthHandler(QuerySignatureHelper, AuthHandler):\n    \"\"\"Provides Signature V0 Signing\"\"\"\n\n    SignatureVersion = 0\n    capability = ['sign-v0']\n\n    def _calc_signature(self, params, *args):\n        boto.log.debug('using _calc_signature_0')\n        hmac = self._get_hmac()\n        s = params['Action'] + params['Timestamp']\n        hmac.update(s.encode('utf-8'))\n        keys = params.keys()\n        keys.sort(cmp=lambda x, y: cmp(x.lower(), y.lower()))\n        pairs = []\n        for key in keys:\n            val = boto.utils.get_utf8_value(params[key])\n            pairs.append(key + '=' + urllib.parse.quote(val))\n        qs = '&'.join(pairs)\n        return (qs, base64.b64encode(hmac.digest()))\n\n\nclass QuerySignatureV1AuthHandler(QuerySignatureHelper, AuthHandler):\n    \"\"\"\n    Provides Query Signature V1 Authentication.\n    \"\"\"\n\n    SignatureVersion = 1\n    capability = ['sign-v1', 'mturk']\n\n    def __init__(self, *args, **kw):\n        QuerySignatureHelper.__init__(self, *args, **kw)\n        AuthHandler.__init__(self, *args, **kw)\n        self._hmac_256 = None\n\n    def _calc_signature(self, params, *args):\n        boto.log.debug('using _calc_signature_1')\n        hmac = self._get_hmac()\n        keys = list(params.keys())\n        keys.sort(key=lambda x: x.lower())\n        pairs = []\n        for key in keys:\n            hmac.update(key.encode('utf-8'))\n            val = boto.utils.get_utf8_value(params[key])\n            hmac.update(val)\n            pairs.append(key + '=' + urllib.parse.quote(val))\n        qs = '&'.join(pairs)\n        return (qs, base64.b64encode(hmac.digest()))\n\n\nclass QuerySignatureV2AuthHandler(QuerySignatureHelper, AuthHandler):\n    \"\"\"Provides Query Signature V2 Authentication.\"\"\"\n\n    SignatureVersion = 2\n    capability = ['sign-v2', 'ec2', 'ec2', 'emr', 'fps', 'ecs',\n                  'sdb', 'iam', 'rds', 'sns', 'sqs', 'cloudformation']\n\n    def _calc_signature(self, params, verb, path, server_name):\n        boto.log.debug('using _calc_signature_2')\n        string_to_sign = '%s\\n%s\\n%s\\n' % (verb, server_name.lower(), path)\n        hmac = self._get_hmac()\n        params['SignatureMethod'] = self.algorithm()\n        if self._provider.security_token:\n            params['SecurityToken'] = self._provider.security_token\n        keys = sorted(params.keys())\n        pairs = []\n        for key in keys:\n            val = boto.utils.get_utf8_value(params[key])\n            pairs.append(urllib.parse.quote(key, safe='') + '=' +\n                         urllib.parse.quote(val, safe='-_~'))\n        qs = '&'.join(pairs)\n        boto.log.debug('query string: %s' % qs)\n        string_to_sign += qs\n        boto.log.debug('string_to_sign: %s' % string_to_sign)\n        hmac.update(string_to_sign.encode('utf-8'))\n        b64 = base64.b64encode(hmac.digest())\n        boto.log.debug('len(b64)=%d' % len(b64))\n        boto.log.debug('base64 encoded digest: %s' % b64)\n        return (qs, b64)\n\n\nclass POSTPathQSV2AuthHandler(QuerySignatureV2AuthHandler, AuthHandler):\n    \"\"\"\n    Query Signature V2 Authentication relocating signed query\n    into the path and allowing POST requests with Content-Types.\n    \"\"\"\n\n    capability = ['mws']\n\n    def add_auth(self, req, **kwargs):\n        req.params['AWSAccessKeyId'] = self._provider.access_key\n        req.params['SignatureVersion'] = self.SignatureVersion\n        req.params['Timestamp'] = boto.utils.get_ts()\n        qs, signature = self._calc_signature(req.params, req.method,\n                                             req.auth_path, req.host)\n        boto.log.debug('query_string: %s Signature: %s' % (qs, signature))\n        if req.method == 'POST':\n            req.headers['Content-Length'] = str(len(req.body))\n            req.headers['Content-Type'] = req.headers.get('Content-Type',\n                                                          'text/plain')\n        else:\n            req.body = ''\n        # if this is a retried req, the qs from the previous try will\n        # already be there, we need to get rid of that and rebuild it\n        req.path = req.path.split('?')[0]\n        req.path = (req.path + '?' + qs +\n                    '&Signature=' + urllib.parse.quote_plus(signature))\n\n\ndef get_auth_handler(host, config, provider, requested_capability=None):\n    \"\"\"Finds an AuthHandler that is ready to authenticate.\n\n    Lists through all the registered AuthHandlers to find one that is willing\n    to handle for the requested capabilities, config and provider.\n\n    :type host: string\n    :param host: The name of the host\n\n    :type config:\n    :param config:\n\n    :type provider:\n    :param provider:\n\n    Returns:\n        An implementation of AuthHandler.\n\n    Raises:\n        boto.exception.NoAuthHandlerFound\n    \"\"\"\n    ready_handlers = []\n    auth_handlers = boto.plugin.get_plugin(AuthHandler, requested_capability)\n    for handler in auth_handlers:\n        try:\n            ready_handlers.append(handler(host, config, provider))\n        except boto.auth_handler.NotReadyToAuthenticate:\n            pass\n\n    if not ready_handlers:\n        checked_handlers = auth_handlers\n        names = [handler.__name__ for handler in checked_handlers]\n        raise boto.exception.NoAuthHandlerFound(\n            'No handler was ready to authenticate. %d handlers were checked.'\n            ' %s '\n            'Check your credentials' % (len(names), str(names)))\n\n    # We select the last ready auth handler that was loaded, to allow users to\n    # customize how auth works in environments where there are shared boto\n    # config files (e.g., /etc/boto.cfg and ~/.boto): The more general,\n    # system-wide shared configs should be loaded first, and the user's\n    # customizations loaded last. That way, for example, the system-wide\n    # config might include a plugin_directory that includes a service account\n    # auth plugin shared by all users of a Google Compute Engine instance\n    # (allowing sharing of non-user data between various services), and the\n    # user could override this with a .boto config that includes user-specific\n    # credentials (for access to user data).\n    return ready_handlers[-1]\n\n\ndef detect_potential_sigv4(func):\n    def _wrapper(self):\n        if os.environ.get('EC2_USE_SIGV4', False):\n            return ['hmac-v4']\n\n        if boto.config.get('ec2', 'use-sigv4', False):\n            return ['hmac-v4']\n\n        if hasattr(self, 'region'):\n            # If you're making changes here, you should also check\n            # ``boto/iam/connection.py``, as several things there are also\n            # endpoint-related.\n            if getattr(self.region, 'endpoint', ''):\n                for test in SIGV4_DETECT:\n                    if test in self.region.endpoint:\n                        return ['hmac-v4']\n\n        return func(self)\n    return _wrapper\n\n\ndef detect_potential_s3sigv4(func):\n    def _wrapper(self):\n        if os.environ.get('S3_USE_SIGV4', False):\n            return ['hmac-v4-s3']\n\n        if boto.config.get('s3', 'use-sigv4', False):\n            return ['hmac-v4-s3']\n\n        if not hasattr(self, 'host'):\n            return func(self)\n\n        # Keep the old explicit logic in case somebody was adding to the list.\n        for test in SIGV4_DETECT:\n            if test in self.host:\n                return ['hmac-v4-s3']\n\n        # Use default for non-aws hosts. Adding a url scheme is necessary if\n        # not present for urlparse to properly function.\n        host = self.host\n        if not self.host.startswith('http://') or \\\n                self.host.startswith('https://'):\n            host = 'https://' + host\n        netloc = urlparse(host).netloc\n        if not (netloc.endswith('amazonaws.com') or\n                netloc.endswith('amazonaws.com.cn')):\n            return func(self)\n\n        # Use the default for the global endpoint\n        if netloc.endswith('s3.amazonaws.com'):\n            return func(self)\n\n        # Use the default for regions that support sigv4 and sigv2\n        if any(test in self.host for test in S3_AUTH_DETECT):\n            return func(self)\n\n        # Use anonymous if enabled.\n        if hasattr(self, 'anon') and self.anon:\n            return func(self)\n\n        # Default to sigv4 for aws hosts outside of regions that are known\n        # to support sigv2\n        return ['hmac-v4-s3']\n    return _wrapper\n"}], "prompt": "Please write a python function called 'generate_url_sigv4' base the context. Generate a presigned URL with Signature Version 4 for accessing an S3 object. It constructs the necessary parameters and builds an HTTP request. Then, it uses the authentication handler to generate the presigned URL. For presigned URLs we should ignore the port if it's HTTPS:param self: S3Connection. An instance of S3Connection class\n:param expires_in: Integer. The number of seconds until the presigned URL expires.\n:param method: String. The HTTP method to be used for the request.\n:param bucket: String. The name of the S3 bucket.\n:param key: String. The key of the S3 object.\n:param headers: Dictionary. Additional headers to include in the request.\n:param force_http: Bool. Whether to force the use of HTTP instead of HTTPS.\n:param response_headers: Dictionary. Additional response headers to include in the presigned URL.\n:param version_id: String. The version ID of the S3 object.\n:param iso_date: String. The ISO-formatted date to be used for signing the request.\n:return: String. The generated presigned URL..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\n# All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n\nimport xml.sax\nimport base64\nfrom boto.compat import six, urllib\nimport time\n\nfrom boto.auth import detect_potential_s3sigv4\nimport boto.utils\nfrom boto.connection import AWSAuthConnection\nfrom boto import handler\nfrom boto.s3.bucket import Bucket\nfrom boto.s3.key import Key\nfrom boto.resultset import ResultSet\nfrom boto.exception import BotoClientError, S3ResponseError\n\n\ndef check_lowercase_bucketname(n):\n    \"\"\"\n    Bucket names must not contain uppercase characters. We check for\n    this by appending a lowercase character and testing with islower().\n    Note this also covers cases like numeric bucket names with dashes.\n\n    >>> check_lowercase_bucketname(\"Aaaa\")\n    Traceback (most recent call last):\n    ...\n    BotoClientError: S3Error: Bucket names cannot contain upper-case\n    characters when using either the sub-domain or virtual hosting calling\n    format.\n\n    >>> check_lowercase_bucketname(\"1234-5678-9123\")\n    True\n    >>> check_lowercase_bucketname(\"abcdefg1234\")\n    True\n    \"\"\"\n    if not (n + 'a').islower():\n        raise BotoClientError(\"Bucket names cannot contain upper-case \" \\\n            \"characters when using either the sub-domain or virtual \" \\\n            \"hosting calling format.\")\n    return True\n\n\ndef assert_case_insensitive(f):\n    def wrapper(*args, **kwargs):\n        if len(args) == 3 and check_lowercase_bucketname(args[2]):\n            pass\n        return f(*args, **kwargs)\n    return wrapper\n\n\nclass _CallingFormat(object):\n\n    def get_bucket_server(self, server, bucket):\n        return ''\n\n    def build_url_base(self, connection, protocol, server, bucket, key=''):\n        url_base = '%s://' % protocol\n        url_base += self.build_host(server, bucket)\n        url_base += connection.get_path(self.build_path_base(bucket, key))\n        return url_base\n\n    def build_host(self, server, bucket):\n        if bucket == '':\n            return server\n        else:\n            return self.get_bucket_server(server, bucket)\n\n    def build_auth_path(self, bucket, key=''):\n        key = boto.utils.get_utf8_value(key)\n        path = ''\n        if bucket != '':\n            path = '/' + bucket\n        return path + '/%s' % urllib.parse.quote(key)\n\n    def build_path_base(self, bucket, key=''):\n        key = boto.utils.get_utf8_value(key)\n        return '/%s' % urllib.parse.quote(key)\n\n\nclass SubdomainCallingFormat(_CallingFormat):\n\n    @assert_case_insensitive\n    def get_bucket_server(self, server, bucket):\n        return '%s.%s' % (bucket, server)\n\n\nclass VHostCallingFormat(_CallingFormat):\n\n    @assert_case_insensitive\n    def get_bucket_server(self, server, bucket):\n        return bucket\n\n\nclass OrdinaryCallingFormat(_CallingFormat):\n\n    def get_bucket_server(self, server, bucket):\n        return server\n\n    def build_path_base(self, bucket, key=''):\n        key = boto.utils.get_utf8_value(key)\n        path_base = '/'\n        if bucket:\n            path_base += \"%s/\" % bucket\n        return path_base + urllib.parse.quote(key)\n\n\nclass ProtocolIndependentOrdinaryCallingFormat(OrdinaryCallingFormat):\n\n    def build_url_base(self, connection, protocol, server, bucket, key=''):\n        url_base = '//'\n        url_base += self.build_host(server, bucket)\n        url_base += connection.get_path(self.build_path_base(bucket, key))\n        return url_base\n\n\nclass Location(object):\n\n    DEFAULT = ''  # US Classic Region\n    EU = 'EU'  # Ireland\n    EUCentral1 = 'eu-central-1'  # Frankfurt\n    USWest = 'us-west-1'\n    USWest2 = 'us-west-2'\n    SAEast = 'sa-east-1'\n    APNortheast = 'ap-northeast-1'\n    APSoutheast = 'ap-southeast-1'\n    APSoutheast2 = 'ap-southeast-2'\n    CNNorth1 = 'cn-north-1'\n\n\nclass NoHostProvided(object):\n    # An identifying object to help determine whether the user provided a\n    # ``host`` or not. Never instantiated.\n    pass\n\n\nclass HostRequiredError(BotoClientError):\n    pass\n\n\nclass S3Connection(AWSAuthConnection):\n\n    DefaultHost = 's3.amazonaws.com'\n    DefaultCallingFormat = boto.config.get('s3', 'calling_format', 'boto.s3.connection.SubdomainCallingFormat')\n    QueryString = 'Signature=%s&Expires=%d&AWSAccessKeyId=%s'\n\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\n                 proxy_user=None, proxy_pass=None,\n                 host=NoHostProvided, debug=0, https_connection_factory=None,\n                 calling_format=DefaultCallingFormat, path='/',\n                 provider='aws', bucket_class=Bucket, security_token=None,\n                 suppress_consec_slashes=True, anon=False,\n                 validate_certs=None, profile_name=None):\n        no_host_provided = False\n        # Try falling back to the boto config file's value, if present.\n        if host is NoHostProvided:\n            host = boto.config.get('s3', 'host')\n            if host is None:\n                host = self.DefaultHost\n                no_host_provided = True\n        if isinstance(calling_format, six.string_types):\n            calling_format=boto.utils.find_class(calling_format)()\n        self.calling_format = calling_format\n        self.bucket_class = bucket_class\n        self.anon = anon\n        super(S3Connection, self).__init__(host,\n                aws_access_key_id, aws_secret_access_key,\n                is_secure, port, proxy, proxy_port, proxy_user, proxy_pass,\n                debug=debug, https_connection_factory=https_connection_factory,\n                path=path, provider=provider, security_token=security_token,\n                suppress_consec_slashes=suppress_consec_slashes,\n                validate_certs=validate_certs, profile_name=profile_name)\n        # We need to delay until after the call to ``super`` before checking\n        # to see if SigV4 is in use.\n        if no_host_provided:\n            if 'hmac-v4-s3' in self._required_auth_capability():\n                raise HostRequiredError(\n                    \"When using SigV4, you must specify a 'host' parameter.\"\n                )\n\n    @detect_potential_s3sigv4\n    def _required_auth_capability(self):\n        if self.anon:\n            return ['anon']\n        else:\n            return ['s3']\n\n    def __iter__(self):\n        for bucket in self.get_all_buckets():\n            yield bucket\n\n    def __contains__(self, bucket_name):\n        return not (self.lookup(bucket_name) is None)\n\n    def set_bucket_class(self, bucket_class):\n        \"\"\"\n        Set the Bucket class associated with this bucket.  By default, this\n        would be the boto.s3.key.Bucket class but if you want to subclass that\n        for some reason this allows you to associate your new class.\n\n        :type bucket_class: class\n        :param bucket_class: A subclass of Bucket that can be more specific\n        \"\"\"\n        self.bucket_class = bucket_class\n\n    def build_post_policy(self, expiration_time, conditions):\n        \"\"\"\n        Taken from the AWS book Python examples and modified for use with boto\n        \"\"\"\n        assert isinstance(expiration_time, time.struct_time), \\\n            'Policy document must include a valid expiration Time object'\n\n        # Convert conditions object mappings to condition statements\n\n        return '{\"expiration\": \"%s\",\\n\"conditions\": [%s]}' % \\\n            (time.strftime(boto.utils.ISO8601, expiration_time), \",\".join(conditions))\n\n    def build_post_form_args(self, bucket_name, key, expires_in=6000,\n                             acl=None, success_action_redirect=None,\n                             max_content_length=None,\n                             http_method='http', fields=None,\n                             conditions=None, storage_class='STANDARD',\n                             server_side_encryption=None):\n        \"\"\"\n        Taken from the AWS book Python examples and modified for use with boto\n        This only returns the arguments required for the post form, not the\n        actual form.  This does not return the file input field which also\n        needs to be added\n\n        :type bucket_name: string\n        :param bucket_name: Bucket to submit to\n\n        :type key: string\n        :param key:  Key name, optionally add ${filename} to the end to\n            attach the submitted filename\n\n        :type expires_in: integer\n        :param expires_in: Time (in seconds) before this expires, defaults\n            to 6000\n\n        :type acl: string\n        :param acl: A canned ACL.  One of:\n            * private\n            * public-read\n            * public-read-write\n            * authenticated-read\n            * bucket-owner-read\n            * bucket-owner-full-control\n\n        :type success_action_redirect: string\n        :param success_action_redirect: URL to redirect to on success\n\n        :type max_content_length: integer\n        :param max_content_length: Maximum size for this file\n\n        :type http_method: string\n        :param http_method:  HTTP Method to use, \"http\" or \"https\"\n\n        :type storage_class: string\n        :param storage_class: Storage class to use for storing the object.\n            Valid values: STANDARD | REDUCED_REDUNDANCY\n\n        :type server_side_encryption: string\n        :param server_side_encryption: Specifies server-side encryption\n            algorithm to use when Amazon S3 creates an object.\n            Valid values: None | AES256\n\n        :rtype: dict\n        :return: A dictionary containing field names/values as well as\n            a url to POST to\n\n            .. code-block:: python\n\n\n        \"\"\"\n        if fields is None:\n            fields = []\n        if conditions is None:\n            conditions = []\n        expiration = time.gmtime(int(time.time() + expires_in))\n\n        # Generate policy document\n        conditions.append('{\"bucket\": \"%s\"}' % bucket_name)\n        if key.endswith(\"${filename}\"):\n            conditions.append('[\"starts-with\", \"$key\", \"%s\"]' % key[:-len(\"${filename}\")])\n        else:\n            conditions.append('{\"key\": \"%s\"}' % key)\n        if acl:\n            conditions.append('{\"acl\": \"%s\"}' % acl)\n            fields.append({\"name\": \"acl\", \"value\": acl})\n        if success_action_redirect:\n            conditions.append('{\"success_action_redirect\": \"%s\"}' % success_action_redirect)\n            fields.append({\"name\": \"success_action_redirect\", \"value\": success_action_redirect})\n        if max_content_length:\n            conditions.append('[\"content-length-range\", 0, %i]' % max_content_length)\n\n        if self.provider.security_token:\n            fields.append({'name': 'x-amz-security-token',\n                           'value': self.provider.security_token})\n            conditions.append('{\"x-amz-security-token\": \"%s\"}' % self.provider.security_token)\n\n        if storage_class:\n            fields.append({'name': 'x-amz-storage-class',\n                           'value': storage_class})\n            conditions.append('{\"x-amz-storage-class\": \"%s\"}' % storage_class)\n\n        if server_side_encryption:\n            fields.append({'name': 'x-amz-server-side-encryption',\n                           'value': server_side_encryption})\n            conditions.append('{\"x-amz-server-side-encryption\": \"%s\"}' % server_side_encryption)\n\n        policy = self.build_post_policy(expiration, conditions)\n\n        # Add the base64-encoded policy document as the 'policy' field\n        policy_b64 = base64.b64encode(policy)\n        fields.append({\"name\": \"policy\", \"value\": policy_b64})\n\n        # Add the AWS access key as the 'AWSAccessKeyId' field\n        fields.append({\"name\": \"AWSAccessKeyId\",\n                       \"value\": self.aws_access_key_id})\n\n        # Add signature for encoded policy document as the\n        # 'signature' field\n        signature = self._auth_handler.sign_string(policy_b64)\n        fields.append({\"name\": \"signature\", \"value\": signature})\n        fields.append({\"name\": \"key\", \"value\": key})\n\n        # HTTPS protocol will be used if the secure HTTP option is enabled.\n        url = '%s://%s/' % (http_method,\n                            self.calling_format.build_host(self.server_name(),\n                                                           bucket_name))\n\n        return {\"action\": url, \"fields\": fields}\n\n###The function: generate_url_sigv4###\n    def generate_url(self, expires_in, method, bucket='', key='', headers=None,\n                     query_auth=True, force_http=False, response_headers=None,\n                     expires_in_absolute=False, version_id=None):\n        if self._auth_handler.capability[0] == 'hmac-v4-s3' and query_auth:\n            # Handle the special sigv4 case\n            return self.generate_url_sigv4(expires_in, method, bucket=bucket,\n                key=key, headers=headers, force_http=force_http,\n                response_headers=response_headers, version_id=version_id)\n\n        headers = headers or {}\n        if expires_in_absolute:\n            expires = int(expires_in)\n        else:\n            expires = int(time.time() + expires_in)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        auth_path = self.get_path(auth_path)\n        # optional version_id and response_headers need to be added to\n        # the query param list.\n        extra_qp = []\n        if version_id is not None:\n            extra_qp.append(\"versionId=%s\" % version_id)\n        if response_headers:\n            for k, v in response_headers.items():\n                extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n        if extra_qp:\n            delimiter = '?' if '?' not in auth_path else '&'\n            auth_path += delimiter + '&'.join(extra_qp)\n        self.calling_format.build_path_base(bucket, key)\n        if query_auth and not self.anon:\n            c_string = boto.utils.canonical_string(method, auth_path, headers,\n                                                   expires, self.provider)\n            b64_hmac = self._auth_handler.sign_string(c_string)\n            encoded_canonical = urllib.parse.quote(b64_hmac, safe='')\n            query_part = '?' + self.QueryString % (encoded_canonical, expires,\n                                                   self.aws_access_key_id)\n        else:\n            query_part = ''\n        if headers:\n            hdr_prefix = self.provider.header_prefix\n            for k, v in headers.items():\n                if k.startswith(hdr_prefix):\n                    # headers used for sig generation must be\n                    # included in the url also.\n                    extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if extra_qp:\n            delimiter = '?' if not query_part else '&'\n            query_part += delimiter + '&'.join(extra_qp)\n        if force_http:\n            protocol = 'http'\n            port = 80\n        else:\n            protocol = self.protocol\n            port = self.port\n        return self.calling_format.build_url_base(self, protocol,\n                                                  self.server_name(port),\n                                                  bucket, key) + query_part\n\n    def get_all_buckets(self, headers=None):\n        response = self.make_request('GET', headers=headers)\n        body = response.read()\n        if response.status > 300:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n        rs = ResultSet([('Bucket', self.bucket_class)])\n        h = handler.XmlHandler(rs, self)\n        if not isinstance(body, bytes):\n            body = body.encode('utf-8')\n        xml.sax.parseString(body, h)\n        return rs\n\n    def get_canonical_user_id(self, headers=None):\n        \"\"\"\n        Convenience method that returns the \"CanonicalUserID\" of the\n        user who's credentials are associated with the connection.\n        The only way to get this value is to do a GET request on the\n        service which returns all buckets associated with the account.\n        As part of that response, the canonical userid is returned.\n        This method simply does all of that and then returns just the\n        user id.\n\n        :rtype: string\n        :return: A string containing the canonical user id.\n        \"\"\"\n        rs = self.get_all_buckets(headers=headers)\n        return rs.owner.id\n\n    def get_bucket(self, bucket_name, validate=True, headers=None):\n        \"\"\"\n        Retrieves a bucket by name.\n\n        If the bucket does not exist, an ``S3ResponseError`` will be raised. If\n        you are unsure if the bucket exists or not, you can use the\n        ``S3Connection.lookup`` method, which will either return a valid bucket\n        or ``None``.\n\n        If ``validate=False`` is passed, no request is made to the service (no\n        charge/communication delay). This is only safe to do if you are **sure**\n        the bucket exists.\n\n        If the default ``validate=True`` is passed, a request is made to the\n        service to ensure the bucket exists. Prior to Boto v2.25.0, this fetched\n        a list of keys (but with a max limit set to ``0``, always returning an empty\n        list) in the bucket (& included better error messages), at an\n        increased expense. As of Boto v2.25.0, this now performs a HEAD request\n        (less expensive but worse error messages).\n\n        If you were relying on parsing the error message before, you should call\n        something like::\n\n            bucket = conn.get_bucket('<bucket_name>', validate=False)\n            bucket.get_all_keys(maxkeys=0)\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n\n        :type validate: boolean\n        :param validate: If ``True``, it will try to verify the bucket exists\n            on the service-side. (Default: ``True``)\n        \"\"\"\n        if validate:\n            return self.head_bucket(bucket_name, headers=headers)\n        else:\n            return self.bucket_class(self, bucket_name)\n\n    def head_bucket(self, bucket_name, headers=None):\n        \"\"\"\n        Determines if a bucket exists by name.\n\n        If the bucket does not exist, an ``S3ResponseError`` will be raised.\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n\n        :returns: A <Bucket> object\n        \"\"\"\n        response = self.make_request('HEAD', bucket_name, headers=headers)\n        body = response.read()\n        if response.status == 200:\n            return self.bucket_class(self, bucket_name)\n        elif response.status == 403:\n            # For backward-compatibility, we'll populate part of the exception\n            # with the most-common default.\n            err = self.provider.storage_response_error(\n                response.status,\n                response.reason,\n                body\n            )\n            err.error_code = 'AccessDenied'\n            err.error_message = 'Access Denied'\n            raise err\n        elif response.status == 404:\n            # For backward-compatibility, we'll populate part of the exception\n            # with the most-common default.\n            err = self.provider.storage_response_error(\n                response.status,\n                response.reason,\n                body\n            )\n            err.error_code = 'NoSuchBucket'\n            err.error_message = 'The specified bucket does not exist'\n            raise err\n        else:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n\n    def lookup(self, bucket_name, validate=True, headers=None):\n        \"\"\"\n        Attempts to get a bucket from S3.\n\n        Works identically to ``S3Connection.get_bucket``, save for that it\n        will return ``None`` if the bucket does not exist instead of throwing\n        an exception.\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n\n        :type validate: boolean\n        :param validate: If ``True``, it will try to fetch all keys within the\n            given bucket. (Default: ``True``)\n        \"\"\"\n        try:\n            bucket = self.get_bucket(bucket_name, validate, headers=headers)\n        except:\n            bucket = None\n        return bucket\n\n    def create_bucket(self, bucket_name, headers=None,\n                      location=Location.DEFAULT, policy=None):\n        \"\"\"\n        Creates a new located bucket. By default it's in the USA. You can pass\n        Location.EU to create a European bucket (S3) or European Union bucket\n        (GCS).\n\n        :type bucket_name: string\n        :param bucket_name: The name of the new bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to AWS.\n\n        :type location: str\n        :param location: The location of the new bucket.  You can use one of the\n            constants in :class:`boto.s3.connection.Location` (e.g. Location.EU,\n            Location.USWest, etc.).\n\n        :type policy: :class:`boto.s3.acl.CannedACLStrings`\n        :param policy: A canned ACL policy that will be applied to the\n            new key in S3.\n\n        \"\"\"\n        check_lowercase_bucketname(bucket_name)\n\n        if policy:\n            if headers:\n                headers[self.provider.acl_header] = policy\n            else:\n                headers = {self.provider.acl_header: policy}\n        if location == Location.DEFAULT:\n            data = ''\n        else:\n            data = '<CreateBucketConfiguration><LocationConstraint>' + \\\n                    location + '</LocationConstraint></CreateBucketConfiguration>'\n        response = self.make_request('PUT', bucket_name, headers=headers,\n                data=data)\n        body = response.read()\n        if response.status == 409:\n            raise self.provider.storage_create_error(\n                response.status, response.reason, body)\n        if response.status == 200:\n            return self.bucket_class(self, bucket_name)\n        else:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n\n    def delete_bucket(self, bucket, headers=None):\n        \"\"\"\n        Removes an S3 bucket.\n\n        In order to remove the bucket, it must first be empty. If the bucket is\n        not empty, an ``S3ResponseError`` will be raised.\n\n        :type bucket_name: string\n        :param bucket_name: The name of the bucket\n\n        :type headers: dict\n        :param headers: Additional headers to pass along with the request to\n            AWS.\n        \"\"\"\n        response = self.make_request('DELETE', bucket, headers=headers)\n        body = response.read()\n        if response.status != 204:\n            raise self.provider.storage_response_error(\n                response.status, response.reason, body)\n\n    def make_request(self, method, bucket='', key='', headers=None, data='',\n                     query_args=None, sender=None, override_num_retries=None,\n                     retry_handler=None):\n        if isinstance(bucket, self.bucket_class):\n            bucket = bucket.name\n        if isinstance(key, Key):\n            key = key.name\n        path = self.calling_format.build_path_base(bucket, key)\n        boto.log.debug('path=%s' % path)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        boto.log.debug('auth_path=%s' % auth_path)\n        host = self.calling_format.build_host(self.server_name(), bucket)\n        if query_args:\n            path += '?' + query_args\n            boto.log.debug('path=%s' % path)\n            auth_path += '?' + query_args\n            boto.log.debug('auth_path=%s' % auth_path)\n        return super(S3Connection, self).make_request(\n            method, path, headers,\n            data, host, auth_path, sender,\n            override_num_retries=override_num_retries,\n            retry_handler=retry_handler\n        )\n\n        ####cross_file_context:\n        [{'boto.connection.AWSAuthConnection._auth_handler': '# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\\n# Copyright (c) 2010 Google\\n# Copyright (c) 2008 rPath, Inc.\\n# Copyright (c) 2009 The Echo Nest Corporation\\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\\n# Copyright (c) 2011, Nexenta Systems Inc.\\n# All rights reserved.\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a\\n# copy of this software and associated documentation files (the\\n# \"Software\"), to deal in the Software without restriction, including\\n# without limitation the rights to use, copy, modify, merge, publish, dis-\\n# tribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the fol-\\n# lowing conditions:\\n#\\n# The above copyright notice and this permission notice shall be included\\n# in all copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\\n# IN THE SOFTWARE.\\n\\n#\\n# Parts of this code were copied or derived from sample code supplied by AWS.\\n# The following notice applies to that code.\\n#\\n#  This software code is made available \"AS IS\" without warranties of any\\n#  kind.  You may copy, display, modify and redistribute the software\\n#  code either by itself or as incorporated into your code; provided that\\n#  you do not remove any proprietary notices.  Your use of this software\\n#  code is at your own risk and you waive any claim against Amazon\\n#  Digital Services, Inc. or its affiliates with respect to your use of\\n#  this software code. (c) 2006 Amazon Digital Services, Inc. or its\\n#  affiliates.\\n\\n\"\"\"\\nHandles basic connections to AWS\\n\"\"\"\\nfrom datetime import datetime\\nimport errno\\nimport os\\nimport random\\nimport re\\nimport socket\\nimport sys\\nimport time\\nimport xml.sax\\nimport copy\\n\\nfrom boto import auth\\nfrom boto import auth_handler\\nimport boto\\nimport boto.utils\\nimport boto.handler\\nimport boto.cacerts\\n\\nfrom boto import config, UserAgent\\nfrom boto.compat import six, http_client, urlparse, quote, encodebytes\\nfrom boto.exception import AWSConnectionError\\nfrom boto.exception import BotoClientError\\nfrom boto.exception import BotoServerError\\nfrom boto.exception import PleaseRetryException\\nfrom boto.provider import Provider\\nfrom boto.resultset import ResultSet\\n\\nHAVE_HTTPS_CONNECTION = False\\ntry:\\n    import ssl\\n    from boto import https_connection\\n    # Google App Engine runs on Python 2.5 so doesn\\'t have ssl.SSLError.\\n    if hasattr(ssl, \\'SSLError\\'):\\n        HAVE_HTTPS_CONNECTION = True\\nexcept ImportError:\\n    pass\\n\\ntry:\\n    import threading\\nexcept ImportError:\\n    import dummy_threading as threading\\n\\nON_APP_ENGINE = all(key in os.environ for key in (\\n    \\'USER_IS_ADMIN\\', \\'CURRENT_VERSION_ID\\', \\'APPLICATION_ID\\'))\\n\\nPORTS_BY_SECURITY = {True: 443,\\n                     False: 80}\\n\\nDEFAULT_CA_CERTS_FILE = os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)), \"cacerts.txt\")\\n\\n\\nclass HostConnectionPool(object):\\n\\n    \"\"\"\\n    A pool of connections for one remote (host,port,is_secure).\\n\\n    When connections are added to the pool, they are put into a\\n    pending queue.  The _mexe method returns connections to the pool\\n    before the response body has been read, so they connections aren\\'t\\n    ready to send another request yet.  They stay in the pending queue\\n    until they are ready for another request, at which point they are\\n    returned to the pool of ready connections.\\n\\n    The pool of ready connections is an ordered list of\\n    (connection,time) pairs, where the time is the time the connection\\n    was returned from _mexe.  After a certain period of time,\\n    connections are considered stale, and discarded rather than being\\n    reused.  This saves having to wait for the connection to time out\\n    if AWS has decided to close it on the other end because of\\n    inactivity.\\n\\n    Thread Safety:\\n\\n        This class is used only from ConnectionPool while it\\'s mutex\\n        is held.\\n    \"\"\"\\n\\n    def __init__(self):\\n        self.queue = []\\n\\n    def size(self):\\n        \"\"\"\\n        Returns the number of connections in the pool for this host.\\n        Some of the connections may still be in use, and may not be\\n        ready to be returned by get().\\n        \"\"\"\\n        return len(self.queue)\\n\\n    def put(self, conn):\\n        \"\"\"\\n        Adds a connection to the pool, along with the time it was\\n        added.\\n        \"\"\"\\n        self.queue.append((conn, time.time()))\\n\\n    def get(self):\\n        \"\"\"\\n        Returns the next connection in this pool that is ready to be\\n        reused.  Returns None if there aren\\'t any.\\n        \"\"\"\\n        # Discard ready connections that are too old.\\n        self.clean()\\n\\n        # Return the first connection that is ready, and remove it\\n        # from the queue.  Connections that aren\\'t ready are returned\\n        # to the end of the queue with an updated time, on the\\n        # assumption that somebody is actively reading the response.\\n        for _ in range(len(self.queue)):\\n            (conn, _) = self.queue.pop(0)\\n            if self._conn_ready(conn):\\n                return conn\\n            else:\\n                self.put(conn)\\n        return None\\n\\n    def _conn_ready(self, conn):\\n        \"\"\"\\n        There is a nice state diagram at the top of http_client.py.  It\\n        indicates that once the response headers have been read (which\\n        _mexe does before adding the connection to the pool), a\\n        response is attached to the connection, and it stays there\\n        until it\\'s done reading.  This isn\\'t entirely true: even after\\n        the client is done reading, the response may be closed, but\\n        not removed from the connection yet.\\n\\n        This is ugly, reading a private instance variable, but the\\n        state we care about isn\\'t available in any public methods.\\n        \"\"\"\\n        if ON_APP_ENGINE:\\n            # Google AppEngine implementation of HTTPConnection doesn\\'t contain\\n            # _HTTPConnection__response attribute. Moreover, it\\'s not possible\\n            # to determine if given connection is ready. Reusing connections\\n            # simply doesn\\'t make sense with App Engine urlfetch service.\\n            return False\\n        else:\\n            response = getattr(conn, \\'_HTTPConnection__response\\', None)\\n            return (response is None) or response.isclosed()\\n\\n    def clean(self):\\n        \"\"\"\\n        Get rid of stale connections.\\n        \"\"\"\\n        # Note that we do not close the connection here -- somebody\\n        # may still be reading from it.\\n        while len(self.queue) > 0 and self._pair_stale(self.queue[0]):\\n            self.queue.pop(0)\\n\\n    def _pair_stale(self, pair):\\n        \"\"\"\\n        Returns true of the (connection,time) pair is too old to be\\n        used.\\n        \"\"\"\\n        (_conn, return_time) = pair\\n        now = time.time()\\n        return return_time + ConnectionPool.STALE_DURATION < now\\n\\n\\nclass ConnectionPool(object):\\n\\n    \"\"\"\\n    A connection pool that expires connections after a fixed period of\\n    time.  This saves time spent waiting for a connection that AWS has\\n    timed out on the other end.\\n\\n    This class is thread-safe.\\n    \"\"\"\\n\\n    #\\n    # The amout of time between calls to clean.\\n    #\\n\\n    CLEAN_INTERVAL = 5.0\\n\\n    #\\n    # How long before a connection becomes \"stale\" and won\\'t be reused\\n    # again.  The intention is that this time is less that the timeout\\n    # period that AWS uses, so we\\'ll never try to reuse a connection\\n    # and find that AWS is timing it out.\\n    #\\n    # Experimentation in July 2011 shows that AWS starts timing things\\n    # out after three minutes.  The 60 seconds here is conservative so\\n    # we should never hit that 3-minute timout.\\n    #\\n\\n    STALE_DURATION = 60.0\\n\\n    def __init__(self):\\n        # Mapping from (host,port,is_secure) to HostConnectionPool.\\n        # If a pool becomes empty, it is removed.\\n        self.host_to_pool = {}\\n        # The last time the pool was cleaned.\\n        self.last_clean_time = 0.0\\n        self.mutex = threading.Lock()\\n        ConnectionPool.STALE_DURATION = \\\\\\n            config.getfloat(\\'Boto\\', \\'connection_stale_duration\\',\\n                            ConnectionPool.STALE_DURATION)\\n\\n    def __getstate__(self):\\n        pickled_dict = copy.copy(self.__dict__)\\n        pickled_dict[\\'host_to_pool\\'] = {}\\n        del pickled_dict[\\'mutex\\']\\n        return pickled_dict\\n\\n    def __setstate__(self, dct):\\n        self.__init__()\\n\\n    def size(self):\\n        \"\"\"\\n        Returns the number of connections in the pool.\\n        \"\"\"\\n        return sum(pool.size() for pool in self.host_to_pool.values())\\n\\n    def get_http_connection(self, host, port, is_secure):\\n        \"\"\"\\n        Gets a connection from the pool for the named host.  Returns\\n        None if there is no connection that can be reused. It\\'s the caller\\'s\\n        responsibility to call close() on the connection when it\\'s no longer\\n        needed.\\n        \"\"\"\\n        self.clean()\\n        with self.mutex:\\n            key = (host, port, is_secure)\\n            if key not in self.host_to_pool:\\n                return None\\n            return self.host_to_pool[key].get()\\n\\n    def put_http_connection(self, host, port, is_secure, conn):\\n        \"\"\"\\n        Adds a connection to the pool of connections that can be\\n        reused for the named host.\\n        \"\"\"\\n        with self.mutex:\\n            key = (host, port, is_secure)\\n            if key not in self.host_to_pool:\\n                self.host_to_pool[key] = HostConnectionPool()\\n            self.host_to_pool[key].put(conn)\\n\\n    def clean(self):\\n        \"\"\"\\n        Clean up the stale connections in all of the pools, and then\\n        get rid of empty pools.  Pools clean themselves every time a\\n        connection is fetched; this cleaning takes care of pools that\\n        aren\\'t being used any more, so nothing is being gotten from\\n        them.\\n        \"\"\"\\n        with self.mutex:\\n            now = time.time()\\n            if self.last_clean_time + self.CLEAN_INTERVAL < now:\\n                to_remove = []\\n                for (host, pool) in self.host_to_pool.items():\\n                    pool.clean()\\n                    if pool.size() == 0:\\n                        to_remove.append(host)\\n                for host in to_remove:\\n                    del self.host_to_pool[host]\\n                self.last_clean_time = now\\n\\n\\nclass HTTPRequest(object):\\n\\n    def __init__(self, method, protocol, host, port, path, auth_path,\\n                 params, headers, body):\\n        \"\"\"Represents an HTTP request.\\n\\n        :type method: string\\n        :param method: The HTTP method name, \\'GET\\', \\'POST\\', \\'PUT\\' etc.\\n\\n        :type protocol: string\\n        :param protocol: The http protocol used, \\'http\\' or \\'https\\'.\\n\\n        :type host: string\\n        :param host: Host to which the request is addressed. eg. abc.com\\n\\n        :type port: int\\n        :param port: port on which the request is being sent. Zero means unset,\\n            in which case default port will be chosen.\\n\\n        :type path: string\\n        :param path: URL path that is being accessed.\\n\\n        :type auth_path: string\\n        :param path: The part of the URL path used when creating the\\n            authentication string.\\n\\n        :type params: dict\\n        :param params: HTTP url query parameters, with key as name of\\n            the param, and value as value of param.\\n\\n        :type headers: dict\\n        :param headers: HTTP headers, with key as name of the header and value\\n            as value of header.\\n\\n        :type body: string\\n        :param body: Body of the HTTP request. If not present, will be None or\\n            empty string (\\'\\').\\n        \"\"\"\\n        self.method = method\\n        self.protocol = protocol\\n        self.host = host\\n        self.port = port\\n        self.path = path\\n        if auth_path is None:\\n            auth_path = path\\n        self.auth_path = auth_path\\n        self.params = params\\n        # chunked Transfer-Encoding should act only on PUT request.\\n        if headers and \\'Transfer-Encoding\\' in headers and \\\\\\n                headers[\\'Transfer-Encoding\\'] == \\'chunked\\' and \\\\\\n                self.method != \\'PUT\\':\\n            self.headers = headers.copy()\\n            del self.headers[\\'Transfer-Encoding\\']\\n        else:\\n            self.headers = headers\\n        self.body = body\\n\\n    def __str__(self):\\n        return ((\\'method:(%s) protocol:(%s) host(%s) port(%s) path(%s) \\'\\n                 \\'params(%s) headers(%s) body(%s)\\') % (self.method,\\n                 self.protocol, self.host, self.port, self.path, self.params,\\n                 self.headers, self.body))\\n\\n    def authorize(self, connection, **kwargs):\\n        if not getattr(self, \\'_headers_quoted\\', False):\\n            for key in self.headers:\\n                val = self.headers[key]\\n                if isinstance(val, six.text_type):\\n                    safe = \\'!\"#$%&\\\\\\'()*+,/:;<=>?@[\\\\\\\\]^`{|}~ \\'\\n                    self.headers[key] = quote(val.encode(\\'utf-8\\'), safe)\\n            setattr(self, \\'_headers_quoted\\', True)\\n\\n        self.headers[\\'User-Agent\\'] = UserAgent\\n\\n        connection._auth_handler.add_auth(self, **kwargs)\\n\\n        # I\\'m not sure if this is still needed, now that add_auth is\\n        # setting the content-length for POST requests.\\n        if \\'Content-Length\\' not in self.headers:\\n            if \\'Transfer-Encoding\\' not in self.headers or \\\\\\n                    self.headers[\\'Transfer-Encoding\\'] != \\'chunked\\':\\n                self.headers[\\'Content-Length\\'] = str(len(self.body))\\n\\n\\nclass HTTPResponse(http_client.HTTPResponse):\\n\\n    def __init__(self, *args, **kwargs):\\n        http_client.HTTPResponse.__init__(self, *args, **kwargs)\\n        self._cached_response = \\'\\'\\n\\n    def read(self, amt=None):\\n        \"\"\"Read the response.\\n\\n        This method does not have the same behavior as\\n        http_client.HTTPResponse.read.  Instead, if this method is called with\\n        no ``amt`` arg, then the response body will be cached.  Subsequent\\n        calls to ``read()`` with no args **will return the cached response**.\\n\\n        \"\"\"\\n        if amt is None:\\n            # The reason for doing this is that many places in boto call\\n            # response.read() and except to get the response body that they\\n            # can then process.  To make sure this always works as they expect\\n            # we\\'re caching the response so that multiple calls to read()\\n            # will return the full body.  Note that this behavior only\\n            # happens if the amt arg is not specified.\\n            if not self._cached_response:\\n                self._cached_response = http_client.HTTPResponse.read(self)\\n            return self._cached_response\\n        else:\\n            return http_client.HTTPResponse.read(self, amt)\\n\\n\\nclass AWSAuthConnection(object):\\n    def __init__(self, host, aws_access_key_id=None,\\n                 aws_secret_access_key=None,\\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\\n                 proxy_user=None, proxy_pass=None, debug=0,\\n                 https_connection_factory=None, path=\\'/\\',\\n                 provider=\\'aws\\', security_token=None,\\n                 suppress_consec_slashes=True,\\n                 validate_certs=True, profile_name=None):\\n        \"\"\"\\n        :type host: str\\n        :param host: The host to make the connection to\\n\\n        :keyword str aws_access_key_id: Your AWS Access Key ID (provided by\\n            Amazon). If none is specified, the value in your\\n            ``AWS_ACCESS_KEY_ID`` environmental variable is used.\\n        :keyword str aws_secret_access_key: Your AWS Secret Access Key\\n            (provided by Amazon). If none is specified, the value in your\\n            ``AWS_SECRET_ACCESS_KEY`` environmental variable is used.\\n        :keyword str security_token: The security token associated with\\n            temporary credentials issued by STS.  Optional unless using\\n            temporary credentials.  If none is specified, the environment\\n            variable ``AWS_SECURITY_TOKEN`` is used if defined.\\n\\n        :type is_secure: boolean\\n        :param is_secure: Whether the connection is over SSL\\n\\n        :type https_connection_factory: list or tuple\\n        :param https_connection_factory: A pair of an HTTP connection\\n            factory and the exceptions to catch.  The factory should have\\n            a similar interface to L{http_client.HTTPSConnection}.\\n\\n        :param str proxy: Address/hostname for a proxy server\\n\\n        :type proxy_port: int\\n        :param proxy_port: The port to use when connecting over a proxy\\n\\n        :type proxy_user: str\\n        :param proxy_user: The username to connect with on the proxy\\n\\n        :type proxy_pass: str\\n        :param proxy_pass: The password to use when connection over a proxy.\\n\\n        :type port: int\\n        :param port: The port to use to connect\\n\\n        :type suppress_consec_slashes: bool\\n        :param suppress_consec_slashes: If provided, controls whether\\n            consecutive slashes will be suppressed in key paths.\\n\\n        :type validate_certs: bool\\n        :param validate_certs: Controls whether SSL certificates\\n            will be validated or not.  Defaults to True.\\n\\n        :type profile_name: str\\n        :param profile_name: Override usual Credentials section in config\\n            file to use a named set of keys instead.\\n        \"\"\"\\n        self.suppress_consec_slashes = suppress_consec_slashes\\n        self.num_retries = 6\\n        # Override passed-in is_secure setting if value was defined in config.\\n        if config.has_option(\\'Boto\\', \\'is_secure\\'):\\n            is_secure = config.getboolean(\\'Boto\\', \\'is_secure\\')\\n        self.is_secure = is_secure\\n        # Whether or not to validate server certificates.\\n        # The default is now to validate certificates.  This can be\\n        # overridden in the boto config file are by passing an\\n        # explicit validate_certs parameter to the class constructor.\\n        self.https_validate_certificates = config.getbool(\\n            \\'Boto\\', \\'https_validate_certificates\\',\\n            validate_certs)\\n        if self.https_validate_certificates and not HAVE_HTTPS_CONNECTION:\\n            raise BotoClientError(\\n                \"SSL server certificate validation is enabled in boto \"\\n                \"configuration, but Python dependencies required to \"\\n                \"support this feature are not available. Certificate \"\\n                \"validation is only supported when running under Python \"\\n                \"2.6 or later.\")\\n        certs_file = config.get_value(\\n            \\'Boto\\', \\'ca_certificates_file\\', DEFAULT_CA_CERTS_FILE)\\n        if certs_file == \\'system\\':\\n            certs_file = None\\n        self.ca_certificates_file = certs_file\\n        if port:\\n            self.port = port\\n        else:\\n            self.port = PORTS_BY_SECURITY[is_secure]\\n\\n        self.handle_proxy(proxy, proxy_port, proxy_user, proxy_pass)\\n        # define exceptions from http_client that we want to catch and retry\\n        self.http_exceptions = (http_client.HTTPException, socket.error,\\n                                socket.gaierror, http_client.BadStatusLine)\\n        # define subclasses of the above that are not retryable.\\n        self.http_unretryable_exceptions = []\\n        if HAVE_HTTPS_CONNECTION:\\n            self.http_unretryable_exceptions.append(\\n                https_connection.InvalidCertificateException)\\n\\n        # define values in socket exceptions we don\\'t want to catch\\n        self.socket_exception_values = (errno.EINTR,)\\n        if https_connection_factory is not None:\\n            self.https_connection_factory = https_connection_factory[0]\\n            self.http_exceptions += https_connection_factory[1]\\n        else:\\n            self.https_connection_factory = None\\n        if (is_secure):\\n            self.protocol = \\'https\\'\\n        else:\\n            self.protocol = \\'http\\'\\n        self.host = host\\n        self.path = path\\n        # if the value passed in for debug\\n        if not isinstance(debug, six.integer_types):\\n            debug = 0\\n        self.debug = config.getint(\\'Boto\\', \\'debug\\', debug)\\n        self.host_header = None\\n\\n        # Timeout used to tell http_client how long to wait for socket timeouts.\\n        # Default is to leave timeout unchanged, which will in turn result in\\n        # the socket\\'s default global timeout being used. To specify a\\n        # timeout, set http_socket_timeout in Boto config. Regardless,\\n        # timeouts will only be applied if Python is 2.6 or greater.\\n        self.http_connection_kwargs = {}\\n        if (sys.version_info[0], sys.version_info[1]) >= (2, 6):\\n            # If timeout isn\\'t defined in boto config file, use 70 second\\n            # default as recommended by\\n            # http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForActivityTask.html\\n            self.http_connection_kwargs[\\'timeout\\'] = config.getint(\\n                \\'Boto\\', \\'http_socket_timeout\\', 70)\\n\\n        if isinstance(provider, Provider):\\n            # Allow overriding Provider\\n            self.provider = provider\\n        else:\\n            self._provider_type = provider\\n            self.provider = Provider(self._provider_type,\\n                                     aws_access_key_id,\\n                                     aws_secret_access_key,\\n                                     security_token,\\n                                     profile_name)\\n\\n        # Allow config file to override default host, port, and host header.\\n        if self.provider.host:\\n            self.host = self.provider.host\\n        if self.provider.port:\\n            self.port = self.provider.port\\n        if self.provider.host_header:\\n            self.host_header = self.provider.host_header\\n\\n        self._pool = ConnectionPool()\\n        self._connection = (self.host, self.port, self.is_secure)\\n        self._last_rs = None\\n        self._auth_handler = auth.get_auth_handler(\\n            host, config, self.provider, self._required_auth_capability())\\n        if getattr(self, \\'AuthServiceName\\', None) is not None:\\n            self.auth_service_name = self.AuthServiceName\\n        self.request_hook = None\\n\\n    def __repr__(self):\\n        return \\'%s:%s\\' % (self.__class__.__name__, self.host)\\n\\n    def _required_auth_capability(self):\\n        return []\\n\\n    def _get_auth_service_name(self):\\n        return getattr(self._auth_handler, \\'service_name\\')\\n\\n    # For Sigv4, the auth_service_name/auth_region_name properties allow\\n    # the service_name/region_name to be explicitly set instead of being\\n    # derived from the endpoint url.\\n    def _set_auth_service_name(self, value):\\n        self._auth_handler.service_name = value\\n    auth_service_name = property(_get_auth_service_name, _set_auth_service_name)\\n\\n    def _get_auth_region_name(self):\\n        return getattr(self._auth_handler, \\'region_name\\')\\n\\n    def _set_auth_region_name(self, value):\\n        self._auth_handler.region_name = value\\n    auth_region_name = property(_get_auth_region_name, _set_auth_region_name)\\n\\n    def connection(self):\\n        return self.get_http_connection(*self._connection)\\n    connection = property(connection)\\n\\n    def aws_access_key_id(self):\\n        return self.provider.access_key\\n    aws_access_key_id = property(aws_access_key_id)\\n    gs_access_key_id = aws_access_key_id\\n    access_key = aws_access_key_id\\n\\n    def aws_secret_access_key(self):\\n        return self.provider.secret_key\\n    aws_secret_access_key = property(aws_secret_access_key)\\n    gs_secret_access_key = aws_secret_access_key\\n    secret_key = aws_secret_access_key\\n\\n    def profile_name(self):\\n        return self.provider.profile_name\\n    profile_name = property(profile_name)\\n\\n    def get_path(self, path=\\'/\\'):\\n        # The default behavior is to suppress consecutive slashes for reasons\\n        # discussed at\\n        # https://groups.google.com/forum/#!topic/boto-dev/-ft0XPUy0y8\\n        # You can override that behavior with the suppress_consec_slashes param.\\n        if not self.suppress_consec_slashes:\\n            return self.path + re.sub(\\'^(/*)/\\', \"\\\\\\\\1\", path)\\n        pos = path.find(\\'?\\')\\n        if pos >= 0:\\n            params = path[pos:]\\n            path = path[:pos]\\n        else:\\n            params = None\\n        if path[-1] == \\'/\\':\\n            need_trailing = True\\n        else:\\n            need_trailing = False\\n        path_elements = self.path.split(\\'/\\')\\n        path_elements.extend(path.split(\\'/\\'))\\n        path_elements = [p for p in path_elements if p]\\n        path = \\'/\\' + \\'/\\'.join(path_elements)\\n        if path[-1] != \\'/\\' and need_trailing:\\n            path += \\'/\\'\\n        if params:\\n            path = path + params\\n        return path\\n\\n    def server_name(self, port=None):\\n        if not port:\\n            port = self.port\\n        if port == 80:\\n            signature_host = self.host\\n        else:\\n            # This unfortunate little hack can be attributed to\\n            # a difference in the 2.6 version of http_client.  In old\\n            # versions, it would append \":443\" to the hostname sent\\n            # in the Host header and so we needed to make sure we\\n            # did the same when calculating the V2 signature.  In 2.6\\n            # (and higher!)\\n            # it no longer does that.  Hence, this kludge.\\n            if ((ON_APP_ENGINE and sys.version[:3] == \\'2.5\\') or\\n                    sys.version[:3] in (\\'2.6\\', \\'2.7\\')) and port == 443:\\n                signature_host = self.host\\n            else:\\n                signature_host = \\'%s:%d\\' % (self.host, port)\\n        return signature_host\\n\\n    def handle_proxy(self, proxy, proxy_port, proxy_user, proxy_pass):\\n        self.proxy = proxy\\n        self.proxy_port = proxy_port\\n        self.proxy_user = proxy_user\\n        self.proxy_pass = proxy_pass\\n        if \\'http_proxy\\' in os.environ and not self.proxy:\\n            pattern = re.compile(\\n                \\'(?:http://)?\\'\\n                \\'(?:(?P<user>[\\\\w\\\\-\\\\.]+):(?P<pass>.*)@)?\\'\\n                \\'(?P<host>[\\\\w\\\\-\\\\.]+)\\'\\n                \\'(?::(?P<port>\\\\d+))?\\'\\n            )\\n            match = pattern.match(os.environ[\\'http_proxy\\'])\\n            if match:\\n                self.proxy = match.group(\\'host\\')\\n                self.proxy_port = match.group(\\'port\\')\\n                self.proxy_user = match.group(\\'user\\')\\n                self.proxy_pass = match.group(\\'pass\\')\\n        else:\\n            if not self.proxy:\\n                self.proxy = config.get_value(\\'Boto\\', \\'proxy\\', None)\\n            if not self.proxy_port:\\n                self.proxy_port = config.get_value(\\'Boto\\', \\'proxy_port\\', None)\\n            if not self.proxy_user:\\n                self.proxy_user = config.get_value(\\'Boto\\', \\'proxy_user\\', None)\\n            if not self.proxy_pass:\\n                self.proxy_pass = config.get_value(\\'Boto\\', \\'proxy_pass\\', None)\\n\\n        if not self.proxy_port and self.proxy:\\n            print(\"http_proxy environment variable does not specify \"\\n                  \"a port, using default\")\\n            self.proxy_port = self.port\\n\\n        self.no_proxy = os.environ.get(\\'no_proxy\\', \\'\\') or os.environ.get(\\'NO_PROXY\\', \\'\\')\\n        self.use_proxy = (self.proxy is not None)\\n\\n    def get_http_connection(self, host, port, is_secure):\\n        conn = self._pool.get_http_connection(host, port, is_secure)\\n        if conn is not None:\\n            return conn\\n        else:\\n            return self.new_http_connection(host, port, is_secure)\\n\\n    def skip_proxy(self, host):\\n        if not self.no_proxy:\\n            return False\\n\\n        if self.no_proxy == \"*\":\\n            return True\\n\\n        hostonly = host\\n        hostonly = host.split(\\':\\')[0]\\n\\n        for name in self.no_proxy.split(\\',\\'):\\n            if name and (hostonly.endswith(name) or host.endswith(name)):\\n                return True\\n\\n        return False\\n\\n    def new_http_connection(self, host, port, is_secure):\\n        if host is None:\\n            host = self.server_name()\\n\\n        # Make sure the host is really just the host, not including\\n        # the port number\\n        host = boto.utils.parse_host(host)\\n\\n        http_connection_kwargs = self.http_connection_kwargs.copy()\\n\\n        # Connection factories below expect a port keyword argument\\n        http_connection_kwargs[\\'port\\'] = port\\n\\n        # Override host with proxy settings if needed\\n        if self.use_proxy and not is_secure and \\\\\\n                not self.skip_proxy(host):\\n            host = self.proxy\\n            http_connection_kwargs[\\'port\\'] = int(self.proxy_port)\\n\\n        if is_secure:\\n            boto.log.debug(\\n                \\'establishing HTTPS connection: host=%s, kwargs=%s\\',\\n                host, http_connection_kwargs)\\n            if self.use_proxy and not self.skip_proxy(host):\\n                connection = self.proxy_ssl(host, is_secure and 443 or 80)\\n            elif self.https_connection_factory:\\n                connection = self.https_connection_factory(host)\\n            elif self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\\n                connection = https_connection.CertValidatingHTTPSConnection(\\n                    host, ca_certs=self.ca_certificates_file,\\n                    **http_connection_kwargs)\\n            else:\\n                connection = http_client.HTTPSConnection(\\n                    host, **http_connection_kwargs)\\n        else:\\n            boto.log.debug(\\'establishing HTTP connection: kwargs=%s\\' %\\n                           http_connection_kwargs)\\n            if self.https_connection_factory:\\n                # even though the factory says https, this is too handy\\n                # to not be able to allow overriding for http also.\\n                connection = self.https_connection_factory(\\n                    host, **http_connection_kwargs)\\n            else:\\n                connection = http_client.HTTPConnection(\\n                    host, **http_connection_kwargs)\\n        if self.debug > 1:\\n            connection.set_debuglevel(self.debug)\\n        # self.connection must be maintained for backwards-compatibility\\n        # however, it must be dynamically pulled from the connection pool\\n        # set a private variable which will enable that\\n        if host.split(\\':\\')[0] == self.host and is_secure == self.is_secure:\\n            self._connection = (host, port, is_secure)\\n        # Set the response class of the http connection to use our custom\\n        # class.\\n        connection.response_class = HTTPResponse\\n        return connection\\n\\n    def put_http_connection(self, host, port, is_secure, connection):\\n        self._pool.put_http_connection(host, port, is_secure, connection)\\n\\n    def proxy_ssl(self, host=None, port=None):\\n        if host and port:\\n            host = \\'%s:%d\\' % (host, port)\\n        else:\\n            host = \\'%s:%d\\' % (self.host, self.port)\\n        # Seems properly to use timeout for connect too\\n        timeout = self.http_connection_kwargs.get(\"timeout\")\\n        if timeout is not None:\\n            sock = socket.create_connection((self.proxy,\\n                                             int(self.proxy_port)), timeout)\\n        else:\\n            sock = socket.create_connection((self.proxy, int(self.proxy_port)))\\n        boto.log.debug(\"Proxy connection: CONNECT %s HTTP/1.0\\\\r\\\\n\", host)\\n        sock.sendall(\"CONNECT %s HTTP/1.0\\\\r\\\\n\" % host)\\n        sock.sendall(\"User-Agent: %s\\\\r\\\\n\" % UserAgent)\\n        if self.proxy_user and self.proxy_pass:\\n            for k, v in self.get_proxy_auth_header().items():\\n                sock.sendall(\"%s: %s\\\\r\\\\n\" % (k, v))\\n            # See discussion about this config option at\\n            # https://groups.google.com/forum/?fromgroups#!topic/boto-dev/teenFvOq2Cc\\n            if config.getbool(\\'Boto\\', \\'send_crlf_after_proxy_auth_headers\\', False):\\n                sock.sendall(\"\\\\r\\\\n\")\\n        else:\\n            sock.sendall(\"\\\\r\\\\n\")\\n        resp = http_client.HTTPResponse(sock, strict=True, debuglevel=self.debug)\\n        resp.begin()\\n\\n        if resp.status != 200:\\n            # Fake a socket error, use a code that make it obvious it hasn\\'t\\n            # been generated by the socket library\\n            raise socket.error(-71,\\n                               \"Error talking to HTTP proxy %s:%s: %s (%s)\" %\\n                               (self.proxy, self.proxy_port,\\n                                resp.status, resp.reason))\\n\\n        # We can safely close the response, it duped the original socket\\n        resp.close()\\n\\n        h = http_client.HTTPConnection(host)\\n\\n        if self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\\n            msg = \"wrapping ssl socket for proxied connection; \"\\n            if self.ca_certificates_file:\\n                msg += \"CA certificate file=%s\" % self.ca_certificates_file\\n            else:\\n                msg += \"using system provided SSL certs\"\\n            boto.log.debug(msg)\\n            key_file = self.http_connection_kwargs.get(\\'key_file\\', None)\\n            cert_file = self.http_connection_kwargs.get(\\'cert_file\\', None)\\n            sslSock = ssl.wrap_socket(sock, keyfile=key_file,\\n                                      certfile=cert_file,\\n                                      cert_reqs=ssl.CERT_REQUIRED,\\n                                      ca_certs=self.ca_certificates_file)\\n            cert = sslSock.getpeercert()\\n            hostname = self.host.split(\\':\\', 0)[0]\\n            if not https_connection.ValidateCertificateHostname(cert, hostname):\\n                raise https_connection.InvalidCertificateException(\\n                    hostname, cert, \\'hostname mismatch\\')\\n        else:\\n            # Fallback for old Python without ssl.wrap_socket\\n            if hasattr(http_client, \\'ssl\\'):\\n                sslSock = http_client.ssl.SSLSocket(sock)\\n            else:\\n                sslSock = socket.ssl(sock, None, None)\\n                sslSock = http_client.FakeSocket(sock, sslSock)\\n\\n        # This is a bit unclean\\n        h.sock = sslSock\\n        return h\\n\\n    def prefix_proxy_to_path(self, path, host=None):\\n        path = self.protocol + \\'://\\' + (host or self.server_name()) + path\\n        return path\\n\\n    def get_proxy_auth_header(self):\\n        auth = encodebytes(self.proxy_user + \\':\\' + self.proxy_pass)\\n        return {\\'Proxy-Authorization\\': \\'Basic %s\\' % auth}\\n\\n    # For passing proxy information to other connection libraries, e.g. cloudsearch2\\n    def get_proxy_url_with_auth(self):\\n        if not self.use_proxy:\\n            return None\\n\\n        if self.proxy_user or self.proxy_pass:\\n            if self.proxy_pass:\\n                login_info = \\'%s:%s@\\' % (self.proxy_user, self.proxy_pass)\\n            else:\\n                login_info = \\'%s@\\' % self.proxy_user\\n        else:\\n            login_info = \\'\\'\\n\\n        return \\'http://%s%s:%s\\' % (login_info, self.proxy, str(self.proxy_port or self.port))\\n\\n    def set_host_header(self, request):\\n        try:\\n            request.headers[\\'Host\\'] = \\\\\\n                self._auth_handler.host_header(self.host, request)\\n        except AttributeError:\\n            request.headers[\\'Host\\'] = self.host.split(\\':\\', 1)[0]\\n\\n    def set_request_hook(self, hook):\\n        self.request_hook = hook\\n\\n    def _mexe(self, request, sender=None, override_num_retries=None,\\n              retry_handler=None):\\n        \"\"\"\\n        mexe - Multi-execute inside a loop, retrying multiple times to handle\\n               transient Internet errors by simply trying again.\\n               Also handles redirects.\\n\\n        This code was inspired by the S3Utils classes posted to the boto-users\\n        Google group by Larry Bates.  Thanks!\\n\\n        \"\"\"\\n        boto.log.debug(\\'Method: %s\\' % request.method)\\n        boto.log.debug(\\'Path: %s\\' % request.path)\\n        boto.log.debug(\\'Data: %s\\' % request.body)\\n        boto.log.debug(\\'Headers: %s\\' % request.headers)\\n        boto.log.debug(\\'Host: %s\\' % request.host)\\n        boto.log.debug(\\'Port: %s\\' % request.port)\\n        boto.log.debug(\\'Params: %s\\' % request.params)\\n        response = None\\n        body = None\\n        ex = None\\n        if override_num_retries is None:\\n            num_retries = config.getint(\\'Boto\\', \\'num_retries\\', self.num_retries)\\n        else:\\n            num_retries = override_num_retries\\n        i = 0\\n        connection = self.get_http_connection(request.host, request.port,\\n                                              self.is_secure)\\n\\n        # Convert body to bytes if needed\\n        if not isinstance(request.body, bytes) and hasattr(request.body,\\n                                                           \\'encode\\'):\\n            request.body = request.body.encode(\\'utf-8\\')\\n\\n        while i <= num_retries:\\n            # Use binary exponential backoff to desynchronize client requests.\\n            next_sleep = min(random.random() * (2 ** i),\\n                             boto.config.get(\\'Boto\\', \\'max_retry_delay\\', 60))\\n            try:\\n                # we now re-sign each request before it is retried\\n                boto.log.debug(\\'Token: %s\\' % self.provider.security_token)\\n                request.authorize(connection=self)\\n                # Only force header for non-s3 connections, because s3 uses\\n                # an older signing method + bucket resource URLs that include\\n                # the port info. All others should be now be up to date and\\n                # not include the port.\\n                if \\'s3\\' not in self._required_auth_capability():\\n                    if not getattr(self, \\'anon\\', False):\\n                        if not request.headers.get(\\'Host\\'):\\n                            self.set_host_header(request)\\n                boto.log.debug(\\'Final headers: %s\\' % request.headers)\\n                request.start_time = datetime.now()\\n                if callable(sender):\\n                    response = sender(connection, request.method, request.path,\\n                                      request.body, request.headers)\\n                else:\\n                    connection.request(request.method, request.path,\\n                                       request.body, request.headers)\\n                    response = connection.getresponse()\\n                boto.log.debug(\\'Response headers: %s\\' % response.getheaders())\\n                location = response.getheader(\\'location\\')\\n                # -- gross hack --\\n                # http_client gets confused with chunked responses to HEAD requests\\n                # so I have to fake it out\\n                if request.method == \\'HEAD\\' and getattr(response,\\n                                                        \\'chunked\\', False):\\n                    response.chunked = 0\\n                if callable(retry_handler):\\n                    status = retry_handler(response, i, next_sleep)\\n                    if status:\\n                        msg, i, next_sleep = status\\n                        if msg:\\n                            boto.log.debug(msg)\\n                        time.sleep(next_sleep)\\n                        continue\\n                if response.status in [500, 502, 503, 504]:\\n                    msg = \\'Received %d response.  \\' % response.status\\n                    msg += \\'Retrying in %3.1f seconds\\' % next_sleep\\n                    boto.log.debug(msg)\\n                    body = response.read()\\n                    if isinstance(body, bytes):\\n                        body = body.decode(\\'utf-8\\')\\n                elif response.status < 300 or response.status >= 400 or \\\\\\n                        not location:\\n                    # don\\'t return connection to the pool if response contains\\n                    # Connection:close header, because the connection has been\\n                    # closed and default reconnect behavior may do something\\n                    # different than new_http_connection. Also, it\\'s probably\\n                    # less efficient to try to reuse a closed connection.\\n                    conn_header_value = response.getheader(\\'connection\\')\\n                    if conn_header_value == \\'close\\':\\n                        connection.close()\\n                    else:\\n                        self.put_http_connection(request.host, request.port,\\n                                                 self.is_secure, connection)\\n                    if self.request_hook is not None:\\n                        self.request_hook.handle_request_data(request, response)\\n                    return response\\n                else:\\n                    scheme, request.host, request.path, \\\\\\n                        params, query, fragment = urlparse(location)\\n                    if query:\\n                        request.path += \\'?\\' + query\\n                    # urlparse can return both host and port in netloc, so if\\n                    # that\\'s the case we need to split them up properly\\n                    if \\':\\' in request.host:\\n                        request.host, request.port = request.host.split(\\':\\', 1)\\n                    msg = \\'Redirecting: %s\\' % scheme + \\'://\\'\\n                    msg += request.host + request.path\\n                    boto.log.debug(msg)\\n                    connection = self.get_http_connection(request.host,\\n                                                          request.port,\\n                                                          scheme == \\'https\\')\\n                    response = None\\n                    continue\\n            except PleaseRetryException as e:\\n                boto.log.debug(\\'encountered a retry exception: %s\\' % e)\\n                connection = self.new_http_connection(request.host, request.port,\\n                                                      self.is_secure)\\n                response = e.response\\n                ex = e\\n            except self.http_exceptions as e:\\n                for unretryable in self.http_unretryable_exceptions:\\n                    if isinstance(e, unretryable):\\n                        boto.log.debug(\\n                            \\'encountered unretryable %s exception, re-raising\\' %\\n                            e.__class__.__name__)\\n                        raise\\n                boto.log.debug(\\'encountered %s exception, reconnecting\\' %\\n                               e.__class__.__name__)\\n                connection = self.new_http_connection(request.host, request.port,\\n                                                      self.is_secure)\\n                ex = e\\n            time.sleep(next_sleep)\\n            i += 1\\n        # If we made it here, it\\'s because we have exhausted our retries\\n        # and stil haven\\'t succeeded.  So, if we have a response object,\\n        # use it to raise an exception.\\n        # Otherwise, raise the exception that must have already happened.\\n        if self.request_hook is not None:\\n            self.request_hook.handle_request_data(request, response, error=True)\\n        if response:\\n            raise BotoServerError(response.status, response.reason, body)\\n        elif ex:\\n            raise ex\\n        else:\\n            msg = \\'Please report this exception as a Boto Issue!\\'\\n            raise BotoClientError(msg)\\n\\n    def build_base_http_request(self, method, path, auth_path,\\n                                params=None, headers=None, data=\\'\\', host=None):\\n        path = self.get_path(path)\\n        if auth_path is not None:\\n            auth_path = self.get_path(auth_path)\\n        if params is None:\\n            params = {}\\n        else:\\n            params = params.copy()\\n        if headers is None:\\n            headers = {}\\n        else:\\n            headers = headers.copy()\\n        if self.host_header and not boto.utils.find_matching_headers(\\'host\\', headers):\\n            headers[\\'host\\'] = self.host_header\\n        host = host or self.host\\n        if self.use_proxy and not self.skip_proxy(host):\\n            if not auth_path:\\n                auth_path = path\\n            path = self.prefix_proxy_to_path(path, host)\\n            if self.proxy_user and self.proxy_pass and not self.is_secure:\\n                # If is_secure, we don\\'t have to set the proxy authentication\\n                # header here, we did that in the CONNECT to the proxy.\\n                headers.update(self.get_proxy_auth_header())\\n        return HTTPRequest(method, self.protocol, host, self.port,\\n                           path, auth_path, params, headers, data)\\n\\n    def make_request(self, method, path, headers=None, data=\\'\\', host=None,\\n                     auth_path=None, sender=None, override_num_retries=None,\\n                     params=None, retry_handler=None):\\n        \"\"\"Makes a request to the server, with stock multiple-retry logic.\"\"\"\\n        if params is None:\\n            params = {}\\n        http_request = self.build_base_http_request(method, path, auth_path,\\n                                                    params, headers, data, host)\\n        return self._mexe(http_request, sender, override_num_retries,\\n                          retry_handler=retry_handler)\\n\\n    def close(self):\\n        \"\"\"(Optional) Close any open HTTP connections.  This is non-destructive,\\n        and making a new request will open a connection again.\"\"\"\\n\\n        boto.log.debug(\\'closing all HTTP connections\\')\\n        self._connection = None  # compat field\\n\\n\\nclass AWSQueryConnection(AWSAuthConnection):\\n\\n    APIVersion = \\'\\'\\n    ResponseError = BotoServerError\\n\\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\\n                 proxy_user=None, proxy_pass=None, host=None, debug=0,\\n                 https_connection_factory=None, path=\\'/\\', security_token=None,\\n                 validate_certs=True, profile_name=None, provider=\\'aws\\'):\\n        super(AWSQueryConnection, self).__init__(\\n            host, aws_access_key_id,\\n            aws_secret_access_key,\\n            is_secure, port, proxy,\\n            proxy_port, proxy_user, proxy_pass,\\n            debug, https_connection_factory, path,\\n            security_token=security_token,\\n            validate_certs=validate_certs,\\n            profile_name=profile_name,\\n            provider=provider)\\n\\n    def _required_auth_capability(self):\\n        return []\\n\\n    def get_utf8_value(self, value):\\n        return boto.utils.get_utf8_value(value)\\n\\n    def make_request(self, action, params=None, path=\\'/\\', verb=\\'GET\\'):\\n        http_request = self.build_base_http_request(verb, path, None,\\n                                                    params, {}, \\'\\',\\n                                                    self.host)\\n        if action:\\n            http_request.params[\\'Action\\'] = action\\n        if self.APIVersion:\\n            http_request.params[\\'Version\\'] = self.APIVersion\\n        return self._mexe(http_request)\\n\\n    def build_list_params(self, params, items, label):\\n        if isinstance(items, six.string_types):\\n            items = [items]\\n        for i in range(1, len(items) + 1):\\n            params[\\'%s.%d\\' % (label, i)] = items[i - 1]\\n\\n    def build_complex_list_params(self, params, items, label, names):\\n        \"\"\"Serialize a list of structures.\\n\\n        For example::\\n\\n            items = [(\\'foo\\', \\'bar\\', \\'baz\\'), (\\'foo2\\', \\'bar2\\', \\'baz2\\')]\\n            label = \\'ParamName.member\\'\\n            names = (\\'One\\', \\'Two\\', \\'Three\\')\\n            self.build_complex_list_params(params, items, label, names)\\n\\n        would result in the params dict being updated with these params::\\n\\n            ParamName.member.1.One = foo\\n            ParamName.member.1.Two = bar\\n            ParamName.member.1.Three = baz\\n\\n            ParamName.member.2.One = foo2\\n            ParamName.member.2.Two = bar2\\n            ParamName.member.2.Three = baz2\\n\\n        :type params: dict\\n        :param params: The params dict.  The complex list params\\n            will be added to this dict.\\n\\n        :type items: list of tuples\\n        :param items: The list to serialize.\\n\\n        :type label: string\\n        :param label: The prefix to apply to the parameter.\\n\\n        :type names: tuple of strings\\n        :param names: The names associated with each tuple element.\\n\\n        \"\"\"\\n        for i, item in enumerate(items, 1):\\n            current_prefix = \\'%s.%s\\' % (label, i)\\n            for key, value in zip(names, item):\\n                full_key = \\'%s.%s\\' % (current_prefix, key)\\n                params[full_key] = value\\n\\n    # generics\\n\\n    def get_list(self, action, params, markers, path=\\'/\\',\\n                 parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            rs = ResultSet(markers)\\n            h = boto.handler.XmlHandler(rs, parent)\\n            if isinstance(body, six.text_type):\\n                body = body.encode(\\'utf-8\\')\\n            xml.sax.parseString(body, h)\\n            return rs\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n\\n    def get_object(self, action, params, cls, path=\\'/\\',\\n                   parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            obj = cls(parent)\\n            h = boto.handler.XmlHandler(obj, parent)\\n            if isinstance(body, six.text_type):\\n                body = body.encode(\\'utf-8\\')\\n            xml.sax.parseString(body, h)\\n            return obj\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n\\n    def get_status(self, action, params, path=\\'/\\', parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            rs = ResultSet()\\n            h = boto.handler.XmlHandler(rs, parent)\\n            xml.sax.parseString(body, h)\\n            return rs.status\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n'}, {'boto.connection.AWSAuthConnection.build_base_http_request': '# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\\n# Copyright (c) 2010 Google\\n# Copyright (c) 2008 rPath, Inc.\\n# Copyright (c) 2009 The Echo Nest Corporation\\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\\n# Copyright (c) 2011, Nexenta Systems Inc.\\n# All rights reserved.\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a\\n# copy of this software and associated documentation files (the\\n# \"Software\"), to deal in the Software without restriction, including\\n# without limitation the rights to use, copy, modify, merge, publish, dis-\\n# tribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the fol-\\n# lowing conditions:\\n#\\n# The above copyright notice and this permission notice shall be included\\n# in all copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\\n# IN THE SOFTWARE.\\n\\n#\\n# Parts of this code were copied or derived from sample code supplied by AWS.\\n# The following notice applies to that code.\\n#\\n#  This software code is made available \"AS IS\" without warranties of any\\n#  kind.  You may copy, display, modify and redistribute the software\\n#  code either by itself or as incorporated into your code; provided that\\n#  you do not remove any proprietary notices.  Your use of this software\\n#  code is at your own risk and you waive any claim against Amazon\\n#  Digital Services, Inc. or its affiliates with respect to your use of\\n#  this software code. (c) 2006 Amazon Digital Services, Inc. or its\\n#  affiliates.\\n\\n\"\"\"\\nHandles basic connections to AWS\\n\"\"\"\\nfrom datetime import datetime\\nimport errno\\nimport os\\nimport random\\nimport re\\nimport socket\\nimport sys\\nimport time\\nimport xml.sax\\nimport copy\\n\\nfrom boto import auth\\nfrom boto import auth_handler\\nimport boto\\nimport boto.utils\\nimport boto.handler\\nimport boto.cacerts\\n\\nfrom boto import config, UserAgent\\nfrom boto.compat import six, http_client, urlparse, quote, encodebytes\\nfrom boto.exception import AWSConnectionError\\nfrom boto.exception import BotoClientError\\nfrom boto.exception import BotoServerError\\nfrom boto.exception import PleaseRetryException\\nfrom boto.provider import Provider\\nfrom boto.resultset import ResultSet\\n\\nHAVE_HTTPS_CONNECTION = False\\ntry:\\n    import ssl\\n    from boto import https_connection\\n    # Google App Engine runs on Python 2.5 so doesn\\'t have ssl.SSLError.\\n    if hasattr(ssl, \\'SSLError\\'):\\n        HAVE_HTTPS_CONNECTION = True\\nexcept ImportError:\\n    pass\\n\\ntry:\\n    import threading\\nexcept ImportError:\\n    import dummy_threading as threading\\n\\nON_APP_ENGINE = all(key in os.environ for key in (\\n    \\'USER_IS_ADMIN\\', \\'CURRENT_VERSION_ID\\', \\'APPLICATION_ID\\'))\\n\\nPORTS_BY_SECURITY = {True: 443,\\n                     False: 80}\\n\\nDEFAULT_CA_CERTS_FILE = os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)), \"cacerts.txt\")\\n\\n\\nclass HostConnectionPool(object):\\n\\n    \"\"\"\\n    A pool of connections for one remote (host,port,is_secure).\\n\\n    When connections are added to the pool, they are put into a\\n    pending queue.  The _mexe method returns connections to the pool\\n    before the response body has been read, so they connections aren\\'t\\n    ready to send another request yet.  They stay in the pending queue\\n    until they are ready for another request, at which point they are\\n    returned to the pool of ready connections.\\n\\n    The pool of ready connections is an ordered list of\\n    (connection,time) pairs, where the time is the time the connection\\n    was returned from _mexe.  After a certain period of time,\\n    connections are considered stale, and discarded rather than being\\n    reused.  This saves having to wait for the connection to time out\\n    if AWS has decided to close it on the other end because of\\n    inactivity.\\n\\n    Thread Safety:\\n\\n        This class is used only from ConnectionPool while it\\'s mutex\\n        is held.\\n    \"\"\"\\n\\n    def __init__(self):\\n        self.queue = []\\n\\n    def size(self):\\n        \"\"\"\\n        Returns the number of connections in the pool for this host.\\n        Some of the connections may still be in use, and may not be\\n        ready to be returned by get().\\n        \"\"\"\\n        return len(self.queue)\\n\\n    def put(self, conn):\\n        \"\"\"\\n        Adds a connection to the pool, along with the time it was\\n        added.\\n        \"\"\"\\n        self.queue.append((conn, time.time()))\\n\\n    def get(self):\\n        \"\"\"\\n        Returns the next connection in this pool that is ready to be\\n        reused.  Returns None if there aren\\'t any.\\n        \"\"\"\\n        # Discard ready connections that are too old.\\n        self.clean()\\n\\n        # Return the first connection that is ready, and remove it\\n        # from the queue.  Connections that aren\\'t ready are returned\\n        # to the end of the queue with an updated time, on the\\n        # assumption that somebody is actively reading the response.\\n        for _ in range(len(self.queue)):\\n            (conn, _) = self.queue.pop(0)\\n            if self._conn_ready(conn):\\n                return conn\\n            else:\\n                self.put(conn)\\n        return None\\n\\n    def _conn_ready(self, conn):\\n        \"\"\"\\n        There is a nice state diagram at the top of http_client.py.  It\\n        indicates that once the response headers have been read (which\\n        _mexe does before adding the connection to the pool), a\\n        response is attached to the connection, and it stays there\\n        until it\\'s done reading.  This isn\\'t entirely true: even after\\n        the client is done reading, the response may be closed, but\\n        not removed from the connection yet.\\n\\n        This is ugly, reading a private instance variable, but the\\n        state we care about isn\\'t available in any public methods.\\n        \"\"\"\\n        if ON_APP_ENGINE:\\n            # Google AppEngine implementation of HTTPConnection doesn\\'t contain\\n            # _HTTPConnection__response attribute. Moreover, it\\'s not possible\\n            # to determine if given connection is ready. Reusing connections\\n            # simply doesn\\'t make sense with App Engine urlfetch service.\\n            return False\\n        else:\\n            response = getattr(conn, \\'_HTTPConnection__response\\', None)\\n            return (response is None) or response.isclosed()\\n\\n    def clean(self):\\n        \"\"\"\\n        Get rid of stale connections.\\n        \"\"\"\\n        # Note that we do not close the connection here -- somebody\\n        # may still be reading from it.\\n        while len(self.queue) > 0 and self._pair_stale(self.queue[0]):\\n            self.queue.pop(0)\\n\\n    def _pair_stale(self, pair):\\n        \"\"\"\\n        Returns true of the (connection,time) pair is too old to be\\n        used.\\n        \"\"\"\\n        (_conn, return_time) = pair\\n        now = time.time()\\n        return return_time + ConnectionPool.STALE_DURATION < now\\n\\n\\nclass ConnectionPool(object):\\n\\n    \"\"\"\\n    A connection pool that expires connections after a fixed period of\\n    time.  This saves time spent waiting for a connection that AWS has\\n    timed out on the other end.\\n\\n    This class is thread-safe.\\n    \"\"\"\\n\\n    #\\n    # The amout of time between calls to clean.\\n    #\\n\\n    CLEAN_INTERVAL = 5.0\\n\\n    #\\n    # How long before a connection becomes \"stale\" and won\\'t be reused\\n    # again.  The intention is that this time is less that the timeout\\n    # period that AWS uses, so we\\'ll never try to reuse a connection\\n    # and find that AWS is timing it out.\\n    #\\n    # Experimentation in July 2011 shows that AWS starts timing things\\n    # out after three minutes.  The 60 seconds here is conservative so\\n    # we should never hit that 3-minute timout.\\n    #\\n\\n    STALE_DURATION = 60.0\\n\\n    def __init__(self):\\n        # Mapping from (host,port,is_secure) to HostConnectionPool.\\n        # If a pool becomes empty, it is removed.\\n        self.host_to_pool = {}\\n        # The last time the pool was cleaned.\\n        self.last_clean_time = 0.0\\n        self.mutex = threading.Lock()\\n        ConnectionPool.STALE_DURATION = \\\\\\n            config.getfloat(\\'Boto\\', \\'connection_stale_duration\\',\\n                            ConnectionPool.STALE_DURATION)\\n\\n    def __getstate__(self):\\n        pickled_dict = copy.copy(self.__dict__)\\n        pickled_dict[\\'host_to_pool\\'] = {}\\n        del pickled_dict[\\'mutex\\']\\n        return pickled_dict\\n\\n    def __setstate__(self, dct):\\n        self.__init__()\\n\\n    def size(self):\\n        \"\"\"\\n        Returns the number of connections in the pool.\\n        \"\"\"\\n        return sum(pool.size() for pool in self.host_to_pool.values())\\n\\n    def get_http_connection(self, host, port, is_secure):\\n        \"\"\"\\n        Gets a connection from the pool for the named host.  Returns\\n        None if there is no connection that can be reused. It\\'s the caller\\'s\\n        responsibility to call close() on the connection when it\\'s no longer\\n        needed.\\n        \"\"\"\\n        self.clean()\\n        with self.mutex:\\n            key = (host, port, is_secure)\\n            if key not in self.host_to_pool:\\n                return None\\n            return self.host_to_pool[key].get()\\n\\n    def put_http_connection(self, host, port, is_secure, conn):\\n        \"\"\"\\n        Adds a connection to the pool of connections that can be\\n        reused for the named host.\\n        \"\"\"\\n        with self.mutex:\\n            key = (host, port, is_secure)\\n            if key not in self.host_to_pool:\\n                self.host_to_pool[key] = HostConnectionPool()\\n            self.host_to_pool[key].put(conn)\\n\\n    def clean(self):\\n        \"\"\"\\n        Clean up the stale connections in all of the pools, and then\\n        get rid of empty pools.  Pools clean themselves every time a\\n        connection is fetched; this cleaning takes care of pools that\\n        aren\\'t being used any more, so nothing is being gotten from\\n        them.\\n        \"\"\"\\n        with self.mutex:\\n            now = time.time()\\n            if self.last_clean_time + self.CLEAN_INTERVAL < now:\\n                to_remove = []\\n                for (host, pool) in self.host_to_pool.items():\\n                    pool.clean()\\n                    if pool.size() == 0:\\n                        to_remove.append(host)\\n                for host in to_remove:\\n                    del self.host_to_pool[host]\\n                self.last_clean_time = now\\n\\n\\nclass HTTPRequest(object):\\n\\n    def __init__(self, method, protocol, host, port, path, auth_path,\\n                 params, headers, body):\\n        \"\"\"Represents an HTTP request.\\n\\n        :type method: string\\n        :param method: The HTTP method name, \\'GET\\', \\'POST\\', \\'PUT\\' etc.\\n\\n        :type protocol: string\\n        :param protocol: The http protocol used, \\'http\\' or \\'https\\'.\\n\\n        :type host: string\\n        :param host: Host to which the request is addressed. eg. abc.com\\n\\n        :type port: int\\n        :param port: port on which the request is being sent. Zero means unset,\\n            in which case default port will be chosen.\\n\\n        :type path: string\\n        :param path: URL path that is being accessed.\\n\\n        :type auth_path: string\\n        :param path: The part of the URL path used when creating the\\n            authentication string.\\n\\n        :type params: dict\\n        :param params: HTTP url query parameters, with key as name of\\n            the param, and value as value of param.\\n\\n        :type headers: dict\\n        :param headers: HTTP headers, with key as name of the header and value\\n            as value of header.\\n\\n        :type body: string\\n        :param body: Body of the HTTP request. If not present, will be None or\\n            empty string (\\'\\').\\n        \"\"\"\\n        self.method = method\\n        self.protocol = protocol\\n        self.host = host\\n        self.port = port\\n        self.path = path\\n        if auth_path is None:\\n            auth_path = path\\n        self.auth_path = auth_path\\n        self.params = params\\n        # chunked Transfer-Encoding should act only on PUT request.\\n        if headers and \\'Transfer-Encoding\\' in headers and \\\\\\n                headers[\\'Transfer-Encoding\\'] == \\'chunked\\' and \\\\\\n                self.method != \\'PUT\\':\\n            self.headers = headers.copy()\\n            del self.headers[\\'Transfer-Encoding\\']\\n        else:\\n            self.headers = headers\\n        self.body = body\\n\\n    def __str__(self):\\n        return ((\\'method:(%s) protocol:(%s) host(%s) port(%s) path(%s) \\'\\n                 \\'params(%s) headers(%s) body(%s)\\') % (self.method,\\n                 self.protocol, self.host, self.port, self.path, self.params,\\n                 self.headers, self.body))\\n\\n    def authorize(self, connection, **kwargs):\\n        if not getattr(self, \\'_headers_quoted\\', False):\\n            for key in self.headers:\\n                val = self.headers[key]\\n                if isinstance(val, six.text_type):\\n                    safe = \\'!\"#$%&\\\\\\'()*+,/:;<=>?@[\\\\\\\\]^`{|}~ \\'\\n                    self.headers[key] = quote(val.encode(\\'utf-8\\'), safe)\\n            setattr(self, \\'_headers_quoted\\', True)\\n\\n        self.headers[\\'User-Agent\\'] = UserAgent\\n\\n        connection._auth_handler.add_auth(self, **kwargs)\\n\\n        # I\\'m not sure if this is still needed, now that add_auth is\\n        # setting the content-length for POST requests.\\n        if \\'Content-Length\\' not in self.headers:\\n            if \\'Transfer-Encoding\\' not in self.headers or \\\\\\n                    self.headers[\\'Transfer-Encoding\\'] != \\'chunked\\':\\n                self.headers[\\'Content-Length\\'] = str(len(self.body))\\n\\n\\nclass HTTPResponse(http_client.HTTPResponse):\\n\\n    def __init__(self, *args, **kwargs):\\n        http_client.HTTPResponse.__init__(self, *args, **kwargs)\\n        self._cached_response = \\'\\'\\n\\n    def read(self, amt=None):\\n        \"\"\"Read the response.\\n\\n        This method does not have the same behavior as\\n        http_client.HTTPResponse.read.  Instead, if this method is called with\\n        no ``amt`` arg, then the response body will be cached.  Subsequent\\n        calls to ``read()`` with no args **will return the cached response**.\\n\\n        \"\"\"\\n        if amt is None:\\n            # The reason for doing this is that many places in boto call\\n            # response.read() and except to get the response body that they\\n            # can then process.  To make sure this always works as they expect\\n            # we\\'re caching the response so that multiple calls to read()\\n            # will return the full body.  Note that this behavior only\\n            # happens if the amt arg is not specified.\\n            if not self._cached_response:\\n                self._cached_response = http_client.HTTPResponse.read(self)\\n            return self._cached_response\\n        else:\\n            return http_client.HTTPResponse.read(self, amt)\\n\\n\\nclass AWSAuthConnection(object):\\n    def __init__(self, host, aws_access_key_id=None,\\n                 aws_secret_access_key=None,\\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\\n                 proxy_user=None, proxy_pass=None, debug=0,\\n                 https_connection_factory=None, path=\\'/\\',\\n                 provider=\\'aws\\', security_token=None,\\n                 suppress_consec_slashes=True,\\n                 validate_certs=True, profile_name=None):\\n        \"\"\"\\n        :type host: str\\n        :param host: The host to make the connection to\\n\\n        :keyword str aws_access_key_id: Your AWS Access Key ID (provided by\\n            Amazon). If none is specified, the value in your\\n            ``AWS_ACCESS_KEY_ID`` environmental variable is used.\\n        :keyword str aws_secret_access_key: Your AWS Secret Access Key\\n            (provided by Amazon). If none is specified, the value in your\\n            ``AWS_SECRET_ACCESS_KEY`` environmental variable is used.\\n        :keyword str security_token: The security token associated with\\n            temporary credentials issued by STS.  Optional unless using\\n            temporary credentials.  If none is specified, the environment\\n            variable ``AWS_SECURITY_TOKEN`` is used if defined.\\n\\n        :type is_secure: boolean\\n        :param is_secure: Whether the connection is over SSL\\n\\n        :type https_connection_factory: list or tuple\\n        :param https_connection_factory: A pair of an HTTP connection\\n            factory and the exceptions to catch.  The factory should have\\n            a similar interface to L{http_client.HTTPSConnection}.\\n\\n        :param str proxy: Address/hostname for a proxy server\\n\\n        :type proxy_port: int\\n        :param proxy_port: The port to use when connecting over a proxy\\n\\n        :type proxy_user: str\\n        :param proxy_user: The username to connect with on the proxy\\n\\n        :type proxy_pass: str\\n        :param proxy_pass: The password to use when connection over a proxy.\\n\\n        :type port: int\\n        :param port: The port to use to connect\\n\\n        :type suppress_consec_slashes: bool\\n        :param suppress_consec_slashes: If provided, controls whether\\n            consecutive slashes will be suppressed in key paths.\\n\\n        :type validate_certs: bool\\n        :param validate_certs: Controls whether SSL certificates\\n            will be validated or not.  Defaults to True.\\n\\n        :type profile_name: str\\n        :param profile_name: Override usual Credentials section in config\\n            file to use a named set of keys instead.\\n        \"\"\"\\n        self.suppress_consec_slashes = suppress_consec_slashes\\n        self.num_retries = 6\\n        # Override passed-in is_secure setting if value was defined in config.\\n        if config.has_option(\\'Boto\\', \\'is_secure\\'):\\n            is_secure = config.getboolean(\\'Boto\\', \\'is_secure\\')\\n        self.is_secure = is_secure\\n        # Whether or not to validate server certificates.\\n        # The default is now to validate certificates.  This can be\\n        # overridden in the boto config file are by passing an\\n        # explicit validate_certs parameter to the class constructor.\\n        self.https_validate_certificates = config.getbool(\\n            \\'Boto\\', \\'https_validate_certificates\\',\\n            validate_certs)\\n        if self.https_validate_certificates and not HAVE_HTTPS_CONNECTION:\\n            raise BotoClientError(\\n                \"SSL server certificate validation is enabled in boto \"\\n                \"configuration, but Python dependencies required to \"\\n                \"support this feature are not available. Certificate \"\\n                \"validation is only supported when running under Python \"\\n                \"2.6 or later.\")\\n        certs_file = config.get_value(\\n            \\'Boto\\', \\'ca_certificates_file\\', DEFAULT_CA_CERTS_FILE)\\n        if certs_file == \\'system\\':\\n            certs_file = None\\n        self.ca_certificates_file = certs_file\\n        if port:\\n            self.port = port\\n        else:\\n            self.port = PORTS_BY_SECURITY[is_secure]\\n\\n        self.handle_proxy(proxy, proxy_port, proxy_user, proxy_pass)\\n        # define exceptions from http_client that we want to catch and retry\\n        self.http_exceptions = (http_client.HTTPException, socket.error,\\n                                socket.gaierror, http_client.BadStatusLine)\\n        # define subclasses of the above that are not retryable.\\n        self.http_unretryable_exceptions = []\\n        if HAVE_HTTPS_CONNECTION:\\n            self.http_unretryable_exceptions.append(\\n                https_connection.InvalidCertificateException)\\n\\n        # define values in socket exceptions we don\\'t want to catch\\n        self.socket_exception_values = (errno.EINTR,)\\n        if https_connection_factory is not None:\\n            self.https_connection_factory = https_connection_factory[0]\\n            self.http_exceptions += https_connection_factory[1]\\n        else:\\n            self.https_connection_factory = None\\n        if (is_secure):\\n            self.protocol = \\'https\\'\\n        else:\\n            self.protocol = \\'http\\'\\n        self.host = host\\n        self.path = path\\n        # if the value passed in for debug\\n        if not isinstance(debug, six.integer_types):\\n            debug = 0\\n        self.debug = config.getint(\\'Boto\\', \\'debug\\', debug)\\n        self.host_header = None\\n\\n        # Timeout used to tell http_client how long to wait for socket timeouts.\\n        # Default is to leave timeout unchanged, which will in turn result in\\n        # the socket\\'s default global timeout being used. To specify a\\n        # timeout, set http_socket_timeout in Boto config. Regardless,\\n        # timeouts will only be applied if Python is 2.6 or greater.\\n        self.http_connection_kwargs = {}\\n        if (sys.version_info[0], sys.version_info[1]) >= (2, 6):\\n            # If timeout isn\\'t defined in boto config file, use 70 second\\n            # default as recommended by\\n            # http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForActivityTask.html\\n            self.http_connection_kwargs[\\'timeout\\'] = config.getint(\\n                \\'Boto\\', \\'http_socket_timeout\\', 70)\\n\\n        if isinstance(provider, Provider):\\n            # Allow overriding Provider\\n            self.provider = provider\\n        else:\\n            self._provider_type = provider\\n            self.provider = Provider(self._provider_type,\\n                                     aws_access_key_id,\\n                                     aws_secret_access_key,\\n                                     security_token,\\n                                     profile_name)\\n\\n        # Allow config file to override default host, port, and host header.\\n        if self.provider.host:\\n            self.host = self.provider.host\\n        if self.provider.port:\\n            self.port = self.provider.port\\n        if self.provider.host_header:\\n            self.host_header = self.provider.host_header\\n\\n        self._pool = ConnectionPool()\\n        self._connection = (self.host, self.port, self.is_secure)\\n        self._last_rs = None\\n        self._auth_handler = auth.get_auth_handler(\\n            host, config, self.provider, self._required_auth_capability())\\n        if getattr(self, \\'AuthServiceName\\', None) is not None:\\n            self.auth_service_name = self.AuthServiceName\\n        self.request_hook = None\\n\\n    def __repr__(self):\\n        return \\'%s:%s\\' % (self.__class__.__name__, self.host)\\n\\n    def _required_auth_capability(self):\\n        return []\\n\\n    def _get_auth_service_name(self):\\n        return getattr(self._auth_handler, \\'service_name\\')\\n\\n    # For Sigv4, the auth_service_name/auth_region_name properties allow\\n    # the service_name/region_name to be explicitly set instead of being\\n    # derived from the endpoint url.\\n    def _set_auth_service_name(self, value):\\n        self._auth_handler.service_name = value\\n    auth_service_name = property(_get_auth_service_name, _set_auth_service_name)\\n\\n    def _get_auth_region_name(self):\\n        return getattr(self._auth_handler, \\'region_name\\')\\n\\n    def _set_auth_region_name(self, value):\\n        self._auth_handler.region_name = value\\n    auth_region_name = property(_get_auth_region_name, _set_auth_region_name)\\n\\n    def connection(self):\\n        return self.get_http_connection(*self._connection)\\n    connection = property(connection)\\n\\n    def aws_access_key_id(self):\\n        return self.provider.access_key\\n    aws_access_key_id = property(aws_access_key_id)\\n    gs_access_key_id = aws_access_key_id\\n    access_key = aws_access_key_id\\n\\n    def aws_secret_access_key(self):\\n        return self.provider.secret_key\\n    aws_secret_access_key = property(aws_secret_access_key)\\n    gs_secret_access_key = aws_secret_access_key\\n    secret_key = aws_secret_access_key\\n\\n    def profile_name(self):\\n        return self.provider.profile_name\\n    profile_name = property(profile_name)\\n\\n    def get_path(self, path=\\'/\\'):\\n        # The default behavior is to suppress consecutive slashes for reasons\\n        # discussed at\\n        # https://groups.google.com/forum/#!topic/boto-dev/-ft0XPUy0y8\\n        # You can override that behavior with the suppress_consec_slashes param.\\n        if not self.suppress_consec_slashes:\\n            return self.path + re.sub(\\'^(/*)/\\', \"\\\\\\\\1\", path)\\n        pos = path.find(\\'?\\')\\n        if pos >= 0:\\n            params = path[pos:]\\n            path = path[:pos]\\n        else:\\n            params = None\\n        if path[-1] == \\'/\\':\\n            need_trailing = True\\n        else:\\n            need_trailing = False\\n        path_elements = self.path.split(\\'/\\')\\n        path_elements.extend(path.split(\\'/\\'))\\n        path_elements = [p for p in path_elements if p]\\n        path = \\'/\\' + \\'/\\'.join(path_elements)\\n        if path[-1] != \\'/\\' and need_trailing:\\n            path += \\'/\\'\\n        if params:\\n            path = path + params\\n        return path\\n\\n    def server_name(self, port=None):\\n        if not port:\\n            port = self.port\\n        if port == 80:\\n            signature_host = self.host\\n        else:\\n            # This unfortunate little hack can be attributed to\\n            # a difference in the 2.6 version of http_client.  In old\\n            # versions, it would append \":443\" to the hostname sent\\n            # in the Host header and so we needed to make sure we\\n            # did the same when calculating the V2 signature.  In 2.6\\n            # (and higher!)\\n            # it no longer does that.  Hence, this kludge.\\n            if ((ON_APP_ENGINE and sys.version[:3] == \\'2.5\\') or\\n                    sys.version[:3] in (\\'2.6\\', \\'2.7\\')) and port == 443:\\n                signature_host = self.host\\n            else:\\n                signature_host = \\'%s:%d\\' % (self.host, port)\\n        return signature_host\\n\\n    def handle_proxy(self, proxy, proxy_port, proxy_user, proxy_pass):\\n        self.proxy = proxy\\n        self.proxy_port = proxy_port\\n        self.proxy_user = proxy_user\\n        self.proxy_pass = proxy_pass\\n        if \\'http_proxy\\' in os.environ and not self.proxy:\\n            pattern = re.compile(\\n                \\'(?:http://)?\\'\\n                \\'(?:(?P<user>[\\\\w\\\\-\\\\.]+):(?P<pass>.*)@)?\\'\\n                \\'(?P<host>[\\\\w\\\\-\\\\.]+)\\'\\n                \\'(?::(?P<port>\\\\d+))?\\'\\n            )\\n            match = pattern.match(os.environ[\\'http_proxy\\'])\\n            if match:\\n                self.proxy = match.group(\\'host\\')\\n                self.proxy_port = match.group(\\'port\\')\\n                self.proxy_user = match.group(\\'user\\')\\n                self.proxy_pass = match.group(\\'pass\\')\\n        else:\\n            if not self.proxy:\\n                self.proxy = config.get_value(\\'Boto\\', \\'proxy\\', None)\\n            if not self.proxy_port:\\n                self.proxy_port = config.get_value(\\'Boto\\', \\'proxy_port\\', None)\\n            if not self.proxy_user:\\n                self.proxy_user = config.get_value(\\'Boto\\', \\'proxy_user\\', None)\\n            if not self.proxy_pass:\\n                self.proxy_pass = config.get_value(\\'Boto\\', \\'proxy_pass\\', None)\\n\\n        if not self.proxy_port and self.proxy:\\n            print(\"http_proxy environment variable does not specify \"\\n                  \"a port, using default\")\\n            self.proxy_port = self.port\\n\\n        self.no_proxy = os.environ.get(\\'no_proxy\\', \\'\\') or os.environ.get(\\'NO_PROXY\\', \\'\\')\\n        self.use_proxy = (self.proxy is not None)\\n\\n    def get_http_connection(self, host, port, is_secure):\\n        conn = self._pool.get_http_connection(host, port, is_secure)\\n        if conn is not None:\\n            return conn\\n        else:\\n            return self.new_http_connection(host, port, is_secure)\\n\\n    def skip_proxy(self, host):\\n        if not self.no_proxy:\\n            return False\\n\\n        if self.no_proxy == \"*\":\\n            return True\\n\\n        hostonly = host\\n        hostonly = host.split(\\':\\')[0]\\n\\n        for name in self.no_proxy.split(\\',\\'):\\n            if name and (hostonly.endswith(name) or host.endswith(name)):\\n                return True\\n\\n        return False\\n\\n    def new_http_connection(self, host, port, is_secure):\\n        if host is None:\\n            host = self.server_name()\\n\\n        # Make sure the host is really just the host, not including\\n        # the port number\\n        host = boto.utils.parse_host(host)\\n\\n        http_connection_kwargs = self.http_connection_kwargs.copy()\\n\\n        # Connection factories below expect a port keyword argument\\n        http_connection_kwargs[\\'port\\'] = port\\n\\n        # Override host with proxy settings if needed\\n        if self.use_proxy and not is_secure and \\\\\\n                not self.skip_proxy(host):\\n            host = self.proxy\\n            http_connection_kwargs[\\'port\\'] = int(self.proxy_port)\\n\\n        if is_secure:\\n            boto.log.debug(\\n                \\'establishing HTTPS connection: host=%s, kwargs=%s\\',\\n                host, http_connection_kwargs)\\n            if self.use_proxy and not self.skip_proxy(host):\\n                connection = self.proxy_ssl(host, is_secure and 443 or 80)\\n            elif self.https_connection_factory:\\n                connection = self.https_connection_factory(host)\\n            elif self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\\n                connection = https_connection.CertValidatingHTTPSConnection(\\n                    host, ca_certs=self.ca_certificates_file,\\n                    **http_connection_kwargs)\\n            else:\\n                connection = http_client.HTTPSConnection(\\n                    host, **http_connection_kwargs)\\n        else:\\n            boto.log.debug(\\'establishing HTTP connection: kwargs=%s\\' %\\n                           http_connection_kwargs)\\n            if self.https_connection_factory:\\n                # even though the factory says https, this is too handy\\n                # to not be able to allow overriding for http also.\\n                connection = self.https_connection_factory(\\n                    host, **http_connection_kwargs)\\n            else:\\n                connection = http_client.HTTPConnection(\\n                    host, **http_connection_kwargs)\\n        if self.debug > 1:\\n            connection.set_debuglevel(self.debug)\\n        # self.connection must be maintained for backwards-compatibility\\n        # however, it must be dynamically pulled from the connection pool\\n        # set a private variable which will enable that\\n        if host.split(\\':\\')[0] == self.host and is_secure == self.is_secure:\\n            self._connection = (host, port, is_secure)\\n        # Set the response class of the http connection to use our custom\\n        # class.\\n        connection.response_class = HTTPResponse\\n        return connection\\n\\n    def put_http_connection(self, host, port, is_secure, connection):\\n        self._pool.put_http_connection(host, port, is_secure, connection)\\n\\n    def proxy_ssl(self, host=None, port=None):\\n        if host and port:\\n            host = \\'%s:%d\\' % (host, port)\\n        else:\\n            host = \\'%s:%d\\' % (self.host, self.port)\\n        # Seems properly to use timeout for connect too\\n        timeout = self.http_connection_kwargs.get(\"timeout\")\\n        if timeout is not None:\\n            sock = socket.create_connection((self.proxy,\\n                                             int(self.proxy_port)), timeout)\\n        else:\\n            sock = socket.create_connection((self.proxy, int(self.proxy_port)))\\n        boto.log.debug(\"Proxy connection: CONNECT %s HTTP/1.0\\\\r\\\\n\", host)\\n        sock.sendall(\"CONNECT %s HTTP/1.0\\\\r\\\\n\" % host)\\n        sock.sendall(\"User-Agent: %s\\\\r\\\\n\" % UserAgent)\\n        if self.proxy_user and self.proxy_pass:\\n            for k, v in self.get_proxy_auth_header().items():\\n                sock.sendall(\"%s: %s\\\\r\\\\n\" % (k, v))\\n            # See discussion about this config option at\\n            # https://groups.google.com/forum/?fromgroups#!topic/boto-dev/teenFvOq2Cc\\n            if config.getbool(\\'Boto\\', \\'send_crlf_after_proxy_auth_headers\\', False):\\n                sock.sendall(\"\\\\r\\\\n\")\\n        else:\\n            sock.sendall(\"\\\\r\\\\n\")\\n        resp = http_client.HTTPResponse(sock, strict=True, debuglevel=self.debug)\\n        resp.begin()\\n\\n        if resp.status != 200:\\n            # Fake a socket error, use a code that make it obvious it hasn\\'t\\n            # been generated by the socket library\\n            raise socket.error(-71,\\n                               \"Error talking to HTTP proxy %s:%s: %s (%s)\" %\\n                               (self.proxy, self.proxy_port,\\n                                resp.status, resp.reason))\\n\\n        # We can safely close the response, it duped the original socket\\n        resp.close()\\n\\n        h = http_client.HTTPConnection(host)\\n\\n        if self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\\n            msg = \"wrapping ssl socket for proxied connection; \"\\n            if self.ca_certificates_file:\\n                msg += \"CA certificate file=%s\" % self.ca_certificates_file\\n            else:\\n                msg += \"using system provided SSL certs\"\\n            boto.log.debug(msg)\\n            key_file = self.http_connection_kwargs.get(\\'key_file\\', None)\\n            cert_file = self.http_connection_kwargs.get(\\'cert_file\\', None)\\n            sslSock = ssl.wrap_socket(sock, keyfile=key_file,\\n                                      certfile=cert_file,\\n                                      cert_reqs=ssl.CERT_REQUIRED,\\n                                      ca_certs=self.ca_certificates_file)\\n            cert = sslSock.getpeercert()\\n            hostname = self.host.split(\\':\\', 0)[0]\\n            if not https_connection.ValidateCertificateHostname(cert, hostname):\\n                raise https_connection.InvalidCertificateException(\\n                    hostname, cert, \\'hostname mismatch\\')\\n        else:\\n            # Fallback for old Python without ssl.wrap_socket\\n            if hasattr(http_client, \\'ssl\\'):\\n                sslSock = http_client.ssl.SSLSocket(sock)\\n            else:\\n                sslSock = socket.ssl(sock, None, None)\\n                sslSock = http_client.FakeSocket(sock, sslSock)\\n\\n        # This is a bit unclean\\n        h.sock = sslSock\\n        return h\\n\\n    def prefix_proxy_to_path(self, path, host=None):\\n        path = self.protocol + \\'://\\' + (host or self.server_name()) + path\\n        return path\\n\\n    def get_proxy_auth_header(self):\\n        auth = encodebytes(self.proxy_user + \\':\\' + self.proxy_pass)\\n        return {\\'Proxy-Authorization\\': \\'Basic %s\\' % auth}\\n\\n    # For passing proxy information to other connection libraries, e.g. cloudsearch2\\n    def get_proxy_url_with_auth(self):\\n        if not self.use_proxy:\\n            return None\\n\\n        if self.proxy_user or self.proxy_pass:\\n            if self.proxy_pass:\\n                login_info = \\'%s:%s@\\' % (self.proxy_user, self.proxy_pass)\\n            else:\\n                login_info = \\'%s@\\' % self.proxy_user\\n        else:\\n            login_info = \\'\\'\\n\\n        return \\'http://%s%s:%s\\' % (login_info, self.proxy, str(self.proxy_port or self.port))\\n\\n    def set_host_header(self, request):\\n        try:\\n            request.headers[\\'Host\\'] = \\\\\\n                self._auth_handler.host_header(self.host, request)\\n        except AttributeError:\\n            request.headers[\\'Host\\'] = self.host.split(\\':\\', 1)[0]\\n\\n    def set_request_hook(self, hook):\\n        self.request_hook = hook\\n\\n    def _mexe(self, request, sender=None, override_num_retries=None,\\n              retry_handler=None):\\n        \"\"\"\\n        mexe - Multi-execute inside a loop, retrying multiple times to handle\\n               transient Internet errors by simply trying again.\\n               Also handles redirects.\\n\\n        This code was inspired by the S3Utils classes posted to the boto-users\\n        Google group by Larry Bates.  Thanks!\\n\\n        \"\"\"\\n        boto.log.debug(\\'Method: %s\\' % request.method)\\n        boto.log.debug(\\'Path: %s\\' % request.path)\\n        boto.log.debug(\\'Data: %s\\' % request.body)\\n        boto.log.debug(\\'Headers: %s\\' % request.headers)\\n        boto.log.debug(\\'Host: %s\\' % request.host)\\n        boto.log.debug(\\'Port: %s\\' % request.port)\\n        boto.log.debug(\\'Params: %s\\' % request.params)\\n        response = None\\n        body = None\\n        ex = None\\n        if override_num_retries is None:\\n            num_retries = config.getint(\\'Boto\\', \\'num_retries\\', self.num_retries)\\n        else:\\n            num_retries = override_num_retries\\n        i = 0\\n        connection = self.get_http_connection(request.host, request.port,\\n                                              self.is_secure)\\n\\n        # Convert body to bytes if needed\\n        if not isinstance(request.body, bytes) and hasattr(request.body,\\n                                                           \\'encode\\'):\\n            request.body = request.body.encode(\\'utf-8\\')\\n\\n        while i <= num_retries:\\n            # Use binary exponential backoff to desynchronize client requests.\\n            next_sleep = min(random.random() * (2 ** i),\\n                             boto.config.get(\\'Boto\\', \\'max_retry_delay\\', 60))\\n            try:\\n                # we now re-sign each request before it is retried\\n                boto.log.debug(\\'Token: %s\\' % self.provider.security_token)\\n                request.authorize(connection=self)\\n                # Only force header for non-s3 connections, because s3 uses\\n                # an older signing method + bucket resource URLs that include\\n                # the port info. All others should be now be up to date and\\n                # not include the port.\\n                if \\'s3\\' not in self._required_auth_capability():\\n                    if not getattr(self, \\'anon\\', False):\\n                        if not request.headers.get(\\'Host\\'):\\n                            self.set_host_header(request)\\n                boto.log.debug(\\'Final headers: %s\\' % request.headers)\\n                request.start_time = datetime.now()\\n                if callable(sender):\\n                    response = sender(connection, request.method, request.path,\\n                                      request.body, request.headers)\\n                else:\\n                    connection.request(request.method, request.path,\\n                                       request.body, request.headers)\\n                    response = connection.getresponse()\\n                boto.log.debug(\\'Response headers: %s\\' % response.getheaders())\\n                location = response.getheader(\\'location\\')\\n                # -- gross hack --\\n                # http_client gets confused with chunked responses to HEAD requests\\n                # so I have to fake it out\\n                if request.method == \\'HEAD\\' and getattr(response,\\n                                                        \\'chunked\\', False):\\n                    response.chunked = 0\\n                if callable(retry_handler):\\n                    status = retry_handler(response, i, next_sleep)\\n                    if status:\\n                        msg, i, next_sleep = status\\n                        if msg:\\n                            boto.log.debug(msg)\\n                        time.sleep(next_sleep)\\n                        continue\\n                if response.status in [500, 502, 503, 504]:\\n                    msg = \\'Received %d response.  \\' % response.status\\n                    msg += \\'Retrying in %3.1f seconds\\' % next_sleep\\n                    boto.log.debug(msg)\\n                    body = response.read()\\n                    if isinstance(body, bytes):\\n                        body = body.decode(\\'utf-8\\')\\n                elif response.status < 300 or response.status >= 400 or \\\\\\n                        not location:\\n                    # don\\'t return connection to the pool if response contains\\n                    # Connection:close header, because the connection has been\\n                    # closed and default reconnect behavior may do something\\n                    # different than new_http_connection. Also, it\\'s probably\\n                    # less efficient to try to reuse a closed connection.\\n                    conn_header_value = response.getheader(\\'connection\\')\\n                    if conn_header_value == \\'close\\':\\n                        connection.close()\\n                    else:\\n                        self.put_http_connection(request.host, request.port,\\n                                                 self.is_secure, connection)\\n                    if self.request_hook is not None:\\n                        self.request_hook.handle_request_data(request, response)\\n                    return response\\n                else:\\n                    scheme, request.host, request.path, \\\\\\n                        params, query, fragment = urlparse(location)\\n                    if query:\\n                        request.path += \\'?\\' + query\\n                    # urlparse can return both host and port in netloc, so if\\n                    # that\\'s the case we need to split them up properly\\n                    if \\':\\' in request.host:\\n                        request.host, request.port = request.host.split(\\':\\', 1)\\n                    msg = \\'Redirecting: %s\\' % scheme + \\'://\\'\\n                    msg += request.host + request.path\\n                    boto.log.debug(msg)\\n                    connection = self.get_http_connection(request.host,\\n                                                          request.port,\\n                                                          scheme == \\'https\\')\\n                    response = None\\n                    continue\\n            except PleaseRetryException as e:\\n                boto.log.debug(\\'encountered a retry exception: %s\\' % e)\\n                connection = self.new_http_connection(request.host, request.port,\\n                                                      self.is_secure)\\n                response = e.response\\n                ex = e\\n            except self.http_exceptions as e:\\n                for unretryable in self.http_unretryable_exceptions:\\n                    if isinstance(e, unretryable):\\n                        boto.log.debug(\\n                            \\'encountered unretryable %s exception, re-raising\\' %\\n                            e.__class__.__name__)\\n                        raise\\n                boto.log.debug(\\'encountered %s exception, reconnecting\\' %\\n                               e.__class__.__name__)\\n                connection = self.new_http_connection(request.host, request.port,\\n                                                      self.is_secure)\\n                ex = e\\n            time.sleep(next_sleep)\\n            i += 1\\n        # If we made it here, it\\'s because we have exhausted our retries\\n        # and stil haven\\'t succeeded.  So, if we have a response object,\\n        # use it to raise an exception.\\n        # Otherwise, raise the exception that must have already happened.\\n        if self.request_hook is not None:\\n            self.request_hook.handle_request_data(request, response, error=True)\\n        if response:\\n            raise BotoServerError(response.status, response.reason, body)\\n        elif ex:\\n            raise ex\\n        else:\\n            msg = \\'Please report this exception as a Boto Issue!\\'\\n            raise BotoClientError(msg)\\n\\n    def build_base_http_request(self, method, path, auth_path,\\n                                params=None, headers=None, data=\\'\\', host=None):\\n        path = self.get_path(path)\\n        if auth_path is not None:\\n            auth_path = self.get_path(auth_path)\\n        if params is None:\\n            params = {}\\n        else:\\n            params = params.copy()\\n        if headers is None:\\n            headers = {}\\n        else:\\n            headers = headers.copy()\\n        if self.host_header and not boto.utils.find_matching_headers(\\'host\\', headers):\\n            headers[\\'host\\'] = self.host_header\\n        host = host or self.host\\n        if self.use_proxy and not self.skip_proxy(host):\\n            if not auth_path:\\n                auth_path = path\\n            path = self.prefix_proxy_to_path(path, host)\\n            if self.proxy_user and self.proxy_pass and not self.is_secure:\\n                # If is_secure, we don\\'t have to set the proxy authentication\\n                # header here, we did that in the CONNECT to the proxy.\\n                headers.update(self.get_proxy_auth_header())\\n        return HTTPRequest(method, self.protocol, host, self.port,\\n                           path, auth_path, params, headers, data)\\n\\n    def make_request(self, method, path, headers=None, data=\\'\\', host=None,\\n                     auth_path=None, sender=None, override_num_retries=None,\\n                     params=None, retry_handler=None):\\n        \"\"\"Makes a request to the server, with stock multiple-retry logic.\"\"\"\\n        if params is None:\\n            params = {}\\n        http_request = self.build_base_http_request(method, path, auth_path,\\n                                                    params, headers, data, host)\\n        return self._mexe(http_request, sender, override_num_retries,\\n                          retry_handler=retry_handler)\\n\\n    def close(self):\\n        \"\"\"(Optional) Close any open HTTP connections.  This is non-destructive,\\n        and making a new request will open a connection again.\"\"\"\\n\\n        boto.log.debug(\\'closing all HTTP connections\\')\\n        self._connection = None  # compat field\\n\\n\\nclass AWSQueryConnection(AWSAuthConnection):\\n\\n    APIVersion = \\'\\'\\n    ResponseError = BotoServerError\\n\\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\\n                 proxy_user=None, proxy_pass=None, host=None, debug=0,\\n                 https_connection_factory=None, path=\\'/\\', security_token=None,\\n                 validate_certs=True, profile_name=None, provider=\\'aws\\'):\\n        super(AWSQueryConnection, self).__init__(\\n            host, aws_access_key_id,\\n            aws_secret_access_key,\\n            is_secure, port, proxy,\\n            proxy_port, proxy_user, proxy_pass,\\n            debug, https_connection_factory, path,\\n            security_token=security_token,\\n            validate_certs=validate_certs,\\n            profile_name=profile_name,\\n            provider=provider)\\n\\n    def _required_auth_capability(self):\\n        return []\\n\\n    def get_utf8_value(self, value):\\n        return boto.utils.get_utf8_value(value)\\n\\n    def make_request(self, action, params=None, path=\\'/\\', verb=\\'GET\\'):\\n        http_request = self.build_base_http_request(verb, path, None,\\n                                                    params, {}, \\'\\',\\n                                                    self.host)\\n        if action:\\n            http_request.params[\\'Action\\'] = action\\n        if self.APIVersion:\\n            http_request.params[\\'Version\\'] = self.APIVersion\\n        return self._mexe(http_request)\\n\\n    def build_list_params(self, params, items, label):\\n        if isinstance(items, six.string_types):\\n            items = [items]\\n        for i in range(1, len(items) + 1):\\n            params[\\'%s.%d\\' % (label, i)] = items[i - 1]\\n\\n    def build_complex_list_params(self, params, items, label, names):\\n        \"\"\"Serialize a list of structures.\\n\\n        For example::\\n\\n            items = [(\\'foo\\', \\'bar\\', \\'baz\\'), (\\'foo2\\', \\'bar2\\', \\'baz2\\')]\\n            label = \\'ParamName.member\\'\\n            names = (\\'One\\', \\'Two\\', \\'Three\\')\\n            self.build_complex_list_params(params, items, label, names)\\n\\n        would result in the params dict being updated with these params::\\n\\n            ParamName.member.1.One = foo\\n            ParamName.member.1.Two = bar\\n            ParamName.member.1.Three = baz\\n\\n            ParamName.member.2.One = foo2\\n            ParamName.member.2.Two = bar2\\n            ParamName.member.2.Three = baz2\\n\\n        :type params: dict\\n        :param params: The params dict.  The complex list params\\n            will be added to this dict.\\n\\n        :type items: list of tuples\\n        :param items: The list to serialize.\\n\\n        :type label: string\\n        :param label: The prefix to apply to the parameter.\\n\\n        :type names: tuple of strings\\n        :param names: The names associated with each tuple element.\\n\\n        \"\"\"\\n        for i, item in enumerate(items, 1):\\n            current_prefix = \\'%s.%s\\' % (label, i)\\n            for key, value in zip(names, item):\\n                full_key = \\'%s.%s\\' % (current_prefix, key)\\n                params[full_key] = value\\n\\n    # generics\\n\\n    def get_list(self, action, params, markers, path=\\'/\\',\\n                 parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            rs = ResultSet(markers)\\n            h = boto.handler.XmlHandler(rs, parent)\\n            if isinstance(body, six.text_type):\\n                body = body.encode(\\'utf-8\\')\\n            xml.sax.parseString(body, h)\\n            return rs\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n\\n    def get_object(self, action, params, cls, path=\\'/\\',\\n                   parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            obj = cls(parent)\\n            h = boto.handler.XmlHandler(obj, parent)\\n            if isinstance(body, six.text_type):\\n                body = body.encode(\\'utf-8\\')\\n            xml.sax.parseString(body, h)\\n            return obj\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n\\n    def get_status(self, action, params, path=\\'/\\', parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            rs = ResultSet()\\n            h = boto.handler.XmlHandler(rs, parent)\\n            xml.sax.parseString(body, h)\\n            return rs.status\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n'}, {'boto.connection.AWSAuthConnection.server_name': '# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/\\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.\\n# Copyright (c) 2010 Google\\n# Copyright (c) 2008 rPath, Inc.\\n# Copyright (c) 2009 The Echo Nest Corporation\\n# Copyright (c) 2010, Eucalyptus Systems, Inc.\\n# Copyright (c) 2011, Nexenta Systems Inc.\\n# All rights reserved.\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a\\n# copy of this software and associated documentation files (the\\n# \"Software\"), to deal in the Software without restriction, including\\n# without limitation the rights to use, copy, modify, merge, publish, dis-\\n# tribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the fol-\\n# lowing conditions:\\n#\\n# The above copyright notice and this permission notice shall be included\\n# in all copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\\n# IN THE SOFTWARE.\\n\\n#\\n# Parts of this code were copied or derived from sample code supplied by AWS.\\n# The following notice applies to that code.\\n#\\n#  This software code is made available \"AS IS\" without warranties of any\\n#  kind.  You may copy, display, modify and redistribute the software\\n#  code either by itself or as incorporated into your code; provided that\\n#  you do not remove any proprietary notices.  Your use of this software\\n#  code is at your own risk and you waive any claim against Amazon\\n#  Digital Services, Inc. or its affiliates with respect to your use of\\n#  this software code. (c) 2006 Amazon Digital Services, Inc. or its\\n#  affiliates.\\n\\n\"\"\"\\nHandles basic connections to AWS\\n\"\"\"\\nfrom datetime import datetime\\nimport errno\\nimport os\\nimport random\\nimport re\\nimport socket\\nimport sys\\nimport time\\nimport xml.sax\\nimport copy\\n\\nfrom boto import auth\\nfrom boto import auth_handler\\nimport boto\\nimport boto.utils\\nimport boto.handler\\nimport boto.cacerts\\n\\nfrom boto import config, UserAgent\\nfrom boto.compat import six, http_client, urlparse, quote, encodebytes\\nfrom boto.exception import AWSConnectionError\\nfrom boto.exception import BotoClientError\\nfrom boto.exception import BotoServerError\\nfrom boto.exception import PleaseRetryException\\nfrom boto.provider import Provider\\nfrom boto.resultset import ResultSet\\n\\nHAVE_HTTPS_CONNECTION = False\\ntry:\\n    import ssl\\n    from boto import https_connection\\n    # Google App Engine runs on Python 2.5 so doesn\\'t have ssl.SSLError.\\n    if hasattr(ssl, \\'SSLError\\'):\\n        HAVE_HTTPS_CONNECTION = True\\nexcept ImportError:\\n    pass\\n\\ntry:\\n    import threading\\nexcept ImportError:\\n    import dummy_threading as threading\\n\\nON_APP_ENGINE = all(key in os.environ for key in (\\n    \\'USER_IS_ADMIN\\', \\'CURRENT_VERSION_ID\\', \\'APPLICATION_ID\\'))\\n\\nPORTS_BY_SECURITY = {True: 443,\\n                     False: 80}\\n\\nDEFAULT_CA_CERTS_FILE = os.path.join(os.path.dirname(os.path.abspath(boto.cacerts.__file__)), \"cacerts.txt\")\\n\\n\\nclass HostConnectionPool(object):\\n\\n    \"\"\"\\n    A pool of connections for one remote (host,port,is_secure).\\n\\n    When connections are added to the pool, they are put into a\\n    pending queue.  The _mexe method returns connections to the pool\\n    before the response body has been read, so they connections aren\\'t\\n    ready to send another request yet.  They stay in the pending queue\\n    until they are ready for another request, at which point they are\\n    returned to the pool of ready connections.\\n\\n    The pool of ready connections is an ordered list of\\n    (connection,time) pairs, where the time is the time the connection\\n    was returned from _mexe.  After a certain period of time,\\n    connections are considered stale, and discarded rather than being\\n    reused.  This saves having to wait for the connection to time out\\n    if AWS has decided to close it on the other end because of\\n    inactivity.\\n\\n    Thread Safety:\\n\\n        This class is used only from ConnectionPool while it\\'s mutex\\n        is held.\\n    \"\"\"\\n\\n    def __init__(self):\\n        self.queue = []\\n\\n    def size(self):\\n        \"\"\"\\n        Returns the number of connections in the pool for this host.\\n        Some of the connections may still be in use, and may not be\\n        ready to be returned by get().\\n        \"\"\"\\n        return len(self.queue)\\n\\n    def put(self, conn):\\n        \"\"\"\\n        Adds a connection to the pool, along with the time it was\\n        added.\\n        \"\"\"\\n        self.queue.append((conn, time.time()))\\n\\n    def get(self):\\n        \"\"\"\\n        Returns the next connection in this pool that is ready to be\\n        reused.  Returns None if there aren\\'t any.\\n        \"\"\"\\n        # Discard ready connections that are too old.\\n        self.clean()\\n\\n        # Return the first connection that is ready, and remove it\\n        # from the queue.  Connections that aren\\'t ready are returned\\n        # to the end of the queue with an updated time, on the\\n        # assumption that somebody is actively reading the response.\\n        for _ in range(len(self.queue)):\\n            (conn, _) = self.queue.pop(0)\\n            if self._conn_ready(conn):\\n                return conn\\n            else:\\n                self.put(conn)\\n        return None\\n\\n    def _conn_ready(self, conn):\\n        \"\"\"\\n        There is a nice state diagram at the top of http_client.py.  It\\n        indicates that once the response headers have been read (which\\n        _mexe does before adding the connection to the pool), a\\n        response is attached to the connection, and it stays there\\n        until it\\'s done reading.  This isn\\'t entirely true: even after\\n        the client is done reading, the response may be closed, but\\n        not removed from the connection yet.\\n\\n        This is ugly, reading a private instance variable, but the\\n        state we care about isn\\'t available in any public methods.\\n        \"\"\"\\n        if ON_APP_ENGINE:\\n            # Google AppEngine implementation of HTTPConnection doesn\\'t contain\\n            # _HTTPConnection__response attribute. Moreover, it\\'s not possible\\n            # to determine if given connection is ready. Reusing connections\\n            # simply doesn\\'t make sense with App Engine urlfetch service.\\n            return False\\n        else:\\n            response = getattr(conn, \\'_HTTPConnection__response\\', None)\\n            return (response is None) or response.isclosed()\\n\\n    def clean(self):\\n        \"\"\"\\n        Get rid of stale connections.\\n        \"\"\"\\n        # Note that we do not close the connection here -- somebody\\n        # may still be reading from it.\\n        while len(self.queue) > 0 and self._pair_stale(self.queue[0]):\\n            self.queue.pop(0)\\n\\n    def _pair_stale(self, pair):\\n        \"\"\"\\n        Returns true of the (connection,time) pair is too old to be\\n        used.\\n        \"\"\"\\n        (_conn, return_time) = pair\\n        now = time.time()\\n        return return_time + ConnectionPool.STALE_DURATION < now\\n\\n\\nclass ConnectionPool(object):\\n\\n    \"\"\"\\n    A connection pool that expires connections after a fixed period of\\n    time.  This saves time spent waiting for a connection that AWS has\\n    timed out on the other end.\\n\\n    This class is thread-safe.\\n    \"\"\"\\n\\n    #\\n    # The amout of time between calls to clean.\\n    #\\n\\n    CLEAN_INTERVAL = 5.0\\n\\n    #\\n    # How long before a connection becomes \"stale\" and won\\'t be reused\\n    # again.  The intention is that this time is less that the timeout\\n    # period that AWS uses, so we\\'ll never try to reuse a connection\\n    # and find that AWS is timing it out.\\n    #\\n    # Experimentation in July 2011 shows that AWS starts timing things\\n    # out after three minutes.  The 60 seconds here is conservative so\\n    # we should never hit that 3-minute timout.\\n    #\\n\\n    STALE_DURATION = 60.0\\n\\n    def __init__(self):\\n        # Mapping from (host,port,is_secure) to HostConnectionPool.\\n        # If a pool becomes empty, it is removed.\\n        self.host_to_pool = {}\\n        # The last time the pool was cleaned.\\n        self.last_clean_time = 0.0\\n        self.mutex = threading.Lock()\\n        ConnectionPool.STALE_DURATION = \\\\\\n            config.getfloat(\\'Boto\\', \\'connection_stale_duration\\',\\n                            ConnectionPool.STALE_DURATION)\\n\\n    def __getstate__(self):\\n        pickled_dict = copy.copy(self.__dict__)\\n        pickled_dict[\\'host_to_pool\\'] = {}\\n        del pickled_dict[\\'mutex\\']\\n        return pickled_dict\\n\\n    def __setstate__(self, dct):\\n        self.__init__()\\n\\n    def size(self):\\n        \"\"\"\\n        Returns the number of connections in the pool.\\n        \"\"\"\\n        return sum(pool.size() for pool in self.host_to_pool.values())\\n\\n    def get_http_connection(self, host, port, is_secure):\\n        \"\"\"\\n        Gets a connection from the pool for the named host.  Returns\\n        None if there is no connection that can be reused. It\\'s the caller\\'s\\n        responsibility to call close() on the connection when it\\'s no longer\\n        needed.\\n        \"\"\"\\n        self.clean()\\n        with self.mutex:\\n            key = (host, port, is_secure)\\n            if key not in self.host_to_pool:\\n                return None\\n            return self.host_to_pool[key].get()\\n\\n    def put_http_connection(self, host, port, is_secure, conn):\\n        \"\"\"\\n        Adds a connection to the pool of connections that can be\\n        reused for the named host.\\n        \"\"\"\\n        with self.mutex:\\n            key = (host, port, is_secure)\\n            if key not in self.host_to_pool:\\n                self.host_to_pool[key] = HostConnectionPool()\\n            self.host_to_pool[key].put(conn)\\n\\n    def clean(self):\\n        \"\"\"\\n        Clean up the stale connections in all of the pools, and then\\n        get rid of empty pools.  Pools clean themselves every time a\\n        connection is fetched; this cleaning takes care of pools that\\n        aren\\'t being used any more, so nothing is being gotten from\\n        them.\\n        \"\"\"\\n        with self.mutex:\\n            now = time.time()\\n            if self.last_clean_time + self.CLEAN_INTERVAL < now:\\n                to_remove = []\\n                for (host, pool) in self.host_to_pool.items():\\n                    pool.clean()\\n                    if pool.size() == 0:\\n                        to_remove.append(host)\\n                for host in to_remove:\\n                    del self.host_to_pool[host]\\n                self.last_clean_time = now\\n\\n\\nclass HTTPRequest(object):\\n\\n    def __init__(self, method, protocol, host, port, path, auth_path,\\n                 params, headers, body):\\n        \"\"\"Represents an HTTP request.\\n\\n        :type method: string\\n        :param method: The HTTP method name, \\'GET\\', \\'POST\\', \\'PUT\\' etc.\\n\\n        :type protocol: string\\n        :param protocol: The http protocol used, \\'http\\' or \\'https\\'.\\n\\n        :type host: string\\n        :param host: Host to which the request is addressed. eg. abc.com\\n\\n        :type port: int\\n        :param port: port on which the request is being sent. Zero means unset,\\n            in which case default port will be chosen.\\n\\n        :type path: string\\n        :param path: URL path that is being accessed.\\n\\n        :type auth_path: string\\n        :param path: The part of the URL path used when creating the\\n            authentication string.\\n\\n        :type params: dict\\n        :param params: HTTP url query parameters, with key as name of\\n            the param, and value as value of param.\\n\\n        :type headers: dict\\n        :param headers: HTTP headers, with key as name of the header and value\\n            as value of header.\\n\\n        :type body: string\\n        :param body: Body of the HTTP request. If not present, will be None or\\n            empty string (\\'\\').\\n        \"\"\"\\n        self.method = method\\n        self.protocol = protocol\\n        self.host = host\\n        self.port = port\\n        self.path = path\\n        if auth_path is None:\\n            auth_path = path\\n        self.auth_path = auth_path\\n        self.params = params\\n        # chunked Transfer-Encoding should act only on PUT request.\\n        if headers and \\'Transfer-Encoding\\' in headers and \\\\\\n                headers[\\'Transfer-Encoding\\'] == \\'chunked\\' and \\\\\\n                self.method != \\'PUT\\':\\n            self.headers = headers.copy()\\n            del self.headers[\\'Transfer-Encoding\\']\\n        else:\\n            self.headers = headers\\n        self.body = body\\n\\n    def __str__(self):\\n        return ((\\'method:(%s) protocol:(%s) host(%s) port(%s) path(%s) \\'\\n                 \\'params(%s) headers(%s) body(%s)\\') % (self.method,\\n                 self.protocol, self.host, self.port, self.path, self.params,\\n                 self.headers, self.body))\\n\\n    def authorize(self, connection, **kwargs):\\n        if not getattr(self, \\'_headers_quoted\\', False):\\n            for key in self.headers:\\n                val = self.headers[key]\\n                if isinstance(val, six.text_type):\\n                    safe = \\'!\"#$%&\\\\\\'()*+,/:;<=>?@[\\\\\\\\]^`{|}~ \\'\\n                    self.headers[key] = quote(val.encode(\\'utf-8\\'), safe)\\n            setattr(self, \\'_headers_quoted\\', True)\\n\\n        self.headers[\\'User-Agent\\'] = UserAgent\\n\\n        connection._auth_handler.add_auth(self, **kwargs)\\n\\n        # I\\'m not sure if this is still needed, now that add_auth is\\n        # setting the content-length for POST requests.\\n        if \\'Content-Length\\' not in self.headers:\\n            if \\'Transfer-Encoding\\' not in self.headers or \\\\\\n                    self.headers[\\'Transfer-Encoding\\'] != \\'chunked\\':\\n                self.headers[\\'Content-Length\\'] = str(len(self.body))\\n\\n\\nclass HTTPResponse(http_client.HTTPResponse):\\n\\n    def __init__(self, *args, **kwargs):\\n        http_client.HTTPResponse.__init__(self, *args, **kwargs)\\n        self._cached_response = \\'\\'\\n\\n    def read(self, amt=None):\\n        \"\"\"Read the response.\\n\\n        This method does not have the same behavior as\\n        http_client.HTTPResponse.read.  Instead, if this method is called with\\n        no ``amt`` arg, then the response body will be cached.  Subsequent\\n        calls to ``read()`` with no args **will return the cached response**.\\n\\n        \"\"\"\\n        if amt is None:\\n            # The reason for doing this is that many places in boto call\\n            # response.read() and except to get the response body that they\\n            # can then process.  To make sure this always works as they expect\\n            # we\\'re caching the response so that multiple calls to read()\\n            # will return the full body.  Note that this behavior only\\n            # happens if the amt arg is not specified.\\n            if not self._cached_response:\\n                self._cached_response = http_client.HTTPResponse.read(self)\\n            return self._cached_response\\n        else:\\n            return http_client.HTTPResponse.read(self, amt)\\n\\n\\nclass AWSAuthConnection(object):\\n    def __init__(self, host, aws_access_key_id=None,\\n                 aws_secret_access_key=None,\\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\\n                 proxy_user=None, proxy_pass=None, debug=0,\\n                 https_connection_factory=None, path=\\'/\\',\\n                 provider=\\'aws\\', security_token=None,\\n                 suppress_consec_slashes=True,\\n                 validate_certs=True, profile_name=None):\\n        \"\"\"\\n        :type host: str\\n        :param host: The host to make the connection to\\n\\n        :keyword str aws_access_key_id: Your AWS Access Key ID (provided by\\n            Amazon). If none is specified, the value in your\\n            ``AWS_ACCESS_KEY_ID`` environmental variable is used.\\n        :keyword str aws_secret_access_key: Your AWS Secret Access Key\\n            (provided by Amazon). If none is specified, the value in your\\n            ``AWS_SECRET_ACCESS_KEY`` environmental variable is used.\\n        :keyword str security_token: The security token associated with\\n            temporary credentials issued by STS.  Optional unless using\\n            temporary credentials.  If none is specified, the environment\\n            variable ``AWS_SECURITY_TOKEN`` is used if defined.\\n\\n        :type is_secure: boolean\\n        :param is_secure: Whether the connection is over SSL\\n\\n        :type https_connection_factory: list or tuple\\n        :param https_connection_factory: A pair of an HTTP connection\\n            factory and the exceptions to catch.  The factory should have\\n            a similar interface to L{http_client.HTTPSConnection}.\\n\\n        :param str proxy: Address/hostname for a proxy server\\n\\n        :type proxy_port: int\\n        :param proxy_port: The port to use when connecting over a proxy\\n\\n        :type proxy_user: str\\n        :param proxy_user: The username to connect with on the proxy\\n\\n        :type proxy_pass: str\\n        :param proxy_pass: The password to use when connection over a proxy.\\n\\n        :type port: int\\n        :param port: The port to use to connect\\n\\n        :type suppress_consec_slashes: bool\\n        :param suppress_consec_slashes: If provided, controls whether\\n            consecutive slashes will be suppressed in key paths.\\n\\n        :type validate_certs: bool\\n        :param validate_certs: Controls whether SSL certificates\\n            will be validated or not.  Defaults to True.\\n\\n        :type profile_name: str\\n        :param profile_name: Override usual Credentials section in config\\n            file to use a named set of keys instead.\\n        \"\"\"\\n        self.suppress_consec_slashes = suppress_consec_slashes\\n        self.num_retries = 6\\n        # Override passed-in is_secure setting if value was defined in config.\\n        if config.has_option(\\'Boto\\', \\'is_secure\\'):\\n            is_secure = config.getboolean(\\'Boto\\', \\'is_secure\\')\\n        self.is_secure = is_secure\\n        # Whether or not to validate server certificates.\\n        # The default is now to validate certificates.  This can be\\n        # overridden in the boto config file are by passing an\\n        # explicit validate_certs parameter to the class constructor.\\n        self.https_validate_certificates = config.getbool(\\n            \\'Boto\\', \\'https_validate_certificates\\',\\n            validate_certs)\\n        if self.https_validate_certificates and not HAVE_HTTPS_CONNECTION:\\n            raise BotoClientError(\\n                \"SSL server certificate validation is enabled in boto \"\\n                \"configuration, but Python dependencies required to \"\\n                \"support this feature are not available. Certificate \"\\n                \"validation is only supported when running under Python \"\\n                \"2.6 or later.\")\\n        certs_file = config.get_value(\\n            \\'Boto\\', \\'ca_certificates_file\\', DEFAULT_CA_CERTS_FILE)\\n        if certs_file == \\'system\\':\\n            certs_file = None\\n        self.ca_certificates_file = certs_file\\n        if port:\\n            self.port = port\\n        else:\\n            self.port = PORTS_BY_SECURITY[is_secure]\\n\\n        self.handle_proxy(proxy, proxy_port, proxy_user, proxy_pass)\\n        # define exceptions from http_client that we want to catch and retry\\n        self.http_exceptions = (http_client.HTTPException, socket.error,\\n                                socket.gaierror, http_client.BadStatusLine)\\n        # define subclasses of the above that are not retryable.\\n        self.http_unretryable_exceptions = []\\n        if HAVE_HTTPS_CONNECTION:\\n            self.http_unretryable_exceptions.append(\\n                https_connection.InvalidCertificateException)\\n\\n        # define values in socket exceptions we don\\'t want to catch\\n        self.socket_exception_values = (errno.EINTR,)\\n        if https_connection_factory is not None:\\n            self.https_connection_factory = https_connection_factory[0]\\n            self.http_exceptions += https_connection_factory[1]\\n        else:\\n            self.https_connection_factory = None\\n        if (is_secure):\\n            self.protocol = \\'https\\'\\n        else:\\n            self.protocol = \\'http\\'\\n        self.host = host\\n        self.path = path\\n        # if the value passed in for debug\\n        if not isinstance(debug, six.integer_types):\\n            debug = 0\\n        self.debug = config.getint(\\'Boto\\', \\'debug\\', debug)\\n        self.host_header = None\\n\\n        # Timeout used to tell http_client how long to wait for socket timeouts.\\n        # Default is to leave timeout unchanged, which will in turn result in\\n        # the socket\\'s default global timeout being used. To specify a\\n        # timeout, set http_socket_timeout in Boto config. Regardless,\\n        # timeouts will only be applied if Python is 2.6 or greater.\\n        self.http_connection_kwargs = {}\\n        if (sys.version_info[0], sys.version_info[1]) >= (2, 6):\\n            # If timeout isn\\'t defined in boto config file, use 70 second\\n            # default as recommended by\\n            # http://docs.aws.amazon.com/amazonswf/latest/apireference/API_PollForActivityTask.html\\n            self.http_connection_kwargs[\\'timeout\\'] = config.getint(\\n                \\'Boto\\', \\'http_socket_timeout\\', 70)\\n\\n        if isinstance(provider, Provider):\\n            # Allow overriding Provider\\n            self.provider = provider\\n        else:\\n            self._provider_type = provider\\n            self.provider = Provider(self._provider_type,\\n                                     aws_access_key_id,\\n                                     aws_secret_access_key,\\n                                     security_token,\\n                                     profile_name)\\n\\n        # Allow config file to override default host, port, and host header.\\n        if self.provider.host:\\n            self.host = self.provider.host\\n        if self.provider.port:\\n            self.port = self.provider.port\\n        if self.provider.host_header:\\n            self.host_header = self.provider.host_header\\n\\n        self._pool = ConnectionPool()\\n        self._connection = (self.host, self.port, self.is_secure)\\n        self._last_rs = None\\n        self._auth_handler = auth.get_auth_handler(\\n            host, config, self.provider, self._required_auth_capability())\\n        if getattr(self, \\'AuthServiceName\\', None) is not None:\\n            self.auth_service_name = self.AuthServiceName\\n        self.request_hook = None\\n\\n    def __repr__(self):\\n        return \\'%s:%s\\' % (self.__class__.__name__, self.host)\\n\\n    def _required_auth_capability(self):\\n        return []\\n\\n    def _get_auth_service_name(self):\\n        return getattr(self._auth_handler, \\'service_name\\')\\n\\n    # For Sigv4, the auth_service_name/auth_region_name properties allow\\n    # the service_name/region_name to be explicitly set instead of being\\n    # derived from the endpoint url.\\n    def _set_auth_service_name(self, value):\\n        self._auth_handler.service_name = value\\n    auth_service_name = property(_get_auth_service_name, _set_auth_service_name)\\n\\n    def _get_auth_region_name(self):\\n        return getattr(self._auth_handler, \\'region_name\\')\\n\\n    def _set_auth_region_name(self, value):\\n        self._auth_handler.region_name = value\\n    auth_region_name = property(_get_auth_region_name, _set_auth_region_name)\\n\\n    def connection(self):\\n        return self.get_http_connection(*self._connection)\\n    connection = property(connection)\\n\\n    def aws_access_key_id(self):\\n        return self.provider.access_key\\n    aws_access_key_id = property(aws_access_key_id)\\n    gs_access_key_id = aws_access_key_id\\n    access_key = aws_access_key_id\\n\\n    def aws_secret_access_key(self):\\n        return self.provider.secret_key\\n    aws_secret_access_key = property(aws_secret_access_key)\\n    gs_secret_access_key = aws_secret_access_key\\n    secret_key = aws_secret_access_key\\n\\n    def profile_name(self):\\n        return self.provider.profile_name\\n    profile_name = property(profile_name)\\n\\n    def get_path(self, path=\\'/\\'):\\n        # The default behavior is to suppress consecutive slashes for reasons\\n        # discussed at\\n        # https://groups.google.com/forum/#!topic/boto-dev/-ft0XPUy0y8\\n        # You can override that behavior with the suppress_consec_slashes param.\\n        if not self.suppress_consec_slashes:\\n            return self.path + re.sub(\\'^(/*)/\\', \"\\\\\\\\1\", path)\\n        pos = path.find(\\'?\\')\\n        if pos >= 0:\\n            params = path[pos:]\\n            path = path[:pos]\\n        else:\\n            params = None\\n        if path[-1] == \\'/\\':\\n            need_trailing = True\\n        else:\\n            need_trailing = False\\n        path_elements = self.path.split(\\'/\\')\\n        path_elements.extend(path.split(\\'/\\'))\\n        path_elements = [p for p in path_elements if p]\\n        path = \\'/\\' + \\'/\\'.join(path_elements)\\n        if path[-1] != \\'/\\' and need_trailing:\\n            path += \\'/\\'\\n        if params:\\n            path = path + params\\n        return path\\n\\n    def server_name(self, port=None):\\n        if not port:\\n            port = self.port\\n        if port == 80:\\n            signature_host = self.host\\n        else:\\n            # This unfortunate little hack can be attributed to\\n            # a difference in the 2.6 version of http_client.  In old\\n            # versions, it would append \":443\" to the hostname sent\\n            # in the Host header and so we needed to make sure we\\n            # did the same when calculating the V2 signature.  In 2.6\\n            # (and higher!)\\n            # it no longer does that.  Hence, this kludge.\\n            if ((ON_APP_ENGINE and sys.version[:3] == \\'2.5\\') or\\n                    sys.version[:3] in (\\'2.6\\', \\'2.7\\')) and port == 443:\\n                signature_host = self.host\\n            else:\\n                signature_host = \\'%s:%d\\' % (self.host, port)\\n        return signature_host\\n\\n    def handle_proxy(self, proxy, proxy_port, proxy_user, proxy_pass):\\n        self.proxy = proxy\\n        self.proxy_port = proxy_port\\n        self.proxy_user = proxy_user\\n        self.proxy_pass = proxy_pass\\n        if \\'http_proxy\\' in os.environ and not self.proxy:\\n            pattern = re.compile(\\n                \\'(?:http://)?\\'\\n                \\'(?:(?P<user>[\\\\w\\\\-\\\\.]+):(?P<pass>.*)@)?\\'\\n                \\'(?P<host>[\\\\w\\\\-\\\\.]+)\\'\\n                \\'(?::(?P<port>\\\\d+))?\\'\\n            )\\n            match = pattern.match(os.environ[\\'http_proxy\\'])\\n            if match:\\n                self.proxy = match.group(\\'host\\')\\n                self.proxy_port = match.group(\\'port\\')\\n                self.proxy_user = match.group(\\'user\\')\\n                self.proxy_pass = match.group(\\'pass\\')\\n        else:\\n            if not self.proxy:\\n                self.proxy = config.get_value(\\'Boto\\', \\'proxy\\', None)\\n            if not self.proxy_port:\\n                self.proxy_port = config.get_value(\\'Boto\\', \\'proxy_port\\', None)\\n            if not self.proxy_user:\\n                self.proxy_user = config.get_value(\\'Boto\\', \\'proxy_user\\', None)\\n            if not self.proxy_pass:\\n                self.proxy_pass = config.get_value(\\'Boto\\', \\'proxy_pass\\', None)\\n\\n        if not self.proxy_port and self.proxy:\\n            print(\"http_proxy environment variable does not specify \"\\n                  \"a port, using default\")\\n            self.proxy_port = self.port\\n\\n        self.no_proxy = os.environ.get(\\'no_proxy\\', \\'\\') or os.environ.get(\\'NO_PROXY\\', \\'\\')\\n        self.use_proxy = (self.proxy is not None)\\n\\n    def get_http_connection(self, host, port, is_secure):\\n        conn = self._pool.get_http_connection(host, port, is_secure)\\n        if conn is not None:\\n            return conn\\n        else:\\n            return self.new_http_connection(host, port, is_secure)\\n\\n    def skip_proxy(self, host):\\n        if not self.no_proxy:\\n            return False\\n\\n        if self.no_proxy == \"*\":\\n            return True\\n\\n        hostonly = host\\n        hostonly = host.split(\\':\\')[0]\\n\\n        for name in self.no_proxy.split(\\',\\'):\\n            if name and (hostonly.endswith(name) or host.endswith(name)):\\n                return True\\n\\n        return False\\n\\n    def new_http_connection(self, host, port, is_secure):\\n        if host is None:\\n            host = self.server_name()\\n\\n        # Make sure the host is really just the host, not including\\n        # the port number\\n        host = boto.utils.parse_host(host)\\n\\n        http_connection_kwargs = self.http_connection_kwargs.copy()\\n\\n        # Connection factories below expect a port keyword argument\\n        http_connection_kwargs[\\'port\\'] = port\\n\\n        # Override host with proxy settings if needed\\n        if self.use_proxy and not is_secure and \\\\\\n                not self.skip_proxy(host):\\n            host = self.proxy\\n            http_connection_kwargs[\\'port\\'] = int(self.proxy_port)\\n\\n        if is_secure:\\n            boto.log.debug(\\n                \\'establishing HTTPS connection: host=%s, kwargs=%s\\',\\n                host, http_connection_kwargs)\\n            if self.use_proxy and not self.skip_proxy(host):\\n                connection = self.proxy_ssl(host, is_secure and 443 or 80)\\n            elif self.https_connection_factory:\\n                connection = self.https_connection_factory(host)\\n            elif self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\\n                connection = https_connection.CertValidatingHTTPSConnection(\\n                    host, ca_certs=self.ca_certificates_file,\\n                    **http_connection_kwargs)\\n            else:\\n                connection = http_client.HTTPSConnection(\\n                    host, **http_connection_kwargs)\\n        else:\\n            boto.log.debug(\\'establishing HTTP connection: kwargs=%s\\' %\\n                           http_connection_kwargs)\\n            if self.https_connection_factory:\\n                # even though the factory says https, this is too handy\\n                # to not be able to allow overriding for http also.\\n                connection = self.https_connection_factory(\\n                    host, **http_connection_kwargs)\\n            else:\\n                connection = http_client.HTTPConnection(\\n                    host, **http_connection_kwargs)\\n        if self.debug > 1:\\n            connection.set_debuglevel(self.debug)\\n        # self.connection must be maintained for backwards-compatibility\\n        # however, it must be dynamically pulled from the connection pool\\n        # set a private variable which will enable that\\n        if host.split(\\':\\')[0] == self.host and is_secure == self.is_secure:\\n            self._connection = (host, port, is_secure)\\n        # Set the response class of the http connection to use our custom\\n        # class.\\n        connection.response_class = HTTPResponse\\n        return connection\\n\\n    def put_http_connection(self, host, port, is_secure, connection):\\n        self._pool.put_http_connection(host, port, is_secure, connection)\\n\\n    def proxy_ssl(self, host=None, port=None):\\n        if host and port:\\n            host = \\'%s:%d\\' % (host, port)\\n        else:\\n            host = \\'%s:%d\\' % (self.host, self.port)\\n        # Seems properly to use timeout for connect too\\n        timeout = self.http_connection_kwargs.get(\"timeout\")\\n        if timeout is not None:\\n            sock = socket.create_connection((self.proxy,\\n                                             int(self.proxy_port)), timeout)\\n        else:\\n            sock = socket.create_connection((self.proxy, int(self.proxy_port)))\\n        boto.log.debug(\"Proxy connection: CONNECT %s HTTP/1.0\\\\r\\\\n\", host)\\n        sock.sendall(\"CONNECT %s HTTP/1.0\\\\r\\\\n\" % host)\\n        sock.sendall(\"User-Agent: %s\\\\r\\\\n\" % UserAgent)\\n        if self.proxy_user and self.proxy_pass:\\n            for k, v in self.get_proxy_auth_header().items():\\n                sock.sendall(\"%s: %s\\\\r\\\\n\" % (k, v))\\n            # See discussion about this config option at\\n            # https://groups.google.com/forum/?fromgroups#!topic/boto-dev/teenFvOq2Cc\\n            if config.getbool(\\'Boto\\', \\'send_crlf_after_proxy_auth_headers\\', False):\\n                sock.sendall(\"\\\\r\\\\n\")\\n        else:\\n            sock.sendall(\"\\\\r\\\\n\")\\n        resp = http_client.HTTPResponse(sock, strict=True, debuglevel=self.debug)\\n        resp.begin()\\n\\n        if resp.status != 200:\\n            # Fake a socket error, use a code that make it obvious it hasn\\'t\\n            # been generated by the socket library\\n            raise socket.error(-71,\\n                               \"Error talking to HTTP proxy %s:%s: %s (%s)\" %\\n                               (self.proxy, self.proxy_port,\\n                                resp.status, resp.reason))\\n\\n        # We can safely close the response, it duped the original socket\\n        resp.close()\\n\\n        h = http_client.HTTPConnection(host)\\n\\n        if self.https_validate_certificates and HAVE_HTTPS_CONNECTION:\\n            msg = \"wrapping ssl socket for proxied connection; \"\\n            if self.ca_certificates_file:\\n                msg += \"CA certificate file=%s\" % self.ca_certificates_file\\n            else:\\n                msg += \"using system provided SSL certs\"\\n            boto.log.debug(msg)\\n            key_file = self.http_connection_kwargs.get(\\'key_file\\', None)\\n            cert_file = self.http_connection_kwargs.get(\\'cert_file\\', None)\\n            sslSock = ssl.wrap_socket(sock, keyfile=key_file,\\n                                      certfile=cert_file,\\n                                      cert_reqs=ssl.CERT_REQUIRED,\\n                                      ca_certs=self.ca_certificates_file)\\n            cert = sslSock.getpeercert()\\n            hostname = self.host.split(\\':\\', 0)[0]\\n            if not https_connection.ValidateCertificateHostname(cert, hostname):\\n                raise https_connection.InvalidCertificateException(\\n                    hostname, cert, \\'hostname mismatch\\')\\n        else:\\n            # Fallback for old Python without ssl.wrap_socket\\n            if hasattr(http_client, \\'ssl\\'):\\n                sslSock = http_client.ssl.SSLSocket(sock)\\n            else:\\n                sslSock = socket.ssl(sock, None, None)\\n                sslSock = http_client.FakeSocket(sock, sslSock)\\n\\n        # This is a bit unclean\\n        h.sock = sslSock\\n        return h\\n\\n    def prefix_proxy_to_path(self, path, host=None):\\n        path = self.protocol + \\'://\\' + (host or self.server_name()) + path\\n        return path\\n\\n    def get_proxy_auth_header(self):\\n        auth = encodebytes(self.proxy_user + \\':\\' + self.proxy_pass)\\n        return {\\'Proxy-Authorization\\': \\'Basic %s\\' % auth}\\n\\n    # For passing proxy information to other connection libraries, e.g. cloudsearch2\\n    def get_proxy_url_with_auth(self):\\n        if not self.use_proxy:\\n            return None\\n\\n        if self.proxy_user or self.proxy_pass:\\n            if self.proxy_pass:\\n                login_info = \\'%s:%s@\\' % (self.proxy_user, self.proxy_pass)\\n            else:\\n                login_info = \\'%s@\\' % self.proxy_user\\n        else:\\n            login_info = \\'\\'\\n\\n        return \\'http://%s%s:%s\\' % (login_info, self.proxy, str(self.proxy_port or self.port))\\n\\n    def set_host_header(self, request):\\n        try:\\n            request.headers[\\'Host\\'] = \\\\\\n                self._auth_handler.host_header(self.host, request)\\n        except AttributeError:\\n            request.headers[\\'Host\\'] = self.host.split(\\':\\', 1)[0]\\n\\n    def set_request_hook(self, hook):\\n        self.request_hook = hook\\n\\n    def _mexe(self, request, sender=None, override_num_retries=None,\\n              retry_handler=None):\\n        \"\"\"\\n        mexe - Multi-execute inside a loop, retrying multiple times to handle\\n               transient Internet errors by simply trying again.\\n               Also handles redirects.\\n\\n        This code was inspired by the S3Utils classes posted to the boto-users\\n        Google group by Larry Bates.  Thanks!\\n\\n        \"\"\"\\n        boto.log.debug(\\'Method: %s\\' % request.method)\\n        boto.log.debug(\\'Path: %s\\' % request.path)\\n        boto.log.debug(\\'Data: %s\\' % request.body)\\n        boto.log.debug(\\'Headers: %s\\' % request.headers)\\n        boto.log.debug(\\'Host: %s\\' % request.host)\\n        boto.log.debug(\\'Port: %s\\' % request.port)\\n        boto.log.debug(\\'Params: %s\\' % request.params)\\n        response = None\\n        body = None\\n        ex = None\\n        if override_num_retries is None:\\n            num_retries = config.getint(\\'Boto\\', \\'num_retries\\', self.num_retries)\\n        else:\\n            num_retries = override_num_retries\\n        i = 0\\n        connection = self.get_http_connection(request.host, request.port,\\n                                              self.is_secure)\\n\\n        # Convert body to bytes if needed\\n        if not isinstance(request.body, bytes) and hasattr(request.body,\\n                                                           \\'encode\\'):\\n            request.body = request.body.encode(\\'utf-8\\')\\n\\n        while i <= num_retries:\\n            # Use binary exponential backoff to desynchronize client requests.\\n            next_sleep = min(random.random() * (2 ** i),\\n                             boto.config.get(\\'Boto\\', \\'max_retry_delay\\', 60))\\n            try:\\n                # we now re-sign each request before it is retried\\n                boto.log.debug(\\'Token: %s\\' % self.provider.security_token)\\n                request.authorize(connection=self)\\n                # Only force header for non-s3 connections, because s3 uses\\n                # an older signing method + bucket resource URLs that include\\n                # the port info. All others should be now be up to date and\\n                # not include the port.\\n                if \\'s3\\' not in self._required_auth_capability():\\n                    if not getattr(self, \\'anon\\', False):\\n                        if not request.headers.get(\\'Host\\'):\\n                            self.set_host_header(request)\\n                boto.log.debug(\\'Final headers: %s\\' % request.headers)\\n                request.start_time = datetime.now()\\n                if callable(sender):\\n                    response = sender(connection, request.method, request.path,\\n                                      request.body, request.headers)\\n                else:\\n                    connection.request(request.method, request.path,\\n                                       request.body, request.headers)\\n                    response = connection.getresponse()\\n                boto.log.debug(\\'Response headers: %s\\' % response.getheaders())\\n                location = response.getheader(\\'location\\')\\n                # -- gross hack --\\n                # http_client gets confused with chunked responses to HEAD requests\\n                # so I have to fake it out\\n                if request.method == \\'HEAD\\' and getattr(response,\\n                                                        \\'chunked\\', False):\\n                    response.chunked = 0\\n                if callable(retry_handler):\\n                    status = retry_handler(response, i, next_sleep)\\n                    if status:\\n                        msg, i, next_sleep = status\\n                        if msg:\\n                            boto.log.debug(msg)\\n                        time.sleep(next_sleep)\\n                        continue\\n                if response.status in [500, 502, 503, 504]:\\n                    msg = \\'Received %d response.  \\' % response.status\\n                    msg += \\'Retrying in %3.1f seconds\\' % next_sleep\\n                    boto.log.debug(msg)\\n                    body = response.read()\\n                    if isinstance(body, bytes):\\n                        body = body.decode(\\'utf-8\\')\\n                elif response.status < 300 or response.status >= 400 or \\\\\\n                        not location:\\n                    # don\\'t return connection to the pool if response contains\\n                    # Connection:close header, because the connection has been\\n                    # closed and default reconnect behavior may do something\\n                    # different than new_http_connection. Also, it\\'s probably\\n                    # less efficient to try to reuse a closed connection.\\n                    conn_header_value = response.getheader(\\'connection\\')\\n                    if conn_header_value == \\'close\\':\\n                        connection.close()\\n                    else:\\n                        self.put_http_connection(request.host, request.port,\\n                                                 self.is_secure, connection)\\n                    if self.request_hook is not None:\\n                        self.request_hook.handle_request_data(request, response)\\n                    return response\\n                else:\\n                    scheme, request.host, request.path, \\\\\\n                        params, query, fragment = urlparse(location)\\n                    if query:\\n                        request.path += \\'?\\' + query\\n                    # urlparse can return both host and port in netloc, so if\\n                    # that\\'s the case we need to split them up properly\\n                    if \\':\\' in request.host:\\n                        request.host, request.port = request.host.split(\\':\\', 1)\\n                    msg = \\'Redirecting: %s\\' % scheme + \\'://\\'\\n                    msg += request.host + request.path\\n                    boto.log.debug(msg)\\n                    connection = self.get_http_connection(request.host,\\n                                                          request.port,\\n                                                          scheme == \\'https\\')\\n                    response = None\\n                    continue\\n            except PleaseRetryException as e:\\n                boto.log.debug(\\'encountered a retry exception: %s\\' % e)\\n                connection = self.new_http_connection(request.host, request.port,\\n                                                      self.is_secure)\\n                response = e.response\\n                ex = e\\n            except self.http_exceptions as e:\\n                for unretryable in self.http_unretryable_exceptions:\\n                    if isinstance(e, unretryable):\\n                        boto.log.debug(\\n                            \\'encountered unretryable %s exception, re-raising\\' %\\n                            e.__class__.__name__)\\n                        raise\\n                boto.log.debug(\\'encountered %s exception, reconnecting\\' %\\n                               e.__class__.__name__)\\n                connection = self.new_http_connection(request.host, request.port,\\n                                                      self.is_secure)\\n                ex = e\\n            time.sleep(next_sleep)\\n            i += 1\\n        # If we made it here, it\\'s because we have exhausted our retries\\n        # and stil haven\\'t succeeded.  So, if we have a response object,\\n        # use it to raise an exception.\\n        # Otherwise, raise the exception that must have already happened.\\n        if self.request_hook is not None:\\n            self.request_hook.handle_request_data(request, response, error=True)\\n        if response:\\n            raise BotoServerError(response.status, response.reason, body)\\n        elif ex:\\n            raise ex\\n        else:\\n            msg = \\'Please report this exception as a Boto Issue!\\'\\n            raise BotoClientError(msg)\\n\\n    def build_base_http_request(self, method, path, auth_path,\\n                                params=None, headers=None, data=\\'\\', host=None):\\n        path = self.get_path(path)\\n        if auth_path is not None:\\n            auth_path = self.get_path(auth_path)\\n        if params is None:\\n            params = {}\\n        else:\\n            params = params.copy()\\n        if headers is None:\\n            headers = {}\\n        else:\\n            headers = headers.copy()\\n        if self.host_header and not boto.utils.find_matching_headers(\\'host\\', headers):\\n            headers[\\'host\\'] = self.host_header\\n        host = host or self.host\\n        if self.use_proxy and not self.skip_proxy(host):\\n            if not auth_path:\\n                auth_path = path\\n            path = self.prefix_proxy_to_path(path, host)\\n            if self.proxy_user and self.proxy_pass and not self.is_secure:\\n                # If is_secure, we don\\'t have to set the proxy authentication\\n                # header here, we did that in the CONNECT to the proxy.\\n                headers.update(self.get_proxy_auth_header())\\n        return HTTPRequest(method, self.protocol, host, self.port,\\n                           path, auth_path, params, headers, data)\\n\\n    def make_request(self, method, path, headers=None, data=\\'\\', host=None,\\n                     auth_path=None, sender=None, override_num_retries=None,\\n                     params=None, retry_handler=None):\\n        \"\"\"Makes a request to the server, with stock multiple-retry logic.\"\"\"\\n        if params is None:\\n            params = {}\\n        http_request = self.build_base_http_request(method, path, auth_path,\\n                                                    params, headers, data, host)\\n        return self._mexe(http_request, sender, override_num_retries,\\n                          retry_handler=retry_handler)\\n\\n    def close(self):\\n        \"\"\"(Optional) Close any open HTTP connections.  This is non-destructive,\\n        and making a new request will open a connection again.\"\"\"\\n\\n        boto.log.debug(\\'closing all HTTP connections\\')\\n        self._connection = None  # compat field\\n\\n\\nclass AWSQueryConnection(AWSAuthConnection):\\n\\n    APIVersion = \\'\\'\\n    ResponseError = BotoServerError\\n\\n    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None,\\n                 is_secure=True, port=None, proxy=None, proxy_port=None,\\n                 proxy_user=None, proxy_pass=None, host=None, debug=0,\\n                 https_connection_factory=None, path=\\'/\\', security_token=None,\\n                 validate_certs=True, profile_name=None, provider=\\'aws\\'):\\n        super(AWSQueryConnection, self).__init__(\\n            host, aws_access_key_id,\\n            aws_secret_access_key,\\n            is_secure, port, proxy,\\n            proxy_port, proxy_user, proxy_pass,\\n            debug, https_connection_factory, path,\\n            security_token=security_token,\\n            validate_certs=validate_certs,\\n            profile_name=profile_name,\\n            provider=provider)\\n\\n    def _required_auth_capability(self):\\n        return []\\n\\n    def get_utf8_value(self, value):\\n        return boto.utils.get_utf8_value(value)\\n\\n    def make_request(self, action, params=None, path=\\'/\\', verb=\\'GET\\'):\\n        http_request = self.build_base_http_request(verb, path, None,\\n                                                    params, {}, \\'\\',\\n                                                    self.host)\\n        if action:\\n            http_request.params[\\'Action\\'] = action\\n        if self.APIVersion:\\n            http_request.params[\\'Version\\'] = self.APIVersion\\n        return self._mexe(http_request)\\n\\n    def build_list_params(self, params, items, label):\\n        if isinstance(items, six.string_types):\\n            items = [items]\\n        for i in range(1, len(items) + 1):\\n            params[\\'%s.%d\\' % (label, i)] = items[i - 1]\\n\\n    def build_complex_list_params(self, params, items, label, names):\\n        \"\"\"Serialize a list of structures.\\n\\n        For example::\\n\\n            items = [(\\'foo\\', \\'bar\\', \\'baz\\'), (\\'foo2\\', \\'bar2\\', \\'baz2\\')]\\n            label = \\'ParamName.member\\'\\n            names = (\\'One\\', \\'Two\\', \\'Three\\')\\n            self.build_complex_list_params(params, items, label, names)\\n\\n        would result in the params dict being updated with these params::\\n\\n            ParamName.member.1.One = foo\\n            ParamName.member.1.Two = bar\\n            ParamName.member.1.Three = baz\\n\\n            ParamName.member.2.One = foo2\\n            ParamName.member.2.Two = bar2\\n            ParamName.member.2.Three = baz2\\n\\n        :type params: dict\\n        :param params: The params dict.  The complex list params\\n            will be added to this dict.\\n\\n        :type items: list of tuples\\n        :param items: The list to serialize.\\n\\n        :type label: string\\n        :param label: The prefix to apply to the parameter.\\n\\n        :type names: tuple of strings\\n        :param names: The names associated with each tuple element.\\n\\n        \"\"\"\\n        for i, item in enumerate(items, 1):\\n            current_prefix = \\'%s.%s\\' % (label, i)\\n            for key, value in zip(names, item):\\n                full_key = \\'%s.%s\\' % (current_prefix, key)\\n                params[full_key] = value\\n\\n    # generics\\n\\n    def get_list(self, action, params, markers, path=\\'/\\',\\n                 parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            rs = ResultSet(markers)\\n            h = boto.handler.XmlHandler(rs, parent)\\n            if isinstance(body, six.text_type):\\n                body = body.encode(\\'utf-8\\')\\n            xml.sax.parseString(body, h)\\n            return rs\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n\\n    def get_object(self, action, params, cls, path=\\'/\\',\\n                   parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            obj = cls(parent)\\n            h = boto.handler.XmlHandler(obj, parent)\\n            if isinstance(body, six.text_type):\\n                body = body.encode(\\'utf-8\\')\\n            xml.sax.parseString(body, h)\\n            return obj\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n\\n    def get_status(self, action, params, path=\\'/\\', parent=None, verb=\\'GET\\'):\\n        if not parent:\\n            parent = self\\n        response = self.make_request(action, params, path, verb)\\n        body = response.read()\\n        boto.log.debug(body)\\n        if not body:\\n            boto.log.error(\\'Null body %s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n        elif response.status == 200:\\n            rs = ResultSet()\\n            h = boto.handler.XmlHandler(rs, parent)\\n            xml.sax.parseString(body, h)\\n            return rs.status\\n        else:\\n            boto.log.error(\\'%s %s\\' % (response.status, response.reason))\\n            boto.log.error(\\'%s\\' % body)\\n            raise self.ResponseError(response.status, response.reason, body)\\n'}, {'boto.auth.S3HmacAuthV4Handler.presign': '# Copyright 2010 Google Inc.\\n# Copyright (c) 2011 Mitch Garnaat http://garnaat.org/\\n# Copyright (c) 2011, Eucalyptus Systems, Inc.\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a\\n# copy of this software and associated documentation files (the\\n# \"Software\"), to deal in the Software without restriction, including\\n# without limitation the rights to use, copy, modify, merge, publish, dis-\\n# tribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the fol-\\n# lowing conditions:\\n#\\n# The above copyright notice and this permission notice shall be included\\n# in all copies or substantial portions of the Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\\n# IN THE SOFTWARE.\\n\\n\\n\"\"\"\\nHandles authentication required to AWS and GS\\n\"\"\"\\n\\nimport base64\\nimport boto\\nimport boto.auth_handler\\nimport boto.exception\\nimport boto.plugin\\nimport boto.utils\\nimport copy\\nimport datetime\\nfrom email.utils import formatdate\\nimport hmac\\nimport os\\nimport posixpath\\n\\nfrom boto.compat import urllib, encodebytes, parse_qs_safe, urlparse\\nfrom boto.auth_handler import AuthHandler\\nfrom boto.exception import BotoClientError\\n\\ntry:\\n    from hashlib import sha1 as sha\\n    from hashlib import sha256 as sha256\\nexcept ImportError:\\n    import sha\\n    sha256 = None\\n\\n\\n# Region detection strings to determine if SigV2 should be used\\n# by default\\nS3_AUTH_DETECT = [\\n    \\'-ap-northeast-1\\',\\n    \\'.ap-northeast-1\\',\\n    \\'-ap-southeast-1\\',\\n    \\'.ap-southeast-1\\',\\n    \\'-ap-southeast-2\\',\\n    \\'.ap-southeast-2\\',\\n    \\'-eu-west-1\\',\\n    \\'.eu-west-1\\',\\n    \\'-external-1\\',\\n    \\'.external-1\\',\\n    \\'-sa-east-1\\',\\n    \\'.sa-east-1\\',\\n    \\'-us-east-1\\',\\n    \\'.us-east-1\\',\\n    \\'-us-gov-west-1\\',\\n    \\'.us-gov-west-1\\',\\n    \\'-us-west-1\\',\\n    \\'.us-west-1\\',\\n    \\'-us-west-2\\',\\n    \\'.us-west-2\\'\\n]\\n\\n\\nSIGV4_DETECT = [\\n    \\'.cn-\\',\\n    # In eu-central and ap-northeast-2 we support both host styles for S3\\n    \\'.eu-central\\',\\n    \\'-eu-central\\',\\n    \\'.ap-northeast-2\\',\\n    \\'-ap-northeast-2\\',\\n    \\'.ap-south-1\\',\\n    \\'-ap-south-1\\',\\n    \\'.us-east-2\\',\\n    \\'-us-east-2\\',\\n    \\'-ca-central\\',\\n    \\'.ca-central\\',\\n    \\'.eu-west-2\\',\\n    \\'-eu-west-2\\',\\n]\\n\\n\\nclass HmacKeys(object):\\n    \"\"\"Key based Auth handler helper.\"\"\"\\n\\n    def __init__(self, host, config, provider):\\n        if provider.access_key is None or provider.secret_key is None:\\n            raise boto.auth_handler.NotReadyToAuthenticate()\\n        self.host = host\\n        self.update_provider(provider)\\n\\n    def update_provider(self, provider):\\n        self._provider = provider\\n        self._hmac = hmac.new(self._provider.secret_key.encode(\\'utf-8\\'),\\n                              digestmod=sha)\\n        if sha256:\\n            self._hmac_256 = hmac.new(self._provider.secret_key.encode(\\'utf-8\\'),\\n                                      digestmod=sha256)\\n        else:\\n            self._hmac_256 = None\\n\\n    def algorithm(self):\\n        if self._hmac_256:\\n            return \\'HmacSHA256\\'\\n        else:\\n            return \\'HmacSHA1\\'\\n\\n    def _get_hmac(self):\\n        if self._hmac_256:\\n            digestmod = sha256\\n        else:\\n            digestmod = sha\\n        return hmac.new(self._provider.secret_key.encode(\\'utf-8\\'),\\n                        digestmod=digestmod)\\n\\n    def sign_string(self, string_to_sign):\\n        new_hmac = self._get_hmac()\\n        new_hmac.update(string_to_sign.encode(\\'utf-8\\'))\\n        return encodebytes(new_hmac.digest()).decode(\\'utf-8\\').strip()\\n\\n    def __getstate__(self):\\n        pickled_dict = copy.copy(self.__dict__)\\n        del pickled_dict[\\'_hmac\\']\\n        del pickled_dict[\\'_hmac_256\\']\\n        return pickled_dict\\n\\n    def __setstate__(self, dct):\\n        self.__dict__ = dct\\n        self.update_provider(self._provider)\\n\\n\\nclass AnonAuthHandler(AuthHandler, HmacKeys):\\n    \"\"\"\\n    Implements Anonymous requests.\\n    \"\"\"\\n\\n    capability = [\\'anon\\']\\n\\n    def __init__(self, host, config, provider):\\n        super(AnonAuthHandler, self).__init__(host, config, provider)\\n\\n    def add_auth(self, http_request, **kwargs):\\n        pass\\n\\n\\nclass HmacAuthV1Handler(AuthHandler, HmacKeys):\\n    \"\"\"    Implements the HMAC request signing used by S3 and GS.\"\"\"\\n\\n    capability = [\\'hmac-v1\\', \\'s3\\']\\n\\n    def __init__(self, host, config, provider):\\n        AuthHandler.__init__(self, host, config, provider)\\n        HmacKeys.__init__(self, host, config, provider)\\n        self._hmac_256 = None\\n\\n    def update_provider(self, provider):\\n        super(HmacAuthV1Handler, self).update_provider(provider)\\n        self._hmac_256 = None\\n\\n    def add_auth(self, http_request, **kwargs):\\n        headers = http_request.headers\\n        method = http_request.method\\n        auth_path = http_request.auth_path\\n        if \\'Date\\' not in headers:\\n            headers[\\'Date\\'] = formatdate(usegmt=True)\\n\\n        if self._provider.security_token:\\n            key = self._provider.security_token_header\\n            headers[key] = self._provider.security_token\\n        string_to_sign = boto.utils.canonical_string(method, auth_path,\\n                                                     headers, None,\\n                                                     self._provider)\\n        boto.log.debug(\\'StringToSign:\\\\n%s\\' % string_to_sign)\\n        b64_hmac = self.sign_string(string_to_sign)\\n        auth_hdr = self._provider.auth_header\\n        auth = (\"%s %s:%s\" % (auth_hdr, self._provider.access_key, b64_hmac))\\n        boto.log.debug(\\'Signature:\\\\n%s\\' % auth)\\n        headers[\\'Authorization\\'] = auth\\n\\n\\nclass HmacAuthV2Handler(AuthHandler, HmacKeys):\\n    \"\"\"\\n    Implements the simplified HMAC authorization used by CloudFront.\\n    \"\"\"\\n    capability = [\\'hmac-v2\\', \\'cloudfront\\']\\n\\n    def __init__(self, host, config, provider):\\n        AuthHandler.__init__(self, host, config, provider)\\n        HmacKeys.__init__(self, host, config, provider)\\n        self._hmac_256 = None\\n\\n    def update_provider(self, provider):\\n        super(HmacAuthV2Handler, self).update_provider(provider)\\n        self._hmac_256 = None\\n\\n    def add_auth(self, http_request, **kwargs):\\n        headers = http_request.headers\\n        if \\'Date\\' not in headers:\\n            headers[\\'Date\\'] = formatdate(usegmt=True)\\n        if self._provider.security_token:\\n            key = self._provider.security_token_header\\n            headers[key] = self._provider.security_token\\n\\n        b64_hmac = self.sign_string(headers[\\'Date\\'])\\n        auth_hdr = self._provider.auth_header\\n        headers[\\'Authorization\\'] = (\"%s %s:%s\" %\\n                                    (auth_hdr,\\n                                     self._provider.access_key, b64_hmac))\\n\\n\\nclass HmacAuthV3Handler(AuthHandler, HmacKeys):\\n    \"\"\"Implements the new Version 3 HMAC authorization used by Route53.\"\"\"\\n\\n    capability = [\\'hmac-v3\\', \\'route53\\', \\'ses\\']\\n\\n    def __init__(self, host, config, provider):\\n        AuthHandler.__init__(self, host, config, provider)\\n        HmacKeys.__init__(self, host, config, provider)\\n\\n    def add_auth(self, http_request, **kwargs):\\n        headers = http_request.headers\\n        if \\'Date\\' not in headers:\\n            headers[\\'Date\\'] = formatdate(usegmt=True)\\n\\n        if self._provider.security_token:\\n            key = self._provider.security_token_header\\n            headers[key] = self._provider.security_token\\n\\n        b64_hmac = self.sign_string(headers[\\'Date\\'])\\n        s = \"AWS3-HTTPS AWSAccessKeyId=%s,\" % self._provider.access_key\\n        s += \"Algorithm=%s,Signature=%s\" % (self.algorithm(), b64_hmac)\\n        headers[\\'X-Amzn-Authorization\\'] = s\\n\\n\\nclass HmacAuthV3HTTPHandler(AuthHandler, HmacKeys):\\n    \"\"\"\\n    Implements the new Version 3 HMAC authorization used by DynamoDB.\\n    \"\"\"\\n\\n    capability = [\\'hmac-v3-http\\']\\n\\n    def __init__(self, host, config, provider):\\n        AuthHandler.__init__(self, host, config, provider)\\n        HmacKeys.__init__(self, host, config, provider)\\n\\n    def headers_to_sign(self, http_request):\\n        \"\"\"\\n        Select the headers from the request that need to be included\\n        in the StringToSign.\\n        \"\"\"\\n        headers_to_sign = {\\'Host\\': self.host}\\n        for name, value in http_request.headers.items():\\n            lname = name.lower()\\n            if lname.startswith(\\'x-amz\\'):\\n                headers_to_sign[name] = value\\n        return headers_to_sign\\n\\n    def canonical_headers(self, headers_to_sign):\\n        \"\"\"\\n        Return the headers that need to be included in the StringToSign\\n        in their canonical form by converting all header keys to lower\\n        case, sorting them in alphabetical order and then joining\\n        them into a string, separated by newlines.\\n        \"\"\"\\n        l = sorted([\\'%s:%s\\' % (n.lower().strip(),\\n                    headers_to_sign[n].strip()) for n in headers_to_sign])\\n        return \\'\\\\n\\'.join(l)\\n\\n    def string_to_sign(self, http_request):\\n        \"\"\"\\n        Return the canonical StringToSign as well as a dict\\n        containing the original version of all headers that\\n        were included in the StringToSign.\\n        \"\"\"\\n        headers_to_sign = self.headers_to_sign(http_request)\\n        canonical_headers = self.canonical_headers(headers_to_sign)\\n        string_to_sign = \\'\\\\n\\'.join([http_request.method,\\n                                    http_request.auth_path,\\n                                    \\'\\',\\n                                    canonical_headers,\\n                                    \\'\\',\\n                                    http_request.body])\\n        return string_to_sign, headers_to_sign\\n\\n    def add_auth(self, req, **kwargs):\\n        \"\"\"\\n        Add AWS3 authentication to a request.\\n\\n        :type req: :class`boto.connection.HTTPRequest`\\n        :param req: The HTTPRequest object.\\n        \"\"\"\\n        # This could be a retry.  Make sure the previous\\n        # authorization header is removed first.\\n        if \\'X-Amzn-Authorization\\' in req.headers:\\n            del req.headers[\\'X-Amzn-Authorization\\']\\n        req.headers[\\'X-Amz-Date\\'] = formatdate(usegmt=True)\\n        if self._provider.security_token:\\n            req.headers[\\'X-Amz-Security-Token\\'] = self._provider.security_token\\n        string_to_sign, headers_to_sign = self.string_to_sign(req)\\n        boto.log.debug(\\'StringToSign:\\\\n%s\\' % string_to_sign)\\n        hash_value = sha256(string_to_sign.encode(\\'utf-8\\')).digest()\\n        b64_hmac = self.sign_string(hash_value)\\n        s = \"AWS3 AWSAccessKeyId=%s,\" % self._provider.access_key\\n        s += \"Algorithm=%s,\" % self.algorithm()\\n        s += \"SignedHeaders=%s,\" % \\';\\'.join(headers_to_sign)\\n        s += \"Signature=%s\" % b64_hmac\\n        req.headers[\\'X-Amzn-Authorization\\'] = s\\n\\n\\nclass HmacAuthV4Handler(AuthHandler, HmacKeys):\\n    \"\"\"\\n    Implements the new Version 4 HMAC authorization.\\n    \"\"\"\\n\\n    capability = [\\'hmac-v4\\']\\n\\n    def __init__(self, host, config, provider,\\n                 service_name=None, region_name=None):\\n        AuthHandler.__init__(self, host, config, provider)\\n        HmacKeys.__init__(self, host, config, provider)\\n        # You can set the service_name and region_name to override the\\n        # values which would otherwise come from the endpoint, e.g.\\n        # <service>.<region>.amazonaws.com.\\n        self.service_name = service_name\\n        self.region_name = region_name\\n\\n    def _sign(self, key, msg, hex=False):\\n        if not isinstance(key, bytes):\\n            key = key.encode(\\'utf-8\\')\\n\\n        if hex:\\n            sig = hmac.new(key, msg.encode(\\'utf-8\\'), sha256).hexdigest()\\n        else:\\n            sig = hmac.new(key, msg.encode(\\'utf-8\\'), sha256).digest()\\n        return sig\\n\\n    def headers_to_sign(self, http_request):\\n        \"\"\"\\n        Select the headers from the request that need to be included\\n        in the StringToSign.\\n        \"\"\"\\n        host_header_value = self.host_header(self.host, http_request)\\n        if http_request.headers.get(\\'Host\\'):\\n            host_header_value = http_request.headers[\\'Host\\']\\n        headers_to_sign = {\\'Host\\': host_header_value}\\n        for name, value in http_request.headers.items():\\n            lname = name.lower()\\n            if lname.startswith(\\'x-amz\\'):\\n                if isinstance(value, bytes):\\n                    value = value.decode(\\'utf-8\\')\\n                headers_to_sign[name] = value\\n        return headers_to_sign\\n\\n    def host_header(self, host, http_request):\\n        port = http_request.port\\n        secure = http_request.protocol == \\'https\\'\\n        if ((port == 80 and not secure) or (port == 443 and secure)):\\n            return host\\n        return \\'%s:%s\\' % (host, port)\\n\\n    def query_string(self, http_request):\\n        parameter_names = sorted(http_request.params.keys())\\n        pairs = []\\n        for pname in parameter_names:\\n            pval = boto.utils.get_utf8_value(http_request.params[pname])\\n            pairs.append(urllib.parse.quote(pname, safe=\\'\\') + \\'=\\' +\\n                         urllib.parse.quote(pval, safe=\\'-_~\\'))\\n        return \\'&\\'.join(pairs)\\n\\n    def canonical_query_string(self, http_request):\\n        # POST requests pass parameters in through the\\n        # http_request.body field.\\n        if http_request.method == \\'POST\\':\\n            return \"\"\\n        l = []\\n        for param in sorted(http_request.params):\\n            value = boto.utils.get_utf8_value(http_request.params[param])\\n            l.append(\\'%s=%s\\' % (urllib.parse.quote(param, safe=\\'-_.~\\'),\\n                                urllib.parse.quote(value, safe=\\'-_.~\\')))\\n        return \\'&\\'.join(l)\\n\\n    def canonical_headers(self, headers_to_sign):\\n        \"\"\"\\n        Return the headers that need to be included in the StringToSign\\n        in their canonical form by converting all header keys to lower\\n        case, sorting them in alphabetical order and then joining\\n        them into a string, separated by newlines.\\n        \"\"\"\\n        canonical = []\\n\\n        for header in headers_to_sign:\\n            c_name = header.lower().strip()\\n            raw_value = str(headers_to_sign[header])\\n            if \\'\"\\' in raw_value:\\n                c_value = raw_value.strip()\\n            else:\\n                c_value = \\' \\'.join(raw_value.strip().split())\\n            canonical.append(\\'%s:%s\\' % (c_name, c_value))\\n        return \\'\\\\n\\'.join(sorted(canonical))\\n\\n    def signed_headers(self, headers_to_sign):\\n        l = [\\'%s\\' % n.lower().strip() for n in headers_to_sign]\\n        l = sorted(l)\\n        return \\';\\'.join(l)\\n\\n    def canonical_uri(self, http_request):\\n        path = http_request.auth_path\\n        # Normalize the path\\n        # in windows normpath(\\'/\\') will be \\'\\\\\\\\\\' so we chane it back to \\'/\\'\\n        normalized = posixpath.normpath(path).replace(\\'\\\\\\\\\\', \\'/\\')\\n        # Then urlencode whatever\\'s left.\\n        encoded = urllib.parse.quote(normalized)\\n        if len(path) > 1 and path.endswith(\\'/\\'):\\n            encoded += \\'/\\'\\n        return encoded\\n\\n    def payload(self, http_request):\\n        body = http_request.body\\n        # If the body is a file like object, we can use\\n        # boto.utils.compute_hash, which will avoid reading\\n        # the entire body into memory.\\n        if hasattr(body, \\'seek\\') and hasattr(body, \\'read\\'):\\n            return boto.utils.compute_hash(body, hash_algorithm=sha256)[0]\\n        elif not isinstance(body, bytes):\\n            body = body.encode(\\'utf-8\\')\\n        return sha256(body).hexdigest()\\n\\n    def canonical_request(self, http_request):\\n        cr = [http_request.method.upper()]\\n        cr.append(self.canonical_uri(http_request))\\n        cr.append(self.canonical_query_string(http_request))\\n        headers_to_sign = self.headers_to_sign(http_request)\\n        cr.append(self.canonical_headers(headers_to_sign) + \\'\\\\n\\')\\n        cr.append(self.signed_headers(headers_to_sign))\\n        cr.append(self.payload(http_request))\\n        return \\'\\\\n\\'.join(cr)\\n\\n    def scope(self, http_request):\\n        scope = [self._provider.access_key]\\n        scope.append(http_request.timestamp)\\n        scope.append(http_request.region_name)\\n        scope.append(http_request.service_name)\\n        scope.append(\\'aws4_request\\')\\n        return \\'/\\'.join(scope)\\n\\n    def split_host_parts(self, host):\\n        return host.split(\\'.\\')\\n\\n    def determine_region_name(self, host):\\n        parts = self.split_host_parts(host)\\n        if self.region_name is not None:\\n            region_name = self.region_name\\n        elif len(parts) > 1:\\n            if parts[1] == \\'us-gov\\':\\n                region_name = \\'us-gov-west-1\\'\\n            else:\\n                if len(parts) == 3:\\n                    region_name = \\'us-east-1\\'\\n                else:\\n                    region_name = parts[1]\\n        else:\\n            region_name = parts[0]\\n\\n        return region_name\\n\\n    def determine_service_name(self, host):\\n        parts = self.split_host_parts(host)\\n        if self.service_name is not None:\\n            service_name = self.service_name\\n        else:\\n            service_name = parts[0]\\n        return service_name\\n\\n    def credential_scope(self, http_request):\\n        scope = []\\n        http_request.timestamp = http_request.headers[\\'X-Amz-Date\\'][0:8]\\n        scope.append(http_request.timestamp)\\n        # The service_name and region_name either come from:\\n        # * The service_name/region_name attrs or (if these values are None)\\n        # * parsed from the endpoint <service>.<region>.amazonaws.com.\\n        region_name = self.determine_region_name(http_request.host)\\n        service_name = self.determine_service_name(http_request.host)\\n        http_request.service_name = service_name\\n        http_request.region_name = region_name\\n\\n        scope.append(http_request.region_name)\\n        scope.append(http_request.service_name)\\n        scope.append(\\'aws4_request\\')\\n        return \\'/\\'.join(scope)\\n\\n    def string_to_sign(self, http_request, canonical_request):\\n        \"\"\"\\n        Return the canonical StringToSign as well as a dict\\n        containing the original version of all headers that\\n        were included in the StringToSign.\\n        \"\"\"\\n        sts = [\\'AWS4-HMAC-SHA256\\']\\n        sts.append(http_request.headers[\\'X-Amz-Date\\'])\\n        sts.append(self.credential_scope(http_request))\\n        sts.append(sha256(canonical_request.encode(\\'utf-8\\')).hexdigest())\\n        return \\'\\\\n\\'.join(sts)\\n\\n    def signature(self, http_request, string_to_sign):\\n        key = self._provider.secret_key\\n        k_date = self._sign((\\'AWS4\\' + key).encode(\\'utf-8\\'),\\n                            http_request.timestamp)\\n        k_region = self._sign(k_date, http_request.region_name)\\n        k_service = self._sign(k_region, http_request.service_name)\\n        k_signing = self._sign(k_service, \\'aws4_request\\')\\n        return self._sign(k_signing, string_to_sign, hex=True)\\n\\n    def add_auth(self, req, **kwargs):\\n        \"\"\"\\n        Add AWS4 authentication to a request.\\n\\n        :type req: :class`boto.connection.HTTPRequest`\\n        :param req: The HTTPRequest object.\\n        \"\"\"\\n        # This could be a retry.  Make sure the previous\\n        # authorization header is removed first.\\n        if \\'X-Amzn-Authorization\\' in req.headers:\\n            del req.headers[\\'X-Amzn-Authorization\\']\\n        now = datetime.datetime.utcnow()\\n        req.headers[\\'X-Amz-Date\\'] = now.strftime(\\'%Y%m%dT%H%M%SZ\\')\\n        if self._provider.security_token:\\n            req.headers[\\'X-Amz-Security-Token\\'] = self._provider.security_token\\n        qs = self.query_string(req)\\n\\n        qs_to_post = qs\\n\\n        # We do not want to include any params that were mangled into\\n        # the params if performing s3-sigv4 since it does not\\n        # belong in the body of a post for some requests.  Mangled\\n        # refers to items in the query string URL being added to the\\n        # http response params. However, these params get added to\\n        # the body of the request, but the query string URL does not\\n        # belong in the body of the request. ``unmangled_resp`` is the\\n        # response that happened prior to the mangling.  This ``unmangled_req``\\n        # kwarg will only appear for s3-sigv4.\\n        if \\'unmangled_req\\' in kwargs:\\n            qs_to_post = self.query_string(kwargs[\\'unmangled_req\\'])\\n\\n        if qs_to_post and req.method == \\'POST\\':\\n            # Stash request parameters into post body\\n            # before we generate the signature.\\n            req.body = qs_to_post\\n            req.headers[\\'Content-Type\\'] = \\'application/x-www-form-urlencoded; charset=UTF-8\\'\\n            req.headers[\\'Content-Length\\'] = str(len(req.body))\\n        else:\\n            # Safe to modify req.path here since\\n            # the signature will use req.auth_path.\\n            req.path = req.path.split(\\'?\\')[0]\\n\\n            if qs:\\n                # Don\\'t insert the \\'?\\' unless there\\'s actually a query string\\n                req.path = req.path + \\'?\\' + qs\\n        canonical_request = self.canonical_request(req)\\n        boto.log.debug(\\'CanonicalRequest:\\\\n%s\\' % canonical_request)\\n        string_to_sign = self.string_to_sign(req, canonical_request)\\n        boto.log.debug(\\'StringToSign:\\\\n%s\\' % string_to_sign)\\n        signature = self.signature(req, string_to_sign)\\n        boto.log.debug(\\'Signature:\\\\n%s\\' % signature)\\n        headers_to_sign = self.headers_to_sign(req)\\n        l = [\\'AWS4-HMAC-SHA256 Credential=%s\\' % self.scope(req)]\\n        l.append(\\'SignedHeaders=%s\\' % self.signed_headers(headers_to_sign))\\n        l.append(\\'Signature=%s\\' % signature)\\n        req.headers[\\'Authorization\\'] = \\',\\'.join(l)\\n\\n\\nclass S3HmacAuthV4Handler(HmacAuthV4Handler, AuthHandler):\\n    \"\"\"\\n    Implements a variant of Version 4 HMAC authorization specific to S3.\\n    \"\"\"\\n    capability = [\\'hmac-v4-s3\\']\\n\\n    def __init__(self, *args, **kwargs):\\n        super(S3HmacAuthV4Handler, self).__init__(*args, **kwargs)\\n\\n        if self.region_name:\\n            self.region_name = self.clean_region_name(self.region_name)\\n\\n    def clean_region_name(self, region_name):\\n        if region_name.startswith(\\'s3-\\'):\\n            return region_name[3:]\\n\\n        return region_name\\n\\n    def canonical_uri(self, http_request):\\n        # S3 does **NOT** do path normalization that SigV4 typically does.\\n        # Urlencode the path, **NOT** ``auth_path`` (because vhosting).\\n        path = urllib.parse.urlparse(http_request.path)\\n        # Because some quoting may have already been applied, let\\'s back it out.\\n        unquoted = urllib.parse.unquote(path.path)\\n        # Requote, this time addressing all characters.\\n        encoded = urllib.parse.quote(unquoted, safe=\\'/~\\')\\n        return encoded\\n\\n    def canonical_query_string(self, http_request):\\n        # Note that we just do not return an empty string for\\n        # POST request. Query strings in url are included in canonical\\n        # query string.\\n        l = []\\n        for param in sorted(http_request.params):\\n            value = boto.utils.get_utf8_value(http_request.params[param])\\n            l.append(\\'%s=%s\\' % (urllib.parse.quote(param, safe=\\'-_.~\\'),\\n                                urllib.parse.quote(value, safe=\\'-_.~\\')))\\n        return \\'&\\'.join(l)\\n\\n    def host_header(self, host, http_request):\\n        port = http_request.port\\n        secure = http_request.protocol == \\'https\\'\\n        if ((port == 80 and not secure) or (port == 443 and secure)):\\n            return http_request.host\\n        return \\'%s:%s\\' % (http_request.host, port)\\n\\n    def headers_to_sign(self, http_request):\\n        \"\"\"\\n        Select the headers from the request that need to be included\\n        in the StringToSign.\\n        \"\"\"\\n        host_header_value = self.host_header(self.host, http_request)\\n        headers_to_sign = {\\'Host\\': host_header_value}\\n        for name, value in http_request.headers.items():\\n            lname = name.lower()\\n            # Hooray for the only difference! The main SigV4 signer only does\\n            # ``Host`` + ``x-amz-*``. But S3 wants pretty much everything\\n            # signed, except for authorization itself.\\n            if lname not in [\\'authorization\\']:\\n                headers_to_sign[name] = value\\n        return headers_to_sign\\n\\n    def determine_region_name(self, host):\\n        # S3\\'s different format(s) of representing region/service from the\\n        # rest of AWS makes this hurt too.\\n        #\\n        # Possible domain formats:\\n        # - s3.amazonaws.com (Classic)\\n        # - s3-us-west-2.amazonaws.com (Specific region)\\n        # - bukkit.s3.amazonaws.com (Vhosted Classic)\\n        # - bukkit.s3-ap-northeast-1.amazonaws.com (Vhosted specific region)\\n        # - s3.cn-north-1.amazonaws.com.cn - (Beijing region)\\n        # - bukkit.s3.cn-north-1.amazonaws.com.cn - (Vhosted Beijing region)\\n        parts = self.split_host_parts(host)\\n\\n        if self.region_name is not None:\\n            region_name = self.region_name\\n        else:\\n            # Classic URLs - s3-us-west-2.amazonaws.com\\n            if len(parts) == 3:\\n                region_name = self.clean_region_name(parts[0])\\n\\n                # Special-case for Classic.\\n                if region_name == \\'s3\\':\\n                    region_name = \\'us-east-1\\'\\n            else:\\n                # Iterate over the parts in reverse order.\\n                for offset, part in enumerate(reversed(parts)):\\n                    part = part.lower()\\n\\n                    # Look for the first thing starting with \\'s3\\'.\\n                    # Until there\\'s a ``.s3`` TLD, we should be OK. :P\\n                    if part == \\'s3\\':\\n                        # If it\\'s by itself, the region is the previous part.\\n                        region_name = parts[-offset]\\n\\n                        # Unless it\\'s Vhosted classic\\n                        if region_name == \\'amazonaws\\':\\n                            region_name = \\'us-east-1\\'\\n\\n                        break\\n                    elif part.startswith(\\'s3-\\'):\\n                        region_name = self.clean_region_name(part)\\n                        break\\n\\n        return region_name\\n\\n    def determine_service_name(self, host):\\n        # Should this signing mechanism ever be used for anything else, this\\n        # will fail. Consider utilizing the logic from the parent class should\\n        # you find yourself here.\\n        return \\'s3\\'\\n\\n    def mangle_path_and_params(self, req):\\n        \"\"\"\\n        Returns a copy of the request object with fixed ``auth_path/params``\\n        attributes from the original.\\n        \"\"\"\\n        modified_req = copy.copy(req)\\n\\n        # Unlike the most other services, in S3, ``req.params`` isn\\'t the only\\n        # source of query string parameters.\\n        # Because of the ``query_args``, we may already have a query string\\n        # **ON** the ``path/auth_path``.\\n        # Rip them apart, so the ``auth_path/params`` can be signed\\n        # appropriately.\\n        parsed_path = urllib.parse.urlparse(modified_req.auth_path)\\n        modified_req.auth_path = parsed_path.path\\n\\n        if modified_req.params is None:\\n            modified_req.params = {}\\n        else:\\n            # To keep the original request object untouched. We must make\\n            # a copy of the params dictionary. Because the copy of the\\n            # original request directly refers to the params dictionary\\n            # of the original request.\\n            copy_params = req.params.copy()\\n            modified_req.params = copy_params\\n\\n        raw_qs = parsed_path.query\\n        existing_qs = parse_qs_safe(\\n            raw_qs,\\n            keep_blank_values=True\\n        )\\n\\n        # ``parse_qs`` will return lists. Don\\'t do that unless there\\'s a real,\\n        # live list provided.\\n        for key, value in existing_qs.items():\\n            if isinstance(value, (list, tuple)):\\n                if len(value) == 1:\\n                    existing_qs[key] = value[0]\\n\\n        modified_req.params.update(existing_qs)\\n        return modified_req\\n\\n    def payload(self, http_request):\\n        if http_request.headers.get(\\'x-amz-content-sha256\\'):\\n            return http_request.headers[\\'x-amz-content-sha256\\']\\n\\n        return super(S3HmacAuthV4Handler, self).payload(http_request)\\n\\n    def add_auth(self, req, **kwargs):\\n        if \\'x-amz-content-sha256\\' not in req.headers:\\n            if \\'_sha256\\' in req.headers:\\n                req.headers[\\'x-amz-content-sha256\\'] = req.headers.pop(\\'_sha256\\')\\n            else:\\n                req.headers[\\'x-amz-content-sha256\\'] = self.payload(req)\\n        updated_req = self.mangle_path_and_params(req)\\n        return super(S3HmacAuthV4Handler, self).add_auth(updated_req,\\n                                                         unmangled_req=req,\\n                                                         **kwargs)\\n\\n    def presign(self, req, expires, iso_date=None):\\n        \"\"\"\\n        Presign a request using SigV4 query params. Takes in an HTTP request\\n        and an expiration time in seconds and returns a URL.\\n\\n        http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\\n        \"\"\"\\n        if iso_date is None:\\n            iso_date = datetime.datetime.utcnow().strftime(\\'%Y%m%dT%H%M%SZ\\')\\n\\n        region = self.determine_region_name(req.host)\\n        service = self.determine_service_name(req.host)\\n\\n        params = {\\n            \\'X-Amz-Algorithm\\': \\'AWS4-HMAC-SHA256\\',\\n            \\'X-Amz-Credential\\': \\'%s/%s/%s/%s/aws4_request\\' % (\\n                self._provider.access_key,\\n                iso_date[:8],\\n                region,\\n                service\\n            ),\\n            \\'X-Amz-Date\\': iso_date,\\n            \\'X-Amz-Expires\\': expires,\\n            \\'X-Amz-SignedHeaders\\': \\'host\\'\\n        }\\n\\n        if self._provider.security_token:\\n            params[\\'X-Amz-Security-Token\\'] = self._provider.security_token\\n\\n        headers_to_sign = self.headers_to_sign(req)\\n        l = sorted([\\'%s\\' % n.lower().strip() for n in headers_to_sign])\\n        params[\\'X-Amz-SignedHeaders\\'] = \\';\\'.join(l)\\n \\n        req.params.update(params)\\n\\n        cr = self.canonical_request(req)\\n\\n        # We need to replace the payload SHA with a constant\\n        cr = \\'\\\\n\\'.join(cr.split(\\'\\\\n\\')[:-1]) + \\'\\\\nUNSIGNED-PAYLOAD\\'\\n\\n        # Date header is expected for string_to_sign, but unused otherwise\\n        req.headers[\\'X-Amz-Date\\'] = iso_date\\n\\n        sts = self.string_to_sign(req, cr)\\n        signature = self.signature(req, sts)\\n\\n        # Add signature to params now that we have it\\n        req.params[\\'X-Amz-Signature\\'] = signature\\n\\n        return \\'%s://%s%s?%s\\' % (req.protocol, req.host, req.path,\\n                                 urllib.parse.urlencode(req.params))\\n\\n\\nclass STSAnonHandler(AuthHandler):\\n    \"\"\"\\n    Provides pure query construction (no actual signing).\\n\\n    Used for making anonymous STS request for operations like\\n    ``assume_role_with_web_identity``.\\n    \"\"\"\\n\\n    capability = [\\'sts-anon\\']\\n\\n    def _escape_value(self, value):\\n        # This is changed from a previous version because this string is\\n        # being passed to the query string and query strings must\\n        # be url encoded. In particular STS requires the saml_response to\\n        # be urlencoded when calling assume_role_with_saml.\\n        return urllib.parse.quote(value)\\n\\n    def _build_query_string(self, params):\\n        keys = list(params.keys())\\n        keys.sort(key=lambda x: x.lower())\\n        pairs = []\\n        for key in keys:\\n            val = boto.utils.get_utf8_value(params[key])\\n            pairs.append(key + \\'=\\' + self._escape_value(val.decode(\\'utf-8\\')))\\n        return \\'&\\'.join(pairs)\\n\\n    def add_auth(self, http_request, **kwargs):\\n        headers = http_request.headers\\n        qs = self._build_query_string(\\n            http_request.params\\n        )\\n        boto.log.debug(\\'query_string in body: %s\\' % qs)\\n        headers[\\'Content-Type\\'] = \\'application/x-www-form-urlencoded\\'\\n        # This will be  a POST so the query string should go into the body\\n        # as opposed to being in the uri\\n        http_request.body = qs\\n\\n\\nclass QuerySignatureHelper(HmacKeys):\\n    \"\"\"\\n    Helper for Query signature based Auth handler.\\n\\n    Concrete sub class need to implement _calc_sigature method.\\n    \"\"\"\\n\\n    def add_auth(self, http_request, **kwargs):\\n        headers = http_request.headers\\n        params = http_request.params\\n        params[\\'AWSAccessKeyId\\'] = self._provider.access_key\\n        params[\\'SignatureVersion\\'] = self.SignatureVersion\\n        params[\\'Timestamp\\'] = boto.utils.get_ts()\\n        qs, signature = self._calc_signature(\\n            http_request.params, http_request.method,\\n            http_request.auth_path, http_request.host)\\n        boto.log.debug(\\'query_string: %s Signature: %s\\' % (qs, signature))\\n        if http_request.method == \\'POST\\':\\n            headers[\\'Content-Type\\'] = \\'application/x-www-form-urlencoded; charset=UTF-8\\'\\n            http_request.body = qs + \\'&Signature=\\' + urllib.parse.quote_plus(signature)\\n            http_request.headers[\\'Content-Length\\'] = str(len(http_request.body))\\n        else:\\n            http_request.body = \\'\\'\\n            # if this is a retried request, the qs from the previous try will\\n            # already be there, we need to get rid of that and rebuild it\\n            http_request.path = http_request.path.split(\\'?\\')[0]\\n            http_request.path = (http_request.path + \\'?\\' + qs +\\n                                 \\'&Signature=\\' + urllib.parse.quote_plus(signature))\\n\\n\\nclass QuerySignatureV0AuthHandler(QuerySignatureHelper, AuthHandler):\\n    \"\"\"Provides Signature V0 Signing\"\"\"\\n\\n    SignatureVersion = 0\\n    capability = [\\'sign-v0\\']\\n\\n    def _calc_signature(self, params, *args):\\n        boto.log.debug(\\'using _calc_signature_0\\')\\n        hmac = self._get_hmac()\\n        s = params[\\'Action\\'] + params[\\'Timestamp\\']\\n        hmac.update(s.encode(\\'utf-8\\'))\\n        keys = params.keys()\\n        keys.sort(cmp=lambda x, y: cmp(x.lower(), y.lower()))\\n        pairs = []\\n        for key in keys:\\n            val = boto.utils.get_utf8_value(params[key])\\n            pairs.append(key + \\'=\\' + urllib.parse.quote(val))\\n        qs = \\'&\\'.join(pairs)\\n        return (qs, base64.b64encode(hmac.digest()))\\n\\n\\nclass QuerySignatureV1AuthHandler(QuerySignatureHelper, AuthHandler):\\n    \"\"\"\\n    Provides Query Signature V1 Authentication.\\n    \"\"\"\\n\\n    SignatureVersion = 1\\n    capability = [\\'sign-v1\\', \\'mturk\\']\\n\\n    def __init__(self, *args, **kw):\\n        QuerySignatureHelper.__init__(self, *args, **kw)\\n        AuthHandler.__init__(self, *args, **kw)\\n        self._hmac_256 = None\\n\\n    def _calc_signature(self, params, *args):\\n        boto.log.debug(\\'using _calc_signature_1\\')\\n        hmac = self._get_hmac()\\n        keys = list(params.keys())\\n        keys.sort(key=lambda x: x.lower())\\n        pairs = []\\n        for key in keys:\\n            hmac.update(key.encode(\\'utf-8\\'))\\n            val = boto.utils.get_utf8_value(params[key])\\n            hmac.update(val)\\n            pairs.append(key + \\'=\\' + urllib.parse.quote(val))\\n        qs = \\'&\\'.join(pairs)\\n        return (qs, base64.b64encode(hmac.digest()))\\n\\n\\nclass QuerySignatureV2AuthHandler(QuerySignatureHelper, AuthHandler):\\n    \"\"\"Provides Query Signature V2 Authentication.\"\"\"\\n\\n    SignatureVersion = 2\\n    capability = [\\'sign-v2\\', \\'ec2\\', \\'ec2\\', \\'emr\\', \\'fps\\', \\'ecs\\',\\n                  \\'sdb\\', \\'iam\\', \\'rds\\', \\'sns\\', \\'sqs\\', \\'cloudformation\\']\\n\\n    def _calc_signature(self, params, verb, path, server_name):\\n        boto.log.debug(\\'using _calc_signature_2\\')\\n        string_to_sign = \\'%s\\\\n%s\\\\n%s\\\\n\\' % (verb, server_name.lower(), path)\\n        hmac = self._get_hmac()\\n        params[\\'SignatureMethod\\'] = self.algorithm()\\n        if self._provider.security_token:\\n            params[\\'SecurityToken\\'] = self._provider.security_token\\n        keys = sorted(params.keys())\\n        pairs = []\\n        for key in keys:\\n            val = boto.utils.get_utf8_value(params[key])\\n            pairs.append(urllib.parse.quote(key, safe=\\'\\') + \\'=\\' +\\n                         urllib.parse.quote(val, safe=\\'-_~\\'))\\n        qs = \\'&\\'.join(pairs)\\n        boto.log.debug(\\'query string: %s\\' % qs)\\n        string_to_sign += qs\\n        boto.log.debug(\\'string_to_sign: %s\\' % string_to_sign)\\n        hmac.update(string_to_sign.encode(\\'utf-8\\'))\\n        b64 = base64.b64encode(hmac.digest())\\n        boto.log.debug(\\'len(b64)=%d\\' % len(b64))\\n        boto.log.debug(\\'base64 encoded digest: %s\\' % b64)\\n        return (qs, b64)\\n\\n\\nclass POSTPathQSV2AuthHandler(QuerySignatureV2AuthHandler, AuthHandler):\\n    \"\"\"\\n    Query Signature V2 Authentication relocating signed query\\n    into the path and allowing POST requests with Content-Types.\\n    \"\"\"\\n\\n    capability = [\\'mws\\']\\n\\n    def add_auth(self, req, **kwargs):\\n        req.params[\\'AWSAccessKeyId\\'] = self._provider.access_key\\n        req.params[\\'SignatureVersion\\'] = self.SignatureVersion\\n        req.params[\\'Timestamp\\'] = boto.utils.get_ts()\\n        qs, signature = self._calc_signature(req.params, req.method,\\n                                             req.auth_path, req.host)\\n        boto.log.debug(\\'query_string: %s Signature: %s\\' % (qs, signature))\\n        if req.method == \\'POST\\':\\n            req.headers[\\'Content-Length\\'] = str(len(req.body))\\n            req.headers[\\'Content-Type\\'] = req.headers.get(\\'Content-Type\\',\\n                                                          \\'text/plain\\')\\n        else:\\n            req.body = \\'\\'\\n        # if this is a retried req, the qs from the previous try will\\n        # already be there, we need to get rid of that and rebuild it\\n        req.path = req.path.split(\\'?\\')[0]\\n        req.path = (req.path + \\'?\\' + qs +\\n                    \\'&Signature=\\' + urllib.parse.quote_plus(signature))\\n\\n\\ndef get_auth_handler(host, config, provider, requested_capability=None):\\n    \"\"\"Finds an AuthHandler that is ready to authenticate.\\n\\n    Lists through all the registered AuthHandlers to find one that is willing\\n    to handle for the requested capabilities, config and provider.\\n\\n    :type host: string\\n    :param host: The name of the host\\n\\n    :type config:\\n    :param config:\\n\\n    :type provider:\\n    :param provider:\\n\\n    Returns:\\n        An implementation of AuthHandler.\\n\\n    Raises:\\n        boto.exception.NoAuthHandlerFound\\n    \"\"\"\\n    ready_handlers = []\\n    auth_handlers = boto.plugin.get_plugin(AuthHandler, requested_capability)\\n    for handler in auth_handlers:\\n        try:\\n            ready_handlers.append(handler(host, config, provider))\\n        except boto.auth_handler.NotReadyToAuthenticate:\\n            pass\\n\\n    if not ready_handlers:\\n        checked_handlers = auth_handlers\\n        names = [handler.__name__ for handler in checked_handlers]\\n        raise boto.exception.NoAuthHandlerFound(\\n            \\'No handler was ready to authenticate. %d handlers were checked.\\'\\n            \\' %s \\'\\n            \\'Check your credentials\\' % (len(names), str(names)))\\n\\n    # We select the last ready auth handler that was loaded, to allow users to\\n    # customize how auth works in environments where there are shared boto\\n    # config files (e.g., /etc/boto.cfg and ~/.boto): The more general,\\n    # system-wide shared configs should be loaded first, and the user\\'s\\n    # customizations loaded last. That way, for example, the system-wide\\n    # config might include a plugin_directory that includes a service account\\n    # auth plugin shared by all users of a Google Compute Engine instance\\n    # (allowing sharing of non-user data between various services), and the\\n    # user could override this with a .boto config that includes user-specific\\n    # credentials (for access to user data).\\n    return ready_handlers[-1]\\n\\n\\ndef detect_potential_sigv4(func):\\n    def _wrapper(self):\\n        if os.environ.get(\\'EC2_USE_SIGV4\\', False):\\n            return [\\'hmac-v4\\']\\n\\n        if boto.config.get(\\'ec2\\', \\'use-sigv4\\', False):\\n            return [\\'hmac-v4\\']\\n\\n        if hasattr(self, \\'region\\'):\\n            # If you\\'re making changes here, you should also check\\n            # ``boto/iam/connection.py``, as several things there are also\\n            # endpoint-related.\\n            if getattr(self.region, \\'endpoint\\', \\'\\'):\\n                for test in SIGV4_DETECT:\\n                    if test in self.region.endpoint:\\n                        return [\\'hmac-v4\\']\\n\\n        return func(self)\\n    return _wrapper\\n\\n\\ndef detect_potential_s3sigv4(func):\\n    def _wrapper(self):\\n        if os.environ.get(\\'S3_USE_SIGV4\\', False):\\n            return [\\'hmac-v4-s3\\']\\n\\n        if boto.config.get(\\'s3\\', \\'use-sigv4\\', False):\\n            return [\\'hmac-v4-s3\\']\\n\\n        if not hasattr(self, \\'host\\'):\\n            return func(self)\\n\\n        # Keep the old explicit logic in case somebody was adding to the list.\\n        for test in SIGV4_DETECT:\\n            if test in self.host:\\n                return [\\'hmac-v4-s3\\']\\n\\n        # Use default for non-aws hosts. Adding a url scheme is necessary if\\n        # not present for urlparse to properly function.\\n        host = self.host\\n        if not self.host.startswith(\\'http://\\') or \\\\\\n                self.host.startswith(\\'https://\\'):\\n            host = \\'https://\\' + host\\n        netloc = urlparse(host).netloc\\n        if not (netloc.endswith(\\'amazonaws.com\\') or\\n                netloc.endswith(\\'amazonaws.com.cn\\')):\\n            return func(self)\\n\\n        # Use the default for the global endpoint\\n        if netloc.endswith(\\'s3.amazonaws.com\\'):\\n            return func(self)\\n\\n        # Use the default for regions that support sigv4 and sigv2\\n        if any(test in self.host for test in S3_AUTH_DETECT):\\n            return func(self)\\n\\n        # Use anonymous if enabled.\\n        if hasattr(self, \\'anon\\') and self.anon:\\n            return func(self)\\n\\n        # Default to sigv4 for aws hosts outside of regions that are known\\n        # to support sigv2\\n        return [\\'hmac-v4-s3\\']\\n    return _wrapper\\n'}]", "test_list": ["def test_sigv4_presign_headers(self):\n    self.config = {'s3': {'use-sigv4': True}}\n    conn = self.connection_class(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    headers = {'x-amz-meta-key': 'val'}\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt', headers=headers)\n    self.assertIn('host', url)\n    self.assertIn('x-amz-meta-key', url)", "def test_sigv4_presign_respects_is_secure(self):\n    self.config = {'s3': {'use-sigv4': True}}\n    conn = self.connection_class(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com', is_secure=True)\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt')\n    self.assertTrue(url.startswith('https://examplebucket.s3.amazonaws.com/test.txt?'))\n    conn = self.connection_class(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com', is_secure=False)\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt')\n    self.assertTrue(url.startswith('http://examplebucket.s3.amazonaws.com/test.txt?'))", "def test_sigv4_presign_optional_params(self):\n    self.config = {'s3': {'use-sigv4': True}}\n    conn = self.connection_class(aws_access_key_id='less', aws_secret_access_key='more', security_token='token', host='s3.amazonaws.com')\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt', version_id=2)\n    self.assertIn('VersionId=2', url)\n    self.assertIn('X-Amz-Security-Token=token', url)", "def test_sigv4_presign_response_headers(self):\n    self.config = {'s3': {'use-sigv4': True}}\n    conn = self.connection_class(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    response_headers = {'response-content-disposition': 'attachment; filename=\"file.ext\"'}\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt', response_headers=response_headers)\n    self.assertIn('host', url)\n    self.assertIn('response-content-disposition', url)", "def test_sigv4_presign(self):\n    self.config = {'s3': {'use-sigv4': True}}\n    conn = self.connection_class(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt', iso_date='20140625T000000Z')\n    self.assertIn('a937f5fbc125d98ac8f04c49e0204ea1526a7b8ca058000a54c192457be05b7d', url)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'generate_url_sigv4' should validate that 'expires_in' is a positive integer and 'method' is a valid HTTP method (e.g., 'GET', 'POST'). If not, it should raise a ValueError.The error messages include 'expires_in must be a positive integer' and 'expires_in must be a positive integer'", "unit_test": ["def test_generate_url_sigv4_invalid_expires_in():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    try:\n        conn.generate_url_sigv4(-1, 'GET', bucket='examplebucket', key='test.txt')\n    except ValueError as e:\n        assert str(e) == 'expires_in must be a positive integer'\n\ndef test_generate_url_sigv4_invalid_method():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    try:\n        conn.generate_url_sigv4(86400, 'INVALID', bucket='examplebucket', key='test.txt')\n    except ValueError as e:\n        assert str(e) == 'method must be a valid HTTP method'"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_invalid_expires_in"}, "Exception Handling": {"requirement": "The function 'generate_url_sigv4' should handle exceptions related to network issues gracefully and provide a user-friendly error message. It should raise requests.exceptions.RequestException and the message should be 'Network error occurred while generating presigned URL'.", "unit_test": ["def test_generate_url_sigv4_network_exception_handling():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    try:\n        conn.generate_url_sigv4(86400, 'GET', bucket='nonexistentbucket', key='test.txt')\n    except NetworkError as e:\n        assert 'Network error occurred while generating presigned URL' in str(e)"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_network_exception_handling"}, "Edge Case Handling": {"requirement": "The function 'generate_url_sigv4' should correctly handle edge cases such as empty bucket or key values by raising a ValueError and the error message is 'bucket and key cannot be empty'.", "unit_test": ["def test_generate_url_sigv4_empty_bucket_key():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    try:\n        conn.generate_url_sigv4(86400, 'GET', bucket='', key='')\n    except ValueError as e:\n        assert str(e) == 'bucket and key cannot be empty'"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_empty_bucket_key"}, "Functionality Extension": {"requirement": "Extend the 'generate_url_sigv4' function to support custom query parameters in the presigned URL.", "unit_test": ["def test_generate_url_sigv4_custom_query_params():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    url = conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt', query_params={'custom': 'value'})\n    assert 'custom=value' in url"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_custom_query_params"}, "Annotation Coverage": {"requirement": "Ensure that the 'generate_url_sigv4' function has complete type annotations for all parameters and return type.", "unit_test": ["def test_generate_url_sigv4_annotations():\n    import inspect\n    sig = inspect.signature(S3Connection.generate_url_sigv4)\n    assert sig.parameters['expires_in'].annotation == int\n    assert sig.parameters['method'].annotation == str\n    assert sig.return_annotation == str"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'generate_url_sigv4' function should not exceed 10.", "unit_test": ["def test_generate_url_sigv4_complexity():\n    from radon.complexity import cc_visit\n    with open('path_to_file_containing_generate_url_sigv4.py') as f:\n        code = f.read()\n    complexity = cc_visit(code)\n    for func in complexity:\n        if func.name == 'generate_url_sigv4':\n            assert func.complexity <= 10"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_complexity"}, "Code Standard": {"requirement": "The 'generate_url_sigv4' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_generate_url_sigv4_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_generate_url_sigv4.py'])\n    assert result.total_errors == 0"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_check_code_style"}, "Context Usage Verification": {"requirement": "Verify that 'generate_url_sigv4' uses the 'build_base_http_request' and '_auth_handler' attribute from 'S3Connection'.", "unit_test": ["def test_generate_url_sigv4_uses_calling_format():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    conn.calling_format = Mock()\n    conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt')\n    assert conn.calling_format.build_url_base.called"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_uses_calling_format"}, "Context Usage Correctness Verification": {"requirement": "Ensure that the 'build_base_http_request' and '_auth_handler' are both correctly used to build the URL in 'generate_url_sigv4'.", "unit_test": ["def test_generate_url_sigv4_correct_calling_format_usage():\n    conn = S3Connection(aws_access_key_id='less', aws_secret_access_key='more', host='s3.amazonaws.com')\n    conn.calling_format = Mock()\n    conn.generate_url_sigv4(86400, 'GET', bucket='examplebucket', key='test.txt')\n    conn.calling_format.build_url_base.assert_called_with(conn, 'https', 's3.amazonaws.com', 'examplebucket', 'test.txt')"], "test": "tests/unit/s3/test_connection.py::TestSigV4Presigned::test_generate_url_sigv4_correct_calling_format_usage"}}}
{"namespace": "gunicorn.config.Config.logger_class", "type": "method", "project_path": "Utilities/gunicorn", "completion_path": "Utilities/gunicorn/gunicorn/config.py", "signature_position": [148, 148], "body_position": [149, 167], "dependency": {"intra_class": ["gunicorn.config.Config.settings"], "intra_file": ["gunicorn.config.LoggerClass", "gunicorn.config.LoggerClass.default"], "cross_file": ["gunicorn.util", "gunicorn.util.load_class"]}, "requirement": {"Functionality": "This function retrieves the logger class based on the configuration settings. It first checks the 'logger_class' setting and if it is \"simple\", it uses the default logger class. If the default logger class is being used andstatsd is on, it automatically switches to the gunicorn.instrument.statsd.Statsd class. Then, it loads the logger class (with default: \"gunicorn.glogging.Logger\" and section: \"gunicorn.loggers\") and install it if can, finally returns it.", "Arguments": ":param self: Config. An instance of the Config class.\n:return: The logger class based on the configuration settings."}, "tests": ["tests/test_config.py::test_statsd_changes_logger", "tests/test_config.py::test_property_access", "tests/test_config.py::test_always_use_configured_logger"], "indent": 8, "domain": "Utilities", "code": "    @property\n    def proc_name(self):\n        pn = self.settings['proc_name'].get()\n        if pn is not None:\n            return pn\n        else:\n            return self.settings['default_proc_name'].get()\n\n    @property\n    def logger_class(self):\n        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n", "intra_context": "# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\n\n# Please remember to run \"make -C docs html\" after update \"desc\" attributes.\n\nimport argparse\nimport copy\nimport grp\nimport inspect\nimport os\nimport pwd\nimport re\nimport shlex\nimport ssl\nimport sys\nimport textwrap\n\nfrom gunicorn import __version__, util\nfrom gunicorn.errors import ConfigError\nfrom gunicorn.reloader import reloader_engines\n\nKNOWN_SETTINGS = []\nPLATFORM = sys.platform\n\n\ndef make_settings(ignore=None):\n    settings = {}\n    ignore = ignore or ()\n    for s in KNOWN_SETTINGS:\n        setting = s()\n        if setting.name in ignore:\n            continue\n        settings[setting.name] = setting.copy()\n    return settings\n\n\ndef auto_int(_, x):\n    # for compatible with octal numbers in python3\n    if re.match(r'0(\\d)', x, re.IGNORECASE):\n        x = x.replace('0', '0o', 1)\n    return int(x, 0)\n\n\nclass Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n    def __str__(self):\n        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n    # def __str__(self):\n    #         lines = []\n    #         kmax = max(len(k) for k in self.settings)\n    #         for k, v in sorted(self.settings.items()):\n    #             if callable(v):\n    #                 value_str = f'<{v.__name__}()>'\n    #             else:\n    #                 value_str = repr(v)\n    #             lines.append(f'{k:{kmax}} = {value_str}')\n    #         return \"\\n\".join(lines)\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n###The function: logger_class###\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n\n    @property\n    def is_ssl(self):\n        return self.certfile or self.keyfile\n\n    @property\n    def ssl_options(self):\n        opts = {}\n        for name, value in self.settings.items():\n            if value.section == 'SSL':\n                opts[name] = value.get()\n        return opts\n\n    @property\n    def env(self):\n        raw_env = self.settings['raw_env'].get()\n        env = {}\n\n        if not raw_env:\n            return env\n\n        for e in raw_env:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = s.split('=', 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n\n            env[k] = v\n\n        return env\n\n    @property\n    def sendfile(self):\n        if self.settings['sendfile'].get() is not None:\n            return False\n\n        if 'SENDFILE' in os.environ:\n            sendfile = os.environ['SENDFILE'].lower()\n            return sendfile in ['y', '1', 'yes', 'true']\n\n        return True\n\n    @property\n    def reuse_port(self):\n        return self.settings['reuse_port'].get()\n\n    @property\n    def paste_global_conf(self):\n        raw_global_conf = self.settings['raw_paste_global_conf'].get()\n        if raw_global_conf is None:\n            return None\n\n        global_conf = {}\n        for e in raw_global_conf:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = re.split(r'(?<!\\\\)=', s, 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n            k = k.replace('\\\\=', '=')\n            v = v.replace('\\\\=', '=')\n            global_conf[k] = v\n\n        return global_conf\n\n\nclass SettingMeta(type):\n    def __new__(cls, name, bases, attrs):\n        super_new = super().__new__\n        parents = [b for b in bases if isinstance(b, SettingMeta)]\n        if not parents:\n            return super_new(cls, name, bases, attrs)\n\n        attrs[\"order\"] = len(KNOWN_SETTINGS)\n        attrs[\"validator\"] = staticmethod(attrs[\"validator\"])\n\n        new_class = super_new(cls, name, bases, attrs)\n        new_class.fmt_desc(attrs.get(\"desc\", \"\"))\n        KNOWN_SETTINGS.append(new_class)\n        return new_class\n\n    def fmt_desc(cls, desc):\n        desc = textwrap.dedent(desc).strip()\n        setattr(cls, \"desc\", desc)\n        setattr(cls, \"short\", desc.splitlines()[0])\n\n\nclass Setting(object):\n    name = None\n    value = None\n    section = None\n    cli = None\n    validator = None\n    type = None\n    meta = None\n    action = None\n    default = None\n    short = None\n    desc = None\n    nargs = None\n    const = None\n\n    def __init__(self):\n        if self.default is not None:\n            self.set(self.default)\n\n    def add_option(self, parser):\n        if not self.cli:\n            return\n        args = tuple(self.cli)\n\n        help_txt = \"%s [%s]\" % (self.short, self.default)\n        help_txt = help_txt.replace(\"%\", \"%%\")\n\n        kwargs = {\n            \"dest\": self.name,\n            \"action\": self.action or \"store\",\n            \"type\": self.type or str,\n            \"default\": None,\n            \"help\": help_txt\n        }\n\n        if self.meta is not None:\n            kwargs['metavar'] = self.meta\n\n        if kwargs[\"action\"] != \"store\":\n            kwargs.pop(\"type\")\n\n        if self.nargs is not None:\n            kwargs[\"nargs\"] = self.nargs\n\n        if self.const is not None:\n            kwargs[\"const\"] = self.const\n\n        parser.add_argument(*args, **kwargs)\n\n    def copy(self):\n        return copy.copy(self)\n\n    def get(self):\n        return self.value\n\n    def set(self, val):\n        if not callable(self.validator):\n            raise TypeError('Invalid validator: %s' % self.name)\n        self.value = self.validator(val)\n\n    def __lt__(self, other):\n        return (self.section == other.section and\n                self.order < other.order)\n    __cmp__ = __lt__\n\n    def __repr__(self):\n        return \"<%s.%s object at %x with value %r>\" % (\n            self.__class__.__module__,\n            self.__class__.__name__,\n            id(self),\n            self.value,\n        )\n\n\nSetting = SettingMeta('Setting', (Setting,), {})\n\n\ndef validate_bool(val):\n    if val is None:\n        return\n\n    if isinstance(val, bool):\n        return val\n    if not isinstance(val, str):\n        raise TypeError(\"Invalid type for casting: %s\" % val)\n    if val.lower().strip() == \"true\":\n        return True\n    elif val.lower().strip() == \"false\":\n        return False\n    else:\n        raise ValueError(\"Invalid boolean: %s\" % val)\n\n\ndef validate_dict(val):\n    if not isinstance(val, dict):\n        raise TypeError(\"Value is not a dictionary: %s \" % val)\n    return val\n\n\ndef validate_pos_int(val):\n    if not isinstance(val, int):\n        val = int(val, 0)\n    else:\n        # Booleans are ints!\n        val = int(val)\n    if val < 0:\n        raise ValueError(\"Value must be positive: %s\" % val)\n    return val\n\n\ndef validate_ssl_version(val):\n    if val != SSLVersion.default:\n        sys.stderr.write(\"Warning: option `ssl_version` is deprecated and it is ignored. Use ssl_context instead.\\n\")\n    return val\n\n\ndef validate_string(val):\n    if val is None:\n        return None\n    if not isinstance(val, str):\n        raise TypeError(\"Not a string: %s\" % val)\n    return val.strip()\n\n\ndef validate_file_exists(val):\n    if val is None:\n        return None\n    if not os.path.exists(val):\n        raise ValueError(\"File %s does not exists.\" % val)\n    return val\n\n\ndef validate_list_string(val):\n    if not val:\n        return []\n\n    # legacy syntax\n    if isinstance(val, str):\n        val = [val]\n\n    return [validate_string(v) for v in val]\n\n\ndef validate_list_of_existing_files(val):\n    return [validate_file_exists(v) for v in validate_list_string(val)]\n\n\ndef validate_string_to_list(val):\n    val = validate_string(val)\n\n    if not val:\n        return []\n\n    return [v.strip() for v in val.split(\",\") if v]\n\n\ndef validate_class(val):\n    if inspect.isfunction(val) or inspect.ismethod(val):\n        val = val()\n    if inspect.isclass(val):\n        return val\n    return validate_string(val)\n\n\ndef validate_callable(arity):\n    def _validate_callable(val):\n        if isinstance(val, str):\n            try:\n                mod_name, obj_name = val.rsplit(\".\", 1)\n            except ValueError:\n                raise TypeError(\"Value '%s' is not import string. \"\n                                \"Format: module[.submodules...].object\" % val)\n            try:\n                mod = __import__(mod_name, fromlist=[obj_name])\n                val = getattr(mod, obj_name)\n            except ImportError as e:\n                raise TypeError(str(e))\n            except AttributeError:\n                raise TypeError(\"Can not load '%s' from '%s'\"\n                                \"\" % (obj_name, mod_name))\n        if not callable(val):\n            raise TypeError(\"Value is not callable: %s\" % val)\n        if arity != -1 and arity != util.get_arity(val):\n            raise TypeError(\"Value must have an arity of: %s\" % arity)\n        return val\n    return _validate_callable\n\n\ndef validate_user(val):\n    if val is None:\n        return os.geteuid()\n    if isinstance(val, int):\n        return val\n    elif val.isdigit():\n        return int(val)\n    else:\n        try:\n            return pwd.getpwnam(val).pw_uid\n        except KeyError:\n            raise ConfigError(\"No such user: '%s'\" % val)\n\n\ndef validate_group(val):\n    if val is None:\n        return os.getegid()\n\n    if isinstance(val, int):\n        return val\n    elif val.isdigit():\n        return int(val)\n    else:\n        try:\n            return grp.getgrnam(val).gr_gid\n        except KeyError:\n            raise ConfigError(\"No such group: '%s'\" % val)\n\n\ndef validate_post_request(val):\n    val = validate_callable(-1)(val)\n\n    largs = util.get_arity(val)\n    if largs == 4:\n        return val\n    elif largs == 3:\n        return lambda worker, req, env, _r: val(worker, req, env)\n    elif largs == 2:\n        return lambda worker, req, _e, _r: val(worker, req)\n    else:\n        raise TypeError(\"Value must have an arity of: 4\")\n\n\ndef validate_chdir(val):\n    # valid if the value is a string\n    val = validate_string(val)\n\n    # transform relative paths\n    path = os.path.abspath(os.path.normpath(os.path.join(util.getcwd(), val)))\n\n    # test if the path exists\n    if not os.path.exists(path):\n        raise ConfigError(\"can't chdir to %r\" % val)\n\n    return path\n\n\ndef validate_statsd_address(val):\n    val = validate_string(val)\n    if val is None:\n        return None\n\n    # As of major release 20, util.parse_address would recognize unix:PORT\n    # as a UDS address, breaking backwards compatibility. We defend against\n    # that regression here (this is also unit-tested).\n    # Feel free to remove in the next major release.\n    unix_hostname_regression = re.match(r'^unix:(\\d+)$', val)\n    if unix_hostname_regression:\n        return ('unix', int(unix_hostname_regression.group(1)))\n\n    try:\n        address = util.parse_address(val, default_port='8125')\n    except RuntimeError:\n        raise TypeError(\"Value must be one of ('host:port', 'unix://PATH')\")\n\n    return address\n\n\ndef validate_reload_engine(val):\n    if val not in reloader_engines:\n        raise ConfigError(\"Invalid reload_engine: %r\" % val)\n\n    return val\n\n\ndef get_default_config_file():\n    config_path = os.path.join(os.path.abspath(os.getcwd()),\n                               'gunicorn.conf.py')\n    if os.path.exists(config_path):\n        return config_path\n    return None\n\n\nclass ConfigFile(Setting):\n    name = \"config\"\n    section = \"Config File\"\n    cli = [\"-c\", \"--config\"]\n    meta = \"CONFIG\"\n    validator = validate_string\n    default = \"./gunicorn.conf.py\"\n    desc = \"\"\"\\\n        :ref:`The Gunicorn config file<configuration_file>`.\n\n        A string of the form ``PATH``, ``file:PATH``, or ``python:MODULE_NAME``.\n\n        Only has an effect when specified on the command line or as part of an\n        application specific configuration.\n\n        By default, a file named ``gunicorn.conf.py`` will be read from the same\n        directory where gunicorn is being run.\n\n        .. versionchanged:: 19.4\n           Loading the config from a Python module requires the ``python:``\n           prefix.\n        \"\"\"\n\n\nclass WSGIApp(Setting):\n    name = \"wsgi_app\"\n    section = \"Config File\"\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A WSGI application path in pattern ``$(MODULE_NAME):$(VARIABLE_NAME)``.\n\n        .. versionadded:: 20.1.0\n        \"\"\"\n\n\nclass Bind(Setting):\n    name = \"bind\"\n    action = \"append\"\n    section = \"Server Socket\"\n    cli = [\"-b\", \"--bind\"]\n    meta = \"ADDRESS\"\n    validator = validate_list_string\n\n    if 'PORT' in os.environ:\n        default = ['0.0.0.0:{0}'.format(os.environ.get('PORT'))]\n    else:\n        default = ['127.0.0.1:8000']\n\n    desc = \"\"\"\\\n        The socket to bind.\n\n        A string of the form: ``HOST``, ``HOST:PORT``, ``unix:PATH``,\n        ``fd://FD``. An IP is a valid ``HOST``.\n\n        .. versionchanged:: 20.0\n           Support for ``fd://FD`` got added.\n\n        Multiple addresses can be bound. ex.::\n\n            $ gunicorn -b 127.0.0.1:8000 -b [::1]:8000 test:app\n\n        will bind the `test:app` application on localhost both on ipv6\n        and ipv4 interfaces.\n\n        If the ``PORT`` environment variable is defined, the default\n        is ``['0.0.0.0:$PORT']``. If it is not defined, the default\n        is ``['127.0.0.1:8000']``.\n        \"\"\"\n\n\nclass Backlog(Setting):\n    name = \"backlog\"\n    section = \"Server Socket\"\n    cli = [\"--backlog\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 2048\n    desc = \"\"\"\\\n        The maximum number of pending connections.\n\n        This refers to the number of clients that can be waiting to be served.\n        Exceeding this number results in the client getting an error when\n        attempting to connect. It should only affect servers under significant\n        load.\n\n        Must be a positive integer. Generally set in the 64-2048 range.\n        \"\"\"\n\n\nclass Workers(Setting):\n    name = \"workers\"\n    section = \"Worker Processes\"\n    cli = [\"-w\", \"--workers\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = int(os.environ.get(\"WEB_CONCURRENCY\", 1))\n    desc = \"\"\"\\\n        The number of worker processes for handling requests.\n\n        A positive integer generally in the ``2-4 x $(NUM_CORES)`` range.\n        You'll want to vary this a bit to find the best for your particular\n        application's work load.\n\n        By default, the value of the ``WEB_CONCURRENCY`` environment variable,\n        which is set by some Platform-as-a-Service providers such as Heroku. If\n        it is not defined, the default is ``1``.\n        \"\"\"\n\n\nclass WorkerClass(Setting):\n    name = \"worker_class\"\n    section = \"Worker Processes\"\n    cli = [\"-k\", \"--worker-class\"]\n    meta = \"STRING\"\n    validator = validate_class\n    default = \"sync\"\n    desc = \"\"\"\\\n        The type of workers to use.\n\n        The default class (``sync``) should handle most \"normal\" types of\n        workloads. You'll want to read :doc:`design` for information on when\n        you might want to choose one of the other worker classes. Required\n        libraries may be installed using setuptools' ``extras_require`` feature.\n\n        A string referring to one of the following bundled classes:\n\n        * ``sync``\n        * ``eventlet`` - Requires eventlet >= 0.24.1 (or install it via\n          ``pip install gunicorn[eventlet]``)\n        * ``gevent``   - Requires gevent >= 1.4 (or install it via\n          ``pip install gunicorn[gevent]``)\n        * ``tornado``  - Requires tornado >= 0.2 (or install it via\n          ``pip install gunicorn[tornado]``)\n        * ``gthread``  - Python 2 requires the futures package to be installed\n          (or install it via ``pip install gunicorn[gthread]``)\n\n        Optionally, you can provide your own worker by giving Gunicorn a\n        Python path to a subclass of ``gunicorn.workers.base.Worker``.\n        This alternative syntax will load the gevent class:\n        ``gunicorn.workers.ggevent.GeventWorker``.\n        \"\"\"\n\n\nclass WorkerThreads(Setting):\n    name = \"threads\"\n    section = \"Worker Processes\"\n    cli = [\"--threads\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 1\n    desc = \"\"\"\\\n        The number of worker threads for handling requests.\n\n        Run each worker with the specified number of threads.\n\n        A positive integer generally in the ``2-4 x $(NUM_CORES)`` range.\n        You'll want to vary this a bit to find the best for your particular\n        application's work load.\n\n        If it is not defined, the default is ``1``.\n\n        This setting only affects the Gthread worker type.\n\n        .. note::\n           If you try to use the ``sync`` worker type and set the ``threads``\n           setting to more than 1, the ``gthread`` worker type will be used\n           instead.\n        \"\"\"\n\n\nclass WorkerConnections(Setting):\n    name = \"worker_connections\"\n    section = \"Worker Processes\"\n    cli = [\"--worker-connections\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 1000\n    desc = \"\"\"\\\n        The maximum number of simultaneous clients.\n\n        This setting only affects the ``gthread``, ``eventlet`` and ``gevent`` worker types.\n        \"\"\"\n\n\nclass MaxRequests(Setting):\n    name = \"max_requests\"\n    section = \"Worker Processes\"\n    cli = [\"--max-requests\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 0\n    desc = \"\"\"\\\n        The maximum number of requests a worker will process before restarting.\n\n        Any value greater than zero will limit the number of requests a worker\n        will process before automatically restarting. This is a simple method\n        to help limit the damage of memory leaks.\n\n        If this is set to zero (the default) then the automatic worker\n        restarts are disabled.\n        \"\"\"\n\n\nclass MaxRequestsJitter(Setting):\n    name = \"max_requests_jitter\"\n    section = \"Worker Processes\"\n    cli = [\"--max-requests-jitter\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 0\n    desc = \"\"\"\\\n        The maximum jitter to add to the *max_requests* setting.\n\n        The jitter causes the restart per worker to be randomized by\n        ``randint(0, max_requests_jitter)``. This is intended to stagger worker\n        restarts to avoid all workers restarting at the same time.\n\n        .. versionadded:: 19.2\n        \"\"\"\n\n\nclass Timeout(Setting):\n    name = \"timeout\"\n    section = \"Worker Processes\"\n    cli = [\"-t\", \"--timeout\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 30\n    desc = \"\"\"\\\n        Workers silent for more than this many seconds are killed and restarted.\n\n        Value is a positive number or 0. Setting it to 0 has the effect of\n        infinite timeouts by disabling timeouts for all workers entirely.\n\n        Generally, the default of thirty seconds should suffice. Only set this\n        noticeably higher if you're sure of the repercussions for sync workers.\n        For the non sync workers it just means that the worker process is still\n        communicating and is not tied to the length of time required to handle a\n        single request.\n        \"\"\"\n\n\nclass GracefulTimeout(Setting):\n    name = \"graceful_timeout\"\n    section = \"Worker Processes\"\n    cli = [\"--graceful-timeout\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 30\n    desc = \"\"\"\\\n        Timeout for graceful workers restart.\n\n        After receiving a restart signal, workers have this much time to finish\n        serving requests. Workers still alive after the timeout (starting from\n        the receipt of the restart signal) are force killed.\n        \"\"\"\n\n\nclass Keepalive(Setting):\n    name = \"keepalive\"\n    section = \"Worker Processes\"\n    cli = [\"--keep-alive\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 2\n    desc = \"\"\"\\\n        The number of seconds to wait for requests on a Keep-Alive connection.\n\n        Generally set in the 1-5 seconds range for servers with direct connection\n        to the client (e.g. when you don't have separate load balancer). When\n        Gunicorn is deployed behind a load balancer, it often makes sense to\n        set this to a higher value.\n\n        .. note::\n           ``sync`` worker does not support persistent connections and will\n           ignore this option.\n        \"\"\"\n\n\nclass LimitRequestLine(Setting):\n    name = \"limit_request_line\"\n    section = \"Security\"\n    cli = [\"--limit-request-line\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 4094\n    desc = \"\"\"\\\n        The maximum size of HTTP request line in bytes.\n\n        This parameter is used to limit the allowed size of a client's\n        HTTP request-line. Since the request-line consists of the HTTP\n        method, URI, and protocol version, this directive places a\n        restriction on the length of a request-URI allowed for a request\n        on the server. A server needs this value to be large enough to\n        hold any of its resource names, including any information that\n        might be passed in the query part of a GET request. Value is a number\n        from 0 (unlimited) to 8190.\n\n        This parameter can be used to prevent any DDOS attack.\n        \"\"\"\n\n\nclass LimitRequestFields(Setting):\n    name = \"limit_request_fields\"\n    section = \"Security\"\n    cli = [\"--limit-request-fields\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 100\n    desc = \"\"\"\\\n        Limit the number of HTTP headers fields in a request.\n\n        This parameter is used to limit the number of headers in a request to\n        prevent DDOS attack. Used with the *limit_request_field_size* it allows\n        more safety. By default this value is 100 and can't be larger than\n        32768.\n        \"\"\"\n\n\nclass LimitRequestFieldSize(Setting):\n    name = \"limit_request_field_size\"\n    section = \"Security\"\n    cli = [\"--limit-request-field_size\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 8190\n    desc = \"\"\"\\\n        Limit the allowed size of an HTTP request header field.\n\n        Value is a positive number or 0. Setting it to 0 will allow unlimited\n        header field sizes.\n\n        .. warning::\n           Setting this parameter to a very high or unlimited value can open\n           up for DDOS attacks.\n        \"\"\"\n\n\nclass Reload(Setting):\n    name = \"reload\"\n    section = 'Debugging'\n    cli = ['--reload']\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n\n    desc = '''\\\n        Restart workers when code changes.\n\n        This setting is intended for development. It will cause workers to be\n        restarted whenever application code changes.\n\n        The reloader is incompatible with application preloading. When using a\n        paste configuration be sure that the server block does not import any\n        application code or the reload will not work as designed.\n\n        The default behavior is to attempt inotify with a fallback to file\n        system polling. Generally, inotify should be preferred if available\n        because it consumes less system resources.\n\n        .. note::\n           In order to use the inotify reloader, you must have the ``inotify``\n           package installed.\n        '''\n\n\nclass ReloadEngine(Setting):\n    name = \"reload_engine\"\n    section = \"Debugging\"\n    cli = [\"--reload-engine\"]\n    meta = \"STRING\"\n    validator = validate_reload_engine\n    default = \"auto\"\n    desc = \"\"\"\\\n        The implementation that should be used to power :ref:`reload`.\n\n        Valid engines are:\n\n        * ``'auto'``\n        * ``'poll'``\n        * ``'inotify'`` (requires inotify)\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass ReloadExtraFiles(Setting):\n    name = \"reload_extra_files\"\n    action = \"append\"\n    section = \"Debugging\"\n    cli = [\"--reload-extra-file\"]\n    meta = \"FILES\"\n    validator = validate_list_of_existing_files\n    default = []\n    desc = \"\"\"\\\n        Extends :ref:`reload` option to also watch and reload on additional files\n        (e.g., templates, configurations, specifications, etc.).\n\n        .. versionadded:: 19.8\n        \"\"\"\n\n\nclass Spew(Setting):\n    name = \"spew\"\n    section = \"Debugging\"\n    cli = [\"--spew\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Install a trace function that spews every line executed by the server.\n\n        This is the nuclear option.\n        \"\"\"\n\n\nclass ConfigCheck(Setting):\n    name = \"check_config\"\n    section = \"Debugging\"\n    cli = [\"--check-config\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Check the configuration and exit. The exit status is 0 if the\n        configuration is correct, and 1 if the configuration is incorrect.\n        \"\"\"\n\n\nclass PrintConfig(Setting):\n    name = \"print_config\"\n    section = \"Debugging\"\n    cli = [\"--print-config\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Print the configuration settings as fully resolved. Implies :ref:`check-config`.\n        \"\"\"\n\n\nclass PreloadApp(Setting):\n    name = \"preload_app\"\n    section = \"Server Mechanics\"\n    cli = [\"--preload\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Load application code before the worker processes are forked.\n\n        By preloading an application you can save some RAM resources as well as\n        speed up server boot times. Although, if you defer application loading\n        to each worker process, you can reload your application code easily by\n        restarting workers.\n        \"\"\"\n\n\nclass Sendfile(Setting):\n    name = \"sendfile\"\n    section = \"Server Mechanics\"\n    cli = [\"--no-sendfile\"]\n    validator = validate_bool\n    action = \"store_const\"\n    const = False\n\n    desc = \"\"\"\\\n        Disables the use of ``sendfile()``.\n\n        If not set, the value of the ``SENDFILE`` environment variable is used\n        to enable or disable its usage.\n\n        .. versionadded:: 19.2\n        .. versionchanged:: 19.4\n           Swapped ``--sendfile`` with ``--no-sendfile`` to actually allow\n           disabling.\n        .. versionchanged:: 19.6\n           added support for the ``SENDFILE`` environment variable\n        \"\"\"\n\n\nclass ReusePort(Setting):\n    name = \"reuse_port\"\n    section = \"Server Mechanics\"\n    cli = [\"--reuse-port\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n\n    desc = \"\"\"\\\n        Set the ``SO_REUSEPORT`` flag on the listening socket.\n\n        .. versionadded:: 19.8\n        \"\"\"\n\n\nclass Chdir(Setting):\n    name = \"chdir\"\n    section = \"Server Mechanics\"\n    cli = [\"--chdir\"]\n    validator = validate_chdir\n    default = util.getcwd()\n    default_doc = \"``'.'``\"\n    desc = \"\"\"\\\n        Change directory to specified directory before loading apps.\n        \"\"\"\n\n\nclass Daemon(Setting):\n    name = \"daemon\"\n    section = \"Server Mechanics\"\n    cli = [\"-D\", \"--daemon\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Daemonize the Gunicorn process.\n\n        Detaches the server from the controlling terminal and enters the\n        background.\n        \"\"\"\n\n\nclass Env(Setting):\n    name = \"raw_env\"\n    action = \"append\"\n    section = \"Server Mechanics\"\n    cli = [\"-e\", \"--env\"]\n    meta = \"ENV\"\n    validator = validate_list_string\n    default = []\n\n    desc = \"\"\"\\\n        Set environment variables in the execution environment.\n\n        Should be a list of strings in the ``key=value`` format.\n\n        For example on the command line:\n\n        .. code-block:: console\n\n            $ gunicorn -b 127.0.0.1:8000 --env FOO=1 test:app\n\n        Or in the configuration file:\n\n        .. code-block:: python\n\n            raw_env = [\"FOO=1\"]\n        \"\"\"\n\n\nclass Pidfile(Setting):\n    name = \"pidfile\"\n    section = \"Server Mechanics\"\n    cli = [\"-p\", \"--pid\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A filename to use for the PID file.\n\n        If not set, no PID file will be written.\n        \"\"\"\n\n\nclass WorkerTmpDir(Setting):\n    name = \"worker_tmp_dir\"\n    section = \"Server Mechanics\"\n    cli = [\"--worker-tmp-dir\"]\n    meta = \"DIR\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A directory to use for the worker heartbeat temporary file.\n\n        If not set, the default temporary directory will be used.\n\n        .. note::\n           The current heartbeat system involves calling ``os.fchmod`` on\n           temporary file handlers and may block a worker for arbitrary time\n           if the directory is on a disk-backed filesystem.\n\n           See :ref:`blocking-os-fchmod` for more detailed information\n           and a solution for avoiding this problem.\n        \"\"\"\n\n\nclass User(Setting):\n    name = \"user\"\n    section = \"Server Mechanics\"\n    cli = [\"-u\", \"--user\"]\n    meta = \"USER\"\n    validator = validate_user\n    default = os.geteuid()\n    default_doc = \"``os.geteuid()``\"\n    desc = \"\"\"\\\n        Switch worker processes to run as this user.\n\n        A valid user id (as an integer) or the name of a user that can be\n        retrieved with a call to ``pwd.getpwnam(value)`` or ``None`` to not\n        change the worker process user.\n        \"\"\"\n\n\nclass Group(Setting):\n    name = \"group\"\n    section = \"Server Mechanics\"\n    cli = [\"-g\", \"--group\"]\n    meta = \"GROUP\"\n    validator = validate_group\n    default = os.getegid()\n    default_doc = \"``os.getegid()``\"\n    desc = \"\"\"\\\n        Switch worker process to run as this group.\n\n        A valid group id (as an integer) or the name of a user that can be\n        retrieved with a call to ``pwd.getgrnam(value)`` or ``None`` to not\n        change the worker processes group.\n        \"\"\"\n\n\nclass Umask(Setting):\n    name = \"umask\"\n    section = \"Server Mechanics\"\n    cli = [\"-m\", \"--umask\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = auto_int\n    default = 0\n    desc = \"\"\"\\\n        A bit mask for the file mode on files written by Gunicorn.\n\n        Note that this affects unix socket permissions.\n\n        A valid value for the ``os.umask(mode)`` call or a string compatible\n        with ``int(value, 0)`` (``0`` means Python guesses the base, so values\n        like ``0``, ``0xFF``, ``0022`` are valid for decimal, hex, and octal\n        representations)\n        \"\"\"\n\n\nclass Initgroups(Setting):\n    name = \"initgroups\"\n    section = \"Server Mechanics\"\n    cli = [\"--initgroups\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n\n    desc = \"\"\"\\\n        If true, set the worker process's group access list with all of the\n        groups of which the specified username is a member, plus the specified\n        group id.\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass TmpUploadDir(Setting):\n    name = \"tmp_upload_dir\"\n    section = \"Server Mechanics\"\n    meta = \"DIR\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        Directory to store temporary request data as they are read.\n\n        This may disappear in the near future.\n\n        This path should be writable by the process permissions set for Gunicorn\n        workers. If not specified, Gunicorn will choose a system generated\n        temporary directory.\n        \"\"\"\n\n\nclass SecureSchemeHeader(Setting):\n    name = \"secure_scheme_headers\"\n    section = \"Server Mechanics\"\n    validator = validate_dict\n    default = {\n        \"X-FORWARDED-PROTOCOL\": \"ssl\",\n        \"X-FORWARDED-PROTO\": \"https\",\n        \"X-FORWARDED-SSL\": \"on\"\n    }\n    desc = \"\"\"\\\n\n        A dictionary containing headers and values that the front-end proxy\n        uses to indicate HTTPS requests. If the source IP is permitted by\n        ``forwarded-allow-ips`` (below), *and* at least one request header matches\n        a key-value pair listed in this dictionary, then Gunicorn will set\n        ``wsgi.url_scheme`` to ``https``, so your application can tell that the\n        request is secure.\n\n        If the other headers listed in this dictionary are not present in the request, they will be ignored,\n        but if the other headers are present and do not match the provided values, then\n        the request will fail to parse. See the note below for more detailed examples of this behaviour.\n\n        The dictionary should map upper-case header names to exact string\n        values. The value comparisons are case-sensitive, unlike the header\n        names, so make sure they're exactly what your front-end proxy sends\n        when handling HTTPS requests.\n\n        It is important that your front-end proxy configuration ensures that\n        the headers defined here can not be passed directly from the client.\n        \"\"\"\n\n\nclass ForwardedAllowIPS(Setting):\n    name = \"forwarded_allow_ips\"\n    section = \"Server Mechanics\"\n    cli = [\"--forwarded-allow-ips\"]\n    meta = \"STRING\"\n    validator = validate_string_to_list\n    default = os.environ.get(\"FORWARDED_ALLOW_IPS\", \"127.0.0.1\")\n    desc = \"\"\"\\\n        Front-end's IPs from which allowed to handle set secure headers.\n        (comma separate).\n\n        Set to ``*`` to disable checking of Front-end IPs (useful for setups\n        where you don't know in advance the IP address of Front-end, but\n        you still trust the environment).\n\n        By default, the value of the ``FORWARDED_ALLOW_IPS`` environment\n        variable. If it is not defined, the default is ``\"127.0.0.1\"``.\n\n        .. note::\n\n            The interplay between the request headers, the value of ``forwarded_allow_ips``, and the value of\n            ``secure_scheme_headers`` is complex. Various scenarios are documented below to further elaborate.\n            In each case, we have a request from the remote address 134.213.44.18, and the default value of\n            ``secure_scheme_headers``:\n\n            .. code::\n\n                secure_scheme_headers = {\n                    'X-FORWARDED-PROTOCOL': 'ssl',\n                    'X-FORWARDED-PROTO': 'https',\n                    'X-FORWARDED-SSL': 'on'\n                }\n\n\n            .. list-table::\n                :header-rows: 1\n                :align: center\n                :widths: auto\n\n                * - ``forwarded-allow-ips``\n                  - Secure Request Headers\n                  - Result\n                  - Explanation\n                * - .. code::\n\n                        [\"127.0.0.1\"]\n                  - .. code::\n\n                        X-Forwarded-Proto: https\n                  - .. code::\n\n                        wsgi.url_scheme = \"http\"\n                  - IP address was not allowed\n                * - .. code::\n\n                        \"*\"\n                  - <none>\n                  - .. code::\n\n                        wsgi.url_scheme = \"http\"\n                  - IP address allowed, but no secure headers provided\n                * - .. code::\n\n                        \"*\"\n                  - .. code::\n\n                        X-Forwarded-Proto: https\n                  - .. code::\n\n                        wsgi.url_scheme = \"https\"\n                  - IP address allowed, one request header matched\n                * - .. code::\n\n                        [\"134.213.44.18\"]\n                  - .. code::\n\n                        X-Forwarded-Ssl: on\n                        X-Forwarded-Proto: http\n                  - ``InvalidSchemeHeaders()`` raised\n                  - IP address allowed, but the two secure headers disagreed on if HTTPS was used\n\n\n        \"\"\"\n\n\nclass AccessLog(Setting):\n    name = \"accesslog\"\n    section = \"Logging\"\n    cli = [\"--access-logfile\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        The Access log file to write to.\n\n        ``'-'`` means log to stdout.\n        \"\"\"\n\n\nclass DisableRedirectAccessToSyslog(Setting):\n    name = \"disable_redirect_access_to_syslog\"\n    section = \"Logging\"\n    cli = [\"--disable-redirect-access-to-syslog\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n    desc = \"\"\"\\\n    Disable redirect access logs to syslog.\n\n    .. versionadded:: 19.8\n    \"\"\"\n\n\nclass AccessLogFormat(Setting):\n    name = \"access_log_format\"\n    section = \"Logging\"\n    cli = [\"--access-logformat\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = '%(h)s %(l)s %(u)s %(t)s \"%(r)s\" %(s)s %(b)s \"%(f)s\" \"%(a)s\"'\n    desc = \"\"\"\\\n        The access log format.\n\n        ===========  ===========\n        Identifier   Description\n        ===========  ===========\n        h            remote address\n        l            ``'-'``\n        u            user name\n        t            date of the request\n        r            status line (e.g. ``GET / HTTP/1.1``)\n        m            request method\n        U            URL path without query string\n        q            query string\n        H            protocol\n        s            status\n        B            response length\n        b            response length or ``'-'`` (CLF format)\n        f            referer\n        a            user agent\n        T            request time in seconds\n        M            request time in milliseconds\n        D            request time in microseconds\n        L            request time in decimal seconds\n        p            process ID\n        {header}i    request header\n        {header}o    response header\n        {variable}e  environment variable\n        ===========  ===========\n\n        Use lowercase for header and environment variable names, and put\n        ``{...}x`` names inside ``%(...)s``. For example::\n\n            %({x-forwarded-for}i)s\n        \"\"\"\n\n\nclass ErrorLog(Setting):\n    name = \"errorlog\"\n    section = \"Logging\"\n    cli = [\"--error-logfile\", \"--log-file\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = '-'\n    desc = \"\"\"\\\n        The Error log file to write to.\n\n        Using ``'-'`` for FILE makes gunicorn log to stderr.\n\n        .. versionchanged:: 19.2\n           Log to stderr by default.\n\n        \"\"\"\n\n\nclass Loglevel(Setting):\n    name = \"loglevel\"\n    section = \"Logging\"\n    cli = [\"--log-level\"]\n    meta = \"LEVEL\"\n    validator = validate_string\n    default = \"info\"\n    desc = \"\"\"\\\n        The granularity of Error log outputs.\n\n        Valid level names are:\n\n        * ``'debug'``\n        * ``'info'``\n        * ``'warning'``\n        * ``'error'``\n        * ``'critical'``\n        \"\"\"\n\n\nclass CaptureOutput(Setting):\n    name = \"capture_output\"\n    section = \"Logging\"\n    cli = [\"--capture-output\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n    desc = \"\"\"\\\n        Redirect stdout/stderr to specified file in :ref:`errorlog`.\n\n        .. versionadded:: 19.6\n        \"\"\"\n\n\nclass LoggerClass(Setting):\n    name = \"logger_class\"\n    section = \"Logging\"\n    cli = [\"--logger-class\"]\n    meta = \"STRING\"\n    validator = validate_class\n    default = \"gunicorn.glogging.Logger\"\n    desc = \"\"\"\\\n        The logger you want to use to log events in Gunicorn.\n\n        The default class (``gunicorn.glogging.Logger``) handles most\n        normal usages in logging. It provides error and access logging.\n\n        You can provide your own logger by giving Gunicorn a Python path to a\n        class that quacks like ``gunicorn.glogging.Logger``.\n        \"\"\"\n\n\nclass LogConfig(Setting):\n    name = \"logconfig\"\n    section = \"Logging\"\n    cli = [\"--log-config\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    The log config file to use.\n    Gunicorn uses the standard Python logging module's Configuration\n    file format.\n    \"\"\"\n\n\nclass LogConfigDict(Setting):\n    name = \"logconfig_dict\"\n    section = \"Logging\"\n    validator = validate_dict\n    default = {}\n    desc = \"\"\"\\\n    The log config dictionary to use, using the standard Python\n    logging module's dictionary configuration format. This option\n    takes precedence over the :ref:`logconfig` and :ref:`logConfigJson` options,\n    which uses the older file configuration format and JSON\n    respectively.\n\n    Format: https://docs.python.org/3/library/logging.config.html#logging.config.dictConfig\n\n    For more context you can look at the default configuration dictionary for logging,\n    which can be found at ``gunicorn.glogging.CONFIG_DEFAULTS``.\n\n    .. versionadded:: 19.8\n    \"\"\"\n\n\nclass LogConfigJson(Setting):\n    name = \"logconfig_json\"\n    section = \"Logging\"\n    cli = [\"--log-config-json\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    The log config to read config from a JSON file\n\n    Format: https://docs.python.org/3/library/logging.config.html#logging.config.jsonConfig\n\n    .. versionadded:: 20.0\n    \"\"\"\n\n\nclass SyslogTo(Setting):\n    name = \"syslog_addr\"\n    section = \"Logging\"\n    cli = [\"--log-syslog-to\"]\n    meta = \"SYSLOG_ADDR\"\n    validator = validate_string\n\n    if PLATFORM == \"darwin\":\n        default = \"unix:///var/run/syslog\"\n    elif PLATFORM in ('freebsd', 'dragonfly', ):\n        default = \"unix:///var/run/log\"\n    elif PLATFORM == \"openbsd\":\n        default = \"unix:///dev/log\"\n    else:\n        default = \"udp://localhost:514\"\n\n    desc = \"\"\"\\\n    Address to send syslog messages.\n\n    Address is a string of the form:\n\n    * ``unix://PATH#TYPE`` : for unix domain socket. ``TYPE`` can be ``stream``\n      for the stream driver or ``dgram`` for the dgram driver.\n      ``stream`` is the default.\n    * ``udp://HOST:PORT`` : for UDP sockets\n    * ``tcp://HOST:PORT`` : for TCP sockets\n\n    \"\"\"\n\n\nclass Syslog(Setting):\n    name = \"syslog\"\n    section = \"Logging\"\n    cli = [\"--log-syslog\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n    desc = \"\"\"\\\n    Send *Gunicorn* logs to syslog.\n\n    .. versionchanged:: 19.8\n       You can now disable sending access logs by using the\n       :ref:`disable-redirect-access-to-syslog` setting.\n    \"\"\"\n\n\nclass SyslogPrefix(Setting):\n    name = \"syslog_prefix\"\n    section = \"Logging\"\n    cli = [\"--log-syslog-prefix\"]\n    meta = \"SYSLOG_PREFIX\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    Makes Gunicorn use the parameter as program-name in the syslog entries.\n\n    All entries will be prefixed by ``gunicorn.<prefix>``. By default the\n    program name is the name of the process.\n    \"\"\"\n\n\nclass SyslogFacility(Setting):\n    name = \"syslog_facility\"\n    section = \"Logging\"\n    cli = [\"--log-syslog-facility\"]\n    meta = \"SYSLOG_FACILITY\"\n    validator = validate_string\n    default = \"user\"\n    desc = \"\"\"\\\n    Syslog facility name\n    \"\"\"\n\n\nclass EnableStdioInheritance(Setting):\n    name = \"enable_stdio_inheritance\"\n    section = \"Logging\"\n    cli = [\"-R\", \"--enable-stdio-inheritance\"]\n    validator = validate_bool\n    default = False\n    action = \"store_true\"\n    desc = \"\"\"\\\n    Enable stdio inheritance.\n\n    Enable inheritance for stdio file descriptors in daemon mode.\n\n    Note: To disable the Python stdout buffering, you can to set the user\n    environment variable ``PYTHONUNBUFFERED`` .\n    \"\"\"\n\n\n# statsD monitoring\nclass StatsdHost(Setting):\n    name = \"statsd_host\"\n    section = \"Logging\"\n    cli = [\"--statsd-host\"]\n    meta = \"STATSD_ADDR\"\n    default = None\n    validator = validate_statsd_address\n    desc = \"\"\"\\\n    The address of the StatsD server to log to.\n\n    Address is a string of the form:\n\n    * ``unix://PATH`` : for a unix domain socket.\n    * ``HOST:PORT`` : for a network address\n\n    .. versionadded:: 19.1\n    \"\"\"\n\n\n# Datadog Statsd (dogstatsd) tags. https://docs.datadoghq.com/developers/dogstatsd/\nclass DogstatsdTags(Setting):\n    name = \"dogstatsd_tags\"\n    section = \"Logging\"\n    cli = [\"--dogstatsd-tags\"]\n    meta = \"DOGSTATSD_TAGS\"\n    default = \"\"\n    validator = validate_string\n    desc = \"\"\"\\\n    A comma-delimited list of datadog statsd (dogstatsd) tags to append to\n    statsd metrics.\n\n    .. versionadded:: 20\n    \"\"\"\n\n\nclass StatsdPrefix(Setting):\n    name = \"statsd_prefix\"\n    section = \"Logging\"\n    cli = [\"--statsd-prefix\"]\n    meta = \"STATSD_PREFIX\"\n    default = \"\"\n    validator = validate_string\n    desc = \"\"\"\\\n    Prefix to use when emitting statsd metrics (a trailing ``.`` is added,\n    if not provided).\n\n    .. versionadded:: 19.2\n    \"\"\"\n\n\nclass Procname(Setting):\n    name = \"proc_name\"\n    section = \"Process Naming\"\n    cli = [\"-n\", \"--name\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A base to use with setproctitle for process naming.\n\n        This affects things like ``ps`` and ``top``. If you're going to be\n        running more than one instance of Gunicorn you'll probably want to set a\n        name to tell them apart. This requires that you install the setproctitle\n        module.\n\n        If not set, the *default_proc_name* setting will be used.\n        \"\"\"\n\n\nclass DefaultProcName(Setting):\n    name = \"default_proc_name\"\n    section = \"Process Naming\"\n    validator = validate_string\n    default = \"gunicorn\"\n    desc = \"\"\"\\\n        Internal setting that is adjusted for each type of application.\n        \"\"\"\n\n\nclass PythonPath(Setting):\n    name = \"pythonpath\"\n    section = \"Server Mechanics\"\n    cli = [\"--pythonpath\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A comma-separated list of directories to add to the Python path.\n\n        e.g.\n        ``'/home/djangoprojects/myproject,/home/python/mylibrary'``.\n        \"\"\"\n\n\nclass Paste(Setting):\n    name = \"paste\"\n    section = \"Server Mechanics\"\n    cli = [\"--paste\", \"--paster\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        Load a PasteDeploy config file. The argument may contain a ``#``\n        symbol followed by the name of an app section from the config file,\n        e.g. ``production.ini#admin``.\n\n        At this time, using alternate server blocks is not supported. Use the\n        command line arguments to control server configuration instead.\n        \"\"\"\n\n\nclass OnStarting(Setting):\n    name = \"on_starting\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def on_starting(server):\n        pass\n    default = staticmethod(on_starting)\n    desc = \"\"\"\\\n        Called just before the master process is initialized.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass OnReload(Setting):\n    name = \"on_reload\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def on_reload(server):\n        pass\n    default = staticmethod(on_reload)\n    desc = \"\"\"\\\n        Called to recycle workers during a reload via SIGHUP.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass WhenReady(Setting):\n    name = \"when_ready\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def when_ready(server):\n        pass\n    default = staticmethod(when_ready)\n    desc = \"\"\"\\\n        Called just after the server is started.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass Prefork(Setting):\n    name = \"pre_fork\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def pre_fork(server, worker):\n        pass\n    default = staticmethod(pre_fork)\n    desc = \"\"\"\\\n        Called just before a worker is forked.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        new Worker.\n        \"\"\"\n\n\nclass Postfork(Setting):\n    name = \"post_fork\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def post_fork(server, worker):\n        pass\n    default = staticmethod(post_fork)\n    desc = \"\"\"\\\n        Called just after a worker has been forked.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        new Worker.\n        \"\"\"\n\n\nclass PostWorkerInit(Setting):\n    name = \"post_worker_init\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def post_worker_init(worker):\n        pass\n\n    default = staticmethod(post_worker_init)\n    desc = \"\"\"\\\n        Called just after a worker has initialized the application.\n\n        The callable needs to accept one instance variable for the initialized\n        Worker.\n        \"\"\"\n\n\nclass WorkerInt(Setting):\n    name = \"worker_int\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def worker_int(worker):\n        pass\n\n    default = staticmethod(worker_int)\n    desc = \"\"\"\\\n        Called just after a worker exited on SIGINT or SIGQUIT.\n\n        The callable needs to accept one instance variable for the initialized\n        Worker.\n        \"\"\"\n\n\nclass WorkerAbort(Setting):\n    name = \"worker_abort\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def worker_abort(worker):\n        pass\n\n    default = staticmethod(worker_abort)\n    desc = \"\"\"\\\n        Called when a worker received the SIGABRT signal.\n\n        This call generally happens on timeout.\n\n        The callable needs to accept one instance variable for the initialized\n        Worker.\n        \"\"\"\n\n\nclass PreExec(Setting):\n    name = \"pre_exec\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def pre_exec(server):\n        pass\n    default = staticmethod(pre_exec)\n    desc = \"\"\"\\\n        Called just before a new master process is forked.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass PreRequest(Setting):\n    name = \"pre_request\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def pre_request(worker, req):\n        worker.log.debug(\"%s %s\", req.method, req.path)\n    default = staticmethod(pre_request)\n    desc = \"\"\"\\\n        Called just before a worker processes the request.\n\n        The callable needs to accept two instance variables for the Worker and\n        the Request.\n        \"\"\"\n\n\nclass PostRequest(Setting):\n    name = \"post_request\"\n    section = \"Server Hooks\"\n    validator = validate_post_request\n    type = callable\n\n    def post_request(worker, req, environ, resp):\n        pass\n    default = staticmethod(post_request)\n    desc = \"\"\"\\\n        Called after a worker processes the request.\n\n        The callable needs to accept two instance variables for the Worker and\n        the Request.\n        \"\"\"\n\n\nclass ChildExit(Setting):\n    name = \"child_exit\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def child_exit(server, worker):\n        pass\n    default = staticmethod(child_exit)\n    desc = \"\"\"\\\n        Called just after a worker has been exited, in the master process.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        the just-exited Worker.\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass WorkerExit(Setting):\n    name = \"worker_exit\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def worker_exit(server, worker):\n        pass\n    default = staticmethod(worker_exit)\n    desc = \"\"\"\\\n        Called just after a worker has been exited, in the worker process.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        the just-exited Worker.\n        \"\"\"\n\n\nclass NumWorkersChanged(Setting):\n    name = \"nworkers_changed\"\n    section = \"Server Hooks\"\n    validator = validate_callable(3)\n    type = callable\n\n    def nworkers_changed(server, new_value, old_value):\n        pass\n    default = staticmethod(nworkers_changed)\n    desc = \"\"\"\\\n        Called just after *num_workers* has been changed.\n\n        The callable needs to accept an instance variable of the Arbiter and\n        two integers of number of workers after and before change.\n\n        If the number of workers is set for the first time, *old_value* would\n        be ``None``.\n        \"\"\"\n\n\nclass OnExit(Setting):\n    name = \"on_exit\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n\n    def on_exit(server):\n        pass\n\n    default = staticmethod(on_exit)\n    desc = \"\"\"\\\n        Called just before exiting Gunicorn.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass NewSSLContext(Setting):\n    name = \"ssl_context\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def ssl_context(config, default_ssl_context_factory):\n        return default_ssl_context_factory()\n\n    default = staticmethod(ssl_context)\n    desc = \"\"\"\\\n        Called when SSLContext is needed.\n\n        Allows customizing SSL context.\n\n        The callable needs to accept an instance variable for the Config and\n        a factory function that returns default SSLContext which is initialized\n        with certificates, private key, cert_reqs, and ciphers according to\n        config and can be further customized by the callable.\n        The callable needs to return SSLContext object.\n\n        Following example shows a configuration file that sets the minimum TLS version to 1.3:\n\n        .. code-block:: python\n\n            def ssl_context(conf, default_ssl_context_factory):\n                import ssl\n                context = default_ssl_context_factory()\n                context.minimum_version = ssl.TLSVersion.TLSv1_3\n                return context\n\n        .. versionadded:: 20.2\n        \"\"\"\n\n\nclass ProxyProtocol(Setting):\n    name = \"proxy_protocol\"\n    section = \"Server Mechanics\"\n    cli = [\"--proxy-protocol\"]\n    validator = validate_bool\n    default = False\n    action = \"store_true\"\n    desc = \"\"\"\\\n        Enable detect PROXY protocol (PROXY mode).\n\n        Allow using HTTP and Proxy together. It may be useful for work with\n        stunnel as HTTPS frontend and Gunicorn as HTTP server.\n\n        PROXY protocol: http://haproxy.1wt.eu/download/1.5/doc/proxy-protocol.txt\n\n        Example for stunnel config::\n\n            [https]\n            protocol = proxy\n            accept  = 443\n            connect = 80\n            cert = /etc/ssl/certs/stunnel.pem\n            key = /etc/ssl/certs/stunnel.key\n        \"\"\"\n\n\nclass ProxyAllowFrom(Setting):\n    name = \"proxy_allow_ips\"\n    section = \"Server Mechanics\"\n    cli = [\"--proxy-allow-from\"]\n    validator = validate_string_to_list\n    default = \"127.0.0.1\"\n    desc = \"\"\"\\\n        Front-end's IPs from which allowed accept proxy requests (comma separate).\n\n        Set to ``*`` to disable checking of Front-end IPs (useful for setups\n        where you don't know in advance the IP address of Front-end, but\n        you still trust the environment)\n        \"\"\"\n\n\nclass KeyFile(Setting):\n    name = \"keyfile\"\n    section = \"SSL\"\n    cli = [\"--keyfile\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    SSL key file\n    \"\"\"\n\n\nclass CertFile(Setting):\n    name = \"certfile\"\n    section = \"SSL\"\n    cli = [\"--certfile\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    SSL certificate file\n    \"\"\"\n\n\nclass SSLVersion(Setting):\n    name = \"ssl_version\"\n    section = \"SSL\"\n    cli = [\"--ssl-version\"]\n    validator = validate_ssl_version\n\n    if hasattr(ssl, \"PROTOCOL_TLS\"):\n        default = ssl.PROTOCOL_TLS\n    else:\n        default = ssl.PROTOCOL_SSLv23\n\n    default = ssl.PROTOCOL_SSLv23\n    desc = \"\"\"\\\n    SSL version to use (see stdlib ssl module's).\n\n    .. deprecated:: 20.2\n       The option is deprecated and it is currently ignored. Use :ref:`ssl-context` instead.\n\n    ============= ============\n    --ssl-version Description\n    ============= ============\n    SSLv3         SSLv3 is not-secure and is strongly discouraged.\n    SSLv23        Alias for TLS. Deprecated in Python 3.6, use TLS.\n    TLS           Negotiate highest possible version between client/server.\n                  Can yield SSL. (Python 3.6+)\n    TLSv1         TLS 1.0\n    TLSv1_1       TLS 1.1 (Python 3.4+)\n    TLSv1_2       TLS 1.2 (Python 3.4+)\n    TLS_SERVER    Auto-negotiate the highest protocol version like TLS,\n                  but only support server-side SSLSocket connections.\n                  (Python 3.6+)\n    ============= ============\n\n    .. versionchanged:: 19.7\n       The default value has been changed from ``ssl.PROTOCOL_TLSv1`` to\n       ``ssl.PROTOCOL_SSLv23``.\n    .. versionchanged:: 20.0\n       This setting now accepts string names based on ``ssl.PROTOCOL_``\n       constants.\n    .. versionchanged:: 20.0.1\n       The default value has been changed from ``ssl.PROTOCOL_SSLv23`` to\n       ``ssl.PROTOCOL_TLS`` when Python >= 3.6 .\n    \"\"\"\n\n\nclass CertReqs(Setting):\n    name = \"cert_reqs\"\n    section = \"SSL\"\n    cli = [\"--cert-reqs\"]\n    validator = validate_pos_int\n    default = ssl.CERT_NONE\n    desc = \"\"\"\\\n    Whether client certificate is required (see stdlib ssl module's)\n\n    ===========  ===========================\n    --cert-reqs      Description\n    ===========  ===========================\n    `0`          no client veirifcation\n    `1`          ssl.CERT_OPTIONAL\n    `2`          ssl.CERT_REQUIRED\n    ===========  ===========================\n    \"\"\"\n\n\nclass CACerts(Setting):\n    name = \"ca_certs\"\n    section = \"SSL\"\n    cli = [\"--ca-certs\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    CA certificates file\n    \"\"\"\n\n\nclass SuppressRaggedEOFs(Setting):\n    name = \"suppress_ragged_eofs\"\n    section = \"SSL\"\n    cli = [\"--suppress-ragged-eofs\"]\n    action = \"store_true\"\n    default = True\n    validator = validate_bool\n    desc = \"\"\"\\\n    Suppress ragged EOFs (see stdlib ssl module's)\n    \"\"\"\n\n\nclass DoHandshakeOnConnect(Setting):\n    name = \"do_handshake_on_connect\"\n    section = \"SSL\"\n    cli = [\"--do-handshake-on-connect\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n    Whether to perform SSL handshake on socket connect (see stdlib ssl module's)\n    \"\"\"\n\n\nclass Ciphers(Setting):\n    name = \"ciphers\"\n    section = \"SSL\"\n    cli = [\"--ciphers\"]\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    SSL Cipher suite to use, in the format of an OpenSSL cipher list.\n\n    By default we use the default cipher list from Python's ``ssl`` module,\n    which contains ciphers considered strong at the time of each Python\n    release.\n\n    As a recommended alternative, the Open Web App Security Project (OWASP)\n    offers `a vetted set of strong cipher strings rated A+ to C-\n    <https://www.owasp.org/index.php/TLS_Cipher_String_Cheat_Sheet>`_.\n    OWASP provides details on user-agent compatibility at each security level.\n\n    See the `OpenSSL Cipher List Format Documentation\n    <https://www.openssl.org/docs/manmaster/man1/ciphers.html#CIPHER-LIST-FORMAT>`_\n    for details on the format of an OpenSSL cipher list.\n    \"\"\"\n\n\nclass PasteGlobalConf(Setting):\n    name = \"raw_paste_global_conf\"\n    action = \"append\"\n    section = \"Server Mechanics\"\n    cli = [\"--paste-global\"]\n    meta = \"CONF\"\n    validator = validate_list_string\n    default = []\n\n    desc = \"\"\"\\\n        Set a PasteDeploy global config variable in ``key=value`` form.\n\n        The option can be specified multiple times.\n\n        The variables are passed to the the PasteDeploy entrypoint. Example::\n\n            $ gunicorn -b 127.0.0.1:8000 --paste development.ini --paste-global FOO=1 --paste-global BAR=2\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass StripHeaderSpaces(Setting):\n    name = \"strip_header_spaces\"\n    section = \"Server Mechanics\"\n    cli = [\"--strip-header-spaces\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Strip spaces present between the header name and the the ``:``.\n\n        This is known to induce vulnerabilities and is not compliant with the HTTP/1.1 standard.\n        See https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn.\n\n        Use with care and only if necessary.\n        \"\"\"\n", "cross_context": [{"gunicorn.util": "# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\nimport ast\nimport email.utils\nimport errno\nimport fcntl\nimport html\nimport importlib\nimport inspect\nimport io\nimport logging\nimport os\nimport pwd\nimport random\nimport re\nimport socket\nimport sys\nimport textwrap\nimport time\nimport traceback\nimport warnings\n\ntry:\n    import importlib.metadata as importlib_metadata\nexcept (ModuleNotFoundError, ImportError):\n    import importlib_metadata\n\nfrom gunicorn.errors import AppImportError\nfrom gunicorn.workers import SUPPORTED_WORKERS\nimport urllib.parse\n\nREDIRECT_TO = getattr(os, 'devnull', '/dev/null')\n\n# Server and Date aren't technically hop-by-hop\n# headers, but they are in the purview of the\n# origin server which the WSGI spec says we should\n# act like. So we drop them and add our own.\n#\n# In the future, concatenation server header values\n# might be better, but nothing else does it and\n# dropping them is easier.\nhop_headers = set(\"\"\"\n    connection keep-alive proxy-authenticate proxy-authorization\n    te trailers transfer-encoding upgrade\n    server date\n    \"\"\".split())\n\ntry:\n    from setproctitle import setproctitle\n\n    def _setproctitle(title):\n        setproctitle(\"gunicorn: %s\" % title)\nexcept ImportError:\n    def _setproctitle(title):\n        pass\n\n\ndef load_entry_point(distribution, group, name):\n    dist_obj = importlib_metadata.distribution(distribution)\n    eps = [ep for ep in dist_obj.entry_points\n           if ep.group == group and ep.name == name]\n    if not eps:\n        raise ImportError(\"Entry point %r not found\" % ((group, name),))\n    return eps[0].load()\n\n\ndef load_class(uri, default=\"gunicorn.workers.sync.SyncWorker\",\n               section=\"gunicorn.workers\"):\n    if inspect.isclass(uri):\n        return uri\n    if uri.startswith(\"egg:\"):\n        # uses entry points\n        entry_str = uri.split(\"egg:\")[1]\n        try:\n            dist, name = entry_str.rsplit(\"#\", 1)\n        except ValueError:\n            dist = entry_str\n            name = default\n\n        try:\n            return load_entry_point(dist, section, name)\n        except Exception:\n            exc = traceback.format_exc()\n            msg = \"class uri %r invalid or not found: \\n\\n[%s]\"\n            raise RuntimeError(msg % (uri, exc))\n    else:\n        components = uri.split('.')\n        if len(components) == 1:\n            while True:\n                if uri.startswith(\"#\"):\n                    uri = uri[1:]\n\n                if uri in SUPPORTED_WORKERS:\n                    components = SUPPORTED_WORKERS[uri].split(\".\")\n                    break\n\n                try:\n                    return load_entry_point(\n                        \"gunicorn\", section, uri\n                    )\n                except Exception:\n                    exc = traceback.format_exc()\n                    msg = \"class uri %r invalid or not found: \\n\\n[%s]\"\n                    raise RuntimeError(msg % (uri, exc))\n\n        klass = components.pop(-1)\n\n        try:\n            mod = importlib.import_module('.'.join(components))\n        except Exception:\n            exc = traceback.format_exc()\n            msg = \"class uri %r invalid or not found: \\n\\n[%s]\"\n            raise RuntimeError(msg % (uri, exc))\n        return getattr(mod, klass)\n\n\npositionals = (\n    inspect.Parameter.POSITIONAL_ONLY,\n    inspect.Parameter.POSITIONAL_OR_KEYWORD,\n)\n\n\ndef get_arity(f):\n    sig = inspect.signature(f)\n    arity = 0\n\n    for param in sig.parameters.values():\n        if param.kind in positionals:\n            arity += 1\n\n    return arity\n\n\ndef get_username(uid):\n    \"\"\" get the username for a user id\"\"\"\n    return pwd.getpwuid(uid).pw_name\n\n\ndef set_owner_process(uid, gid, initgroups=False):\n    \"\"\" set user and group of workers processes \"\"\"\n\n    if gid:\n        if uid:\n            try:\n                username = get_username(uid)\n            except KeyError:\n                initgroups = False\n\n        # versions of python < 2.6.2 don't manage unsigned int for\n        # groups like on osx or fedora\n        gid = abs(gid) & 0x7FFFFFFF\n\n        if initgroups:\n            os.initgroups(username, gid)\n        elif gid != os.getgid():\n            os.setgid(gid)\n\n    if uid and uid != os.getuid():\n        os.setuid(uid)\n\n\ndef chown(path, uid, gid):\n    os.chown(path, uid, gid)\n\n\nif sys.platform.startswith(\"win\"):\n    def _waitfor(func, pathname, waitall=False):\n        # Perform the operation\n        func(pathname)\n        # Now setup the wait loop\n        if waitall:\n            dirname = pathname\n        else:\n            dirname, name = os.path.split(pathname)\n            dirname = dirname or '.'\n        # Check for `pathname` to be removed from the filesystem.\n        # The exponential backoff of the timeout amounts to a total\n        # of ~1 second after which the deletion is probably an error\n        # anyway.\n        # Testing on a i7@4.3GHz shows that usually only 1 iteration is\n        # required when contention occurs.\n        timeout = 0.001\n        while timeout < 1.0:\n            # Note we are only testing for the existence of the file(s) in\n            # the contents of the directory regardless of any security or\n            # access rights.  If we have made it this far, we have sufficient\n            # permissions to do that much using Python's equivalent of the\n            # Windows API FindFirstFile.\n            # Other Windows APIs can fail or give incorrect results when\n            # dealing with files that are pending deletion.\n            L = os.listdir(dirname)\n            if not L if waitall else name in L:\n                return\n            # Increase the timeout and try again\n            time.sleep(timeout)\n            timeout *= 2\n        warnings.warn('tests may fail, delete still pending for ' + pathname,\n                      RuntimeWarning, stacklevel=4)\n\n    def _unlink(filename):\n        _waitfor(os.unlink, filename)\nelse:\n    _unlink = os.unlink\n\n\ndef unlink(filename):\n    try:\n        _unlink(filename)\n    except OSError as error:\n        # The filename need not exist.\n        if error.errno not in (errno.ENOENT, errno.ENOTDIR):\n            raise\n\n\ndef is_ipv6(addr):\n    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:  # not a valid address\n        return False\n    except ValueError:  # ipv6 not supported on this platform\n        return False\n    return True\n\n\ndef parse_address(netloc, default_port='8000'):\n    if re.match(r'unix:(//)?', netloc):\n        return re.split(r'unix:(//)?', netloc)[-1]\n\n    if netloc.startswith(\"fd://\"):\n        fd = netloc[5:]\n        try:\n            return int(fd)\n        except ValueError:\n            raise RuntimeError(\"%r is not a valid file descriptor.\" % fd) from None\n\n    if netloc.startswith(\"tcp://\"):\n        netloc = netloc.split(\"tcp://\")[1]\n    host, port = netloc, default_port\n\n    if '[' in netloc and ']' in netloc:\n        host = netloc.split(']')[0][1:]\n        port = (netloc.split(']:') + [default_port])[1]\n    elif ':' in netloc:\n        host, port = (netloc.split(':') + [default_port])[:2]\n    elif netloc == \"\":\n        host, port = \"0.0.0.0\", default_port\n\n    try:\n        port = int(port)\n    except ValueError:\n        raise RuntimeError(\"%r is not a valid port number.\" % port)\n\n    return host.lower(), port\n\n\ndef close_on_exec(fd):\n    flags = fcntl.fcntl(fd, fcntl.F_GETFD)\n    flags |= fcntl.FD_CLOEXEC\n    fcntl.fcntl(fd, fcntl.F_SETFD, flags)\n\n\ndef set_non_blocking(fd):\n    flags = fcntl.fcntl(fd, fcntl.F_GETFL) | os.O_NONBLOCK\n    fcntl.fcntl(fd, fcntl.F_SETFL, flags)\n\n\ndef close(sock):\n    try:\n        sock.close()\n    except socket.error:\n        pass\n\n\ntry:\n    from os import closerange\nexcept ImportError:\n    def closerange(fd_low, fd_high):\n        # Iterate through and close all file descriptors.\n        for fd in range(fd_low, fd_high):\n            try:\n                os.close(fd)\n            except OSError:  # ERROR, fd wasn't open to begin with (ignored)\n                pass\n\n\ndef write_chunk(sock, data):\n    if isinstance(data, str):\n        data = data.encode('utf-8')\n    chunk_size = \"%X\\r\\n\" % len(data)\n    chunk = b\"\".join([chunk_size.encode('utf-8'), data, b\"\\r\\n\"])\n    sock.sendall(chunk)\n\n\ndef write(sock, data, chunked=False):\n    if chunked:\n        return write_chunk(sock, data)\n    sock.sendall(data)\n\n\ndef write_nonblock(sock, data, chunked=False):\n    timeout = sock.gettimeout()\n    if timeout != 0.0:\n        try:\n            sock.setblocking(0)\n            return write(sock, data, chunked)\n        finally:\n            sock.setblocking(1)\n    else:\n        return write(sock, data, chunked)\n\n\ndef write_error(sock, status_int, reason, mesg):\n    html_error = textwrap.dedent(\"\"\"\\\n    <html>\n      <head>\n        <title>%(reason)s</title>\n      </head>\n      <body>\n        <h1><p>%(reason)s</p></h1>\n        %(mesg)s\n      </body>\n    </html>\n    \"\"\") % {\"reason\": reason, \"mesg\": html.escape(mesg)}\n\n    http = textwrap.dedent(\"\"\"\\\n    HTTP/1.1 %s %s\\r\n    Connection: close\\r\n    Content-Type: text/html\\r\n    Content-Length: %d\\r\n    \\r\n    %s\"\"\") % (str(status_int), reason, len(html_error), html_error)\n    write_nonblock(sock, http.encode('latin1'))\n\n\ndef _called_with_wrong_args(f):\n    \"\"\"Check whether calling a function raised a ``TypeError`` because\n    the call failed or because something in the function raised the\n    error.\n\n    :param f: The function that was called.\n    :return: ``True`` if the call failed.\n    \"\"\"\n    tb = sys.exc_info()[2]\n\n    try:\n        while tb is not None:\n            if tb.tb_frame.f_code is f.__code__:\n                # In the function, it was called successfully.\n                return False\n\n            tb = tb.tb_next\n\n        # Didn't reach the function.\n        return True\n    finally:\n        # Delete tb to break a circular reference in Python 2.\n        # https://docs.python.org/2/library/sys.html#sys.exc_info\n        del tb\n\n\ndef import_app(module):\n    parts = module.split(\":\", 1)\n    if len(parts) == 1:\n        obj = \"application\"\n    else:\n        module, obj = parts[0], parts[1]\n\n    try:\n        mod = importlib.import_module(module)\n    except ImportError:\n        if module.endswith(\".py\") and os.path.exists(module):\n            msg = \"Failed to find application, did you mean '%s:%s'?\"\n            raise ImportError(msg % (module.rsplit(\".\", 1)[0], obj))\n        raise\n\n    # Parse obj as a single expression to determine if it's a valid\n    # attribute name or function call.\n    try:\n        expression = ast.parse(obj, mode=\"eval\").body\n    except SyntaxError:\n        raise AppImportError(\n            \"Failed to parse %r as an attribute name or function call.\" % obj\n        )\n\n    if isinstance(expression, ast.Name):\n        name = expression.id\n        args = kwargs = None\n    elif isinstance(expression, ast.Call):\n        # Ensure the function name is an attribute name only.\n        if not isinstance(expression.func, ast.Name):\n            raise AppImportError(\"Function reference must be a simple name: %r\" % obj)\n\n        name = expression.func.id\n\n        # Parse the positional and keyword arguments as literals.\n        try:\n            args = [ast.literal_eval(arg) for arg in expression.args]\n            kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in expression.keywords}\n        except ValueError:\n            # literal_eval gives cryptic error messages, show a generic\n            # message with the full expression instead.\n            raise AppImportError(\n                \"Failed to parse arguments as literal values: %r\" % obj\n            )\n    else:\n        raise AppImportError(\n            \"Failed to parse %r as an attribute name or function call.\" % obj\n        )\n\n    is_debug = logging.root.level == logging.DEBUG\n    try:\n        app = getattr(mod, name)\n    except AttributeError:\n        if is_debug:\n            traceback.print_exception(*sys.exc_info())\n        raise AppImportError(\"Failed to find attribute %r in %r.\" % (name, module))\n\n    # If the expression was a function call, call the retrieved object\n    # to get the real application.\n    if args is not None:\n        try:\n            app = app(*args, **kwargs)\n        except TypeError as e:\n            # If the TypeError was due to bad arguments to the factory\n            # function, show Python's nice error message without a\n            # traceback.\n            if _called_with_wrong_args(app):\n                raise AppImportError(\n                    \"\".join(traceback.format_exception_only(TypeError, e)).strip()\n                )\n\n            # Otherwise it was raised from within the function, show the\n            # full traceback.\n            raise\n\n    if app is None:\n        raise AppImportError(\"Failed to find application object: %r\" % obj)\n\n    if not callable(app):\n        raise AppImportError(\"Application object must be callable.\")\n    return app\n\n\ndef getcwd():\n    # get current path, try to use PWD env first\n    try:\n        a = os.stat(os.environ['PWD'])\n        b = os.stat(os.getcwd())\n        if a.st_ino == b.st_ino and a.st_dev == b.st_dev:\n            cwd = os.environ['PWD']\n        else:\n            cwd = os.getcwd()\n    except Exception:\n        cwd = os.getcwd()\n    return cwd\n\n\ndef http_date(timestamp=None):\n    \"\"\"Return the current date and time formatted for a message header.\"\"\"\n    if timestamp is None:\n        timestamp = time.time()\n    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)\n    return s\n\n\ndef is_hoppish(header):\n    return header.lower().strip() in hop_headers\n\n\ndef daemonize(enable_stdio_inheritance=False):\n    \"\"\"\\\n    Standard daemonization of a process.\n    http://www.faqs.org/faqs/unix-faq/programmer/faq/ section 1.7\n    \"\"\"\n    if 'GUNICORN_FD' not in os.environ:\n        if os.fork():\n            os._exit(0)\n        os.setsid()\n\n        if os.fork():\n            os._exit(0)\n\n        os.umask(0o22)\n\n        # In both the following any file descriptors above stdin\n        # stdout and stderr are left untouched. The inheritance\n        # option simply allows one to have output go to a file\n        # specified by way of shell redirection when not wanting\n        # to use --error-log option.\n\n        if not enable_stdio_inheritance:\n            # Remap all of stdin, stdout and stderr on to\n            # /dev/null. The expectation is that users have\n            # specified the --error-log option.\n\n            closerange(0, 3)\n\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\n            # PEP 446, make fd for /dev/null inheritable\n            os.set_inheritable(fd_null, True)\n\n            # expect fd_null to be always 0 here, but in-case not ...\n            if fd_null != 0:\n                os.dup2(fd_null, 0)\n\n            os.dup2(fd_null, 1)\n            os.dup2(fd_null, 2)\n\n        else:\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\n\n            # Always redirect stdin to /dev/null as we would\n            # never expect to need to read interactive input.\n\n            if fd_null != 0:\n                os.close(0)\n                os.dup2(fd_null, 0)\n\n            # If stdout and stderr are still connected to\n            # their original file descriptors we check to see\n            # if they are associated with terminal devices.\n            # When they are we map them to /dev/null so that\n            # are still detached from any controlling terminal\n            # properly. If not we preserve them as they are.\n            #\n            # If stdin and stdout were not hooked up to the\n            # original file descriptors, then all bets are\n            # off and all we can really do is leave them as\n            # they were.\n            #\n            # This will allow 'gunicorn ... > output.log 2>&1'\n            # to work with stdout/stderr going to the file\n            # as expected.\n            #\n            # Note that if using --error-log option, the log\n            # file specified through shell redirection will\n            # only be used up until the log file specified\n            # by the option takes over. As it replaces stdout\n            # and stderr at the file descriptor level, then\n            # anything using stdout or stderr, including having\n            # cached a reference to them, will still work.\n\n            def redirect(stream, fd_expect):\n                try:\n                    fd = stream.fileno()\n                    if fd == fd_expect and stream.isatty():\n                        os.close(fd)\n                        os.dup2(fd_null, fd)\n                except AttributeError:\n                    pass\n\n            redirect(sys.stdout, 1)\n            redirect(sys.stderr, 2)\n\n\ndef seed():\n    try:\n        random.seed(os.urandom(64))\n    except NotImplementedError:\n        random.seed('%s.%s' % (time.time(), os.getpid()))\n\n\ndef check_is_writable(path):\n    try:\n        with open(path, 'a') as f:\n            f.close()\n    except IOError as e:\n        raise RuntimeError(\"Error: '%s' isn't writable [%r]\" % (path, e))\n\n\ndef to_bytestring(value, encoding=\"utf8\"):\n    \"\"\"Converts a string argument to a byte string\"\"\"\n    if isinstance(value, bytes):\n        return value\n    if not isinstance(value, str):\n        raise TypeError('%r is not a string' % value)\n\n    return value.encode(encoding)\n\n\ndef has_fileno(obj):\n    if not hasattr(obj, \"fileno\"):\n        return False\n\n    # check BytesIO case and maybe others\n    try:\n        obj.fileno()\n    except (AttributeError, IOError, io.UnsupportedOperation):\n        return False\n\n    return True\n\n\ndef warn(msg):\n    print(\"!!!\", file=sys.stderr)\n\n    lines = msg.splitlines()\n    for i, line in enumerate(lines):\n        if i == 0:\n            line = \"WARNING: %s\" % line\n        print(\"!!! %s\" % line, file=sys.stderr)\n\n    print(\"!!!\\n\", file=sys.stderr)\n    sys.stderr.flush()\n\n\ndef make_fail_app(msg):\n    msg = to_bytestring(msg)\n\n    def app(environ, start_response):\n        start_response(\"500 Internal Server Error\", [\n            (\"Content-Type\", \"text/plain\"),\n            (\"Content-Length\", str(len(msg)))\n        ])\n        return [msg]\n\n    return app\n\n\ndef split_request_uri(uri):\n    if uri.startswith(\"//\"):\n        # When the path starts with //, urlsplit considers it as a\n        # relative uri while the RFC says we should consider it as abs_path\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\n        # We use temporary dot prefix to workaround this behaviour\n        parts = urllib.parse.urlsplit(\".\" + uri)\n        return parts._replace(path=parts.path[1:])\n\n    return urllib.parse.urlsplit(uri)\n\n\n# From six.reraise\ndef reraise(tp, value, tb=None):\n    try:\n        if value is None:\n            value = tp()\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n    finally:\n        value = None\n        tb = None\n\n\ndef bytes_to_str(b):\n    if isinstance(b, str):\n        return b\n    return str(b, 'latin1')\n\n\ndef unquote_to_wsgi_str(string):\n    return urllib.parse.unquote_to_bytes(string).decode('latin-1')\n"}, {"gunicorn.util.load_class": "# -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\nimport ast\nimport email.utils\nimport errno\nimport fcntl\nimport html\nimport importlib\nimport inspect\nimport io\nimport logging\nimport os\nimport pwd\nimport random\nimport re\nimport socket\nimport sys\nimport textwrap\nimport time\nimport traceback\nimport warnings\n\ntry:\n    import importlib.metadata as importlib_metadata\nexcept (ModuleNotFoundError, ImportError):\n    import importlib_metadata\n\nfrom gunicorn.errors import AppImportError\nfrom gunicorn.workers import SUPPORTED_WORKERS\nimport urllib.parse\n\nREDIRECT_TO = getattr(os, 'devnull', '/dev/null')\n\n# Server and Date aren't technically hop-by-hop\n# headers, but they are in the purview of the\n# origin server which the WSGI spec says we should\n# act like. So we drop them and add our own.\n#\n# In the future, concatenation server header values\n# might be better, but nothing else does it and\n# dropping them is easier.\nhop_headers = set(\"\"\"\n    connection keep-alive proxy-authenticate proxy-authorization\n    te trailers transfer-encoding upgrade\n    server date\n    \"\"\".split())\n\ntry:\n    from setproctitle import setproctitle\n\n    def _setproctitle(title):\n        setproctitle(\"gunicorn: %s\" % title)\nexcept ImportError:\n    def _setproctitle(title):\n        pass\n\n\ndef load_entry_point(distribution, group, name):\n    dist_obj = importlib_metadata.distribution(distribution)\n    eps = [ep for ep in dist_obj.entry_points\n           if ep.group == group and ep.name == name]\n    if not eps:\n        raise ImportError(\"Entry point %r not found\" % ((group, name),))\n    return eps[0].load()\n\n\ndef load_class(uri, default=\"gunicorn.workers.sync.SyncWorker\",\n               section=\"gunicorn.workers\"):\n    if inspect.isclass(uri):\n        return uri\n    if uri.startswith(\"egg:\"):\n        # uses entry points\n        entry_str = uri.split(\"egg:\")[1]\n        try:\n            dist, name = entry_str.rsplit(\"#\", 1)\n        except ValueError:\n            dist = entry_str\n            name = default\n\n        try:\n            return load_entry_point(dist, section, name)\n        except Exception:\n            exc = traceback.format_exc()\n            msg = \"class uri %r invalid or not found: \\n\\n[%s]\"\n            raise RuntimeError(msg % (uri, exc))\n    else:\n        components = uri.split('.')\n        if len(components) == 1:\n            while True:\n                if uri.startswith(\"#\"):\n                    uri = uri[1:]\n\n                if uri in SUPPORTED_WORKERS:\n                    components = SUPPORTED_WORKERS[uri].split(\".\")\n                    break\n\n                try:\n                    return load_entry_point(\n                        \"gunicorn\", section, uri\n                    )\n                except Exception:\n                    exc = traceback.format_exc()\n                    msg = \"class uri %r invalid or not found: \\n\\n[%s]\"\n                    raise RuntimeError(msg % (uri, exc))\n\n        klass = components.pop(-1)\n\n        try:\n            mod = importlib.import_module('.'.join(components))\n        except Exception:\n            exc = traceback.format_exc()\n            msg = \"class uri %r invalid or not found: \\n\\n[%s]\"\n            raise RuntimeError(msg % (uri, exc))\n        return getattr(mod, klass)\n\n\npositionals = (\n    inspect.Parameter.POSITIONAL_ONLY,\n    inspect.Parameter.POSITIONAL_OR_KEYWORD,\n)\n\n\ndef get_arity(f):\n    sig = inspect.signature(f)\n    arity = 0\n\n    for param in sig.parameters.values():\n        if param.kind in positionals:\n            arity += 1\n\n    return arity\n\n\ndef get_username(uid):\n    \"\"\" get the username for a user id\"\"\"\n    return pwd.getpwuid(uid).pw_name\n\n\ndef set_owner_process(uid, gid, initgroups=False):\n    \"\"\" set user and group of workers processes \"\"\"\n\n    if gid:\n        if uid:\n            try:\n                username = get_username(uid)\n            except KeyError:\n                initgroups = False\n\n        # versions of python < 2.6.2 don't manage unsigned int for\n        # groups like on osx or fedora\n        gid = abs(gid) & 0x7FFFFFFF\n\n        if initgroups:\n            os.initgroups(username, gid)\n        elif gid != os.getgid():\n            os.setgid(gid)\n\n    if uid and uid != os.getuid():\n        os.setuid(uid)\n\n\ndef chown(path, uid, gid):\n    os.chown(path, uid, gid)\n\n\nif sys.platform.startswith(\"win\"):\n    def _waitfor(func, pathname, waitall=False):\n        # Perform the operation\n        func(pathname)\n        # Now setup the wait loop\n        if waitall:\n            dirname = pathname\n        else:\n            dirname, name = os.path.split(pathname)\n            dirname = dirname or '.'\n        # Check for `pathname` to be removed from the filesystem.\n        # The exponential backoff of the timeout amounts to a total\n        # of ~1 second after which the deletion is probably an error\n        # anyway.\n        # Testing on a i7@4.3GHz shows that usually only 1 iteration is\n        # required when contention occurs.\n        timeout = 0.001\n        while timeout < 1.0:\n            # Note we are only testing for the existence of the file(s) in\n            # the contents of the directory regardless of any security or\n            # access rights.  If we have made it this far, we have sufficient\n            # permissions to do that much using Python's equivalent of the\n            # Windows API FindFirstFile.\n            # Other Windows APIs can fail or give incorrect results when\n            # dealing with files that are pending deletion.\n            L = os.listdir(dirname)\n            if not L if waitall else name in L:\n                return\n            # Increase the timeout and try again\n            time.sleep(timeout)\n            timeout *= 2\n        warnings.warn('tests may fail, delete still pending for ' + pathname,\n                      RuntimeWarning, stacklevel=4)\n\n    def _unlink(filename):\n        _waitfor(os.unlink, filename)\nelse:\n    _unlink = os.unlink\n\n\ndef unlink(filename):\n    try:\n        _unlink(filename)\n    except OSError as error:\n        # The filename need not exist.\n        if error.errno not in (errno.ENOENT, errno.ENOTDIR):\n            raise\n\n\ndef is_ipv6(addr):\n    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:  # not a valid address\n        return False\n    except ValueError:  # ipv6 not supported on this platform\n        return False\n    return True\n\n\ndef parse_address(netloc, default_port='8000'):\n    if re.match(r'unix:(//)?', netloc):\n        return re.split(r'unix:(//)?', netloc)[-1]\n\n    if netloc.startswith(\"fd://\"):\n        fd = netloc[5:]\n        try:\n            return int(fd)\n        except ValueError:\n            raise RuntimeError(\"%r is not a valid file descriptor.\" % fd) from None\n\n    if netloc.startswith(\"tcp://\"):\n        netloc = netloc.split(\"tcp://\")[1]\n    host, port = netloc, default_port\n\n    if '[' in netloc and ']' in netloc:\n        host = netloc.split(']')[0][1:]\n        port = (netloc.split(']:') + [default_port])[1]\n    elif ':' in netloc:\n        host, port = (netloc.split(':') + [default_port])[:2]\n    elif netloc == \"\":\n        host, port = \"0.0.0.0\", default_port\n\n    try:\n        port = int(port)\n    except ValueError:\n        raise RuntimeError(\"%r is not a valid port number.\" % port)\n\n    return host.lower(), port\n\n\ndef close_on_exec(fd):\n    flags = fcntl.fcntl(fd, fcntl.F_GETFD)\n    flags |= fcntl.FD_CLOEXEC\n    fcntl.fcntl(fd, fcntl.F_SETFD, flags)\n\n\ndef set_non_blocking(fd):\n    flags = fcntl.fcntl(fd, fcntl.F_GETFL) | os.O_NONBLOCK\n    fcntl.fcntl(fd, fcntl.F_SETFL, flags)\n\n\ndef close(sock):\n    try:\n        sock.close()\n    except socket.error:\n        pass\n\n\ntry:\n    from os import closerange\nexcept ImportError:\n    def closerange(fd_low, fd_high):\n        # Iterate through and close all file descriptors.\n        for fd in range(fd_low, fd_high):\n            try:\n                os.close(fd)\n            except OSError:  # ERROR, fd wasn't open to begin with (ignored)\n                pass\n\n\ndef write_chunk(sock, data):\n    if isinstance(data, str):\n        data = data.encode('utf-8')\n    chunk_size = \"%X\\r\\n\" % len(data)\n    chunk = b\"\".join([chunk_size.encode('utf-8'), data, b\"\\r\\n\"])\n    sock.sendall(chunk)\n\n\ndef write(sock, data, chunked=False):\n    if chunked:\n        return write_chunk(sock, data)\n    sock.sendall(data)\n\n\ndef write_nonblock(sock, data, chunked=False):\n    timeout = sock.gettimeout()\n    if timeout != 0.0:\n        try:\n            sock.setblocking(0)\n            return write(sock, data, chunked)\n        finally:\n            sock.setblocking(1)\n    else:\n        return write(sock, data, chunked)\n\n\ndef write_error(sock, status_int, reason, mesg):\n    html_error = textwrap.dedent(\"\"\"\\\n    <html>\n      <head>\n        <title>%(reason)s</title>\n      </head>\n      <body>\n        <h1><p>%(reason)s</p></h1>\n        %(mesg)s\n      </body>\n    </html>\n    \"\"\") % {\"reason\": reason, \"mesg\": html.escape(mesg)}\n\n    http = textwrap.dedent(\"\"\"\\\n    HTTP/1.1 %s %s\\r\n    Connection: close\\r\n    Content-Type: text/html\\r\n    Content-Length: %d\\r\n    \\r\n    %s\"\"\") % (str(status_int), reason, len(html_error), html_error)\n    write_nonblock(sock, http.encode('latin1'))\n\n\ndef _called_with_wrong_args(f):\n    \"\"\"Check whether calling a function raised a ``TypeError`` because\n    the call failed or because something in the function raised the\n    error.\n\n    :param f: The function that was called.\n    :return: ``True`` if the call failed.\n    \"\"\"\n    tb = sys.exc_info()[2]\n\n    try:\n        while tb is not None:\n            if tb.tb_frame.f_code is f.__code__:\n                # In the function, it was called successfully.\n                return False\n\n            tb = tb.tb_next\n\n        # Didn't reach the function.\n        return True\n    finally:\n        # Delete tb to break a circular reference in Python 2.\n        # https://docs.python.org/2/library/sys.html#sys.exc_info\n        del tb\n\n\ndef import_app(module):\n    parts = module.split(\":\", 1)\n    if len(parts) == 1:\n        obj = \"application\"\n    else:\n        module, obj = parts[0], parts[1]\n\n    try:\n        mod = importlib.import_module(module)\n    except ImportError:\n        if module.endswith(\".py\") and os.path.exists(module):\n            msg = \"Failed to find application, did you mean '%s:%s'?\"\n            raise ImportError(msg % (module.rsplit(\".\", 1)[0], obj))\n        raise\n\n    # Parse obj as a single expression to determine if it's a valid\n    # attribute name or function call.\n    try:\n        expression = ast.parse(obj, mode=\"eval\").body\n    except SyntaxError:\n        raise AppImportError(\n            \"Failed to parse %r as an attribute name or function call.\" % obj\n        )\n\n    if isinstance(expression, ast.Name):\n        name = expression.id\n        args = kwargs = None\n    elif isinstance(expression, ast.Call):\n        # Ensure the function name is an attribute name only.\n        if not isinstance(expression.func, ast.Name):\n            raise AppImportError(\"Function reference must be a simple name: %r\" % obj)\n\n        name = expression.func.id\n\n        # Parse the positional and keyword arguments as literals.\n        try:\n            args = [ast.literal_eval(arg) for arg in expression.args]\n            kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in expression.keywords}\n        except ValueError:\n            # literal_eval gives cryptic error messages, show a generic\n            # message with the full expression instead.\n            raise AppImportError(\n                \"Failed to parse arguments as literal values: %r\" % obj\n            )\n    else:\n        raise AppImportError(\n            \"Failed to parse %r as an attribute name or function call.\" % obj\n        )\n\n    is_debug = logging.root.level == logging.DEBUG\n    try:\n        app = getattr(mod, name)\n    except AttributeError:\n        if is_debug:\n            traceback.print_exception(*sys.exc_info())\n        raise AppImportError(\"Failed to find attribute %r in %r.\" % (name, module))\n\n    # If the expression was a function call, call the retrieved object\n    # to get the real application.\n    if args is not None:\n        try:\n            app = app(*args, **kwargs)\n        except TypeError as e:\n            # If the TypeError was due to bad arguments to the factory\n            # function, show Python's nice error message without a\n            # traceback.\n            if _called_with_wrong_args(app):\n                raise AppImportError(\n                    \"\".join(traceback.format_exception_only(TypeError, e)).strip()\n                )\n\n            # Otherwise it was raised from within the function, show the\n            # full traceback.\n            raise\n\n    if app is None:\n        raise AppImportError(\"Failed to find application object: %r\" % obj)\n\n    if not callable(app):\n        raise AppImportError(\"Application object must be callable.\")\n    return app\n\n\ndef getcwd():\n    # get current path, try to use PWD env first\n    try:\n        a = os.stat(os.environ['PWD'])\n        b = os.stat(os.getcwd())\n        if a.st_ino == b.st_ino and a.st_dev == b.st_dev:\n            cwd = os.environ['PWD']\n        else:\n            cwd = os.getcwd()\n    except Exception:\n        cwd = os.getcwd()\n    return cwd\n\n\ndef http_date(timestamp=None):\n    \"\"\"Return the current date and time formatted for a message header.\"\"\"\n    if timestamp is None:\n        timestamp = time.time()\n    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)\n    return s\n\n\ndef is_hoppish(header):\n    return header.lower().strip() in hop_headers\n\n\ndef daemonize(enable_stdio_inheritance=False):\n    \"\"\"\\\n    Standard daemonization of a process.\n    http://www.faqs.org/faqs/unix-faq/programmer/faq/ section 1.7\n    \"\"\"\n    if 'GUNICORN_FD' not in os.environ:\n        if os.fork():\n            os._exit(0)\n        os.setsid()\n\n        if os.fork():\n            os._exit(0)\n\n        os.umask(0o22)\n\n        # In both the following any file descriptors above stdin\n        # stdout and stderr are left untouched. The inheritance\n        # option simply allows one to have output go to a file\n        # specified by way of shell redirection when not wanting\n        # to use --error-log option.\n\n        if not enable_stdio_inheritance:\n            # Remap all of stdin, stdout and stderr on to\n            # /dev/null. The expectation is that users have\n            # specified the --error-log option.\n\n            closerange(0, 3)\n\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\n            # PEP 446, make fd for /dev/null inheritable\n            os.set_inheritable(fd_null, True)\n\n            # expect fd_null to be always 0 here, but in-case not ...\n            if fd_null != 0:\n                os.dup2(fd_null, 0)\n\n            os.dup2(fd_null, 1)\n            os.dup2(fd_null, 2)\n\n        else:\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\n\n            # Always redirect stdin to /dev/null as we would\n            # never expect to need to read interactive input.\n\n            if fd_null != 0:\n                os.close(0)\n                os.dup2(fd_null, 0)\n\n            # If stdout and stderr are still connected to\n            # their original file descriptors we check to see\n            # if they are associated with terminal devices.\n            # When they are we map them to /dev/null so that\n            # are still detached from any controlling terminal\n            # properly. If not we preserve them as they are.\n            #\n            # If stdin and stdout were not hooked up to the\n            # original file descriptors, then all bets are\n            # off and all we can really do is leave them as\n            # they were.\n            #\n            # This will allow 'gunicorn ... > output.log 2>&1'\n            # to work with stdout/stderr going to the file\n            # as expected.\n            #\n            # Note that if using --error-log option, the log\n            # file specified through shell redirection will\n            # only be used up until the log file specified\n            # by the option takes over. As it replaces stdout\n            # and stderr at the file descriptor level, then\n            # anything using stdout or stderr, including having\n            # cached a reference to them, will still work.\n\n            def redirect(stream, fd_expect):\n                try:\n                    fd = stream.fileno()\n                    if fd == fd_expect and stream.isatty():\n                        os.close(fd)\n                        os.dup2(fd_null, fd)\n                except AttributeError:\n                    pass\n\n            redirect(sys.stdout, 1)\n            redirect(sys.stderr, 2)\n\n\ndef seed():\n    try:\n        random.seed(os.urandom(64))\n    except NotImplementedError:\n        random.seed('%s.%s' % (time.time(), os.getpid()))\n\n\ndef check_is_writable(path):\n    try:\n        with open(path, 'a') as f:\n            f.close()\n    except IOError as e:\n        raise RuntimeError(\"Error: '%s' isn't writable [%r]\" % (path, e))\n\n\ndef to_bytestring(value, encoding=\"utf8\"):\n    \"\"\"Converts a string argument to a byte string\"\"\"\n    if isinstance(value, bytes):\n        return value\n    if not isinstance(value, str):\n        raise TypeError('%r is not a string' % value)\n\n    return value.encode(encoding)\n\n\ndef has_fileno(obj):\n    if not hasattr(obj, \"fileno\"):\n        return False\n\n    # check BytesIO case and maybe others\n    try:\n        obj.fileno()\n    except (AttributeError, IOError, io.UnsupportedOperation):\n        return False\n\n    return True\n\n\ndef warn(msg):\n    print(\"!!!\", file=sys.stderr)\n\n    lines = msg.splitlines()\n    for i, line in enumerate(lines):\n        if i == 0:\n            line = \"WARNING: %s\" % line\n        print(\"!!! %s\" % line, file=sys.stderr)\n\n    print(\"!!!\\n\", file=sys.stderr)\n    sys.stderr.flush()\n\n\ndef make_fail_app(msg):\n    msg = to_bytestring(msg)\n\n    def app(environ, start_response):\n        start_response(\"500 Internal Server Error\", [\n            (\"Content-Type\", \"text/plain\"),\n            (\"Content-Length\", str(len(msg)))\n        ])\n        return [msg]\n\n    return app\n\n\ndef split_request_uri(uri):\n    if uri.startswith(\"//\"):\n        # When the path starts with //, urlsplit considers it as a\n        # relative uri while the RFC says we should consider it as abs_path\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\n        # We use temporary dot prefix to workaround this behaviour\n        parts = urllib.parse.urlsplit(\".\" + uri)\n        return parts._replace(path=parts.path[1:])\n\n    return urllib.parse.urlsplit(uri)\n\n\n# From six.reraise\ndef reraise(tp, value, tb=None):\n    try:\n        if value is None:\n            value = tp()\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n    finally:\n        value = None\n        tb = None\n\n\ndef bytes_to_str(b):\n    if isinstance(b, str):\n        return b\n    return str(b, 'latin1')\n\n\ndef unquote_to_wsgi_str(string):\n    return urllib.parse.unquote_to_bytes(string).decode('latin-1')\n"}], "prompt": "Please write a python function called 'logger_class' base the context. This function retrieves the logger class based on the configuration settings. It first checks the 'logger_class' setting and if it is \"simple\", it uses the default logger class. If the default logger class is being used andstatsd is on, it automatically switches to the gunicorn.instrument.statsd.Statsd class. Then, it loads the logger class (with default: \"gunicorn.glogging.Logger\" and section: \"gunicorn.loggers\") and install it if can, finally returns it.:param self: Config. An instance of the Config class.\n:return: The logger class based on the configuration settings..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # -*- coding: utf-8 -\n#\n# This file is part of gunicorn released under the MIT license.\n# See the NOTICE for more information.\n\n# Please remember to run \"make -C docs html\" after update \"desc\" attributes.\n\nimport argparse\nimport copy\nimport grp\nimport inspect\nimport os\nimport pwd\nimport re\nimport shlex\nimport ssl\nimport sys\nimport textwrap\n\nfrom gunicorn import __version__, util\nfrom gunicorn.errors import ConfigError\nfrom gunicorn.reloader import reloader_engines\n\nKNOWN_SETTINGS = []\nPLATFORM = sys.platform\n\n\ndef make_settings(ignore=None):\n    settings = {}\n    ignore = ignore or ()\n    for s in KNOWN_SETTINGS:\n        setting = s()\n        if setting.name in ignore:\n            continue\n        settings[setting.name] = setting.copy()\n    return settings\n\n\ndef auto_int(_, x):\n    # for compatible with octal numbers in python3\n    if re.match(r'0(\\d)', x, re.IGNORECASE):\n        x = x.replace('0', '0o', 1)\n    return int(x, 0)\n\n\nclass Config(object):\n\n    def __init__(self, usage=None, prog=None):\n        self.settings = make_settings()\n        self.usage = usage\n        self.prog = prog or os.path.basename(sys.argv[0])\n        self.env_orig = os.environ.copy()\n\n    def __str__(self):\n        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)\n    # def __str__(self):\n    #         lines = []\n    #         kmax = max(len(k) for k in self.settings)\n    #         for k, v in sorted(self.settings.items()):\n    #             if callable(v):\n    #                 value_str = f'<{v.__name__}()>'\n    #             else:\n    #                 value_str = repr(v)\n    #             lines.append(f'{k:{kmax}} = {value_str}')\n    #         return \"\\n\".join(lines)\n    def __getattr__(self, name):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        return self.settings[name].get()\n\n    def __setattr__(self, name, value):\n        if name != \"settings\" and name in self.settings:\n            raise AttributeError(\"Invalid access!\")\n        super().__setattr__(name, value)\n\n    def set(self, name, value):\n        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n\n    def get_cmd_args_from_env(self):\n        if 'GUNICORN_CMD_ARGS' in self.env_orig:\n            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])\n        return []\n\n    def parser(self):\n        kwargs = {\n            \"usage\": self.usage,\n            \"prog\": self.prog\n        }\n        parser = argparse.ArgumentParser(**kwargs)\n        parser.add_argument(\"-v\", \"--version\",\n                            action=\"version\", default=argparse.SUPPRESS,\n                            version=\"%(prog)s (version \" + __version__ + \")\\n\",\n                            help=\"show program's version number and exit\")\n        parser.add_argument(\"args\", nargs=\"*\", help=argparse.SUPPRESS)\n\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            self.settings[k].add_option(parser)\n\n        return parser\n\n    @property\n    def worker_class_str(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri\n\n    @property\n    def worker_class(self):\n        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class\n\n    @property\n    def address(self):\n        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]\n\n    @property\n    def uid(self):\n        return self.settings['user'].get()\n\n    @property\n    def gid(self):\n        return self.settings['group'].get()\n\n###The function: logger_class###\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class\n\n    @property\n    def is_ssl(self):\n        return self.certfile or self.keyfile\n\n    @property\n    def ssl_options(self):\n        opts = {}\n        for name, value in self.settings.items():\n            if value.section == 'SSL':\n                opts[name] = value.get()\n        return opts\n\n    @property\n    def env(self):\n        raw_env = self.settings['raw_env'].get()\n        env = {}\n\n        if not raw_env:\n            return env\n\n        for e in raw_env:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = s.split('=', 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n\n            env[k] = v\n\n        return env\n\n    @property\n    def sendfile(self):\n        if self.settings['sendfile'].get() is not None:\n            return False\n\n        if 'SENDFILE' in os.environ:\n            sendfile = os.environ['SENDFILE'].lower()\n            return sendfile in ['y', '1', 'yes', 'true']\n\n        return True\n\n    @property\n    def reuse_port(self):\n        return self.settings['reuse_port'].get()\n\n    @property\n    def paste_global_conf(self):\n        raw_global_conf = self.settings['raw_paste_global_conf'].get()\n        if raw_global_conf is None:\n            return None\n\n        global_conf = {}\n        for e in raw_global_conf:\n            s = util.bytes_to_str(e)\n            try:\n                k, v = re.split(r'(?<!\\\\)=', s, 1)\n            except ValueError:\n                raise RuntimeError(\"environment setting %r invalid\" % s)\n            k = k.replace('\\\\=', '=')\n            v = v.replace('\\\\=', '=')\n            global_conf[k] = v\n\n        return global_conf\n\n\nclass SettingMeta(type):\n    def __new__(cls, name, bases, attrs):\n        super_new = super().__new__\n        parents = [b for b in bases if isinstance(b, SettingMeta)]\n        if not parents:\n            return super_new(cls, name, bases, attrs)\n\n        attrs[\"order\"] = len(KNOWN_SETTINGS)\n        attrs[\"validator\"] = staticmethod(attrs[\"validator\"])\n\n        new_class = super_new(cls, name, bases, attrs)\n        new_class.fmt_desc(attrs.get(\"desc\", \"\"))\n        KNOWN_SETTINGS.append(new_class)\n        return new_class\n\n    def fmt_desc(cls, desc):\n        desc = textwrap.dedent(desc).strip()\n        setattr(cls, \"desc\", desc)\n        setattr(cls, \"short\", desc.splitlines()[0])\n\n\nclass Setting(object):\n    name = None\n    value = None\n    section = None\n    cli = None\n    validator = None\n    type = None\n    meta = None\n    action = None\n    default = None\n    short = None\n    desc = None\n    nargs = None\n    const = None\n\n    def __init__(self):\n        if self.default is not None:\n            self.set(self.default)\n\n    def add_option(self, parser):\n        if not self.cli:\n            return\n        args = tuple(self.cli)\n\n        help_txt = \"%s [%s]\" % (self.short, self.default)\n        help_txt = help_txt.replace(\"%\", \"%%\")\n\n        kwargs = {\n            \"dest\": self.name,\n            \"action\": self.action or \"store\",\n            \"type\": self.type or str,\n            \"default\": None,\n            \"help\": help_txt\n        }\n\n        if self.meta is not None:\n            kwargs['metavar'] = self.meta\n\n        if kwargs[\"action\"] != \"store\":\n            kwargs.pop(\"type\")\n\n        if self.nargs is not None:\n            kwargs[\"nargs\"] = self.nargs\n\n        if self.const is not None:\n            kwargs[\"const\"] = self.const\n\n        parser.add_argument(*args, **kwargs)\n\n    def copy(self):\n        return copy.copy(self)\n\n    def get(self):\n        return self.value\n\n    def set(self, val):\n        if not callable(self.validator):\n            raise TypeError('Invalid validator: %s' % self.name)\n        self.value = self.validator(val)\n\n    def __lt__(self, other):\n        return (self.section == other.section and\n                self.order < other.order)\n    __cmp__ = __lt__\n\n    def __repr__(self):\n        return \"<%s.%s object at %x with value %r>\" % (\n            self.__class__.__module__,\n            self.__class__.__name__,\n            id(self),\n            self.value,\n        )\n\n\nSetting = SettingMeta('Setting', (Setting,), {})\n\n\ndef validate_bool(val):\n    if val is None:\n        return\n\n    if isinstance(val, bool):\n        return val\n    if not isinstance(val, str):\n        raise TypeError(\"Invalid type for casting: %s\" % val)\n    if val.lower().strip() == \"true\":\n        return True\n    elif val.lower().strip() == \"false\":\n        return False\n    else:\n        raise ValueError(\"Invalid boolean: %s\" % val)\n\n\ndef validate_dict(val):\n    if not isinstance(val, dict):\n        raise TypeError(\"Value is not a dictionary: %s \" % val)\n    return val\n\n\ndef validate_pos_int(val):\n    if not isinstance(val, int):\n        val = int(val, 0)\n    else:\n        # Booleans are ints!\n        val = int(val)\n    if val < 0:\n        raise ValueError(\"Value must be positive: %s\" % val)\n    return val\n\n\ndef validate_ssl_version(val):\n    if val != SSLVersion.default:\n        sys.stderr.write(\"Warning: option `ssl_version` is deprecated and it is ignored. Use ssl_context instead.\\n\")\n    return val\n\n\ndef validate_string(val):\n    if val is None:\n        return None\n    if not isinstance(val, str):\n        raise TypeError(\"Not a string: %s\" % val)\n    return val.strip()\n\n\ndef validate_file_exists(val):\n    if val is None:\n        return None\n    if not os.path.exists(val):\n        raise ValueError(\"File %s does not exists.\" % val)\n    return val\n\n\ndef validate_list_string(val):\n    if not val:\n        return []\n\n    # legacy syntax\n    if isinstance(val, str):\n        val = [val]\n\n    return [validate_string(v) for v in val]\n\n\ndef validate_list_of_existing_files(val):\n    return [validate_file_exists(v) for v in validate_list_string(val)]\n\n\ndef validate_string_to_list(val):\n    val = validate_string(val)\n\n    if not val:\n        return []\n\n    return [v.strip() for v in val.split(\",\") if v]\n\n\ndef validate_class(val):\n    if inspect.isfunction(val) or inspect.ismethod(val):\n        val = val()\n    if inspect.isclass(val):\n        return val\n    return validate_string(val)\n\n\ndef validate_callable(arity):\n    def _validate_callable(val):\n        if isinstance(val, str):\n            try:\n                mod_name, obj_name = val.rsplit(\".\", 1)\n            except ValueError:\n                raise TypeError(\"Value '%s' is not import string. \"\n                                \"Format: module[.submodules...].object\" % val)\n            try:\n                mod = __import__(mod_name, fromlist=[obj_name])\n                val = getattr(mod, obj_name)\n            except ImportError as e:\n                raise TypeError(str(e))\n            except AttributeError:\n                raise TypeError(\"Can not load '%s' from '%s'\"\n                                \"\" % (obj_name, mod_name))\n        if not callable(val):\n            raise TypeError(\"Value is not callable: %s\" % val)\n        if arity != -1 and arity != util.get_arity(val):\n            raise TypeError(\"Value must have an arity of: %s\" % arity)\n        return val\n    return _validate_callable\n\n\ndef validate_user(val):\n    if val is None:\n        return os.geteuid()\n    if isinstance(val, int):\n        return val\n    elif val.isdigit():\n        return int(val)\n    else:\n        try:\n            return pwd.getpwnam(val).pw_uid\n        except KeyError:\n            raise ConfigError(\"No such user: '%s'\" % val)\n\n\ndef validate_group(val):\n    if val is None:\n        return os.getegid()\n\n    if isinstance(val, int):\n        return val\n    elif val.isdigit():\n        return int(val)\n    else:\n        try:\n            return grp.getgrnam(val).gr_gid\n        except KeyError:\n            raise ConfigError(\"No such group: '%s'\" % val)\n\n\ndef validate_post_request(val):\n    val = validate_callable(-1)(val)\n\n    largs = util.get_arity(val)\n    if largs == 4:\n        return val\n    elif largs == 3:\n        return lambda worker, req, env, _r: val(worker, req, env)\n    elif largs == 2:\n        return lambda worker, req, _e, _r: val(worker, req)\n    else:\n        raise TypeError(\"Value must have an arity of: 4\")\n\n\ndef validate_chdir(val):\n    # valid if the value is a string\n    val = validate_string(val)\n\n    # transform relative paths\n    path = os.path.abspath(os.path.normpath(os.path.join(util.getcwd(), val)))\n\n    # test if the path exists\n    if not os.path.exists(path):\n        raise ConfigError(\"can't chdir to %r\" % val)\n\n    return path\n\n\ndef validate_statsd_address(val):\n    val = validate_string(val)\n    if val is None:\n        return None\n\n    # As of major release 20, util.parse_address would recognize unix:PORT\n    # as a UDS address, breaking backwards compatibility. We defend against\n    # that regression here (this is also unit-tested).\n    # Feel free to remove in the next major release.\n    unix_hostname_regression = re.match(r'^unix:(\\d+)$', val)\n    if unix_hostname_regression:\n        return ('unix', int(unix_hostname_regression.group(1)))\n\n    try:\n        address = util.parse_address(val, default_port='8125')\n    except RuntimeError:\n        raise TypeError(\"Value must be one of ('host:port', 'unix://PATH')\")\n\n    return address\n\n\ndef validate_reload_engine(val):\n    if val not in reloader_engines:\n        raise ConfigError(\"Invalid reload_engine: %r\" % val)\n\n    return val\n\n\ndef get_default_config_file():\n    config_path = os.path.join(os.path.abspath(os.getcwd()),\n                               'gunicorn.conf.py')\n    if os.path.exists(config_path):\n        return config_path\n    return None\n\n\nclass ConfigFile(Setting):\n    name = \"config\"\n    section = \"Config File\"\n    cli = [\"-c\", \"--config\"]\n    meta = \"CONFIG\"\n    validator = validate_string\n    default = \"./gunicorn.conf.py\"\n    desc = \"\"\"\\\n        :ref:`The Gunicorn config file<configuration_file>`.\n\n        A string of the form ``PATH``, ``file:PATH``, or ``python:MODULE_NAME``.\n\n        Only has an effect when specified on the command line or as part of an\n        application specific configuration.\n\n        By default, a file named ``gunicorn.conf.py`` will be read from the same\n        directory where gunicorn is being run.\n\n        .. versionchanged:: 19.4\n           Loading the config from a Python module requires the ``python:``\n           prefix.\n        \"\"\"\n\n\nclass WSGIApp(Setting):\n    name = \"wsgi_app\"\n    section = \"Config File\"\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A WSGI application path in pattern ``$(MODULE_NAME):$(VARIABLE_NAME)``.\n\n        .. versionadded:: 20.1.0\n        \"\"\"\n\n\nclass Bind(Setting):\n    name = \"bind\"\n    action = \"append\"\n    section = \"Server Socket\"\n    cli = [\"-b\", \"--bind\"]\n    meta = \"ADDRESS\"\n    validator = validate_list_string\n\n    if 'PORT' in os.environ:\n        default = ['0.0.0.0:{0}'.format(os.environ.get('PORT'))]\n    else:\n        default = ['127.0.0.1:8000']\n\n    desc = \"\"\"\\\n        The socket to bind.\n\n        A string of the form: ``HOST``, ``HOST:PORT``, ``unix:PATH``,\n        ``fd://FD``. An IP is a valid ``HOST``.\n\n        .. versionchanged:: 20.0\n           Support for ``fd://FD`` got added.\n\n        Multiple addresses can be bound. ex.::\n\n            $ gunicorn -b 127.0.0.1:8000 -b [::1]:8000 test:app\n\n        will bind the `test:app` application on localhost both on ipv6\n        and ipv4 interfaces.\n\n        If the ``PORT`` environment variable is defined, the default\n        is ``['0.0.0.0:$PORT']``. If it is not defined, the default\n        is ``['127.0.0.1:8000']``.\n        \"\"\"\n\n\nclass Backlog(Setting):\n    name = \"backlog\"\n    section = \"Server Socket\"\n    cli = [\"--backlog\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 2048\n    desc = \"\"\"\\\n        The maximum number of pending connections.\n\n        This refers to the number of clients that can be waiting to be served.\n        Exceeding this number results in the client getting an error when\n        attempting to connect. It should only affect servers under significant\n        load.\n\n        Must be a positive integer. Generally set in the 64-2048 range.\n        \"\"\"\n\n\nclass Workers(Setting):\n    name = \"workers\"\n    section = \"Worker Processes\"\n    cli = [\"-w\", \"--workers\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = int(os.environ.get(\"WEB_CONCURRENCY\", 1))\n    desc = \"\"\"\\\n        The number of worker processes for handling requests.\n\n        A positive integer generally in the ``2-4 x $(NUM_CORES)`` range.\n        You'll want to vary this a bit to find the best for your particular\n        application's work load.\n\n        By default, the value of the ``WEB_CONCURRENCY`` environment variable,\n        which is set by some Platform-as-a-Service providers such as Heroku. If\n        it is not defined, the default is ``1``.\n        \"\"\"\n\n\nclass WorkerClass(Setting):\n    name = \"worker_class\"\n    section = \"Worker Processes\"\n    cli = [\"-k\", \"--worker-class\"]\n    meta = \"STRING\"\n    validator = validate_class\n    default = \"sync\"\n    desc = \"\"\"\\\n        The type of workers to use.\n\n        The default class (``sync``) should handle most \"normal\" types of\n        workloads. You'll want to read :doc:`design` for information on when\n        you might want to choose one of the other worker classes. Required\n        libraries may be installed using setuptools' ``extras_require`` feature.\n\n        A string referring to one of the following bundled classes:\n\n        * ``sync``\n        * ``eventlet`` - Requires eventlet >= 0.24.1 (or install it via\n          ``pip install gunicorn[eventlet]``)\n        * ``gevent``   - Requires gevent >= 1.4 (or install it via\n          ``pip install gunicorn[gevent]``)\n        * ``tornado``  - Requires tornado >= 0.2 (or install it via\n          ``pip install gunicorn[tornado]``)\n        * ``gthread``  - Python 2 requires the futures package to be installed\n          (or install it via ``pip install gunicorn[gthread]``)\n\n        Optionally, you can provide your own worker by giving Gunicorn a\n        Python path to a subclass of ``gunicorn.workers.base.Worker``.\n        This alternative syntax will load the gevent class:\n        ``gunicorn.workers.ggevent.GeventWorker``.\n        \"\"\"\n\n\nclass WorkerThreads(Setting):\n    name = \"threads\"\n    section = \"Worker Processes\"\n    cli = [\"--threads\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 1\n    desc = \"\"\"\\\n        The number of worker threads for handling requests.\n\n        Run each worker with the specified number of threads.\n\n        A positive integer generally in the ``2-4 x $(NUM_CORES)`` range.\n        You'll want to vary this a bit to find the best for your particular\n        application's work load.\n\n        If it is not defined, the default is ``1``.\n\n        This setting only affects the Gthread worker type.\n\n        .. note::\n           If you try to use the ``sync`` worker type and set the ``threads``\n           setting to more than 1, the ``gthread`` worker type will be used\n           instead.\n        \"\"\"\n\n\nclass WorkerConnections(Setting):\n    name = \"worker_connections\"\n    section = \"Worker Processes\"\n    cli = [\"--worker-connections\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 1000\n    desc = \"\"\"\\\n        The maximum number of simultaneous clients.\n\n        This setting only affects the ``gthread``, ``eventlet`` and ``gevent`` worker types.\n        \"\"\"\n\n\nclass MaxRequests(Setting):\n    name = \"max_requests\"\n    section = \"Worker Processes\"\n    cli = [\"--max-requests\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 0\n    desc = \"\"\"\\\n        The maximum number of requests a worker will process before restarting.\n\n        Any value greater than zero will limit the number of requests a worker\n        will process before automatically restarting. This is a simple method\n        to help limit the damage of memory leaks.\n\n        If this is set to zero (the default) then the automatic worker\n        restarts are disabled.\n        \"\"\"\n\n\nclass MaxRequestsJitter(Setting):\n    name = \"max_requests_jitter\"\n    section = \"Worker Processes\"\n    cli = [\"--max-requests-jitter\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 0\n    desc = \"\"\"\\\n        The maximum jitter to add to the *max_requests* setting.\n\n        The jitter causes the restart per worker to be randomized by\n        ``randint(0, max_requests_jitter)``. This is intended to stagger worker\n        restarts to avoid all workers restarting at the same time.\n\n        .. versionadded:: 19.2\n        \"\"\"\n\n\nclass Timeout(Setting):\n    name = \"timeout\"\n    section = \"Worker Processes\"\n    cli = [\"-t\", \"--timeout\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 30\n    desc = \"\"\"\\\n        Workers silent for more than this many seconds are killed and restarted.\n\n        Value is a positive number or 0. Setting it to 0 has the effect of\n        infinite timeouts by disabling timeouts for all workers entirely.\n\n        Generally, the default of thirty seconds should suffice. Only set this\n        noticeably higher if you're sure of the repercussions for sync workers.\n        For the non sync workers it just means that the worker process is still\n        communicating and is not tied to the length of time required to handle a\n        single request.\n        \"\"\"\n\n\nclass GracefulTimeout(Setting):\n    name = \"graceful_timeout\"\n    section = \"Worker Processes\"\n    cli = [\"--graceful-timeout\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 30\n    desc = \"\"\"\\\n        Timeout for graceful workers restart.\n\n        After receiving a restart signal, workers have this much time to finish\n        serving requests. Workers still alive after the timeout (starting from\n        the receipt of the restart signal) are force killed.\n        \"\"\"\n\n\nclass Keepalive(Setting):\n    name = \"keepalive\"\n    section = \"Worker Processes\"\n    cli = [\"--keep-alive\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 2\n    desc = \"\"\"\\\n        The number of seconds to wait for requests on a Keep-Alive connection.\n\n        Generally set in the 1-5 seconds range for servers with direct connection\n        to the client (e.g. when you don't have separate load balancer). When\n        Gunicorn is deployed behind a load balancer, it often makes sense to\n        set this to a higher value.\n\n        .. note::\n           ``sync`` worker does not support persistent connections and will\n           ignore this option.\n        \"\"\"\n\n\nclass LimitRequestLine(Setting):\n    name = \"limit_request_line\"\n    section = \"Security\"\n    cli = [\"--limit-request-line\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 4094\n    desc = \"\"\"\\\n        The maximum size of HTTP request line in bytes.\n\n        This parameter is used to limit the allowed size of a client's\n        HTTP request-line. Since the request-line consists of the HTTP\n        method, URI, and protocol version, this directive places a\n        restriction on the length of a request-URI allowed for a request\n        on the server. A server needs this value to be large enough to\n        hold any of its resource names, including any information that\n        might be passed in the query part of a GET request. Value is a number\n        from 0 (unlimited) to 8190.\n\n        This parameter can be used to prevent any DDOS attack.\n        \"\"\"\n\n\nclass LimitRequestFields(Setting):\n    name = \"limit_request_fields\"\n    section = \"Security\"\n    cli = [\"--limit-request-fields\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 100\n    desc = \"\"\"\\\n        Limit the number of HTTP headers fields in a request.\n\n        This parameter is used to limit the number of headers in a request to\n        prevent DDOS attack. Used with the *limit_request_field_size* it allows\n        more safety. By default this value is 100 and can't be larger than\n        32768.\n        \"\"\"\n\n\nclass LimitRequestFieldSize(Setting):\n    name = \"limit_request_field_size\"\n    section = \"Security\"\n    cli = [\"--limit-request-field_size\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = int\n    default = 8190\n    desc = \"\"\"\\\n        Limit the allowed size of an HTTP request header field.\n\n        Value is a positive number or 0. Setting it to 0 will allow unlimited\n        header field sizes.\n\n        .. warning::\n           Setting this parameter to a very high or unlimited value can open\n           up for DDOS attacks.\n        \"\"\"\n\n\nclass Reload(Setting):\n    name = \"reload\"\n    section = 'Debugging'\n    cli = ['--reload']\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n\n    desc = '''\\\n        Restart workers when code changes.\n\n        This setting is intended for development. It will cause workers to be\n        restarted whenever application code changes.\n\n        The reloader is incompatible with application preloading. When using a\n        paste configuration be sure that the server block does not import any\n        application code or the reload will not work as designed.\n\n        The default behavior is to attempt inotify with a fallback to file\n        system polling. Generally, inotify should be preferred if available\n        because it consumes less system resources.\n\n        .. note::\n           In order to use the inotify reloader, you must have the ``inotify``\n           package installed.\n        '''\n\n\nclass ReloadEngine(Setting):\n    name = \"reload_engine\"\n    section = \"Debugging\"\n    cli = [\"--reload-engine\"]\n    meta = \"STRING\"\n    validator = validate_reload_engine\n    default = \"auto\"\n    desc = \"\"\"\\\n        The implementation that should be used to power :ref:`reload`.\n\n        Valid engines are:\n\n        * ``'auto'``\n        * ``'poll'``\n        * ``'inotify'`` (requires inotify)\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass ReloadExtraFiles(Setting):\n    name = \"reload_extra_files\"\n    action = \"append\"\n    section = \"Debugging\"\n    cli = [\"--reload-extra-file\"]\n    meta = \"FILES\"\n    validator = validate_list_of_existing_files\n    default = []\n    desc = \"\"\"\\\n        Extends :ref:`reload` option to also watch and reload on additional files\n        (e.g., templates, configurations, specifications, etc.).\n\n        .. versionadded:: 19.8\n        \"\"\"\n\n\nclass Spew(Setting):\n    name = \"spew\"\n    section = \"Debugging\"\n    cli = [\"--spew\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Install a trace function that spews every line executed by the server.\n\n        This is the nuclear option.\n        \"\"\"\n\n\nclass ConfigCheck(Setting):\n    name = \"check_config\"\n    section = \"Debugging\"\n    cli = [\"--check-config\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Check the configuration and exit. The exit status is 0 if the\n        configuration is correct, and 1 if the configuration is incorrect.\n        \"\"\"\n\n\nclass PrintConfig(Setting):\n    name = \"print_config\"\n    section = \"Debugging\"\n    cli = [\"--print-config\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Print the configuration settings as fully resolved. Implies :ref:`check-config`.\n        \"\"\"\n\n\nclass PreloadApp(Setting):\n    name = \"preload_app\"\n    section = \"Server Mechanics\"\n    cli = [\"--preload\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Load application code before the worker processes are forked.\n\n        By preloading an application you can save some RAM resources as well as\n        speed up server boot times. Although, if you defer application loading\n        to each worker process, you can reload your application code easily by\n        restarting workers.\n        \"\"\"\n\n\nclass Sendfile(Setting):\n    name = \"sendfile\"\n    section = \"Server Mechanics\"\n    cli = [\"--no-sendfile\"]\n    validator = validate_bool\n    action = \"store_const\"\n    const = False\n\n    desc = \"\"\"\\\n        Disables the use of ``sendfile()``.\n\n        If not set, the value of the ``SENDFILE`` environment variable is used\n        to enable or disable its usage.\n\n        .. versionadded:: 19.2\n        .. versionchanged:: 19.4\n           Swapped ``--sendfile`` with ``--no-sendfile`` to actually allow\n           disabling.\n        .. versionchanged:: 19.6\n           added support for the ``SENDFILE`` environment variable\n        \"\"\"\n\n\nclass ReusePort(Setting):\n    name = \"reuse_port\"\n    section = \"Server Mechanics\"\n    cli = [\"--reuse-port\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n\n    desc = \"\"\"\\\n        Set the ``SO_REUSEPORT`` flag on the listening socket.\n\n        .. versionadded:: 19.8\n        \"\"\"\n\n\nclass Chdir(Setting):\n    name = \"chdir\"\n    section = \"Server Mechanics\"\n    cli = [\"--chdir\"]\n    validator = validate_chdir\n    default = util.getcwd()\n    default_doc = \"``'.'``\"\n    desc = \"\"\"\\\n        Change directory to specified directory before loading apps.\n        \"\"\"\n\n\nclass Daemon(Setting):\n    name = \"daemon\"\n    section = \"Server Mechanics\"\n    cli = [\"-D\", \"--daemon\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Daemonize the Gunicorn process.\n\n        Detaches the server from the controlling terminal and enters the\n        background.\n        \"\"\"\n\n\nclass Env(Setting):\n    name = \"raw_env\"\n    action = \"append\"\n    section = \"Server Mechanics\"\n    cli = [\"-e\", \"--env\"]\n    meta = \"ENV\"\n    validator = validate_list_string\n    default = []\n\n    desc = \"\"\"\\\n        Set environment variables in the execution environment.\n\n        Should be a list of strings in the ``key=value`` format.\n\n        For example on the command line:\n\n        .. code-block:: console\n\n            $ gunicorn -b 127.0.0.1:8000 --env FOO=1 test:app\n\n        Or in the configuration file:\n\n        .. code-block:: python\n\n            raw_env = [\"FOO=1\"]\n        \"\"\"\n\n\nclass Pidfile(Setting):\n    name = \"pidfile\"\n    section = \"Server Mechanics\"\n    cli = [\"-p\", \"--pid\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A filename to use for the PID file.\n\n        If not set, no PID file will be written.\n        \"\"\"\n\n\nclass WorkerTmpDir(Setting):\n    name = \"worker_tmp_dir\"\n    section = \"Server Mechanics\"\n    cli = [\"--worker-tmp-dir\"]\n    meta = \"DIR\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A directory to use for the worker heartbeat temporary file.\n\n        If not set, the default temporary directory will be used.\n\n        .. note::\n           The current heartbeat system involves calling ``os.fchmod`` on\n           temporary file handlers and may block a worker for arbitrary time\n           if the directory is on a disk-backed filesystem.\n\n           See :ref:`blocking-os-fchmod` for more detailed information\n           and a solution for avoiding this problem.\n        \"\"\"\n\n\nclass User(Setting):\n    name = \"user\"\n    section = \"Server Mechanics\"\n    cli = [\"-u\", \"--user\"]\n    meta = \"USER\"\n    validator = validate_user\n    default = os.geteuid()\n    default_doc = \"``os.geteuid()``\"\n    desc = \"\"\"\\\n        Switch worker processes to run as this user.\n\n        A valid user id (as an integer) or the name of a user that can be\n        retrieved with a call to ``pwd.getpwnam(value)`` or ``None`` to not\n        change the worker process user.\n        \"\"\"\n\n\nclass Group(Setting):\n    name = \"group\"\n    section = \"Server Mechanics\"\n    cli = [\"-g\", \"--group\"]\n    meta = \"GROUP\"\n    validator = validate_group\n    default = os.getegid()\n    default_doc = \"``os.getegid()``\"\n    desc = \"\"\"\\\n        Switch worker process to run as this group.\n\n        A valid group id (as an integer) or the name of a user that can be\n        retrieved with a call to ``pwd.getgrnam(value)`` or ``None`` to not\n        change the worker processes group.\n        \"\"\"\n\n\nclass Umask(Setting):\n    name = \"umask\"\n    section = \"Server Mechanics\"\n    cli = [\"-m\", \"--umask\"]\n    meta = \"INT\"\n    validator = validate_pos_int\n    type = auto_int\n    default = 0\n    desc = \"\"\"\\\n        A bit mask for the file mode on files written by Gunicorn.\n\n        Note that this affects unix socket permissions.\n\n        A valid value for the ``os.umask(mode)`` call or a string compatible\n        with ``int(value, 0)`` (``0`` means Python guesses the base, so values\n        like ``0``, ``0xFF``, ``0022`` are valid for decimal, hex, and octal\n        representations)\n        \"\"\"\n\n\nclass Initgroups(Setting):\n    name = \"initgroups\"\n    section = \"Server Mechanics\"\n    cli = [\"--initgroups\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n\n    desc = \"\"\"\\\n        If true, set the worker process's group access list with all of the\n        groups of which the specified username is a member, plus the specified\n        group id.\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass TmpUploadDir(Setting):\n    name = \"tmp_upload_dir\"\n    section = \"Server Mechanics\"\n    meta = \"DIR\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        Directory to store temporary request data as they are read.\n\n        This may disappear in the near future.\n\n        This path should be writable by the process permissions set for Gunicorn\n        workers. If not specified, Gunicorn will choose a system generated\n        temporary directory.\n        \"\"\"\n\n\nclass SecureSchemeHeader(Setting):\n    name = \"secure_scheme_headers\"\n    section = \"Server Mechanics\"\n    validator = validate_dict\n    default = {\n        \"X-FORWARDED-PROTOCOL\": \"ssl\",\n        \"X-FORWARDED-PROTO\": \"https\",\n        \"X-FORWARDED-SSL\": \"on\"\n    }\n    desc = \"\"\"\\\n\n        A dictionary containing headers and values that the front-end proxy\n        uses to indicate HTTPS requests. If the source IP is permitted by\n        ``forwarded-allow-ips`` (below), *and* at least one request header matches\n        a key-value pair listed in this dictionary, then Gunicorn will set\n        ``wsgi.url_scheme`` to ``https``, so your application can tell that the\n        request is secure.\n\n        If the other headers listed in this dictionary are not present in the request, they will be ignored,\n        but if the other headers are present and do not match the provided values, then\n        the request will fail to parse. See the note below for more detailed examples of this behaviour.\n\n        The dictionary should map upper-case header names to exact string\n        values. The value comparisons are case-sensitive, unlike the header\n        names, so make sure they're exactly what your front-end proxy sends\n        when handling HTTPS requests.\n\n        It is important that your front-end proxy configuration ensures that\n        the headers defined here can not be passed directly from the client.\n        \"\"\"\n\n\nclass ForwardedAllowIPS(Setting):\n    name = \"forwarded_allow_ips\"\n    section = \"Server Mechanics\"\n    cli = [\"--forwarded-allow-ips\"]\n    meta = \"STRING\"\n    validator = validate_string_to_list\n    default = os.environ.get(\"FORWARDED_ALLOW_IPS\", \"127.0.0.1\")\n    desc = \"\"\"\\\n        Front-end's IPs from which allowed to handle set secure headers.\n        (comma separate).\n\n        Set to ``*`` to disable checking of Front-end IPs (useful for setups\n        where you don't know in advance the IP address of Front-end, but\n        you still trust the environment).\n\n        By default, the value of the ``FORWARDED_ALLOW_IPS`` environment\n        variable. If it is not defined, the default is ``\"127.0.0.1\"``.\n\n        .. note::\n\n            The interplay between the request headers, the value of ``forwarded_allow_ips``, and the value of\n            ``secure_scheme_headers`` is complex. Various scenarios are documented below to further elaborate.\n            In each case, we have a request from the remote address 134.213.44.18, and the default value of\n            ``secure_scheme_headers``:\n\n            .. code::\n\n                secure_scheme_headers = {\n                    'X-FORWARDED-PROTOCOL': 'ssl',\n                    'X-FORWARDED-PROTO': 'https',\n                    'X-FORWARDED-SSL': 'on'\n                }\n\n\n            .. list-table::\n                :header-rows: 1\n                :align: center\n                :widths: auto\n\n                * - ``forwarded-allow-ips``\n                  - Secure Request Headers\n                  - Result\n                  - Explanation\n                * - .. code::\n\n                        [\"127.0.0.1\"]\n                  - .. code::\n\n                        X-Forwarded-Proto: https\n                  - .. code::\n\n                        wsgi.url_scheme = \"http\"\n                  - IP address was not allowed\n                * - .. code::\n\n                        \"*\"\n                  - <none>\n                  - .. code::\n\n                        wsgi.url_scheme = \"http\"\n                  - IP address allowed, but no secure headers provided\n                * - .. code::\n\n                        \"*\"\n                  - .. code::\n\n                        X-Forwarded-Proto: https\n                  - .. code::\n\n                        wsgi.url_scheme = \"https\"\n                  - IP address allowed, one request header matched\n                * - .. code::\n\n                        [\"134.213.44.18\"]\n                  - .. code::\n\n                        X-Forwarded-Ssl: on\n                        X-Forwarded-Proto: http\n                  - ``InvalidSchemeHeaders()`` raised\n                  - IP address allowed, but the two secure headers disagreed on if HTTPS was used\n\n\n        \"\"\"\n\n\nclass AccessLog(Setting):\n    name = \"accesslog\"\n    section = \"Logging\"\n    cli = [\"--access-logfile\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        The Access log file to write to.\n\n        ``'-'`` means log to stdout.\n        \"\"\"\n\n\nclass DisableRedirectAccessToSyslog(Setting):\n    name = \"disable_redirect_access_to_syslog\"\n    section = \"Logging\"\n    cli = [\"--disable-redirect-access-to-syslog\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n    desc = \"\"\"\\\n    Disable redirect access logs to syslog.\n\n    .. versionadded:: 19.8\n    \"\"\"\n\n\nclass AccessLogFormat(Setting):\n    name = \"access_log_format\"\n    section = \"Logging\"\n    cli = [\"--access-logformat\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = '%(h)s %(l)s %(u)s %(t)s \"%(r)s\" %(s)s %(b)s \"%(f)s\" \"%(a)s\"'\n    desc = \"\"\"\\\n        The access log format.\n\n        ===========  ===========\n        Identifier   Description\n        ===========  ===========\n        h            remote address\n        l            ``'-'``\n        u            user name\n        t            date of the request\n        r            status line (e.g. ``GET / HTTP/1.1``)\n        m            request method\n        U            URL path without query string\n        q            query string\n        H            protocol\n        s            status\n        B            response length\n        b            response length or ``'-'`` (CLF format)\n        f            referer\n        a            user agent\n        T            request time in seconds\n        M            request time in milliseconds\n        D            request time in microseconds\n        L            request time in decimal seconds\n        p            process ID\n        {header}i    request header\n        {header}o    response header\n        {variable}e  environment variable\n        ===========  ===========\n\n        Use lowercase for header and environment variable names, and put\n        ``{...}x`` names inside ``%(...)s``. For example::\n\n            %({x-forwarded-for}i)s\n        \"\"\"\n\n\nclass ErrorLog(Setting):\n    name = \"errorlog\"\n    section = \"Logging\"\n    cli = [\"--error-logfile\", \"--log-file\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = '-'\n    desc = \"\"\"\\\n        The Error log file to write to.\n\n        Using ``'-'`` for FILE makes gunicorn log to stderr.\n\n        .. versionchanged:: 19.2\n           Log to stderr by default.\n\n        \"\"\"\n\n\nclass Loglevel(Setting):\n    name = \"loglevel\"\n    section = \"Logging\"\n    cli = [\"--log-level\"]\n    meta = \"LEVEL\"\n    validator = validate_string\n    default = \"info\"\n    desc = \"\"\"\\\n        The granularity of Error log outputs.\n\n        Valid level names are:\n\n        * ``'debug'``\n        * ``'info'``\n        * ``'warning'``\n        * ``'error'``\n        * ``'critical'``\n        \"\"\"\n\n\nclass CaptureOutput(Setting):\n    name = \"capture_output\"\n    section = \"Logging\"\n    cli = [\"--capture-output\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n    desc = \"\"\"\\\n        Redirect stdout/stderr to specified file in :ref:`errorlog`.\n\n        .. versionadded:: 19.6\n        \"\"\"\n\n\nclass LoggerClass(Setting):\n    name = \"logger_class\"\n    section = \"Logging\"\n    cli = [\"--logger-class\"]\n    meta = \"STRING\"\n    validator = validate_class\n    default = \"gunicorn.glogging.Logger\"\n    desc = \"\"\"\\\n        The logger you want to use to log events in Gunicorn.\n\n        The default class (``gunicorn.glogging.Logger``) handles most\n        normal usages in logging. It provides error and access logging.\n\n        You can provide your own logger by giving Gunicorn a Python path to a\n        class that quacks like ``gunicorn.glogging.Logger``.\n        \"\"\"\n\n\nclass LogConfig(Setting):\n    name = \"logconfig\"\n    section = \"Logging\"\n    cli = [\"--log-config\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    The log config file to use.\n    Gunicorn uses the standard Python logging module's Configuration\n    file format.\n    \"\"\"\n\n\nclass LogConfigDict(Setting):\n    name = \"logconfig_dict\"\n    section = \"Logging\"\n    validator = validate_dict\n    default = {}\n    desc = \"\"\"\\\n    The log config dictionary to use, using the standard Python\n    logging module's dictionary configuration format. This option\n    takes precedence over the :ref:`logconfig` and :ref:`logConfigJson` options,\n    which uses the older file configuration format and JSON\n    respectively.\n\n    Format: https://docs.python.org/3/library/logging.config.html#logging.config.dictConfig\n\n    For more context you can look at the default configuration dictionary for logging,\n    which can be found at ``gunicorn.glogging.CONFIG_DEFAULTS``.\n\n    .. versionadded:: 19.8\n    \"\"\"\n\n\nclass LogConfigJson(Setting):\n    name = \"logconfig_json\"\n    section = \"Logging\"\n    cli = [\"--log-config-json\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    The log config to read config from a JSON file\n\n    Format: https://docs.python.org/3/library/logging.config.html#logging.config.jsonConfig\n\n    .. versionadded:: 20.0\n    \"\"\"\n\n\nclass SyslogTo(Setting):\n    name = \"syslog_addr\"\n    section = \"Logging\"\n    cli = [\"--log-syslog-to\"]\n    meta = \"SYSLOG_ADDR\"\n    validator = validate_string\n\n    if PLATFORM == \"darwin\":\n        default = \"unix:///var/run/syslog\"\n    elif PLATFORM in ('freebsd', 'dragonfly', ):\n        default = \"unix:///var/run/log\"\n    elif PLATFORM == \"openbsd\":\n        default = \"unix:///dev/log\"\n    else:\n        default = \"udp://localhost:514\"\n\n    desc = \"\"\"\\\n    Address to send syslog messages.\n\n    Address is a string of the form:\n\n    * ``unix://PATH#TYPE`` : for unix domain socket. ``TYPE`` can be ``stream``\n      for the stream driver or ``dgram`` for the dgram driver.\n      ``stream`` is the default.\n    * ``udp://HOST:PORT`` : for UDP sockets\n    * ``tcp://HOST:PORT`` : for TCP sockets\n\n    \"\"\"\n\n\nclass Syslog(Setting):\n    name = \"syslog\"\n    section = \"Logging\"\n    cli = [\"--log-syslog\"]\n    validator = validate_bool\n    action = 'store_true'\n    default = False\n    desc = \"\"\"\\\n    Send *Gunicorn* logs to syslog.\n\n    .. versionchanged:: 19.8\n       You can now disable sending access logs by using the\n       :ref:`disable-redirect-access-to-syslog` setting.\n    \"\"\"\n\n\nclass SyslogPrefix(Setting):\n    name = \"syslog_prefix\"\n    section = \"Logging\"\n    cli = [\"--log-syslog-prefix\"]\n    meta = \"SYSLOG_PREFIX\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    Makes Gunicorn use the parameter as program-name in the syslog entries.\n\n    All entries will be prefixed by ``gunicorn.<prefix>``. By default the\n    program name is the name of the process.\n    \"\"\"\n\n\nclass SyslogFacility(Setting):\n    name = \"syslog_facility\"\n    section = \"Logging\"\n    cli = [\"--log-syslog-facility\"]\n    meta = \"SYSLOG_FACILITY\"\n    validator = validate_string\n    default = \"user\"\n    desc = \"\"\"\\\n    Syslog facility name\n    \"\"\"\n\n\nclass EnableStdioInheritance(Setting):\n    name = \"enable_stdio_inheritance\"\n    section = \"Logging\"\n    cli = [\"-R\", \"--enable-stdio-inheritance\"]\n    validator = validate_bool\n    default = False\n    action = \"store_true\"\n    desc = \"\"\"\\\n    Enable stdio inheritance.\n\n    Enable inheritance for stdio file descriptors in daemon mode.\n\n    Note: To disable the Python stdout buffering, you can to set the user\n    environment variable ``PYTHONUNBUFFERED`` .\n    \"\"\"\n\n\n# statsD monitoring\nclass StatsdHost(Setting):\n    name = \"statsd_host\"\n    section = \"Logging\"\n    cli = [\"--statsd-host\"]\n    meta = \"STATSD_ADDR\"\n    default = None\n    validator = validate_statsd_address\n    desc = \"\"\"\\\n    The address of the StatsD server to log to.\n\n    Address is a string of the form:\n\n    * ``unix://PATH`` : for a unix domain socket.\n    * ``HOST:PORT`` : for a network address\n\n    .. versionadded:: 19.1\n    \"\"\"\n\n\n# Datadog Statsd (dogstatsd) tags. https://docs.datadoghq.com/developers/dogstatsd/\nclass DogstatsdTags(Setting):\n    name = \"dogstatsd_tags\"\n    section = \"Logging\"\n    cli = [\"--dogstatsd-tags\"]\n    meta = \"DOGSTATSD_TAGS\"\n    default = \"\"\n    validator = validate_string\n    desc = \"\"\"\\\n    A comma-delimited list of datadog statsd (dogstatsd) tags to append to\n    statsd metrics.\n\n    .. versionadded:: 20\n    \"\"\"\n\n\nclass StatsdPrefix(Setting):\n    name = \"statsd_prefix\"\n    section = \"Logging\"\n    cli = [\"--statsd-prefix\"]\n    meta = \"STATSD_PREFIX\"\n    default = \"\"\n    validator = validate_string\n    desc = \"\"\"\\\n    Prefix to use when emitting statsd metrics (a trailing ``.`` is added,\n    if not provided).\n\n    .. versionadded:: 19.2\n    \"\"\"\n\n\nclass Procname(Setting):\n    name = \"proc_name\"\n    section = \"Process Naming\"\n    cli = [\"-n\", \"--name\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A base to use with setproctitle for process naming.\n\n        This affects things like ``ps`` and ``top``. If you're going to be\n        running more than one instance of Gunicorn you'll probably want to set a\n        name to tell them apart. This requires that you install the setproctitle\n        module.\n\n        If not set, the *default_proc_name* setting will be used.\n        \"\"\"\n\n\nclass DefaultProcName(Setting):\n    name = \"default_proc_name\"\n    section = \"Process Naming\"\n    validator = validate_string\n    default = \"gunicorn\"\n    desc = \"\"\"\\\n        Internal setting that is adjusted for each type of application.\n        \"\"\"\n\n\nclass PythonPath(Setting):\n    name = \"pythonpath\"\n    section = \"Server Mechanics\"\n    cli = [\"--pythonpath\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        A comma-separated list of directories to add to the Python path.\n\n        e.g.\n        ``'/home/djangoprojects/myproject,/home/python/mylibrary'``.\n        \"\"\"\n\n\nclass Paste(Setting):\n    name = \"paste\"\n    section = \"Server Mechanics\"\n    cli = [\"--paste\", \"--paster\"]\n    meta = \"STRING\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n        Load a PasteDeploy config file. The argument may contain a ``#``\n        symbol followed by the name of an app section from the config file,\n        e.g. ``production.ini#admin``.\n\n        At this time, using alternate server blocks is not supported. Use the\n        command line arguments to control server configuration instead.\n        \"\"\"\n\n\nclass OnStarting(Setting):\n    name = \"on_starting\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def on_starting(server):\n        pass\n    default = staticmethod(on_starting)\n    desc = \"\"\"\\\n        Called just before the master process is initialized.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass OnReload(Setting):\n    name = \"on_reload\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def on_reload(server):\n        pass\n    default = staticmethod(on_reload)\n    desc = \"\"\"\\\n        Called to recycle workers during a reload via SIGHUP.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass WhenReady(Setting):\n    name = \"when_ready\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def when_ready(server):\n        pass\n    default = staticmethod(when_ready)\n    desc = \"\"\"\\\n        Called just after the server is started.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass Prefork(Setting):\n    name = \"pre_fork\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def pre_fork(server, worker):\n        pass\n    default = staticmethod(pre_fork)\n    desc = \"\"\"\\\n        Called just before a worker is forked.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        new Worker.\n        \"\"\"\n\n\nclass Postfork(Setting):\n    name = \"post_fork\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def post_fork(server, worker):\n        pass\n    default = staticmethod(post_fork)\n    desc = \"\"\"\\\n        Called just after a worker has been forked.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        new Worker.\n        \"\"\"\n\n\nclass PostWorkerInit(Setting):\n    name = \"post_worker_init\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def post_worker_init(worker):\n        pass\n\n    default = staticmethod(post_worker_init)\n    desc = \"\"\"\\\n        Called just after a worker has initialized the application.\n\n        The callable needs to accept one instance variable for the initialized\n        Worker.\n        \"\"\"\n\n\nclass WorkerInt(Setting):\n    name = \"worker_int\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def worker_int(worker):\n        pass\n\n    default = staticmethod(worker_int)\n    desc = \"\"\"\\\n        Called just after a worker exited on SIGINT or SIGQUIT.\n\n        The callable needs to accept one instance variable for the initialized\n        Worker.\n        \"\"\"\n\n\nclass WorkerAbort(Setting):\n    name = \"worker_abort\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def worker_abort(worker):\n        pass\n\n    default = staticmethod(worker_abort)\n    desc = \"\"\"\\\n        Called when a worker received the SIGABRT signal.\n\n        This call generally happens on timeout.\n\n        The callable needs to accept one instance variable for the initialized\n        Worker.\n        \"\"\"\n\n\nclass PreExec(Setting):\n    name = \"pre_exec\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n    type = callable\n\n    def pre_exec(server):\n        pass\n    default = staticmethod(pre_exec)\n    desc = \"\"\"\\\n        Called just before a new master process is forked.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass PreRequest(Setting):\n    name = \"pre_request\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def pre_request(worker, req):\n        worker.log.debug(\"%s %s\", req.method, req.path)\n    default = staticmethod(pre_request)\n    desc = \"\"\"\\\n        Called just before a worker processes the request.\n\n        The callable needs to accept two instance variables for the Worker and\n        the Request.\n        \"\"\"\n\n\nclass PostRequest(Setting):\n    name = \"post_request\"\n    section = \"Server Hooks\"\n    validator = validate_post_request\n    type = callable\n\n    def post_request(worker, req, environ, resp):\n        pass\n    default = staticmethod(post_request)\n    desc = \"\"\"\\\n        Called after a worker processes the request.\n\n        The callable needs to accept two instance variables for the Worker and\n        the Request.\n        \"\"\"\n\n\nclass ChildExit(Setting):\n    name = \"child_exit\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def child_exit(server, worker):\n        pass\n    default = staticmethod(child_exit)\n    desc = \"\"\"\\\n        Called just after a worker has been exited, in the master process.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        the just-exited Worker.\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass WorkerExit(Setting):\n    name = \"worker_exit\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def worker_exit(server, worker):\n        pass\n    default = staticmethod(worker_exit)\n    desc = \"\"\"\\\n        Called just after a worker has been exited, in the worker process.\n\n        The callable needs to accept two instance variables for the Arbiter and\n        the just-exited Worker.\n        \"\"\"\n\n\nclass NumWorkersChanged(Setting):\n    name = \"nworkers_changed\"\n    section = \"Server Hooks\"\n    validator = validate_callable(3)\n    type = callable\n\n    def nworkers_changed(server, new_value, old_value):\n        pass\n    default = staticmethod(nworkers_changed)\n    desc = \"\"\"\\\n        Called just after *num_workers* has been changed.\n\n        The callable needs to accept an instance variable of the Arbiter and\n        two integers of number of workers after and before change.\n\n        If the number of workers is set for the first time, *old_value* would\n        be ``None``.\n        \"\"\"\n\n\nclass OnExit(Setting):\n    name = \"on_exit\"\n    section = \"Server Hooks\"\n    validator = validate_callable(1)\n\n    def on_exit(server):\n        pass\n\n    default = staticmethod(on_exit)\n    desc = \"\"\"\\\n        Called just before exiting Gunicorn.\n\n        The callable needs to accept a single instance variable for the Arbiter.\n        \"\"\"\n\n\nclass NewSSLContext(Setting):\n    name = \"ssl_context\"\n    section = \"Server Hooks\"\n    validator = validate_callable(2)\n    type = callable\n\n    def ssl_context(config, default_ssl_context_factory):\n        return default_ssl_context_factory()\n\n    default = staticmethod(ssl_context)\n    desc = \"\"\"\\\n        Called when SSLContext is needed.\n\n        Allows customizing SSL context.\n\n        The callable needs to accept an instance variable for the Config and\n        a factory function that returns default SSLContext which is initialized\n        with certificates, private key, cert_reqs, and ciphers according to\n        config and can be further customized by the callable.\n        The callable needs to return SSLContext object.\n\n        Following example shows a configuration file that sets the minimum TLS version to 1.3:\n\n        .. code-block:: python\n\n            def ssl_context(conf, default_ssl_context_factory):\n                import ssl\n                context = default_ssl_context_factory()\n                context.minimum_version = ssl.TLSVersion.TLSv1_3\n                return context\n\n        .. versionadded:: 20.2\n        \"\"\"\n\n\nclass ProxyProtocol(Setting):\n    name = \"proxy_protocol\"\n    section = \"Server Mechanics\"\n    cli = [\"--proxy-protocol\"]\n    validator = validate_bool\n    default = False\n    action = \"store_true\"\n    desc = \"\"\"\\\n        Enable detect PROXY protocol (PROXY mode).\n\n        Allow using HTTP and Proxy together. It may be useful for work with\n        stunnel as HTTPS frontend and Gunicorn as HTTP server.\n\n        PROXY protocol: http://haproxy.1wt.eu/download/1.5/doc/proxy-protocol.txt\n\n        Example for stunnel config::\n\n            [https]\n            protocol = proxy\n            accept  = 443\n            connect = 80\n            cert = /etc/ssl/certs/stunnel.pem\n            key = /etc/ssl/certs/stunnel.key\n        \"\"\"\n\n\nclass ProxyAllowFrom(Setting):\n    name = \"proxy_allow_ips\"\n    section = \"Server Mechanics\"\n    cli = [\"--proxy-allow-from\"]\n    validator = validate_string_to_list\n    default = \"127.0.0.1\"\n    desc = \"\"\"\\\n        Front-end's IPs from which allowed accept proxy requests (comma separate).\n\n        Set to ``*`` to disable checking of Front-end IPs (useful for setups\n        where you don't know in advance the IP address of Front-end, but\n        you still trust the environment)\n        \"\"\"\n\n\nclass KeyFile(Setting):\n    name = \"keyfile\"\n    section = \"SSL\"\n    cli = [\"--keyfile\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    SSL key file\n    \"\"\"\n\n\nclass CertFile(Setting):\n    name = \"certfile\"\n    section = \"SSL\"\n    cli = [\"--certfile\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    SSL certificate file\n    \"\"\"\n\n\nclass SSLVersion(Setting):\n    name = \"ssl_version\"\n    section = \"SSL\"\n    cli = [\"--ssl-version\"]\n    validator = validate_ssl_version\n\n    if hasattr(ssl, \"PROTOCOL_TLS\"):\n        default = ssl.PROTOCOL_TLS\n    else:\n        default = ssl.PROTOCOL_SSLv23\n\n    default = ssl.PROTOCOL_SSLv23\n    desc = \"\"\"\\\n    SSL version to use (see stdlib ssl module's).\n\n    .. deprecated:: 20.2\n       The option is deprecated and it is currently ignored. Use :ref:`ssl-context` instead.\n\n    ============= ============\n    --ssl-version Description\n    ============= ============\n    SSLv3         SSLv3 is not-secure and is strongly discouraged.\n    SSLv23        Alias for TLS. Deprecated in Python 3.6, use TLS.\n    TLS           Negotiate highest possible version between client/server.\n                  Can yield SSL. (Python 3.6+)\n    TLSv1         TLS 1.0\n    TLSv1_1       TLS 1.1 (Python 3.4+)\n    TLSv1_2       TLS 1.2 (Python 3.4+)\n    TLS_SERVER    Auto-negotiate the highest protocol version like TLS,\n                  but only support server-side SSLSocket connections.\n                  (Python 3.6+)\n    ============= ============\n\n    .. versionchanged:: 19.7\n       The default value has been changed from ``ssl.PROTOCOL_TLSv1`` to\n       ``ssl.PROTOCOL_SSLv23``.\n    .. versionchanged:: 20.0\n       This setting now accepts string names based on ``ssl.PROTOCOL_``\n       constants.\n    .. versionchanged:: 20.0.1\n       The default value has been changed from ``ssl.PROTOCOL_SSLv23`` to\n       ``ssl.PROTOCOL_TLS`` when Python >= 3.6 .\n    \"\"\"\n\n\nclass CertReqs(Setting):\n    name = \"cert_reqs\"\n    section = \"SSL\"\n    cli = [\"--cert-reqs\"]\n    validator = validate_pos_int\n    default = ssl.CERT_NONE\n    desc = \"\"\"\\\n    Whether client certificate is required (see stdlib ssl module's)\n\n    ===========  ===========================\n    --cert-reqs      Description\n    ===========  ===========================\n    `0`          no client veirifcation\n    `1`          ssl.CERT_OPTIONAL\n    `2`          ssl.CERT_REQUIRED\n    ===========  ===========================\n    \"\"\"\n\n\nclass CACerts(Setting):\n    name = \"ca_certs\"\n    section = \"SSL\"\n    cli = [\"--ca-certs\"]\n    meta = \"FILE\"\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    CA certificates file\n    \"\"\"\n\n\nclass SuppressRaggedEOFs(Setting):\n    name = \"suppress_ragged_eofs\"\n    section = \"SSL\"\n    cli = [\"--suppress-ragged-eofs\"]\n    action = \"store_true\"\n    default = True\n    validator = validate_bool\n    desc = \"\"\"\\\n    Suppress ragged EOFs (see stdlib ssl module's)\n    \"\"\"\n\n\nclass DoHandshakeOnConnect(Setting):\n    name = \"do_handshake_on_connect\"\n    section = \"SSL\"\n    cli = [\"--do-handshake-on-connect\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n    Whether to perform SSL handshake on socket connect (see stdlib ssl module's)\n    \"\"\"\n\n\nclass Ciphers(Setting):\n    name = \"ciphers\"\n    section = \"SSL\"\n    cli = [\"--ciphers\"]\n    validator = validate_string\n    default = None\n    desc = \"\"\"\\\n    SSL Cipher suite to use, in the format of an OpenSSL cipher list.\n\n    By default we use the default cipher list from Python's ``ssl`` module,\n    which contains ciphers considered strong at the time of each Python\n    release.\n\n    As a recommended alternative, the Open Web App Security Project (OWASP)\n    offers `a vetted set of strong cipher strings rated A+ to C-\n    <https://www.owasp.org/index.php/TLS_Cipher_String_Cheat_Sheet>`_.\n    OWASP provides details on user-agent compatibility at each security level.\n\n    See the `OpenSSL Cipher List Format Documentation\n    <https://www.openssl.org/docs/manmaster/man1/ciphers.html#CIPHER-LIST-FORMAT>`_\n    for details on the format of an OpenSSL cipher list.\n    \"\"\"\n\n\nclass PasteGlobalConf(Setting):\n    name = \"raw_paste_global_conf\"\n    action = \"append\"\n    section = \"Server Mechanics\"\n    cli = [\"--paste-global\"]\n    meta = \"CONF\"\n    validator = validate_list_string\n    default = []\n\n    desc = \"\"\"\\\n        Set a PasteDeploy global config variable in ``key=value`` form.\n\n        The option can be specified multiple times.\n\n        The variables are passed to the the PasteDeploy entrypoint. Example::\n\n            $ gunicorn -b 127.0.0.1:8000 --paste development.ini --paste-global FOO=1 --paste-global BAR=2\n\n        .. versionadded:: 19.7\n        \"\"\"\n\n\nclass StripHeaderSpaces(Setting):\n    name = \"strip_header_spaces\"\n    section = \"Server Mechanics\"\n    cli = [\"--strip-header-spaces\"]\n    validator = validate_bool\n    action = \"store_true\"\n    default = False\n    desc = \"\"\"\\\n        Strip spaces present between the header name and the the ``:``.\n\n        This is known to induce vulnerabilities and is not compliant with the HTTP/1.1 standard.\n        See https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn.\n\n        Use with care and only if necessary.\n        \"\"\"\n\n        ####cross_file_context:\n        [{'gunicorn.util': '# -*- coding: utf-8 -\\n#\\n# This file is part of gunicorn released under the MIT license.\\n# See the NOTICE for more information.\\nimport ast\\nimport email.utils\\nimport errno\\nimport fcntl\\nimport html\\nimport importlib\\nimport inspect\\nimport io\\nimport logging\\nimport os\\nimport pwd\\nimport random\\nimport re\\nimport socket\\nimport sys\\nimport textwrap\\nimport time\\nimport traceback\\nimport warnings\\n\\ntry:\\n    import importlib.metadata as importlib_metadata\\nexcept (ModuleNotFoundError, ImportError):\\n    import importlib_metadata\\n\\nfrom gunicorn.errors import AppImportError\\nfrom gunicorn.workers import SUPPORTED_WORKERS\\nimport urllib.parse\\n\\nREDIRECT_TO = getattr(os, \\'devnull\\', \\'/dev/null\\')\\n\\n# Server and Date aren\\'t technically hop-by-hop\\n# headers, but they are in the purview of the\\n# origin server which the WSGI spec says we should\\n# act like. So we drop them and add our own.\\n#\\n# In the future, concatenation server header values\\n# might be better, but nothing else does it and\\n# dropping them is easier.\\nhop_headers = set(\"\"\"\\n    connection keep-alive proxy-authenticate proxy-authorization\\n    te trailers transfer-encoding upgrade\\n    server date\\n    \"\"\".split())\\n\\ntry:\\n    from setproctitle import setproctitle\\n\\n    def _setproctitle(title):\\n        setproctitle(\"gunicorn: %s\" % title)\\nexcept ImportError:\\n    def _setproctitle(title):\\n        pass\\n\\n\\ndef load_entry_point(distribution, group, name):\\n    dist_obj = importlib_metadata.distribution(distribution)\\n    eps = [ep for ep in dist_obj.entry_points\\n           if ep.group == group and ep.name == name]\\n    if not eps:\\n        raise ImportError(\"Entry point %r not found\" % ((group, name),))\\n    return eps[0].load()\\n\\n\\ndef load_class(uri, default=\"gunicorn.workers.sync.SyncWorker\",\\n               section=\"gunicorn.workers\"):\\n    if inspect.isclass(uri):\\n        return uri\\n    if uri.startswith(\"egg:\"):\\n        # uses entry points\\n        entry_str = uri.split(\"egg:\")[1]\\n        try:\\n            dist, name = entry_str.rsplit(\"#\", 1)\\n        except ValueError:\\n            dist = entry_str\\n            name = default\\n\\n        try:\\n            return load_entry_point(dist, section, name)\\n        except Exception:\\n            exc = traceback.format_exc()\\n            msg = \"class uri %r invalid or not found: \\\\n\\\\n[%s]\"\\n            raise RuntimeError(msg % (uri, exc))\\n    else:\\n        components = uri.split(\\'.\\')\\n        if len(components) == 1:\\n            while True:\\n                if uri.startswith(\"#\"):\\n                    uri = uri[1:]\\n\\n                if uri in SUPPORTED_WORKERS:\\n                    components = SUPPORTED_WORKERS[uri].split(\".\")\\n                    break\\n\\n                try:\\n                    return load_entry_point(\\n                        \"gunicorn\", section, uri\\n                    )\\n                except Exception:\\n                    exc = traceback.format_exc()\\n                    msg = \"class uri %r invalid or not found: \\\\n\\\\n[%s]\"\\n                    raise RuntimeError(msg % (uri, exc))\\n\\n        klass = components.pop(-1)\\n\\n        try:\\n            mod = importlib.import_module(\\'.\\'.join(components))\\n        except Exception:\\n            exc = traceback.format_exc()\\n            msg = \"class uri %r invalid or not found: \\\\n\\\\n[%s]\"\\n            raise RuntimeError(msg % (uri, exc))\\n        return getattr(mod, klass)\\n\\n\\npositionals = (\\n    inspect.Parameter.POSITIONAL_ONLY,\\n    inspect.Parameter.POSITIONAL_OR_KEYWORD,\\n)\\n\\n\\ndef get_arity(f):\\n    sig = inspect.signature(f)\\n    arity = 0\\n\\n    for param in sig.parameters.values():\\n        if param.kind in positionals:\\n            arity += 1\\n\\n    return arity\\n\\n\\ndef get_username(uid):\\n    \"\"\" get the username for a user id\"\"\"\\n    return pwd.getpwuid(uid).pw_name\\n\\n\\ndef set_owner_process(uid, gid, initgroups=False):\\n    \"\"\" set user and group of workers processes \"\"\"\\n\\n    if gid:\\n        if uid:\\n            try:\\n                username = get_username(uid)\\n            except KeyError:\\n                initgroups = False\\n\\n        # versions of python < 2.6.2 don\\'t manage unsigned int for\\n        # groups like on osx or fedora\\n        gid = abs(gid) & 0x7FFFFFFF\\n\\n        if initgroups:\\n            os.initgroups(username, gid)\\n        elif gid != os.getgid():\\n            os.setgid(gid)\\n\\n    if uid and uid != os.getuid():\\n        os.setuid(uid)\\n\\n\\ndef chown(path, uid, gid):\\n    os.chown(path, uid, gid)\\n\\n\\nif sys.platform.startswith(\"win\"):\\n    def _waitfor(func, pathname, waitall=False):\\n        # Perform the operation\\n        func(pathname)\\n        # Now setup the wait loop\\n        if waitall:\\n            dirname = pathname\\n        else:\\n            dirname, name = os.path.split(pathname)\\n            dirname = dirname or \\'.\\'\\n        # Check for `pathname` to be removed from the filesystem.\\n        # The exponential backoff of the timeout amounts to a total\\n        # of ~1 second after which the deletion is probably an error\\n        # anyway.\\n        # Testing on a i7@4.3GHz shows that usually only 1 iteration is\\n        # required when contention occurs.\\n        timeout = 0.001\\n        while timeout < 1.0:\\n            # Note we are only testing for the existence of the file(s) in\\n            # the contents of the directory regardless of any security or\\n            # access rights.  If we have made it this far, we have sufficient\\n            # permissions to do that much using Python\\'s equivalent of the\\n            # Windows API FindFirstFile.\\n            # Other Windows APIs can fail or give incorrect results when\\n            # dealing with files that are pending deletion.\\n            L = os.listdir(dirname)\\n            if not L if waitall else name in L:\\n                return\\n            # Increase the timeout and try again\\n            time.sleep(timeout)\\n            timeout *= 2\\n        warnings.warn(\\'tests may fail, delete still pending for \\' + pathname,\\n                      RuntimeWarning, stacklevel=4)\\n\\n    def _unlink(filename):\\n        _waitfor(os.unlink, filename)\\nelse:\\n    _unlink = os.unlink\\n\\n\\ndef unlink(filename):\\n    try:\\n        _unlink(filename)\\n    except OSError as error:\\n        # The filename need not exist.\\n        if error.errno not in (errno.ENOENT, errno.ENOTDIR):\\n            raise\\n\\n\\ndef is_ipv6(addr):\\n    try:\\n        socket.inet_pton(socket.AF_INET6, addr)\\n    except socket.error:  # not a valid address\\n        return False\\n    except ValueError:  # ipv6 not supported on this platform\\n        return False\\n    return True\\n\\n\\ndef parse_address(netloc, default_port=\\'8000\\'):\\n    if re.match(r\\'unix:(//)?\\', netloc):\\n        return re.split(r\\'unix:(//)?\\', netloc)[-1]\\n\\n    if netloc.startswith(\"fd://\"):\\n        fd = netloc[5:]\\n        try:\\n            return int(fd)\\n        except ValueError:\\n            raise RuntimeError(\"%r is not a valid file descriptor.\" % fd) from None\\n\\n    if netloc.startswith(\"tcp://\"):\\n        netloc = netloc.split(\"tcp://\")[1]\\n    host, port = netloc, default_port\\n\\n    if \\'[\\' in netloc and \\']\\' in netloc:\\n        host = netloc.split(\\']\\')[0][1:]\\n        port = (netloc.split(\\']:\\') + [default_port])[1]\\n    elif \\':\\' in netloc:\\n        host, port = (netloc.split(\\':\\') + [default_port])[:2]\\n    elif netloc == \"\":\\n        host, port = \"0.0.0.0\", default_port\\n\\n    try:\\n        port = int(port)\\n    except ValueError:\\n        raise RuntimeError(\"%r is not a valid port number.\" % port)\\n\\n    return host.lower(), port\\n\\n\\ndef close_on_exec(fd):\\n    flags = fcntl.fcntl(fd, fcntl.F_GETFD)\\n    flags |= fcntl.FD_CLOEXEC\\n    fcntl.fcntl(fd, fcntl.F_SETFD, flags)\\n\\n\\ndef set_non_blocking(fd):\\n    flags = fcntl.fcntl(fd, fcntl.F_GETFL) | os.O_NONBLOCK\\n    fcntl.fcntl(fd, fcntl.F_SETFL, flags)\\n\\n\\ndef close(sock):\\n    try:\\n        sock.close()\\n    except socket.error:\\n        pass\\n\\n\\ntry:\\n    from os import closerange\\nexcept ImportError:\\n    def closerange(fd_low, fd_high):\\n        # Iterate through and close all file descriptors.\\n        for fd in range(fd_low, fd_high):\\n            try:\\n                os.close(fd)\\n            except OSError:  # ERROR, fd wasn\\'t open to begin with (ignored)\\n                pass\\n\\n\\ndef write_chunk(sock, data):\\n    if isinstance(data, str):\\n        data = data.encode(\\'utf-8\\')\\n    chunk_size = \"%X\\\\r\\\\n\" % len(data)\\n    chunk = b\"\".join([chunk_size.encode(\\'utf-8\\'), data, b\"\\\\r\\\\n\"])\\n    sock.sendall(chunk)\\n\\n\\ndef write(sock, data, chunked=False):\\n    if chunked:\\n        return write_chunk(sock, data)\\n    sock.sendall(data)\\n\\n\\ndef write_nonblock(sock, data, chunked=False):\\n    timeout = sock.gettimeout()\\n    if timeout != 0.0:\\n        try:\\n            sock.setblocking(0)\\n            return write(sock, data, chunked)\\n        finally:\\n            sock.setblocking(1)\\n    else:\\n        return write(sock, data, chunked)\\n\\n\\ndef write_error(sock, status_int, reason, mesg):\\n    html_error = textwrap.dedent(\"\"\"\\\\\\n    <html>\\n      <head>\\n        <title>%(reason)s</title>\\n      </head>\\n      <body>\\n        <h1><p>%(reason)s</p></h1>\\n        %(mesg)s\\n      </body>\\n    </html>\\n    \"\"\") % {\"reason\": reason, \"mesg\": html.escape(mesg)}\\n\\n    http = textwrap.dedent(\"\"\"\\\\\\n    HTTP/1.1 %s %s\\\\r\\n    Connection: close\\\\r\\n    Content-Type: text/html\\\\r\\n    Content-Length: %d\\\\r\\n    \\\\r\\n    %s\"\"\") % (str(status_int), reason, len(html_error), html_error)\\n    write_nonblock(sock, http.encode(\\'latin1\\'))\\n\\n\\ndef _called_with_wrong_args(f):\\n    \"\"\"Check whether calling a function raised a ``TypeError`` because\\n    the call failed or because something in the function raised the\\n    error.\\n\\n    :param f: The function that was called.\\n    :return: ``True`` if the call failed.\\n    \"\"\"\\n    tb = sys.exc_info()[2]\\n\\n    try:\\n        while tb is not None:\\n            if tb.tb_frame.f_code is f.__code__:\\n                # In the function, it was called successfully.\\n                return False\\n\\n            tb = tb.tb_next\\n\\n        # Didn\\'t reach the function.\\n        return True\\n    finally:\\n        # Delete tb to break a circular reference in Python 2.\\n        # https://docs.python.org/2/library/sys.html#sys.exc_info\\n        del tb\\n\\n\\ndef import_app(module):\\n    parts = module.split(\":\", 1)\\n    if len(parts) == 1:\\n        obj = \"application\"\\n    else:\\n        module, obj = parts[0], parts[1]\\n\\n    try:\\n        mod = importlib.import_module(module)\\n    except ImportError:\\n        if module.endswith(\".py\") and os.path.exists(module):\\n            msg = \"Failed to find application, did you mean \\'%s:%s\\'?\"\\n            raise ImportError(msg % (module.rsplit(\".\", 1)[0], obj))\\n        raise\\n\\n    # Parse obj as a single expression to determine if it\\'s a valid\\n    # attribute name or function call.\\n    try:\\n        expression = ast.parse(obj, mode=\"eval\").body\\n    except SyntaxError:\\n        raise AppImportError(\\n            \"Failed to parse %r as an attribute name or function call.\" % obj\\n        )\\n\\n    if isinstance(expression, ast.Name):\\n        name = expression.id\\n        args = kwargs = None\\n    elif isinstance(expression, ast.Call):\\n        # Ensure the function name is an attribute name only.\\n        if not isinstance(expression.func, ast.Name):\\n            raise AppImportError(\"Function reference must be a simple name: %r\" % obj)\\n\\n        name = expression.func.id\\n\\n        # Parse the positional and keyword arguments as literals.\\n        try:\\n            args = [ast.literal_eval(arg) for arg in expression.args]\\n            kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in expression.keywords}\\n        except ValueError:\\n            # literal_eval gives cryptic error messages, show a generic\\n            # message with the full expression instead.\\n            raise AppImportError(\\n                \"Failed to parse arguments as literal values: %r\" % obj\\n            )\\n    else:\\n        raise AppImportError(\\n            \"Failed to parse %r as an attribute name or function call.\" % obj\\n        )\\n\\n    is_debug = logging.root.level == logging.DEBUG\\n    try:\\n        app = getattr(mod, name)\\n    except AttributeError:\\n        if is_debug:\\n            traceback.print_exception(*sys.exc_info())\\n        raise AppImportError(\"Failed to find attribute %r in %r.\" % (name, module))\\n\\n    # If the expression was a function call, call the retrieved object\\n    # to get the real application.\\n    if args is not None:\\n        try:\\n            app = app(*args, **kwargs)\\n        except TypeError as e:\\n            # If the TypeError was due to bad arguments to the factory\\n            # function, show Python\\'s nice error message without a\\n            # traceback.\\n            if _called_with_wrong_args(app):\\n                raise AppImportError(\\n                    \"\".join(traceback.format_exception_only(TypeError, e)).strip()\\n                )\\n\\n            # Otherwise it was raised from within the function, show the\\n            # full traceback.\\n            raise\\n\\n    if app is None:\\n        raise AppImportError(\"Failed to find application object: %r\" % obj)\\n\\n    if not callable(app):\\n        raise AppImportError(\"Application object must be callable.\")\\n    return app\\n\\n\\ndef getcwd():\\n    # get current path, try to use PWD env first\\n    try:\\n        a = os.stat(os.environ[\\'PWD\\'])\\n        b = os.stat(os.getcwd())\\n        if a.st_ino == b.st_ino and a.st_dev == b.st_dev:\\n            cwd = os.environ[\\'PWD\\']\\n        else:\\n            cwd = os.getcwd()\\n    except Exception:\\n        cwd = os.getcwd()\\n    return cwd\\n\\n\\ndef http_date(timestamp=None):\\n    \"\"\"Return the current date and time formatted for a message header.\"\"\"\\n    if timestamp is None:\\n        timestamp = time.time()\\n    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)\\n    return s\\n\\n\\ndef is_hoppish(header):\\n    return header.lower().strip() in hop_headers\\n\\n\\ndef daemonize(enable_stdio_inheritance=False):\\n    \"\"\"\\\\\\n    Standard daemonization of a process.\\n    http://www.faqs.org/faqs/unix-faq/programmer/faq/ section 1.7\\n    \"\"\"\\n    if \\'GUNICORN_FD\\' not in os.environ:\\n        if os.fork():\\n            os._exit(0)\\n        os.setsid()\\n\\n        if os.fork():\\n            os._exit(0)\\n\\n        os.umask(0o22)\\n\\n        # In both the following any file descriptors above stdin\\n        # stdout and stderr are left untouched. The inheritance\\n        # option simply allows one to have output go to a file\\n        # specified by way of shell redirection when not wanting\\n        # to use --error-log option.\\n\\n        if not enable_stdio_inheritance:\\n            # Remap all of stdin, stdout and stderr on to\\n            # /dev/null. The expectation is that users have\\n            # specified the --error-log option.\\n\\n            closerange(0, 3)\\n\\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\\n            # PEP 446, make fd for /dev/null inheritable\\n            os.set_inheritable(fd_null, True)\\n\\n            # expect fd_null to be always 0 here, but in-case not ...\\n            if fd_null != 0:\\n                os.dup2(fd_null, 0)\\n\\n            os.dup2(fd_null, 1)\\n            os.dup2(fd_null, 2)\\n\\n        else:\\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\\n\\n            # Always redirect stdin to /dev/null as we would\\n            # never expect to need to read interactive input.\\n\\n            if fd_null != 0:\\n                os.close(0)\\n                os.dup2(fd_null, 0)\\n\\n            # If stdout and stderr are still connected to\\n            # their original file descriptors we check to see\\n            # if they are associated with terminal devices.\\n            # When they are we map them to /dev/null so that\\n            # are still detached from any controlling terminal\\n            # properly. If not we preserve them as they are.\\n            #\\n            # If stdin and stdout were not hooked up to the\\n            # original file descriptors, then all bets are\\n            # off and all we can really do is leave them as\\n            # they were.\\n            #\\n            # This will allow \\'gunicorn ... > output.log 2>&1\\'\\n            # to work with stdout/stderr going to the file\\n            # as expected.\\n            #\\n            # Note that if using --error-log option, the log\\n            # file specified through shell redirection will\\n            # only be used up until the log file specified\\n            # by the option takes over. As it replaces stdout\\n            # and stderr at the file descriptor level, then\\n            # anything using stdout or stderr, including having\\n            # cached a reference to them, will still work.\\n\\n            def redirect(stream, fd_expect):\\n                try:\\n                    fd = stream.fileno()\\n                    if fd == fd_expect and stream.isatty():\\n                        os.close(fd)\\n                        os.dup2(fd_null, fd)\\n                except AttributeError:\\n                    pass\\n\\n            redirect(sys.stdout, 1)\\n            redirect(sys.stderr, 2)\\n\\n\\ndef seed():\\n    try:\\n        random.seed(os.urandom(64))\\n    except NotImplementedError:\\n        random.seed(\\'%s.%s\\' % (time.time(), os.getpid()))\\n\\n\\ndef check_is_writable(path):\\n    try:\\n        with open(path, \\'a\\') as f:\\n            f.close()\\n    except IOError as e:\\n        raise RuntimeError(\"Error: \\'%s\\' isn\\'t writable [%r]\" % (path, e))\\n\\n\\ndef to_bytestring(value, encoding=\"utf8\"):\\n    \"\"\"Converts a string argument to a byte string\"\"\"\\n    if isinstance(value, bytes):\\n        return value\\n    if not isinstance(value, str):\\n        raise TypeError(\\'%r is not a string\\' % value)\\n\\n    return value.encode(encoding)\\n\\n\\ndef has_fileno(obj):\\n    if not hasattr(obj, \"fileno\"):\\n        return False\\n\\n    # check BytesIO case and maybe others\\n    try:\\n        obj.fileno()\\n    except (AttributeError, IOError, io.UnsupportedOperation):\\n        return False\\n\\n    return True\\n\\n\\ndef warn(msg):\\n    print(\"!!!\", file=sys.stderr)\\n\\n    lines = msg.splitlines()\\n    for i, line in enumerate(lines):\\n        if i == 0:\\n            line = \"WARNING: %s\" % line\\n        print(\"!!! %s\" % line, file=sys.stderr)\\n\\n    print(\"!!!\\\\n\", file=sys.stderr)\\n    sys.stderr.flush()\\n\\n\\ndef make_fail_app(msg):\\n    msg = to_bytestring(msg)\\n\\n    def app(environ, start_response):\\n        start_response(\"500 Internal Server Error\", [\\n            (\"Content-Type\", \"text/plain\"),\\n            (\"Content-Length\", str(len(msg)))\\n        ])\\n        return [msg]\\n\\n    return app\\n\\n\\ndef split_request_uri(uri):\\n    if uri.startswith(\"//\"):\\n        # When the path starts with //, urlsplit considers it as a\\n        # relative uri while the RFC says we should consider it as abs_path\\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\\n        # We use temporary dot prefix to workaround this behaviour\\n        parts = urllib.parse.urlsplit(\".\" + uri)\\n        return parts._replace(path=parts.path[1:])\\n\\n    return urllib.parse.urlsplit(uri)\\n\\n\\n# From six.reraise\\ndef reraise(tp, value, tb=None):\\n    try:\\n        if value is None:\\n            value = tp()\\n        if value.__traceback__ is not tb:\\n            raise value.with_traceback(tb)\\n        raise value\\n    finally:\\n        value = None\\n        tb = None\\n\\n\\ndef bytes_to_str(b):\\n    if isinstance(b, str):\\n        return b\\n    return str(b, \\'latin1\\')\\n\\n\\ndef unquote_to_wsgi_str(string):\\n    return urllib.parse.unquote_to_bytes(string).decode(\\'latin-1\\')\\n'}, {'gunicorn.util.load_class': '# -*- coding: utf-8 -\\n#\\n# This file is part of gunicorn released under the MIT license.\\n# See the NOTICE for more information.\\nimport ast\\nimport email.utils\\nimport errno\\nimport fcntl\\nimport html\\nimport importlib\\nimport inspect\\nimport io\\nimport logging\\nimport os\\nimport pwd\\nimport random\\nimport re\\nimport socket\\nimport sys\\nimport textwrap\\nimport time\\nimport traceback\\nimport warnings\\n\\ntry:\\n    import importlib.metadata as importlib_metadata\\nexcept (ModuleNotFoundError, ImportError):\\n    import importlib_metadata\\n\\nfrom gunicorn.errors import AppImportError\\nfrom gunicorn.workers import SUPPORTED_WORKERS\\nimport urllib.parse\\n\\nREDIRECT_TO = getattr(os, \\'devnull\\', \\'/dev/null\\')\\n\\n# Server and Date aren\\'t technically hop-by-hop\\n# headers, but they are in the purview of the\\n# origin server which the WSGI spec says we should\\n# act like. So we drop them and add our own.\\n#\\n# In the future, concatenation server header values\\n# might be better, but nothing else does it and\\n# dropping them is easier.\\nhop_headers = set(\"\"\"\\n    connection keep-alive proxy-authenticate proxy-authorization\\n    te trailers transfer-encoding upgrade\\n    server date\\n    \"\"\".split())\\n\\ntry:\\n    from setproctitle import setproctitle\\n\\n    def _setproctitle(title):\\n        setproctitle(\"gunicorn: %s\" % title)\\nexcept ImportError:\\n    def _setproctitle(title):\\n        pass\\n\\n\\ndef load_entry_point(distribution, group, name):\\n    dist_obj = importlib_metadata.distribution(distribution)\\n    eps = [ep for ep in dist_obj.entry_points\\n           if ep.group == group and ep.name == name]\\n    if not eps:\\n        raise ImportError(\"Entry point %r not found\" % ((group, name),))\\n    return eps[0].load()\\n\\n\\ndef load_class(uri, default=\"gunicorn.workers.sync.SyncWorker\",\\n               section=\"gunicorn.workers\"):\\n    if inspect.isclass(uri):\\n        return uri\\n    if uri.startswith(\"egg:\"):\\n        # uses entry points\\n        entry_str = uri.split(\"egg:\")[1]\\n        try:\\n            dist, name = entry_str.rsplit(\"#\", 1)\\n        except ValueError:\\n            dist = entry_str\\n            name = default\\n\\n        try:\\n            return load_entry_point(dist, section, name)\\n        except Exception:\\n            exc = traceback.format_exc()\\n            msg = \"class uri %r invalid or not found: \\\\n\\\\n[%s]\"\\n            raise RuntimeError(msg % (uri, exc))\\n    else:\\n        components = uri.split(\\'.\\')\\n        if len(components) == 1:\\n            while True:\\n                if uri.startswith(\"#\"):\\n                    uri = uri[1:]\\n\\n                if uri in SUPPORTED_WORKERS:\\n                    components = SUPPORTED_WORKERS[uri].split(\".\")\\n                    break\\n\\n                try:\\n                    return load_entry_point(\\n                        \"gunicorn\", section, uri\\n                    )\\n                except Exception:\\n                    exc = traceback.format_exc()\\n                    msg = \"class uri %r invalid or not found: \\\\n\\\\n[%s]\"\\n                    raise RuntimeError(msg % (uri, exc))\\n\\n        klass = components.pop(-1)\\n\\n        try:\\n            mod = importlib.import_module(\\'.\\'.join(components))\\n        except Exception:\\n            exc = traceback.format_exc()\\n            msg = \"class uri %r invalid or not found: \\\\n\\\\n[%s]\"\\n            raise RuntimeError(msg % (uri, exc))\\n        return getattr(mod, klass)\\n\\n\\npositionals = (\\n    inspect.Parameter.POSITIONAL_ONLY,\\n    inspect.Parameter.POSITIONAL_OR_KEYWORD,\\n)\\n\\n\\ndef get_arity(f):\\n    sig = inspect.signature(f)\\n    arity = 0\\n\\n    for param in sig.parameters.values():\\n        if param.kind in positionals:\\n            arity += 1\\n\\n    return arity\\n\\n\\ndef get_username(uid):\\n    \"\"\" get the username for a user id\"\"\"\\n    return pwd.getpwuid(uid).pw_name\\n\\n\\ndef set_owner_process(uid, gid, initgroups=False):\\n    \"\"\" set user and group of workers processes \"\"\"\\n\\n    if gid:\\n        if uid:\\n            try:\\n                username = get_username(uid)\\n            except KeyError:\\n                initgroups = False\\n\\n        # versions of python < 2.6.2 don\\'t manage unsigned int for\\n        # groups like on osx or fedora\\n        gid = abs(gid) & 0x7FFFFFFF\\n\\n        if initgroups:\\n            os.initgroups(username, gid)\\n        elif gid != os.getgid():\\n            os.setgid(gid)\\n\\n    if uid and uid != os.getuid():\\n        os.setuid(uid)\\n\\n\\ndef chown(path, uid, gid):\\n    os.chown(path, uid, gid)\\n\\n\\nif sys.platform.startswith(\"win\"):\\n    def _waitfor(func, pathname, waitall=False):\\n        # Perform the operation\\n        func(pathname)\\n        # Now setup the wait loop\\n        if waitall:\\n            dirname = pathname\\n        else:\\n            dirname, name = os.path.split(pathname)\\n            dirname = dirname or \\'.\\'\\n        # Check for `pathname` to be removed from the filesystem.\\n        # The exponential backoff of the timeout amounts to a total\\n        # of ~1 second after which the deletion is probably an error\\n        # anyway.\\n        # Testing on a i7@4.3GHz shows that usually only 1 iteration is\\n        # required when contention occurs.\\n        timeout = 0.001\\n        while timeout < 1.0:\\n            # Note we are only testing for the existence of the file(s) in\\n            # the contents of the directory regardless of any security or\\n            # access rights.  If we have made it this far, we have sufficient\\n            # permissions to do that much using Python\\'s equivalent of the\\n            # Windows API FindFirstFile.\\n            # Other Windows APIs can fail or give incorrect results when\\n            # dealing with files that are pending deletion.\\n            L = os.listdir(dirname)\\n            if not L if waitall else name in L:\\n                return\\n            # Increase the timeout and try again\\n            time.sleep(timeout)\\n            timeout *= 2\\n        warnings.warn(\\'tests may fail, delete still pending for \\' + pathname,\\n                      RuntimeWarning, stacklevel=4)\\n\\n    def _unlink(filename):\\n        _waitfor(os.unlink, filename)\\nelse:\\n    _unlink = os.unlink\\n\\n\\ndef unlink(filename):\\n    try:\\n        _unlink(filename)\\n    except OSError as error:\\n        # The filename need not exist.\\n        if error.errno not in (errno.ENOENT, errno.ENOTDIR):\\n            raise\\n\\n\\ndef is_ipv6(addr):\\n    try:\\n        socket.inet_pton(socket.AF_INET6, addr)\\n    except socket.error:  # not a valid address\\n        return False\\n    except ValueError:  # ipv6 not supported on this platform\\n        return False\\n    return True\\n\\n\\ndef parse_address(netloc, default_port=\\'8000\\'):\\n    if re.match(r\\'unix:(//)?\\', netloc):\\n        return re.split(r\\'unix:(//)?\\', netloc)[-1]\\n\\n    if netloc.startswith(\"fd://\"):\\n        fd = netloc[5:]\\n        try:\\n            return int(fd)\\n        except ValueError:\\n            raise RuntimeError(\"%r is not a valid file descriptor.\" % fd) from None\\n\\n    if netloc.startswith(\"tcp://\"):\\n        netloc = netloc.split(\"tcp://\")[1]\\n    host, port = netloc, default_port\\n\\n    if \\'[\\' in netloc and \\']\\' in netloc:\\n        host = netloc.split(\\']\\')[0][1:]\\n        port = (netloc.split(\\']:\\') + [default_port])[1]\\n    elif \\':\\' in netloc:\\n        host, port = (netloc.split(\\':\\') + [default_port])[:2]\\n    elif netloc == \"\":\\n        host, port = \"0.0.0.0\", default_port\\n\\n    try:\\n        port = int(port)\\n    except ValueError:\\n        raise RuntimeError(\"%r is not a valid port number.\" % port)\\n\\n    return host.lower(), port\\n\\n\\ndef close_on_exec(fd):\\n    flags = fcntl.fcntl(fd, fcntl.F_GETFD)\\n    flags |= fcntl.FD_CLOEXEC\\n    fcntl.fcntl(fd, fcntl.F_SETFD, flags)\\n\\n\\ndef set_non_blocking(fd):\\n    flags = fcntl.fcntl(fd, fcntl.F_GETFL) | os.O_NONBLOCK\\n    fcntl.fcntl(fd, fcntl.F_SETFL, flags)\\n\\n\\ndef close(sock):\\n    try:\\n        sock.close()\\n    except socket.error:\\n        pass\\n\\n\\ntry:\\n    from os import closerange\\nexcept ImportError:\\n    def closerange(fd_low, fd_high):\\n        # Iterate through and close all file descriptors.\\n        for fd in range(fd_low, fd_high):\\n            try:\\n                os.close(fd)\\n            except OSError:  # ERROR, fd wasn\\'t open to begin with (ignored)\\n                pass\\n\\n\\ndef write_chunk(sock, data):\\n    if isinstance(data, str):\\n        data = data.encode(\\'utf-8\\')\\n    chunk_size = \"%X\\\\r\\\\n\" % len(data)\\n    chunk = b\"\".join([chunk_size.encode(\\'utf-8\\'), data, b\"\\\\r\\\\n\"])\\n    sock.sendall(chunk)\\n\\n\\ndef write(sock, data, chunked=False):\\n    if chunked:\\n        return write_chunk(sock, data)\\n    sock.sendall(data)\\n\\n\\ndef write_nonblock(sock, data, chunked=False):\\n    timeout = sock.gettimeout()\\n    if timeout != 0.0:\\n        try:\\n            sock.setblocking(0)\\n            return write(sock, data, chunked)\\n        finally:\\n            sock.setblocking(1)\\n    else:\\n        return write(sock, data, chunked)\\n\\n\\ndef write_error(sock, status_int, reason, mesg):\\n    html_error = textwrap.dedent(\"\"\"\\\\\\n    <html>\\n      <head>\\n        <title>%(reason)s</title>\\n      </head>\\n      <body>\\n        <h1><p>%(reason)s</p></h1>\\n        %(mesg)s\\n      </body>\\n    </html>\\n    \"\"\") % {\"reason\": reason, \"mesg\": html.escape(mesg)}\\n\\n    http = textwrap.dedent(\"\"\"\\\\\\n    HTTP/1.1 %s %s\\\\r\\n    Connection: close\\\\r\\n    Content-Type: text/html\\\\r\\n    Content-Length: %d\\\\r\\n    \\\\r\\n    %s\"\"\") % (str(status_int), reason, len(html_error), html_error)\\n    write_nonblock(sock, http.encode(\\'latin1\\'))\\n\\n\\ndef _called_with_wrong_args(f):\\n    \"\"\"Check whether calling a function raised a ``TypeError`` because\\n    the call failed or because something in the function raised the\\n    error.\\n\\n    :param f: The function that was called.\\n    :return: ``True`` if the call failed.\\n    \"\"\"\\n    tb = sys.exc_info()[2]\\n\\n    try:\\n        while tb is not None:\\n            if tb.tb_frame.f_code is f.__code__:\\n                # In the function, it was called successfully.\\n                return False\\n\\n            tb = tb.tb_next\\n\\n        # Didn\\'t reach the function.\\n        return True\\n    finally:\\n        # Delete tb to break a circular reference in Python 2.\\n        # https://docs.python.org/2/library/sys.html#sys.exc_info\\n        del tb\\n\\n\\ndef import_app(module):\\n    parts = module.split(\":\", 1)\\n    if len(parts) == 1:\\n        obj = \"application\"\\n    else:\\n        module, obj = parts[0], parts[1]\\n\\n    try:\\n        mod = importlib.import_module(module)\\n    except ImportError:\\n        if module.endswith(\".py\") and os.path.exists(module):\\n            msg = \"Failed to find application, did you mean \\'%s:%s\\'?\"\\n            raise ImportError(msg % (module.rsplit(\".\", 1)[0], obj))\\n        raise\\n\\n    # Parse obj as a single expression to determine if it\\'s a valid\\n    # attribute name or function call.\\n    try:\\n        expression = ast.parse(obj, mode=\"eval\").body\\n    except SyntaxError:\\n        raise AppImportError(\\n            \"Failed to parse %r as an attribute name or function call.\" % obj\\n        )\\n\\n    if isinstance(expression, ast.Name):\\n        name = expression.id\\n        args = kwargs = None\\n    elif isinstance(expression, ast.Call):\\n        # Ensure the function name is an attribute name only.\\n        if not isinstance(expression.func, ast.Name):\\n            raise AppImportError(\"Function reference must be a simple name: %r\" % obj)\\n\\n        name = expression.func.id\\n\\n        # Parse the positional and keyword arguments as literals.\\n        try:\\n            args = [ast.literal_eval(arg) for arg in expression.args]\\n            kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in expression.keywords}\\n        except ValueError:\\n            # literal_eval gives cryptic error messages, show a generic\\n            # message with the full expression instead.\\n            raise AppImportError(\\n                \"Failed to parse arguments as literal values: %r\" % obj\\n            )\\n    else:\\n        raise AppImportError(\\n            \"Failed to parse %r as an attribute name or function call.\" % obj\\n        )\\n\\n    is_debug = logging.root.level == logging.DEBUG\\n    try:\\n        app = getattr(mod, name)\\n    except AttributeError:\\n        if is_debug:\\n            traceback.print_exception(*sys.exc_info())\\n        raise AppImportError(\"Failed to find attribute %r in %r.\" % (name, module))\\n\\n    # If the expression was a function call, call the retrieved object\\n    # to get the real application.\\n    if args is not None:\\n        try:\\n            app = app(*args, **kwargs)\\n        except TypeError as e:\\n            # If the TypeError was due to bad arguments to the factory\\n            # function, show Python\\'s nice error message without a\\n            # traceback.\\n            if _called_with_wrong_args(app):\\n                raise AppImportError(\\n                    \"\".join(traceback.format_exception_only(TypeError, e)).strip()\\n                )\\n\\n            # Otherwise it was raised from within the function, show the\\n            # full traceback.\\n            raise\\n\\n    if app is None:\\n        raise AppImportError(\"Failed to find application object: %r\" % obj)\\n\\n    if not callable(app):\\n        raise AppImportError(\"Application object must be callable.\")\\n    return app\\n\\n\\ndef getcwd():\\n    # get current path, try to use PWD env first\\n    try:\\n        a = os.stat(os.environ[\\'PWD\\'])\\n        b = os.stat(os.getcwd())\\n        if a.st_ino == b.st_ino and a.st_dev == b.st_dev:\\n            cwd = os.environ[\\'PWD\\']\\n        else:\\n            cwd = os.getcwd()\\n    except Exception:\\n        cwd = os.getcwd()\\n    return cwd\\n\\n\\ndef http_date(timestamp=None):\\n    \"\"\"Return the current date and time formatted for a message header.\"\"\"\\n    if timestamp is None:\\n        timestamp = time.time()\\n    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)\\n    return s\\n\\n\\ndef is_hoppish(header):\\n    return header.lower().strip() in hop_headers\\n\\n\\ndef daemonize(enable_stdio_inheritance=False):\\n    \"\"\"\\\\\\n    Standard daemonization of a process.\\n    http://www.faqs.org/faqs/unix-faq/programmer/faq/ section 1.7\\n    \"\"\"\\n    if \\'GUNICORN_FD\\' not in os.environ:\\n        if os.fork():\\n            os._exit(0)\\n        os.setsid()\\n\\n        if os.fork():\\n            os._exit(0)\\n\\n        os.umask(0o22)\\n\\n        # In both the following any file descriptors above stdin\\n        # stdout and stderr are left untouched. The inheritance\\n        # option simply allows one to have output go to a file\\n        # specified by way of shell redirection when not wanting\\n        # to use --error-log option.\\n\\n        if not enable_stdio_inheritance:\\n            # Remap all of stdin, stdout and stderr on to\\n            # /dev/null. The expectation is that users have\\n            # specified the --error-log option.\\n\\n            closerange(0, 3)\\n\\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\\n            # PEP 446, make fd for /dev/null inheritable\\n            os.set_inheritable(fd_null, True)\\n\\n            # expect fd_null to be always 0 here, but in-case not ...\\n            if fd_null != 0:\\n                os.dup2(fd_null, 0)\\n\\n            os.dup2(fd_null, 1)\\n            os.dup2(fd_null, 2)\\n\\n        else:\\n            fd_null = os.open(REDIRECT_TO, os.O_RDWR)\\n\\n            # Always redirect stdin to /dev/null as we would\\n            # never expect to need to read interactive input.\\n\\n            if fd_null != 0:\\n                os.close(0)\\n                os.dup2(fd_null, 0)\\n\\n            # If stdout and stderr are still connected to\\n            # their original file descriptors we check to see\\n            # if they are associated with terminal devices.\\n            # When they are we map them to /dev/null so that\\n            # are still detached from any controlling terminal\\n            # properly. If not we preserve them as they are.\\n            #\\n            # If stdin and stdout were not hooked up to the\\n            # original file descriptors, then all bets are\\n            # off and all we can really do is leave them as\\n            # they were.\\n            #\\n            # This will allow \\'gunicorn ... > output.log 2>&1\\'\\n            # to work with stdout/stderr going to the file\\n            # as expected.\\n            #\\n            # Note that if using --error-log option, the log\\n            # file specified through shell redirection will\\n            # only be used up until the log file specified\\n            # by the option takes over. As it replaces stdout\\n            # and stderr at the file descriptor level, then\\n            # anything using stdout or stderr, including having\\n            # cached a reference to them, will still work.\\n\\n            def redirect(stream, fd_expect):\\n                try:\\n                    fd = stream.fileno()\\n                    if fd == fd_expect and stream.isatty():\\n                        os.close(fd)\\n                        os.dup2(fd_null, fd)\\n                except AttributeError:\\n                    pass\\n\\n            redirect(sys.stdout, 1)\\n            redirect(sys.stderr, 2)\\n\\n\\ndef seed():\\n    try:\\n        random.seed(os.urandom(64))\\n    except NotImplementedError:\\n        random.seed(\\'%s.%s\\' % (time.time(), os.getpid()))\\n\\n\\ndef check_is_writable(path):\\n    try:\\n        with open(path, \\'a\\') as f:\\n            f.close()\\n    except IOError as e:\\n        raise RuntimeError(\"Error: \\'%s\\' isn\\'t writable [%r]\" % (path, e))\\n\\n\\ndef to_bytestring(value, encoding=\"utf8\"):\\n    \"\"\"Converts a string argument to a byte string\"\"\"\\n    if isinstance(value, bytes):\\n        return value\\n    if not isinstance(value, str):\\n        raise TypeError(\\'%r is not a string\\' % value)\\n\\n    return value.encode(encoding)\\n\\n\\ndef has_fileno(obj):\\n    if not hasattr(obj, \"fileno\"):\\n        return False\\n\\n    # check BytesIO case and maybe others\\n    try:\\n        obj.fileno()\\n    except (AttributeError, IOError, io.UnsupportedOperation):\\n        return False\\n\\n    return True\\n\\n\\ndef warn(msg):\\n    print(\"!!!\", file=sys.stderr)\\n\\n    lines = msg.splitlines()\\n    for i, line in enumerate(lines):\\n        if i == 0:\\n            line = \"WARNING: %s\" % line\\n        print(\"!!! %s\" % line, file=sys.stderr)\\n\\n    print(\"!!!\\\\n\", file=sys.stderr)\\n    sys.stderr.flush()\\n\\n\\ndef make_fail_app(msg):\\n    msg = to_bytestring(msg)\\n\\n    def app(environ, start_response):\\n        start_response(\"500 Internal Server Error\", [\\n            (\"Content-Type\", \"text/plain\"),\\n            (\"Content-Length\", str(len(msg)))\\n        ])\\n        return [msg]\\n\\n    return app\\n\\n\\ndef split_request_uri(uri):\\n    if uri.startswith(\"//\"):\\n        # When the path starts with //, urlsplit considers it as a\\n        # relative uri while the RFC says we should consider it as abs_path\\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\\n        # We use temporary dot prefix to workaround this behaviour\\n        parts = urllib.parse.urlsplit(\".\" + uri)\\n        return parts._replace(path=parts.path[1:])\\n\\n    return urllib.parse.urlsplit(uri)\\n\\n\\n# From six.reraise\\ndef reraise(tp, value, tb=None):\\n    try:\\n        if value is None:\\n            value = tp()\\n        if value.__traceback__ is not tb:\\n            raise value.with_traceback(tb)\\n        raise value\\n    finally:\\n        value = None\\n        tb = None\\n\\n\\ndef bytes_to_str(b):\\n    if isinstance(b, str):\\n        return b\\n    return str(b, \\'latin1\\')\\n\\n\\ndef unquote_to_wsgi_str(string):\\n    return urllib.parse.unquote_to_bytes(string).decode(\\'latin-1\\')\\n'}]", "test_list": ["def test_statsd_changes_logger():\n    c = config.Config()\n    assert c.logger_class == glogging.Logger\n    c.set('statsd_host', 'localhost:12345')\n    assert c.logger_class == statsd.Statsd", "def test_property_access():\n    c = config.Config()\n    for s in config.KNOWN_SETTINGS:\n        getattr(c, s.name)\n    assert c.worker_class == SyncWorker\n    assert c.logger_class == glogging.Logger\n    assert c.workers == 1\n    c.set('workers', 3)\n    assert c.workers == 3\n    assert c.address == [('127.0.0.1', 8000)]\n    assert os.geteuid() == c.uid\n    assert os.getegid() == c.gid\n    assert 'gunicorn' == c.proc_name\n    pytest.raises(AttributeError, getattr, c, 'foo')\n\n    class Baz(object):\n\n        def get(self):\n            return 3.14\n    c.settings['foo'] = Baz()\n    assert c.foo == 3.14\n    pytest.raises(AttributeError, setattr, c, 'proc_name', 'baz')\n    pytest.raises(AttributeError, c.set, 'baz', 'bar')", "def test_always_use_configured_logger():\n    c = config.Config()\n    c.set('logger_class', __name__ + '.MyLogger')\n    assert c.logger_class == MyLogger\n    c.set('statsd_host', 'localhost:12345')\n    assert c.logger_class == MyLogger"], "requirements": {"Input-Output Conditions": {"requirement": "The 'logger_class' function should return a valid logger class based on the configuration settings. It should correctly interpret the 'logger_class' setting and return the appropriate logger class instance.", "unit_test": ["def test_logger_class_output():\n    c = config.Config()\n    c.set('logger_class', 'simple')\n    assert c.logger_class == glogging.Logger\n    c.set('logger_class', 'gunicorn.instrument.statsd.Statsd')\n    assert c.logger_class == statsd.Statsd"], "test": "tests/test_config.py::test_logger_class_output"}, "Exception Handling": {"requirement": "The 'logger_class' function should raise a ValueError if an invalid logger class is specified in the configuration settings.", "unit_test": ["def test_invalid_logger_class():\n    c = config.Config()\n    with pytest.raises(ConfigError):\n        c.set('logger_class', 'invalid.logger.Class')\n        c.logger_class"], "test": "tests/test_config.py::test_invalid_logger_class"}, "Edge Case Handling": {"requirement": "The 'logger_class' function should handle the case where the 'logger_class' setting is not specified and default to 'gunicorn.glogging.Logger'.", "unit_test": ["def test_default_logger_class():\n    c = config.Config()\n    assert c.logger_class == glogging.Logger"], "test": "tests/test_config.py::test_default_logger_class"}, "Functionality Extension": {"requirement": "Extend the 'logger_class' function to support a custom logger class specified by the user, ensuring it is a subclass of the base logger class.", "unit_test": ["def test_custom_logger_class():\n    class CustomLogger(glogging.Logger):\n        pass\n    c = config.Config()\n    c.set('logger_class', __name__ + '.CustomLogger')\n    assert c.logger_class == CustomLogger"], "test": "tests/test_config.py::test_custom_logger_class"}, "Annotation Coverage": {"requirement": "Ensure that the 'logger_class' function has complete type annotations for its parameters and return type.", "unit_test": ["def test_logger_class_annotations():\n    assert hasattr(config.Config.logger_class, '__annotations__')\n    assert config.Config.logger_class.__annotations__['return'] == type"], "test": "tests/test_config.py::test_logger_class_annotations"}, "Code Complexity": {"requirement": "The 'logger_class' function should maintain a cyclomatic complexity of 10 or less to ensure readability and maintainability.", "unit_test": ["def test_logger_class_complexity():\n    from radon.complexity import cc_visit\n    source = inspect.getsource(config.Config.logger_class)\n    complexity = cc_visit(source)\n    assert complexity[0].complexity <= 10"], "test": "tests/test_config.py::test_logger_class_complexity"}, "Code Standard": {"requirement": "The 'logger_class' function should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": ["def test_logger_class_pep8():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/config.py'])\n    assert result.total_errors == 0"], "test": "tests/test_config.py::test_logger_class_pep8"}, "Context Usage Verification": {"requirement": "The 'logger_class' function should utilize the 'gunicorn.config.Config.settings' context to retrieve configuration settings.", "unit_test": ["def test_logger_class_context_usage():\n    c = config.Config()\n    assert 'logger_class' in c.settings\n    assert c.logger_class == glogging.Logger"], "test": "tests/test_config.py::test_logger_class_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'logger_class' function should correctly use the 'gunicorn.config.Config.settings' context to determine the logger class based on the configuration.", "unit_test": ["def test_logger_class_context_correctness():\n    c = config.Config()\n    c.set('logger_class', 'gunicorn.glogging.Logger')\n    assert c.logger_class == glogging.Logger"], "test": "tests/test_config.py::test_logger_class_context_correctness"}}}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "type": "method", "project_path": "Database/alembic", "completion_path": "Database/alembic/alembic/operations/ops.py", "signature_position": [176, 176], "body_position": [177, 189], "dependency": {"intra_class": ["alembic.operations.ops.DropConstraintOp._reverse", "alembic.operations.ops.DropConstraintOp.constraint_name", "alembic.operations.ops.DropConstraintOp.schema", "alembic.operations.ops.DropConstraintOp.table_name"], "intra_file": ["alembic.operations.ops.AddConstraintOp.to_constraint"], "cross_file": ["alembic.util.sqla_compat", "alembic.util.sqla_compat._table_for_constraint"]}, "requirement": {"Functionality": "Converts a DropConstraintOp instance to a Constraint instance. It first checks if the reverse operation is present. If it is, it converts the reverse operation to a Constraint instance and sets the name, table name, and schema of the constraint. Then it returns the constraint. If the reverse operation is not present, it raises a ValueError.", "Arguments": ":param self: DropConstraintOp. An instance of the DropConstraintOp class.\n:return: Constraint. The converted Constraint instance."}, "tests": ["tests/test_autogen_diffs.py::OrigObjectTest::test_drop_check", "tests/test_autogen_diffs.py::OrigObjectTest::test_drop_unique", "tests/test_op.py::ObjectFromToTest::test_create_unique_constraint_add_kw", "tests/test_op.py::ObjectFromToTest::test_drop_unique_constraint_change_name", "tests/test_autogen_diffs.py::OrigObjectTest::test_add_unique"], "indent": 8, "domain": "Database", "code": "    def to_constraint(self) -> Constraint:\n        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )\n", "intra_context": "from __future__ import annotations\n\nfrom abc import abstractmethod\nimport re\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import FrozenSet\nfrom typing import Iterator\nfrom typing import List\nfrom typing import MutableMapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom sqlalchemy.types import NULLTYPE\n\nfrom . import schemaobj\nfrom .base import BatchOperations\nfrom .base import Operations\nfrom .. import util\nfrom ..util import sqla_compat\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from sqlalchemy.sql import Executable\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.elements import conv\n    from sqlalchemy.sql.elements import quoted_name\n    from sqlalchemy.sql.elements import TextClause\n    from sqlalchemy.sql.functions import Function\n    from sqlalchemy.sql.schema import CheckConstraint\n    from sqlalchemy.sql.schema import Column\n    from sqlalchemy.sql.schema import Computed\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import ForeignKeyConstraint\n    from sqlalchemy.sql.schema import Identity\n    from sqlalchemy.sql.schema import Index\n    from sqlalchemy.sql.schema import MetaData\n    from sqlalchemy.sql.schema import PrimaryKeyConstraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.schema import Table\n    from sqlalchemy.sql.schema import UniqueConstraint\n    from sqlalchemy.sql.selectable import TableClause\n    from sqlalchemy.sql.type_api import TypeEngine\n\n    from ..autogenerate.rewriter import Rewriter\n    from ..runtime.migration import MigrationContext\n    from ..script.revision import _RevIdType\n\n\nclass MigrateOperation:\n    \"\"\"base class for migration command and organization objects.\n\n    This system is part of the operation extensibility API.\n\n    .. seealso::\n\n        :ref:`operation_objects`\n\n        :ref:`operation_plugins`\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    @util.memoized_property\n    def info(self):\n        \"\"\"A dictionary that may be used to store arbitrary information\n        along with this :class:`.MigrateOperation` object.\n\n        \"\"\"\n        return {}\n\n    _mutations: FrozenSet[Rewriter] = frozenset()\n\n    def reverse(self) -> MigrateOperation:\n        raise NotImplementedError\n\n    def to_diff_tuple(self) -> Tuple[Any, ...]:\n        raise NotImplementedError\n\n\nclass AddConstraintOp(MigrateOperation):\n    \"\"\"Represent an add constraint operation.\"\"\"\n\n    add_constraint_ops = util.Dispatcher()\n\n    @property\n    def constraint_type(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def register_add_constraint(cls, type_: str) -> Callable:\n        def go(klass):\n            cls.add_constraint_ops.dispatch_for(type_)(klass.from_constraint)\n            return klass\n\n        return go\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> AddConstraintOp:\n        return cls.add_constraint_ops.dispatch(constraint.__visit_name__)(\n            constraint\n        )\n\n    @abstractmethod\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Constraint:\n        pass\n\n    def reverse(self) -> DropConstraintOp:\n        return DropConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(self) -> Tuple[str, Constraint]:\n        return (\"add_constraint\", self.to_constraint())\n\n\n@Operations.register_operation(\"drop_constraint\")\n@BatchOperations.register_operation(\"drop_constraint\", \"batch_drop_constraint\")\nclass DropConstraintOp(MigrateOperation):\n    \"\"\"Represent a drop constraint operation.\"\"\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddConstraintOp] = None,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.constraint_type = type_\n        self.schema = schema\n        self._reverse = _reverse\n\n    def reverse(self) -> AddConstraintOp:\n        return AddConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, SchemaItem]:\n        if self.constraint_type == \"foreignkey\":\n            return (\"remove_fk\", self.to_constraint())\n        else:\n            return (\"remove_constraint\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:\n        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n\n###The function: to_constraint###\n    @classmethod\n    def drop_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: str,\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        r\"\"\"Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param constraint_name: name of the constraint.\n        :param table_name: table name.\n        :param type\\_: optional, required on MySQL.  can be\n         'foreignkey', 'primary', 'unique', or 'check'.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, type_=type_, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        type_: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            type_=type_,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_primary_key\")\n@BatchOperations.register_operation(\n    \"create_primary_key\", \"batch_create_primary_key\"\n)\n@AddConstraintOp.register_add_constraint(\"primary_key_constraint\")\nclass CreatePrimaryKeyOp(AddConstraintOp):\n    \"\"\"Represent a create primary key operation.\"\"\"\n\n    constraint_type = \"primarykey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreatePrimaryKeyOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        pk_constraint = cast(\"PrimaryKeyConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(pk_constraint.name),\n            constraint_table.name,\n            pk_constraint.columns.keys(),\n            schema=constraint_table.schema,\n            **pk_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_primary_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: List[str],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_primary_key(\"pk_my_table\", \"my_table\", [\"id\", \"version\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the primary key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param columns: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, columns, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_primary_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: List[str],\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_primary_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_unique_constraint\")\n@BatchOperations.register_operation(\n    \"create_unique_constraint\", \"batch_create_unique_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"unique_constraint\")\nclass CreateUniqueConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create unique constraint operation.\"\"\"\n\n    constraint_type = \"unique\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateUniqueConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        uq_constraint = cast(\"UniqueConstraint\", constraint)\n\n        kw: dict = {}\n        if uq_constraint.deferrable:\n            kw[\"deferrable\"] = uq_constraint.deferrable\n        if uq_constraint.initially:\n            kw[\"initially\"] = uq_constraint.initially\n        kw.update(uq_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(uq_constraint.name),\n            constraint_table.name,\n            [c.name for c in uq_constraint.columns],\n            schema=constraint_table.schema,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> UniqueConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.unique_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_unique_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param columns: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, columns, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_unique_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: Sequence[str],\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_unique_constraint`\n\n        \"\"\"\n        kw[\"schema\"] = operations.impl.schema\n        op = cls(constraint_name, operations.impl.table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_foreign_key\")\n@BatchOperations.register_operation(\n    \"create_foreign_key\", \"batch_create_foreign_key\"\n)\n@AddConstraintOp.register_add_constraint(\"foreign_key_constraint\")\nclass CreateForeignKeyOp(AddConstraintOp):\n    \"\"\"Represent a create foreign key constraint operation.\"\"\"\n\n    constraint_type = \"foreignkey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.source_table = source_table\n        self.referent_table = referent_table\n        self.local_cols = local_cols\n        self.remote_cols = remote_cols\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, ForeignKeyConstraint]:\n        return (\"add_fk\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreateForeignKeyOp:\n        fk_constraint = cast(\"ForeignKeyConstraint\", constraint)\n        kw: dict = {}\n        if fk_constraint.onupdate:\n            kw[\"onupdate\"] = fk_constraint.onupdate\n        if fk_constraint.ondelete:\n            kw[\"ondelete\"] = fk_constraint.ondelete\n        if fk_constraint.initially:\n            kw[\"initially\"] = fk_constraint.initially\n        if fk_constraint.deferrable:\n            kw[\"deferrable\"] = fk_constraint.deferrable\n        if fk_constraint.use_alter:\n            kw[\"use_alter\"] = fk_constraint.use_alter\n        if fk_constraint.match:\n            kw[\"match\"] = fk_constraint.match\n\n        (\n            source_schema,\n            source_table,\n            source_columns,\n            target_schema,\n            target_table,\n            target_columns,\n            onupdate,\n            ondelete,\n            deferrable,\n            initially,\n        ) = sqla_compat._fk_spec(fk_constraint)\n\n        kw[\"source_schema\"] = source_schema\n        kw[\"referent_schema\"] = target_schema\n        kw.update(fk_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(fk_constraint.name),\n            source_table,\n            target_table,\n            source_columns,\n            target_columns,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> ForeignKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.foreign_key_constraint(\n            self.constraint_name,\n            self.source_table,\n            self.referent_table,\n            self.local_cols,\n            self.remote_cols,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_foreign_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        source_schema: Optional[str] = None,\n        referent_schema: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_foreign_key(\n                \"fk_user_address\",\n                \"address\",\n                \"user\",\n                [\"user_id\"],\n                [\"id\"],\n            )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the foreign key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source_table: String name of the source table.\n        :param referent_table: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        op = cls(\n            constraint_name,\n            source_table,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=source_schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_foreign_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        referent_schema: Optional[str] = None,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``source_schema``\n        arguments from the call.\n\n        e.g.::\n\n            with batch_alter_table(\"address\") as batch_op:\n                batch_op.create_foreign_key(\n                    \"fk_user_address\",\n                    \"user\",\n                    [\"user_id\"],\n                    [\"id\"],\n                )\n\n        .. seealso::\n\n            :meth:`.Operations.create_foreign_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=operations.impl.schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_check_constraint\")\n@BatchOperations.register_operation(\n    \"create_check_constraint\", \"batch_create_check_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"check_constraint\")\n@AddConstraintOp.register_add_constraint(\"table_or_column_check_constraint\")\n@AddConstraintOp.register_add_constraint(\"column_check_constraint\")\nclass CreateCheckConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create check constraint operation.\"\"\"\n\n    constraint_type = \"check\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        condition: Union[str, TextClause, ColumnElement[Any]],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.condition = condition\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateCheckConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        ck_constraint = cast(\"CheckConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(ck_constraint.name),\n            constraint_table.name,\n            cast(\"ColumnElement[Any]\", ck_constraint.sqltext),\n            schema=constraint_table.schema,\n            **ck_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> CheckConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.check_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.condition,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_check_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column(\"name\")) > 5,\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param condition: SQL expression that's the condition of the\n         constraint. Can be a string or SQLAlchemy expression language\n         structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, condition, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_check_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_check_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            condition,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_index\")\n@BatchOperations.register_operation(\"create_index\", \"batch_create_index\")\nclass CreateIndexOp(MigrateOperation):\n    \"\"\"Represent a create index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.unique = unique\n        self.if_not_exists = if_not_exists\n        self.kw = kw\n\n    def reverse(self) -> DropIndexOp:\n        return DropIndexOp.from_index(self.to_index())\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"add_index\", self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> CreateIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n\n    @classmethod\n    def create_index(\n        cls,\n        operations: Operations,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, Function[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_index(\"ik_test\", \"t1\", [\"foo\", \"bar\"])\n\n        Functional indexes can be produced by using the\n        :func:`sqlalchemy.sql.expression.text` construct::\n\n            from alembic import op\n            from sqlalchemy import text\n\n            op.create_index(\"ik_test\", \"t1\", [text(\"lower(foo)\")])\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.\n        :param columns: a list consisting of string column names and/or\n         :func:`~sqlalchemy.sql.expression.text` constructs.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param unique: If True, create a unique index.\n\n        :param quote: Force quoting of this column's name on or off,\n         corresponding to ``True`` or ``False``. When left at its default\n         of ``None``, the column identifier will be quoted according to\n         whether the name is case sensitive (identifiers with at least one\n         upper case character are treated as case sensitive), or if it's a\n         reserved word. This flag is only needed to force quoting of a\n         reserved word which is not known by the SQLAlchemy dialect.\n\n        :param if_not_exists: If True, adds IF NOT EXISTS operator when\n         creating the new index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_index(\n        cls,\n        operations: BatchOperations,\n        index_name: str,\n        columns: List[str],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.create_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_index\")\n@BatchOperations.register_operation(\"drop_index\", \"batch_drop_index\")\nclass DropIndexOp(MigrateOperation):\n    \"\"\"Represent a drop index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Union[quoted_name, str, conv],\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        _reverse: Optional[CreateIndexOp] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.schema = schema\n        self.if_exists = if_exists\n        self._reverse = _reverse\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"remove_index\", self.to_index())\n\n    def reverse(self) -> CreateIndexOp:\n        return CreateIndexOp.from_index(self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> DropIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def drop_index(\n        cls,\n        operations: Operations,\n        index_name: str,\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        :param if_exists: If True, adds IF EXISTS operator when\n         dropping the index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_index(\n        cls, operations: BatchOperations, index_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            table_name=operations.impl.table_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table\")\nclass CreateTableOp(MigrateOperation):\n    \"\"\"Represent a create table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        columns: Sequence[SchemaItem],\n        *,\n        schema: Optional[str] = None,\n        _namespace_metadata: Optional[MetaData] = None,\n        _constraints_included: bool = False,\n        **kw: Any,\n    ) -> None:\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.info = kw.pop(\"info\", {})\n        self.comment = kw.pop(\"comment\", None)\n        self.prefixes = kw.pop(\"prefixes\", None)\n        self.kw = kw\n        self._namespace_metadata = _namespace_metadata\n        self._constraints_included = _constraints_included\n\n    def reverse(self) -> DropTableOp:\n        return DropTableOp.from_table(\n            self.to_table(), _namespace_metadata=self._namespace_metadata\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"add_table\", self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:\n        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=self._constraints_included,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *columns: SchemaItem,\n        **kw: Any,\n    ) -> Table:\n        r\"\"\"Issue a \"create table\" instruction using the current migration\n        context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        Note that :meth:`.create_table` accepts\n        :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        The function also returns a newly created\n        :class:`~sqlalchemy.schema.Table` object, corresponding to the table\n        specification given, which is suitable for\n        immediate SQL operations, in particular\n        :meth:`.Operations.bulk_insert`::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            account_table = op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n            op.bulk_insert(\n                account_table,\n                [\n                    {\"name\": \"A1\", \"description\": \"account 1\"},\n                    {\"name\": \"A2\", \"description\": \"account 2\"},\n                ],\n            )\n\n        :param table_name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        :return: the :class:`~sqlalchemy.schema.Table` object corresponding\n         to the parameters given.\n\n        \"\"\"\n        op = cls(table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_table\")\nclass DropTableOp(MigrateOperation):\n    \"\"\"Represent a drop table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        table_kw: Optional[MutableMapping[Any, Any]] = None,\n        _reverse: Optional[CreateTableOp] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n        self.table_kw = table_kw or {}\n        self.comment = self.table_kw.pop(\"comment\", None)\n        self.info = self.table_kw.pop(\"info\", None)\n        self.prefixes = self.table_kw.pop(\"prefixes\", None)\n        self._reverse = _reverse\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"remove_table\", self.to_table())\n\n    def reverse(self) -> CreateTableOp:\n        return CreateTableOp.from_table(self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n\n    @classmethod\n    def drop_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param table_name: Name of the table\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        op = cls(table_name, schema=schema, table_kw=kw)\n        operations.invoke(op)\n\n\nclass AlterTableOp(MigrateOperation):\n    \"\"\"Represent an alter table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n\n\n@Operations.register_operation(\"rename_table\")\nclass RenameTableOp(AlterTableOp):\n    \"\"\"Represent a rename table operation.\"\"\"\n\n    def __init__(\n        self,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(old_table_name, schema=schema)\n        self.new_table_name = new_table_name\n\n    @classmethod\n    def rename_table(\n        cls,\n        operations: Operations,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(old_table_name, new_table_name, schema=schema)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table_comment\")\n@BatchOperations.register_operation(\n    \"create_table_comment\", \"batch_create_table_comment\"\n)\nclass CreateTableCommentOp(AlterTableOp):\n    \"\"\"Represent a COMMENT ON `table` operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.comment = comment\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def create_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table.\n\n        :param table_name: string name of the target table.\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(\n            table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=schema,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_table_comment(\n        cls,\n        operations: BatchOperations,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table\n        using the current batch migration context.\n\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        if self.existing_comment is None:\n            return DropTableCommentOp(\n                self.table_name,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n        else:\n            return CreateTableCommentOp(\n                self.table_name,\n                self.existing_comment,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name, schema=self.schema, comment=self.comment\n        )\n\n    def to_diff_tuple(self):\n        return (\"add_table_comment\", self.to_table(), self.existing_comment)\n\n\n@Operations.register_operation(\"drop_table_comment\")\n@BatchOperations.register_operation(\n    \"drop_table_comment\", \"batch_drop_table_comment\"\n)\nclass DropTableCommentOp(AlterTableOp):\n    \"\"\"Represent an operation to remove the comment from a table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def drop_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table.\n\n        :param table_name: string name of the target table.\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        .. seealso::\n\n            :meth:`.Operations.create_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(table_name, existing_comment=existing_comment, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_table_comment(\n        cls,\n        operations: BatchOperations,\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table using the current\n        batch operations context.\n\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        return CreateTableCommentOp(\n            self.table_name, self.existing_comment, schema=self.schema\n        )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(self.table_name, schema=self.schema)\n\n    def to_diff_tuple(self):\n        return (\"remove_table_comment\", self.to_table())\n\n\n@Operations.register_operation(\"alter_column\")\n@BatchOperations.register_operation(\"alter_column\", \"batch_alter_column\")\nclass AlterColumnOp(AlterTableOp):\n    \"\"\"Represent an alter column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_type: Optional[Any] = None,\n        existing_server_default: Any = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        modify_nullable: Optional[bool] = None,\n        modify_comment: Optional[Union[str, Literal[False]]] = False,\n        modify_server_default: Any = False,\n        modify_name: Optional[str] = None,\n        modify_type: Optional[Any] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.existing_type = existing_type\n        self.existing_server_default = existing_server_default\n        self.existing_nullable = existing_nullable\n        self.existing_comment = existing_comment\n        self.modify_nullable = modify_nullable\n        self.modify_comment = modify_comment\n        self.modify_server_default = modify_server_default\n        self.modify_name = modify_name\n        self.modify_type = modify_type\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Any:\n        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n\n    def has_changes(self) -> bool:\n        hc1 = (\n            self.modify_nullable is not None\n            or self.modify_server_default is not False\n            or self.modify_type is not None\n            or self.modify_comment is not False\n        )\n        if hc1:\n            return True\n        for kw in self.kw:\n            if kw.startswith(\"modify_\"):\n                return True\n        else:\n            return False\n\n    def reverse(self) -> AlterColumnOp:\n        kw = self.kw.copy()\n        kw[\"existing_type\"] = self.existing_type\n        kw[\"existing_nullable\"] = self.existing_nullable\n        kw[\"existing_server_default\"] = self.existing_server_default\n        kw[\"existing_comment\"] = self.existing_comment\n        if self.modify_type is not None:\n            kw[\"modify_type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            kw[\"modify_nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            kw[\"modify_server_default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            kw[\"modify_comment\"] = self.modify_comment\n\n        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return self.__class__(\n            self.table_name, self.column_name, schema=self.schema, **kw\n        )\n\n    @classmethod\n    def alter_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_type`` argument is\n        used in this case to identify and remove a previous\n        constraint that was bound to the type object.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param comment: optional string text of a new comment to add to the\n         column.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type\\_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLAlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param existing_comment: string text of the existing comment on the\n         column to be maintained.  Required on MySQL if the existing comment\n         on the column is not being changed.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param postgresql_using: String argument which will indicate a\n         SQL expression to render within the Postgresql-specific USING clause\n         within ALTER COLUMN.    This string is taken directly as raw SQL which\n         must explicitly include any necessary quoting or escaping of tokens\n         within the expression.\n\n        \"\"\"\n\n        alt = cls(\n            table_name,\n            column_name,\n            schema=schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n    @classmethod\n    def batch_alter_column(\n        cls,\n        operations: BatchOperations,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue an \"alter column\" instruction using the current\n        batch migration context.\n\n        Parameters are the same as that of :meth:`.Operations.alter_column`,\n        as well as the following option(s):\n\n        :param insert_before: String name of an existing column which this\n         column should be placed before, when creating the new table.\n\n        :param insert_after: String name of an existing column which this\n         column should be placed after, when creating the new table.  If\n         both :paramref:`.BatchOperations.alter_column.insert_before`\n         and :paramref:`.BatchOperations.alter_column.insert_after` are\n         omitted, the column is inserted after the last existing column\n         in the table.\n\n        .. seealso::\n\n            :meth:`.Operations.alter_column`\n\n\n        \"\"\"\n        alt = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            insert_before=insert_before,\n            insert_after=insert_after,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n\n@Operations.register_operation(\"add_column\")\n@BatchOperations.register_operation(\"add_column\", \"batch_add_column\")\nclass AddColumnOp(AlterTableOp):\n    \"\"\"Represent an add column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column = column\n        self.kw = kw\n\n    def reverse(self) -> DropColumnOp:\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\"add_column\", self.schema, self.table_name, self.column)\n\n    def to_column(self) -> Column:\n        return self.column\n\n    @classmethod\n    def from_column(cls, col: Column) -> AddColumnOp:\n        return cls(col.table.name, col, schema=col.table.schema)\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> AddColumnOp:\n        return cls(tname, col, schema=schema)\n\n    @classmethod\n    def add_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\"organization\", Column(\"name\", String()))\n\n        The :meth:`.Operations.add_column` method typically corresponds\n        to the SQL command \"ALTER TABLE... ADD COLUMN\".    Within the scope\n        of this command, the column's name, datatype, nullability,\n        and optional server-generated defaults may be indicated.\n\n        .. note::\n\n            With the exception of NOT NULL constraints or single-column FOREIGN\n            KEY constraints, other kinds of constraints such as PRIMARY KEY,\n            UNIQUE or CHECK constraints **cannot** be generated using this\n            method; for these constraints, refer to operations such as\n            :meth:`.Operations.create_primary_key` and\n            :meth:`.Operations.create_check_constraint`. In particular, the\n            following :class:`~sqlalchemy.schema.Column` parameters are\n            **ignored**:\n\n            * :paramref:`~sqlalchemy.schema.Column.primary_key` - SQL databases\n              typically do not support an ALTER operation that can add\n              individual columns one at a time to an existing primary key\n              constraint, therefore it's less ambiguous to use the\n              :meth:`.Operations.create_primary_key` method, which assumes no\n              existing primary key constraint is present.\n            * :paramref:`~sqlalchemy.schema.Column.unique` - use the\n              :meth:`.Operations.create_unique_constraint` method\n            * :paramref:`~sqlalchemy.schema.Column.index` - use the\n              :meth:`.Operations.create_index` method\n\n\n        The provided :class:`~sqlalchemy.schema.Column` object may include a\n        :class:`~sqlalchemy.schema.ForeignKey` constraint directive,\n        referencing a remote table name. For this specific type of constraint,\n        Alembic will automatically emit a second ALTER statement in order to\n        add the single-column FOREIGN KEY constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\n                \"organization\",\n                Column(\"account_id\", INTEGER, ForeignKey(\"accounts.id\")),\n            )\n\n        The column argument passed to :meth:`.Operations.add_column` is a\n        :class:`~sqlalchemy.schema.Column` construct, used in the same way it's\n        used in SQLAlchemy. In particular, values or functions to be indicated\n        as producing the column's default value on the database side are\n        specified using the ``server_default`` parameter, and not ``default``\n        which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column(\n                \"account\",\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(table_name, column, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_add_column(\n        cls,\n        operations: BatchOperations,\n        column: Column[Any],\n        *,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.add_column`\n\n        \"\"\"\n\n        kw = {}\n        if insert_before:\n            kw[\"insert_before\"] = insert_before\n        if insert_after:\n            kw[\"insert_after\"] = insert_after\n\n        op = cls(\n            operations.impl.table_name,\n            column,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_column\")\n@BatchOperations.register_operation(\"drop_column\", \"batch_drop_column\")\nclass DropColumnOp(AlterTableOp):\n    \"\"\"Represent a drop column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddColumnOp] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.kw = kw\n        self._reverse = _reverse\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\n            \"remove_column\",\n            self.schema,\n            self.table_name,\n            self.to_column(),\n        )\n\n    def reverse(self) -> AddColumnOp:\n        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:\n        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )\n\n    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:\n        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n\n    @classmethod\n    def drop_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\"organization\", \"account_id\")\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec's a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n        \"\"\"\n\n        op = cls(table_name, column_name, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_column(\n        cls, operations: BatchOperations, column_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_column`\n\n        \"\"\"\n        op = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"bulk_insert\")\nclass BulkInsertOp(MigrateOperation):\n    \"\"\"Represent a bulk insert operation.\"\"\"\n\n    def __init__(\n        self,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        self.table = table\n        self.rows = rows\n        self.multiinsert = multiinsert\n\n    @classmethod\n    def bulk_insert(\n        cls,\n        operations: Operations,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        \"\"\"Issue a \"bulk insert\" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\n                \"account\",\n                column(\"id\", Integer),\n                column(\"name\", String),\n                column(\"create_date\", Date),\n            )\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": date(2010, 10, 5),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": date(2007, 5, 27),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": date(2008, 8, 15),\n                    },\n                ],\n            )\n\n        When using --sql mode, some datatypes may not render inline\n        automatically, such as dates and other special types.   When this\n        issue is present, :meth:`.Operations.inline_literal` may be used::\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": op.inline_literal(\"2010-10-05\"),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": op.inline_literal(\"2007-05-27\"),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": op.inline_literal(\"2008-08-15\"),\n                    },\n                ],\n                multiinsert=False,\n            )\n\n        When using :meth:`.Operations.inline_literal` in conjunction with\n        :meth:`.Operations.bulk_insert`, in order for the statement to work\n        in \"online\" (e.g. non --sql) mode, the\n        :paramref:`~.Operations.bulk_insert.multiinsert`\n        flag should be set to ``False``, which will have the effect of\n        individual INSERT statements being emitted to the database, each\n        with a distinct VALUES clause, so that the \"inline\" values can\n        still be rendered, rather than attempting to pass the values\n        as bound parameters.\n\n        :param table: a table object which represents the target of the INSERT.\n\n        :param rows: a list of dictionaries indicating rows.\n\n        :param multiinsert: when at its default of True and --sql mode is not\n           enabled, the INSERT statement will be executed using\n           \"executemany()\" style, where all elements in the list of\n           dictionaries are passed as bound parameters in a single\n           list.   Setting this to False results in individual INSERT\n           statements being emitted per parameter set, and is needed\n           in those cases where non-literal values are present in the\n           parameter sets.\n\n        \"\"\"\n\n        op = cls(table, rows, multiinsert=multiinsert)\n        operations.invoke(op)\n\n\n@Operations.register_operation(\"execute\")\n@BatchOperations.register_operation(\"execute\", \"batch_execute\")\nclass ExecuteSQLOp(MigrateOperation):\n    \"\"\"Represent an execute SQL operation.\"\"\"\n\n    def __init__(\n        self,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.sqltext = sqltext\n        self.execution_options = execution_options\n\n    @classmethod\n    def execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        r\"\"\"Execute the given SQL using the current migration context.\n\n        The given SQL can be a plain string, e.g.::\n\n            op.execute(\"INSERT INTO table (foo) VALUES ('some value')\")\n\n        Or it can be any kind of Core SQL Expression construct, such as\n        below where we use an update construct::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\"account\", column(\"name\", String))\n            op.execute(\n                account.update()\n                .where(account.c.name == op.inline_literal(\"account 1\"))\n                .values({\"name\": op.inline_literal(\"account 2\")})\n            )\n\n        Above, we made use of the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table` and\n        :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it's a recommended practice to at least\n        ensure the definition of a table is self-contained within the migration\n        script, rather than imported from a module that may break compatibility\n        with older migrations.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in \"offline\" mode.     Additionally, parameterized\n        statements are discouraged here, as they *will not work* in offline\n        mode.  Above, we use :meth:`.inline_literal` where parameters are\n        to be used.\n\n        For full interaction with a connected database where parameters can\n        also be used normally, use the \"bind\" available from the context::\n\n            from alembic import op\n\n            connection = op.get_bind()\n\n            connection.execute(\n                account.update()\n                .where(account.c.name == \"account 1\")\n                .values({\"name\": \"account 2\"})\n            )\n\n        Additionally, when passing the statement as a plain string, it is first\n        coerced into a :func:`sqlalchemy.sql.expression.text` construct\n        before being passed along.  In the less likely case that the\n        literal SQL string contains a colon, it must be escaped with a\n        backslash, as::\n\n           op.execute(r\"INSERT INTO table (foo) VALUES ('\\:colon_value')\")\n\n\n        :param sqltext: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update` construct.\n        * a :func:`sqlalchemy.sql.expression.delete` construct.\n        * Any \"executable\" described in SQLAlchemy Core documentation,\n          noting that no result set is returned.\n\n        .. note::  when passing a plain string, the statement is coerced into\n           a :func:`sqlalchemy.sql.expression.text` construct. This construct\n           considers symbols with colons, e.g. ``:foo`` to be bound parameters.\n           To avoid this, ensure that colon symbols are escaped, e.g.\n           ``\\:foo``.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        \"\"\"\n        op = cls(sqltext, execution_options=execution_options)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Execute the given SQL using the current migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.execute`\n\n        \"\"\"\n        return cls.execute(\n            operations, sqltext, execution_options=execution_options\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Union[Executable, str]]:\n        return (\"execute\", self.sqltext)\n\n\nclass OpContainer(MigrateOperation):\n    \"\"\"Represent a sequence of operations operation.\"\"\"\n\n    def __init__(self, ops: Sequence[MigrateOperation] = ()) -> None:\n        self.ops = list(ops)\n\n    def is_empty(self) -> bool:\n        return not self.ops\n\n    def as_diffs(self) -> Any:\n        return list(OpContainer._ops_as_diffs(self))\n\n    @classmethod\n    def _ops_as_diffs(\n        cls, migrations: OpContainer\n    ) -> Iterator[Tuple[Any, ...]]:\n        for op in migrations.ops:\n            if hasattr(op, \"ops\"):\n                yield from cls._ops_as_diffs(cast(\"OpContainer\", op))\n            else:\n                yield op.to_diff_tuple()\n\n\nclass ModifyTableOps(OpContainer):\n    \"\"\"Contains a sequence of operations that all apply to a single Table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        ops: Sequence[MigrateOperation],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(ops)\n        self.table_name = table_name\n        self.schema = schema\n\n    def reverse(self) -> ModifyTableOps:\n        return ModifyTableOps(\n            self.table_name,\n            ops=list(reversed([op.reverse() for op in self.ops])),\n            schema=self.schema,\n        )\n\n\nclass UpgradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'upgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        upgrade_token: str = \"upgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.upgrade_token = upgrade_token\n\n    def reverse_into(self, downgrade_ops: DowngradeOps) -> DowngradeOps:\n        downgrade_ops.ops[:] = list(  # type:ignore[index]\n            reversed([op.reverse() for op in self.ops])\n        )\n        return downgrade_ops\n\n    def reverse(self) -> DowngradeOps:\n        return self.reverse_into(DowngradeOps(ops=[]))\n\n\nclass DowngradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'downgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        downgrade_token: str = \"downgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.downgrade_token = downgrade_token\n\n    def reverse(self):\n        return UpgradeOps(\n            ops=list(reversed([op.reverse() for op in self.ops]))\n        )\n\n\nclass MigrationScript(MigrateOperation):\n    \"\"\"represents a migration script.\n\n    E.g. when autogenerate encounters this object, this corresponds to the\n    production of an actual script file.\n\n    A normal :class:`.MigrationScript` object would contain a single\n    :class:`.UpgradeOps` and a single :class:`.DowngradeOps` directive.\n    These are accessible via the ``.upgrade_ops`` and ``.downgrade_ops``\n    attributes.\n\n    In the case of an autogenerate operation that runs multiple times,\n    such as the multiple database example in the \"multidb\" template,\n    the ``.upgrade_ops`` and ``.downgrade_ops`` attributes are disabled,\n    and instead these objects should be accessed via the ``.upgrade_ops_list``\n    and ``.downgrade_ops_list`` list-based attributes.  These latter\n    attributes are always available at the very least as single-element lists.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    _needs_render: Optional[bool]\n\n    def __init__(\n        self,\n        rev_id: Optional[str],\n        upgrade_ops: UpgradeOps,\n        downgrade_ops: DowngradeOps,\n        *,\n        message: Optional[str] = None,\n        imports: Set[str] = set(),\n        head: Optional[str] = None,\n        splice: Optional[bool] = None,\n        branch_label: Optional[_RevIdType] = None,\n        version_path: Optional[str] = None,\n        depends_on: Optional[_RevIdType] = None,\n    ) -> None:\n        self.rev_id = rev_id\n        self.message = message\n        self.imports = imports\n        self.head = head\n        self.splice = splice\n        self.branch_label = branch_label\n        self.version_path = version_path\n        self.depends_on = depends_on\n        self.upgrade_ops = upgrade_ops\n        self.downgrade_ops = downgrade_ops\n\n    @property\n    def upgrade_ops(self):\n        \"\"\"An instance of :class:`.UpgradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.upgrade_ops_list`\n        \"\"\"\n        if len(self._upgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for UpgradeOps; please use the \"\n                \"upgrade_ops_list attribute.\"\n            )\n        elif not self._upgrade_ops:\n            return None\n        else:\n            return self._upgrade_ops[0]\n\n    @upgrade_ops.setter\n    def upgrade_ops(self, upgrade_ops):\n        self._upgrade_ops = util.to_list(upgrade_ops)\n        for elem in self._upgrade_ops:\n            assert isinstance(elem, UpgradeOps)\n\n    @property\n    def downgrade_ops(self):\n        \"\"\"An instance of :class:`.DowngradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.downgrade_ops_list`\n        \"\"\"\n        if len(self._downgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for DowngradeOps; please use the \"\n                \"downgrade_ops_list attribute.\"\n            )\n        elif not self._downgrade_ops:\n            return None\n        else:\n            return self._downgrade_ops[0]\n\n    @downgrade_ops.setter\n    def downgrade_ops(self, downgrade_ops):\n        self._downgrade_ops = util.to_list(downgrade_ops)\n        for elem in self._downgrade_ops:\n            assert isinstance(elem, DowngradeOps)\n\n    @property\n    def upgrade_ops_list(self) -> List[UpgradeOps]:\n        \"\"\"A list of :class:`.UpgradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.upgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._upgrade_ops\n\n    @property\n    def downgrade_ops_list(self) -> List[DowngradeOps]:\n        \"\"\"A list of :class:`.DowngradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.downgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._downgrade_ops\n", "cross_context": [{"alembic.util.sqla_compat": "from __future__ import annotations\n\nimport contextlib\nimport re\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import Iterator\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import TYPE_CHECKING\nfrom typing import TypeVar\nfrom typing import Union\n\nfrom sqlalchemy import __version__\nfrom sqlalchemy import inspect\nfrom sqlalchemy import schema\nfrom sqlalchemy import sql\nfrom sqlalchemy import types as sqltypes\nfrom sqlalchemy.engine import url\nfrom sqlalchemy.ext.compiler import compiles\nfrom sqlalchemy.schema import CheckConstraint\nfrom sqlalchemy.schema import Column\nfrom sqlalchemy.schema import ForeignKeyConstraint\nfrom sqlalchemy.sql import visitors\nfrom sqlalchemy.sql.base import DialectKWArgs\nfrom sqlalchemy.sql.elements import BindParameter\nfrom sqlalchemy.sql.elements import ColumnClause\nfrom sqlalchemy.sql.elements import quoted_name\nfrom sqlalchemy.sql.elements import TextClause\nfrom sqlalchemy.sql.elements import UnaryExpression\nfrom sqlalchemy.sql.visitors import traverse\nfrom typing_extensions import TypeGuard\n\nif TYPE_CHECKING:\n    from sqlalchemy import Index\n    from sqlalchemy import Table\n    from sqlalchemy.engine import Connection\n    from sqlalchemy.engine import Dialect\n    from sqlalchemy.engine import Transaction\n    from sqlalchemy.engine.reflection import Inspector\n    from sqlalchemy.sql.base import ColumnCollection\n    from sqlalchemy.sql.compiler import SQLCompiler\n    from sqlalchemy.sql.dml import Insert\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.selectable import Select\n    from sqlalchemy.sql.selectable import TableClause\n\n_CE = TypeVar(\"_CE\", bound=Union[\"ColumnElement[Any]\", \"SchemaItem\"])\n\n\ndef _safe_int(value: str) -> Union[int, str]:\n    try:\n        return int(value)\n    except:\n        return value\n\n\n_vers = tuple(\n    [_safe_int(x) for x in re.findall(r\"(\\d+|[abc]\\d)\", __version__)]\n)\nsqla_13 = _vers >= (1, 3)\nsqla_14 = _vers >= (1, 4)\n# https://docs.sqlalchemy.org/en/latest/changelog/changelog_14.html#change-0c6e0cc67dfe6fac5164720e57ef307d\nsqla_14_18 = _vers >= (1, 4, 18)\nsqla_14_26 = _vers >= (1, 4, 26)\nsqla_2 = _vers >= (2,)\nsqlalchemy_version = __version__\n\ntry:\n    from sqlalchemy.sql.naming import _NONE_NAME as _NONE_NAME\nexcept ImportError:\n    from sqlalchemy.sql.elements import _NONE_NAME as _NONE_NAME  # type: ignore  # noqa: E501\n\n\nclass _Unsupported:\n    \"Placeholder for unsupported SQLAlchemy classes\"\n\n\ntry:\n    from sqlalchemy import Computed\nexcept ImportError:\n    if not TYPE_CHECKING:\n\n        class Computed(_Unsupported):\n            pass\n\n    has_computed = False\n    has_computed_reflection = False\nelse:\n    has_computed = True\n    has_computed_reflection = _vers >= (1, 3, 16)\n\ntry:\n    from sqlalchemy import Identity\nexcept ImportError:\n    if not TYPE_CHECKING:\n\n        class Identity(_Unsupported):\n            pass\n\n    has_identity = False\nelse:\n    identity_has_dialect_kwargs = issubclass(Identity, DialectKWArgs)\n\n    def _get_identity_options_dict(\n        identity: Union[Identity, schema.Sequence, None],\n        dialect_kwargs: bool = False,\n    ) -> Dict[str, Any]:\n        if identity is None:\n            return {}\n        elif identity_has_dialect_kwargs:\n            as_dict = identity._as_dict()  # type: ignore\n            if dialect_kwargs:\n                assert isinstance(identity, DialectKWArgs)\n                as_dict.update(identity.dialect_kwargs)\n        else:\n            as_dict = {}\n            if isinstance(identity, Identity):\n                # always=None means something different than always=False\n                as_dict[\"always\"] = identity.always\n                if identity.on_null is not None:\n                    as_dict[\"on_null\"] = identity.on_null\n            # attributes common to Identity and Sequence\n            attrs = (\n                \"start\",\n                \"increment\",\n                \"minvalue\",\n                \"maxvalue\",\n                \"nominvalue\",\n                \"nomaxvalue\",\n                \"cycle\",\n                \"cache\",\n                \"order\",\n            )\n            as_dict.update(\n                {\n                    key: getattr(identity, key, None)\n                    for key in attrs\n                    if getattr(identity, key, None) is not None\n                }\n            )\n        return as_dict\n\n    has_identity = True\n\nif sqla_2:\n    from sqlalchemy.sql.base import _NoneName\nelse:\n    from sqlalchemy.util import symbol as _NoneName  # type: ignore[assignment]\n\n\n_ConstraintName = Union[None, str, _NoneName]\n\n_ConstraintNameDefined = Union[str, _NoneName]\n\n\ndef constraint_name_defined(\n    name: _ConstraintName,\n) -> TypeGuard[_ConstraintNameDefined]:\n    return name is _NONE_NAME or isinstance(name, (str, _NoneName))\n\n\ndef constraint_name_string(\n    name: _ConstraintName,\n) -> TypeGuard[str]:\n    return isinstance(name, str)\n\n\ndef constraint_name_or_none(\n    name: _ConstraintName,\n) -> Optional[str]:\n    return name if constraint_name_string(name) else None\n\n\nAUTOINCREMENT_DEFAULT = \"auto\"\n\n\n@contextlib.contextmanager\ndef _ensure_scope_for_ddl(\n    connection: Optional[Connection],\n) -> Iterator[None]:\n    try:\n        in_transaction = connection.in_transaction  # type: ignore[union-attr]\n    except AttributeError:\n        # catch for MockConnection, None\n        in_transaction = None\n        pass\n\n    # yield outside the catch\n    if in_transaction is None:\n        yield\n    else:\n        if not in_transaction():\n            assert connection is not None\n            with connection.begin():\n                yield\n        else:\n            yield\n\n\ndef url_render_as_string(url, hide_password=True):\n    if sqla_14:\n        return url.render_as_string(hide_password=hide_password)\n    else:\n        return url.__to_string__(hide_password=hide_password)\n\n\ndef _safe_begin_connection_transaction(\n    connection: Connection,\n) -> Transaction:\n    transaction = _get_connection_transaction(connection)\n    if transaction:\n        return transaction\n    else:\n        return connection.begin()\n\n\ndef _safe_commit_connection_transaction(\n    connection: Connection,\n) -> None:\n    transaction = _get_connection_transaction(connection)\n    if transaction:\n        transaction.commit()\n\n\ndef _safe_rollback_connection_transaction(\n    connection: Connection,\n) -> None:\n    transaction = _get_connection_transaction(connection)\n    if transaction:\n        transaction.rollback()\n\n\ndef _get_connection_in_transaction(connection: Optional[Connection]) -> bool:\n    try:\n        in_transaction = connection.in_transaction  # type: ignore\n    except AttributeError:\n        # catch for MockConnection\n        return False\n    else:\n        return in_transaction()\n\n\ndef _idx_table_bound_expressions(idx: Index) -> Iterable[ColumnElement[Any]]:\n    return idx.expressions  # type: ignore\n\n\ndef _copy(schema_item: _CE, **kw) -> _CE:\n    if hasattr(schema_item, \"_copy\"):\n        return schema_item._copy(**kw)  # type: ignore[union-attr]\n    else:\n        return schema_item.copy(**kw)  # type: ignore[union-attr]\n\n\ndef _get_connection_transaction(\n    connection: Connection,\n) -> Optional[Transaction]:\n    if sqla_14:\n        return connection.get_transaction()\n    else:\n        r = connection._root  # type: ignore[attr-defined]\n        return r._Connection__transaction\n\n\ndef _create_url(*arg, **kw) -> url.URL:\n    if hasattr(url.URL, \"create\"):\n        return url.URL.create(*arg, **kw)\n    else:\n        return url.URL(*arg, **kw)\n\n\ndef _connectable_has_table(\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\n) -> bool:\n    if sqla_14:\n        return inspect(connectable).has_table(tablename, schemaname)\n    else:\n        return connectable.dialect.has_table(\n            connectable, tablename, schemaname\n        )\n\n\ndef _exec_on_inspector(inspector, statement, **params):\n    if sqla_14:\n        with inspector._operation_context() as conn:\n            return conn.execute(statement, params)\n    else:\n        return inspector.bind.execute(statement, params)\n\n\ndef _nullability_might_be_unset(metadata_column):\n    if not sqla_14:\n        return metadata_column.nullable\n    else:\n        from sqlalchemy.sql import schema\n\n        return (\n            metadata_column._user_defined_nullable is schema.NULL_UNSPECIFIED\n        )\n\n\ndef _server_default_is_computed(*server_default) -> bool:\n    if not has_computed:\n        return False\n    else:\n        return any(isinstance(sd, Computed) for sd in server_default)\n\n\ndef _server_default_is_identity(*server_default) -> bool:\n    if not sqla_14:\n        return False\n    else:\n        return any(isinstance(sd, Identity) for sd in server_default)\n\n\ndef _table_for_constraint(constraint: Constraint) -> Table:\n    if isinstance(constraint, ForeignKeyConstraint):\n        table = constraint.parent\n        assert table is not None\n        return table  # type: ignore[return-value]\n    else:\n        return constraint.table\n\n\ndef _columns_for_constraint(constraint):\n    if isinstance(constraint, ForeignKeyConstraint):\n        return [fk.parent for fk in constraint.elements]\n    elif isinstance(constraint, CheckConstraint):\n        return _find_columns(constraint.sqltext)\n    else:\n        return list(constraint.columns)\n\n\ndef _reflect_table(inspector: Inspector, table: Table) -> None:\n    if sqla_14:\n        return inspector.reflect_table(table, None)\n    else:\n        return inspector.reflecttable(  # type: ignore[attr-defined]\n            table, None\n        )\n\n\ndef _resolve_for_variant(type_, dialect):\n    if _type_has_variants(type_):\n        base_type, mapping = _get_variant_mapping(type_)\n        return mapping.get(dialect.name, base_type)\n    else:\n        return type_\n\n\nif hasattr(sqltypes.TypeEngine, \"_variant_mapping\"):\n\n    def _type_has_variants(type_):\n        return bool(type_._variant_mapping)\n\n    def _get_variant_mapping(type_):\n        return type_, type_._variant_mapping\n\nelse:\n\n    def _type_has_variants(type_):\n        return type(type_) is sqltypes.Variant\n\n    def _get_variant_mapping(type_):\n        return type_.impl, type_.mapping\n\n\ndef _fk_spec(constraint):\n    source_columns = [\n        constraint.columns[key].name for key in constraint.column_keys\n    ]\n\n    source_table = constraint.parent.name\n    source_schema = constraint.parent.schema\n    target_schema = constraint.elements[0].column.table.schema\n    target_table = constraint.elements[0].column.table.name\n    target_columns = [element.column.name for element in constraint.elements]\n    ondelete = constraint.ondelete\n    onupdate = constraint.onupdate\n    deferrable = constraint.deferrable\n    initially = constraint.initially\n    return (\n        source_schema,\n        source_table,\n        source_columns,\n        target_schema,\n        target_table,\n        target_columns,\n        onupdate,\n        ondelete,\n        deferrable,\n        initially,\n    )\n\n\ndef _fk_is_self_referential(constraint: ForeignKeyConstraint) -> bool:\n    spec = constraint.elements[0]._get_colspec()  # type: ignore[attr-defined]\n    tokens = spec.split(\".\")\n    tokens.pop(-1)  # colname\n    tablekey = \".\".join(tokens)\n    assert constraint.parent is not None\n    return tablekey == constraint.parent.key\n\n\ndef _is_type_bound(constraint: Constraint) -> bool:\n    # this deals with SQLAlchemy #3260, don't copy CHECK constraints\n    # that will be generated by the type.\n    # new feature added for #3260\n    return constraint._type_bound  # type: ignore[attr-defined]\n\n\ndef _find_columns(clause):\n    \"\"\"locate Column objects within the given expression.\"\"\"\n\n    cols = set()\n    traverse(clause, {}, {\"column\": cols.add})\n    return cols\n\n\ndef _remove_column_from_collection(\n    collection: ColumnCollection, column: Union[Column[Any], ColumnClause[Any]]\n) -> None:\n    \"\"\"remove a column from a ColumnCollection.\"\"\"\n\n    # workaround for older SQLAlchemy, remove the\n    # same object that's present\n    assert column.key is not None\n    to_remove = collection[column.key]\n\n    # SQLAlchemy 2.0 will use more ReadOnlyColumnCollection\n    # (renamed from ImmutableColumnCollection)\n    if hasattr(collection, \"_immutable\") or hasattr(collection, \"_readonly\"):\n        collection._parent.remove(to_remove)\n    else:\n        collection.remove(to_remove)\n\n\ndef _textual_index_column(\n    table: Table, text_: Union[str, TextClause, ColumnElement[Any]]\n) -> Union[ColumnElement[Any], Column[Any]]:\n    \"\"\"a workaround for the Index construct's severe lack of flexibility\"\"\"\n    if isinstance(text_, str):\n        c = Column(text_, sqltypes.NULLTYPE)\n        table.append_column(c)\n        return c\n    elif isinstance(text_, TextClause):\n        return _textual_index_element(table, text_)\n    elif isinstance(text_, _textual_index_element):\n        return _textual_index_column(table, text_.text)\n    elif isinstance(text_, sql.ColumnElement):\n        return _copy_expression(text_, table)\n    else:\n        raise ValueError(\"String or text() construct expected\")\n\n\ndef _copy_expression(expression: _CE, target_table: Table) -> _CE:\n    def replace(col):\n        if (\n            isinstance(col, Column)\n            and col.table is not None\n            and col.table is not target_table\n        ):\n            if col.name in target_table.c:\n                return target_table.c[col.name]\n            else:\n                c = _copy(col)\n                target_table.append_column(c)\n                return c\n        else:\n            return None\n\n    return visitors.replacement_traverse(  # type: ignore[call-overload]\n        expression, {}, replace\n    )\n\n\nclass _textual_index_element(sql.ColumnElement):\n    \"\"\"Wrap around a sqlalchemy text() construct in such a way that\n    we appear like a column-oriented SQL expression to an Index\n    construct.\n\n    The issue here is that currently the Postgresql dialect, the biggest\n    recipient of functional indexes, keys all the index expressions to\n    the corresponding column expressions when rendering CREATE INDEX,\n    so the Index we create here needs to have a .columns collection that\n    is the same length as the .expressions collection.  Ultimately\n    SQLAlchemy should support text() expressions in indexes.\n\n    See SQLAlchemy issue 3174.\n\n    \"\"\"\n\n    __visit_name__ = \"_textual_idx_element\"\n\n    def __init__(self, table: Table, text: TextClause) -> None:\n        self.table = table\n        self.text = text\n        self.key = text.text\n        self.fake_column = schema.Column(self.text.text, sqltypes.NULLTYPE)\n        table.append_column(self.fake_column)\n\n    def get_children(self):\n        return [self.fake_column]\n\n\n@compiles(_textual_index_element)\ndef _render_textual_index_column(\n    element: _textual_index_element, compiler: SQLCompiler, **kw\n) -> str:\n    return compiler.process(element.text, **kw)\n\n\nclass _literal_bindparam(BindParameter):\n    pass\n\n\n@compiles(_literal_bindparam)\ndef _render_literal_bindparam(\n    element: _literal_bindparam, compiler: SQLCompiler, **kw\n) -> str:\n    return compiler.render_literal_bindparam(element, **kw)\n\n\ndef _get_index_expressions(idx):\n    return list(idx.expressions)\n\n\ndef _get_index_column_names(idx):\n    return [getattr(exp, \"name\", None) for exp in _get_index_expressions(idx)]\n\n\ndef _column_kwargs(col: Column) -> Mapping:\n    if sqla_13:\n        return col.kwargs\n    else:\n        return {}\n\n\ndef _get_constraint_final_name(\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\n) -> Optional[str]:\n    if constraint.name is None:\n        return None\n    assert dialect is not None\n    if sqla_14:\n        # for SQLAlchemy 1.4 we would like to have the option to expand\n        # the use of \"deferred\" names for constraints as well as to have\n        # some flexibility with \"None\" name and similar; make use of new\n        # SQLAlchemy API to return what would be the final compiled form of\n        # the name for this dialect.\n        return dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n    else:\n        # prior to SQLAlchemy 1.4, work around quoting logic to get at the\n        # final compiled name without quotes.\n        if hasattr(constraint.name, \"quote\"):\n            # might be quoted_name, might be truncated_name, keep it the\n            # same\n            quoted_name_cls: type = type(constraint.name)\n        else:\n            quoted_name_cls = quoted_name\n\n        new_name = quoted_name_cls(str(constraint.name), quote=False)\n        constraint = constraint.__class__(name=new_name)\n\n        if isinstance(constraint, schema.Index):\n            # name should not be quoted.\n            d = dialect.ddl_compiler(dialect, None)  # type: ignore[arg-type]\n            return d._prepared_index_name(  # type: ignore[attr-defined]\n                constraint\n            )\n        else:\n            # name should not be quoted.\n            return dialect.identifier_preparer.format_constraint(constraint)\n\n\ndef _constraint_is_named(\n    constraint: Union[Constraint, Index], dialect: Optional[Dialect]\n) -> bool:\n    if sqla_14:\n        if constraint.name is None:\n            return False\n        assert dialect is not None\n        name = dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n        return name is not None\n    else:\n        return constraint.name is not None\n\n\ndef _is_mariadb(mysql_dialect: Dialect) -> bool:\n    if sqla_14:\n        return mysql_dialect.is_mariadb  # type: ignore[attr-defined]\n    else:\n        return bool(\n            mysql_dialect.server_version_info\n            and mysql_dialect._is_mariadb  # type: ignore[attr-defined]\n        )\n\n\ndef _mariadb_normalized_version_info(mysql_dialect):\n    return mysql_dialect._mariadb_normalized_version_info\n\n\ndef _insert_inline(table: Union[TableClause, Table]) -> Insert:\n    if sqla_14:\n        return table.insert().inline()\n    else:\n        return table.insert(inline=True)  # type: ignore[call-arg]\n\n\nif sqla_14:\n    from sqlalchemy import create_mock_engine\n    from sqlalchemy import select as _select\nelse:\n    from sqlalchemy import create_engine\n\n    def create_mock_engine(url, executor, **kw):  # type: ignore[misc]\n        return create_engine(\n            \"postgresql://\", strategy=\"mock\", executor=executor\n        )\n\n    def _select(*columns, **kw) -> Select:  # type: ignore[no-redef]\n        return sql.select(list(columns), **kw)  # type: ignore[call-overload]\n\n\ndef is_expression_index(index: Index) -> bool:\n    expr: Any\n    for expr in index.expressions:\n        while isinstance(expr, UnaryExpression):\n            expr = expr.element\n        if not isinstance(expr, ColumnClause) or expr.is_literal:\n            return True\n    return False\n"}, {"alembic.util.sqla_compat._table_for_constraint": "from __future__ import annotations\n\nimport contextlib\nimport re\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import Iterator\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import TYPE_CHECKING\nfrom typing import TypeVar\nfrom typing import Union\n\nfrom sqlalchemy import __version__\nfrom sqlalchemy import inspect\nfrom sqlalchemy import schema\nfrom sqlalchemy import sql\nfrom sqlalchemy import types as sqltypes\nfrom sqlalchemy.engine import url\nfrom sqlalchemy.ext.compiler import compiles\nfrom sqlalchemy.schema import CheckConstraint\nfrom sqlalchemy.schema import Column\nfrom sqlalchemy.schema import ForeignKeyConstraint\nfrom sqlalchemy.sql import visitors\nfrom sqlalchemy.sql.base import DialectKWArgs\nfrom sqlalchemy.sql.elements import BindParameter\nfrom sqlalchemy.sql.elements import ColumnClause\nfrom sqlalchemy.sql.elements import quoted_name\nfrom sqlalchemy.sql.elements import TextClause\nfrom sqlalchemy.sql.elements import UnaryExpression\nfrom sqlalchemy.sql.visitors import traverse\nfrom typing_extensions import TypeGuard\n\nif TYPE_CHECKING:\n    from sqlalchemy import Index\n    from sqlalchemy import Table\n    from sqlalchemy.engine import Connection\n    from sqlalchemy.engine import Dialect\n    from sqlalchemy.engine import Transaction\n    from sqlalchemy.engine.reflection import Inspector\n    from sqlalchemy.sql.base import ColumnCollection\n    from sqlalchemy.sql.compiler import SQLCompiler\n    from sqlalchemy.sql.dml import Insert\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.selectable import Select\n    from sqlalchemy.sql.selectable import TableClause\n\n_CE = TypeVar(\"_CE\", bound=Union[\"ColumnElement[Any]\", \"SchemaItem\"])\n\n\ndef _safe_int(value: str) -> Union[int, str]:\n    try:\n        return int(value)\n    except:\n        return value\n\n\n_vers = tuple(\n    [_safe_int(x) for x in re.findall(r\"(\\d+|[abc]\\d)\", __version__)]\n)\nsqla_13 = _vers >= (1, 3)\nsqla_14 = _vers >= (1, 4)\n# https://docs.sqlalchemy.org/en/latest/changelog/changelog_14.html#change-0c6e0cc67dfe6fac5164720e57ef307d\nsqla_14_18 = _vers >= (1, 4, 18)\nsqla_14_26 = _vers >= (1, 4, 26)\nsqla_2 = _vers >= (2,)\nsqlalchemy_version = __version__\n\ntry:\n    from sqlalchemy.sql.naming import _NONE_NAME as _NONE_NAME\nexcept ImportError:\n    from sqlalchemy.sql.elements import _NONE_NAME as _NONE_NAME  # type: ignore  # noqa: E501\n\n\nclass _Unsupported:\n    \"Placeholder for unsupported SQLAlchemy classes\"\n\n\ntry:\n    from sqlalchemy import Computed\nexcept ImportError:\n    if not TYPE_CHECKING:\n\n        class Computed(_Unsupported):\n            pass\n\n    has_computed = False\n    has_computed_reflection = False\nelse:\n    has_computed = True\n    has_computed_reflection = _vers >= (1, 3, 16)\n\ntry:\n    from sqlalchemy import Identity\nexcept ImportError:\n    if not TYPE_CHECKING:\n\n        class Identity(_Unsupported):\n            pass\n\n    has_identity = False\nelse:\n    identity_has_dialect_kwargs = issubclass(Identity, DialectKWArgs)\n\n    def _get_identity_options_dict(\n        identity: Union[Identity, schema.Sequence, None],\n        dialect_kwargs: bool = False,\n    ) -> Dict[str, Any]:\n        if identity is None:\n            return {}\n        elif identity_has_dialect_kwargs:\n            as_dict = identity._as_dict()  # type: ignore\n            if dialect_kwargs:\n                assert isinstance(identity, DialectKWArgs)\n                as_dict.update(identity.dialect_kwargs)\n        else:\n            as_dict = {}\n            if isinstance(identity, Identity):\n                # always=None means something different than always=False\n                as_dict[\"always\"] = identity.always\n                if identity.on_null is not None:\n                    as_dict[\"on_null\"] = identity.on_null\n            # attributes common to Identity and Sequence\n            attrs = (\n                \"start\",\n                \"increment\",\n                \"minvalue\",\n                \"maxvalue\",\n                \"nominvalue\",\n                \"nomaxvalue\",\n                \"cycle\",\n                \"cache\",\n                \"order\",\n            )\n            as_dict.update(\n                {\n                    key: getattr(identity, key, None)\n                    for key in attrs\n                    if getattr(identity, key, None) is not None\n                }\n            )\n        return as_dict\n\n    has_identity = True\n\nif sqla_2:\n    from sqlalchemy.sql.base import _NoneName\nelse:\n    from sqlalchemy.util import symbol as _NoneName  # type: ignore[assignment]\n\n\n_ConstraintName = Union[None, str, _NoneName]\n\n_ConstraintNameDefined = Union[str, _NoneName]\n\n\ndef constraint_name_defined(\n    name: _ConstraintName,\n) -> TypeGuard[_ConstraintNameDefined]:\n    return name is _NONE_NAME or isinstance(name, (str, _NoneName))\n\n\ndef constraint_name_string(\n    name: _ConstraintName,\n) -> TypeGuard[str]:\n    return isinstance(name, str)\n\n\ndef constraint_name_or_none(\n    name: _ConstraintName,\n) -> Optional[str]:\n    return name if constraint_name_string(name) else None\n\n\nAUTOINCREMENT_DEFAULT = \"auto\"\n\n\n@contextlib.contextmanager\ndef _ensure_scope_for_ddl(\n    connection: Optional[Connection],\n) -> Iterator[None]:\n    try:\n        in_transaction = connection.in_transaction  # type: ignore[union-attr]\n    except AttributeError:\n        # catch for MockConnection, None\n        in_transaction = None\n        pass\n\n    # yield outside the catch\n    if in_transaction is None:\n        yield\n    else:\n        if not in_transaction():\n            assert connection is not None\n            with connection.begin():\n                yield\n        else:\n            yield\n\n\ndef url_render_as_string(url, hide_password=True):\n    if sqla_14:\n        return url.render_as_string(hide_password=hide_password)\n    else:\n        return url.__to_string__(hide_password=hide_password)\n\n\ndef _safe_begin_connection_transaction(\n    connection: Connection,\n) -> Transaction:\n    transaction = _get_connection_transaction(connection)\n    if transaction:\n        return transaction\n    else:\n        return connection.begin()\n\n\ndef _safe_commit_connection_transaction(\n    connection: Connection,\n) -> None:\n    transaction = _get_connection_transaction(connection)\n    if transaction:\n        transaction.commit()\n\n\ndef _safe_rollback_connection_transaction(\n    connection: Connection,\n) -> None:\n    transaction = _get_connection_transaction(connection)\n    if transaction:\n        transaction.rollback()\n\n\ndef _get_connection_in_transaction(connection: Optional[Connection]) -> bool:\n    try:\n        in_transaction = connection.in_transaction  # type: ignore\n    except AttributeError:\n        # catch for MockConnection\n        return False\n    else:\n        return in_transaction()\n\n\ndef _idx_table_bound_expressions(idx: Index) -> Iterable[ColumnElement[Any]]:\n    return idx.expressions  # type: ignore\n\n\ndef _copy(schema_item: _CE, **kw) -> _CE:\n    if hasattr(schema_item, \"_copy\"):\n        return schema_item._copy(**kw)  # type: ignore[union-attr]\n    else:\n        return schema_item.copy(**kw)  # type: ignore[union-attr]\n\n\ndef _get_connection_transaction(\n    connection: Connection,\n) -> Optional[Transaction]:\n    if sqla_14:\n        return connection.get_transaction()\n    else:\n        r = connection._root  # type: ignore[attr-defined]\n        return r._Connection__transaction\n\n\ndef _create_url(*arg, **kw) -> url.URL:\n    if hasattr(url.URL, \"create\"):\n        return url.URL.create(*arg, **kw)\n    else:\n        return url.URL(*arg, **kw)\n\n\ndef _connectable_has_table(\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\n) -> bool:\n    if sqla_14:\n        return inspect(connectable).has_table(tablename, schemaname)\n    else:\n        return connectable.dialect.has_table(\n            connectable, tablename, schemaname\n        )\n\n\ndef _exec_on_inspector(inspector, statement, **params):\n    if sqla_14:\n        with inspector._operation_context() as conn:\n            return conn.execute(statement, params)\n    else:\n        return inspector.bind.execute(statement, params)\n\n\ndef _nullability_might_be_unset(metadata_column):\n    if not sqla_14:\n        return metadata_column.nullable\n    else:\n        from sqlalchemy.sql import schema\n\n        return (\n            metadata_column._user_defined_nullable is schema.NULL_UNSPECIFIED\n        )\n\n\ndef _server_default_is_computed(*server_default) -> bool:\n    if not has_computed:\n        return False\n    else:\n        return any(isinstance(sd, Computed) for sd in server_default)\n\n\ndef _server_default_is_identity(*server_default) -> bool:\n    if not sqla_14:\n        return False\n    else:\n        return any(isinstance(sd, Identity) for sd in server_default)\n\n\ndef _table_for_constraint(constraint: Constraint) -> Table:\n    if isinstance(constraint, ForeignKeyConstraint):\n        table = constraint.parent\n        assert table is not None\n        return table  # type: ignore[return-value]\n    else:\n        return constraint.table\n\n\ndef _columns_for_constraint(constraint):\n    if isinstance(constraint, ForeignKeyConstraint):\n        return [fk.parent for fk in constraint.elements]\n    elif isinstance(constraint, CheckConstraint):\n        return _find_columns(constraint.sqltext)\n    else:\n        return list(constraint.columns)\n\n\ndef _reflect_table(inspector: Inspector, table: Table) -> None:\n    if sqla_14:\n        return inspector.reflect_table(table, None)\n    else:\n        return inspector.reflecttable(  # type: ignore[attr-defined]\n            table, None\n        )\n\n\ndef _resolve_for_variant(type_, dialect):\n    if _type_has_variants(type_):\n        base_type, mapping = _get_variant_mapping(type_)\n        return mapping.get(dialect.name, base_type)\n    else:\n        return type_\n\n\nif hasattr(sqltypes.TypeEngine, \"_variant_mapping\"):\n\n    def _type_has_variants(type_):\n        return bool(type_._variant_mapping)\n\n    def _get_variant_mapping(type_):\n        return type_, type_._variant_mapping\n\nelse:\n\n    def _type_has_variants(type_):\n        return type(type_) is sqltypes.Variant\n\n    def _get_variant_mapping(type_):\n        return type_.impl, type_.mapping\n\n\ndef _fk_spec(constraint):\n    source_columns = [\n        constraint.columns[key].name for key in constraint.column_keys\n    ]\n\n    source_table = constraint.parent.name\n    source_schema = constraint.parent.schema\n    target_schema = constraint.elements[0].column.table.schema\n    target_table = constraint.elements[0].column.table.name\n    target_columns = [element.column.name for element in constraint.elements]\n    ondelete = constraint.ondelete\n    onupdate = constraint.onupdate\n    deferrable = constraint.deferrable\n    initially = constraint.initially\n    return (\n        source_schema,\n        source_table,\n        source_columns,\n        target_schema,\n        target_table,\n        target_columns,\n        onupdate,\n        ondelete,\n        deferrable,\n        initially,\n    )\n\n\ndef _fk_is_self_referential(constraint: ForeignKeyConstraint) -> bool:\n    spec = constraint.elements[0]._get_colspec()  # type: ignore[attr-defined]\n    tokens = spec.split(\".\")\n    tokens.pop(-1)  # colname\n    tablekey = \".\".join(tokens)\n    assert constraint.parent is not None\n    return tablekey == constraint.parent.key\n\n\ndef _is_type_bound(constraint: Constraint) -> bool:\n    # this deals with SQLAlchemy #3260, don't copy CHECK constraints\n    # that will be generated by the type.\n    # new feature added for #3260\n    return constraint._type_bound  # type: ignore[attr-defined]\n\n\ndef _find_columns(clause):\n    \"\"\"locate Column objects within the given expression.\"\"\"\n\n    cols = set()\n    traverse(clause, {}, {\"column\": cols.add})\n    return cols\n\n\ndef _remove_column_from_collection(\n    collection: ColumnCollection, column: Union[Column[Any], ColumnClause[Any]]\n) -> None:\n    \"\"\"remove a column from a ColumnCollection.\"\"\"\n\n    # workaround for older SQLAlchemy, remove the\n    # same object that's present\n    assert column.key is not None\n    to_remove = collection[column.key]\n\n    # SQLAlchemy 2.0 will use more ReadOnlyColumnCollection\n    # (renamed from ImmutableColumnCollection)\n    if hasattr(collection, \"_immutable\") or hasattr(collection, \"_readonly\"):\n        collection._parent.remove(to_remove)\n    else:\n        collection.remove(to_remove)\n\n\ndef _textual_index_column(\n    table: Table, text_: Union[str, TextClause, ColumnElement[Any]]\n) -> Union[ColumnElement[Any], Column[Any]]:\n    \"\"\"a workaround for the Index construct's severe lack of flexibility\"\"\"\n    if isinstance(text_, str):\n        c = Column(text_, sqltypes.NULLTYPE)\n        table.append_column(c)\n        return c\n    elif isinstance(text_, TextClause):\n        return _textual_index_element(table, text_)\n    elif isinstance(text_, _textual_index_element):\n        return _textual_index_column(table, text_.text)\n    elif isinstance(text_, sql.ColumnElement):\n        return _copy_expression(text_, table)\n    else:\n        raise ValueError(\"String or text() construct expected\")\n\n\ndef _copy_expression(expression: _CE, target_table: Table) -> _CE:\n    def replace(col):\n        if (\n            isinstance(col, Column)\n            and col.table is not None\n            and col.table is not target_table\n        ):\n            if col.name in target_table.c:\n                return target_table.c[col.name]\n            else:\n                c = _copy(col)\n                target_table.append_column(c)\n                return c\n        else:\n            return None\n\n    return visitors.replacement_traverse(  # type: ignore[call-overload]\n        expression, {}, replace\n    )\n\n\nclass _textual_index_element(sql.ColumnElement):\n    \"\"\"Wrap around a sqlalchemy text() construct in such a way that\n    we appear like a column-oriented SQL expression to an Index\n    construct.\n\n    The issue here is that currently the Postgresql dialect, the biggest\n    recipient of functional indexes, keys all the index expressions to\n    the corresponding column expressions when rendering CREATE INDEX,\n    so the Index we create here needs to have a .columns collection that\n    is the same length as the .expressions collection.  Ultimately\n    SQLAlchemy should support text() expressions in indexes.\n\n    See SQLAlchemy issue 3174.\n\n    \"\"\"\n\n    __visit_name__ = \"_textual_idx_element\"\n\n    def __init__(self, table: Table, text: TextClause) -> None:\n        self.table = table\n        self.text = text\n        self.key = text.text\n        self.fake_column = schema.Column(self.text.text, sqltypes.NULLTYPE)\n        table.append_column(self.fake_column)\n\n    def get_children(self):\n        return [self.fake_column]\n\n\n@compiles(_textual_index_element)\ndef _render_textual_index_column(\n    element: _textual_index_element, compiler: SQLCompiler, **kw\n) -> str:\n    return compiler.process(element.text, **kw)\n\n\nclass _literal_bindparam(BindParameter):\n    pass\n\n\n@compiles(_literal_bindparam)\ndef _render_literal_bindparam(\n    element: _literal_bindparam, compiler: SQLCompiler, **kw\n) -> str:\n    return compiler.render_literal_bindparam(element, **kw)\n\n\ndef _get_index_expressions(idx):\n    return list(idx.expressions)\n\n\ndef _get_index_column_names(idx):\n    return [getattr(exp, \"name\", None) for exp in _get_index_expressions(idx)]\n\n\ndef _column_kwargs(col: Column) -> Mapping:\n    if sqla_13:\n        return col.kwargs\n    else:\n        return {}\n\n\ndef _get_constraint_final_name(\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\n) -> Optional[str]:\n    if constraint.name is None:\n        return None\n    assert dialect is not None\n    if sqla_14:\n        # for SQLAlchemy 1.4 we would like to have the option to expand\n        # the use of \"deferred\" names for constraints as well as to have\n        # some flexibility with \"None\" name and similar; make use of new\n        # SQLAlchemy API to return what would be the final compiled form of\n        # the name for this dialect.\n        return dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n    else:\n        # prior to SQLAlchemy 1.4, work around quoting logic to get at the\n        # final compiled name without quotes.\n        if hasattr(constraint.name, \"quote\"):\n            # might be quoted_name, might be truncated_name, keep it the\n            # same\n            quoted_name_cls: type = type(constraint.name)\n        else:\n            quoted_name_cls = quoted_name\n\n        new_name = quoted_name_cls(str(constraint.name), quote=False)\n        constraint = constraint.__class__(name=new_name)\n\n        if isinstance(constraint, schema.Index):\n            # name should not be quoted.\n            d = dialect.ddl_compiler(dialect, None)  # type: ignore[arg-type]\n            return d._prepared_index_name(  # type: ignore[attr-defined]\n                constraint\n            )\n        else:\n            # name should not be quoted.\n            return dialect.identifier_preparer.format_constraint(constraint)\n\n\ndef _constraint_is_named(\n    constraint: Union[Constraint, Index], dialect: Optional[Dialect]\n) -> bool:\n    if sqla_14:\n        if constraint.name is None:\n            return False\n        assert dialect is not None\n        name = dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n        return name is not None\n    else:\n        return constraint.name is not None\n\n\ndef _is_mariadb(mysql_dialect: Dialect) -> bool:\n    if sqla_14:\n        return mysql_dialect.is_mariadb  # type: ignore[attr-defined]\n    else:\n        return bool(\n            mysql_dialect.server_version_info\n            and mysql_dialect._is_mariadb  # type: ignore[attr-defined]\n        )\n\n\ndef _mariadb_normalized_version_info(mysql_dialect):\n    return mysql_dialect._mariadb_normalized_version_info\n\n\ndef _insert_inline(table: Union[TableClause, Table]) -> Insert:\n    if sqla_14:\n        return table.insert().inline()\n    else:\n        return table.insert(inline=True)  # type: ignore[call-arg]\n\n\nif sqla_14:\n    from sqlalchemy import create_mock_engine\n    from sqlalchemy import select as _select\nelse:\n    from sqlalchemy import create_engine\n\n    def create_mock_engine(url, executor, **kw):  # type: ignore[misc]\n        return create_engine(\n            \"postgresql://\", strategy=\"mock\", executor=executor\n        )\n\n    def _select(*columns, **kw) -> Select:  # type: ignore[no-redef]\n        return sql.select(list(columns), **kw)  # type: ignore[call-overload]\n\n\ndef is_expression_index(index: Index) -> bool:\n    expr: Any\n    for expr in index.expressions:\n        while isinstance(expr, UnaryExpression):\n            expr = expr.element\n        if not isinstance(expr, ColumnClause) or expr.is_literal:\n            return True\n    return False\n"}], "prompt": "Please write a python function called 'to_constraint' base the context. Converts a DropConstraintOp instance to a Constraint instance. It first checks if the reverse operation is present. If it is, it converts the reverse operation to a Constraint instance and sets the name, table name, and schema of the constraint. Then it returns the constraint. If the reverse operation is not present, it raises a ValueError.:param self: DropConstraintOp. An instance of the DropConstraintOp class.\n:return: Constraint. The converted Constraint instance..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        from __future__ import annotations\n\nfrom abc import abstractmethod\nimport re\nfrom typing import Any\nfrom typing import Callable\nfrom typing import cast\nfrom typing import FrozenSet\nfrom typing import Iterator\nfrom typing import List\nfrom typing import MutableMapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Set\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TYPE_CHECKING\nfrom typing import Union\n\nfrom sqlalchemy.types import NULLTYPE\n\nfrom . import schemaobj\nfrom .base import BatchOperations\nfrom .base import Operations\nfrom .. import util\nfrom ..util import sqla_compat\n\nif TYPE_CHECKING:\n    from typing import Literal\n\n    from sqlalchemy.sql import Executable\n    from sqlalchemy.sql.elements import ColumnElement\n    from sqlalchemy.sql.elements import conv\n    from sqlalchemy.sql.elements import quoted_name\n    from sqlalchemy.sql.elements import TextClause\n    from sqlalchemy.sql.functions import Function\n    from sqlalchemy.sql.schema import CheckConstraint\n    from sqlalchemy.sql.schema import Column\n    from sqlalchemy.sql.schema import Computed\n    from sqlalchemy.sql.schema import Constraint\n    from sqlalchemy.sql.schema import ForeignKeyConstraint\n    from sqlalchemy.sql.schema import Identity\n    from sqlalchemy.sql.schema import Index\n    from sqlalchemy.sql.schema import MetaData\n    from sqlalchemy.sql.schema import PrimaryKeyConstraint\n    from sqlalchemy.sql.schema import SchemaItem\n    from sqlalchemy.sql.schema import Table\n    from sqlalchemy.sql.schema import UniqueConstraint\n    from sqlalchemy.sql.selectable import TableClause\n    from sqlalchemy.sql.type_api import TypeEngine\n\n    from ..autogenerate.rewriter import Rewriter\n    from ..runtime.migration import MigrationContext\n    from ..script.revision import _RevIdType\n\n\nclass MigrateOperation:\n    \"\"\"base class for migration command and organization objects.\n\n    This system is part of the operation extensibility API.\n\n    .. seealso::\n\n        :ref:`operation_objects`\n\n        :ref:`operation_plugins`\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    @util.memoized_property\n    def info(self):\n        \"\"\"A dictionary that may be used to store arbitrary information\n        along with this :class:`.MigrateOperation` object.\n\n        \"\"\"\n        return {}\n\n    _mutations: FrozenSet[Rewriter] = frozenset()\n\n    def reverse(self) -> MigrateOperation:\n        raise NotImplementedError\n\n    def to_diff_tuple(self) -> Tuple[Any, ...]:\n        raise NotImplementedError\n\n\nclass AddConstraintOp(MigrateOperation):\n    \"\"\"Represent an add constraint operation.\"\"\"\n\n    add_constraint_ops = util.Dispatcher()\n\n    @property\n    def constraint_type(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def register_add_constraint(cls, type_: str) -> Callable:\n        def go(klass):\n            cls.add_constraint_ops.dispatch_for(type_)(klass.from_constraint)\n            return klass\n\n        return go\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> AddConstraintOp:\n        return cls.add_constraint_ops.dispatch(constraint.__visit_name__)(\n            constraint\n        )\n\n    @abstractmethod\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Constraint:\n        pass\n\n    def reverse(self) -> DropConstraintOp:\n        return DropConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(self) -> Tuple[str, Constraint]:\n        return (\"add_constraint\", self.to_constraint())\n\n\n@Operations.register_operation(\"drop_constraint\")\n@BatchOperations.register_operation(\"drop_constraint\", \"batch_drop_constraint\")\nclass DropConstraintOp(MigrateOperation):\n    \"\"\"Represent a drop constraint operation.\"\"\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddConstraintOp] = None,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.constraint_type = type_\n        self.schema = schema\n        self._reverse = _reverse\n\n    def reverse(self) -> AddConstraintOp:\n        return AddConstraintOp.from_constraint(self.to_constraint())\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, SchemaItem]:\n        if self.constraint_type == \"foreignkey\":\n            return (\"remove_fk\", self.to_constraint())\n        else:\n            return (\"remove_constraint\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:\n        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )\n\n###The function: to_constraint###\n    @classmethod\n    def drop_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: str,\n        table_name: str,\n        type_: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        r\"\"\"Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param constraint_name: name of the constraint.\n        :param table_name: table name.\n        :param type\\_: optional, required on MySQL.  can be\n         'foreignkey', 'primary', 'unique', or 'check'.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, type_=type_, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        type_: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            type_=type_,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_primary_key\")\n@BatchOperations.register_operation(\n    \"create_primary_key\", \"batch_create_primary_key\"\n)\n@AddConstraintOp.register_add_constraint(\"primary_key_constraint\")\nclass CreatePrimaryKeyOp(AddConstraintOp):\n    \"\"\"Represent a create primary key operation.\"\"\"\n\n    constraint_type = \"primarykey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreatePrimaryKeyOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        pk_constraint = cast(\"PrimaryKeyConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(pk_constraint.name),\n            constraint_table.name,\n            pk_constraint.columns.keys(),\n            schema=constraint_table.schema,\n            **pk_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_primary_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: List[str],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_primary_key(\"pk_my_table\", \"my_table\", [\"id\", \"version\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the primary key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param columns: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, columns, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_primary_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: List[str],\n    ) -> None:\n        \"\"\"Issue a \"create primary key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``table_name`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_primary_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_unique_constraint\")\n@BatchOperations.register_operation(\n    \"create_unique_constraint\", \"batch_create_unique_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"unique_constraint\")\nclass CreateUniqueConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create unique constraint operation.\"\"\"\n\n    constraint_type = \"unique\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateUniqueConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        uq_constraint = cast(\"UniqueConstraint\", constraint)\n\n        kw: dict = {}\n        if uq_constraint.deferrable:\n            kw[\"deferrable\"] = uq_constraint.deferrable\n        if uq_constraint.initially:\n            kw[\"initially\"] = uq_constraint.initially\n        kw.update(uq_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(uq_constraint.name),\n            constraint_table.name,\n            [c.name for c in uq_constraint.columns],\n            schema=constraint_table.schema,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> UniqueConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.unique_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_unique_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        columns: Sequence[str],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param columns: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(constraint_name, table_name, columns, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_unique_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        columns: Sequence[str],\n        **kw: Any,\n    ) -> Any:\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_unique_constraint`\n\n        \"\"\"\n        kw[\"schema\"] = operations.impl.schema\n        op = cls(constraint_name, operations.impl.table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_foreign_key\")\n@BatchOperations.register_operation(\n    \"create_foreign_key\", \"batch_create_foreign_key\"\n)\n@AddConstraintOp.register_add_constraint(\"foreign_key_constraint\")\nclass CreateForeignKeyOp(AddConstraintOp):\n    \"\"\"Represent a create foreign key constraint operation.\"\"\"\n\n    constraint_type = \"foreignkey\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.source_table = source_table\n        self.referent_table = referent_table\n        self.local_cols = local_cols\n        self.remote_cols = remote_cols\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, ForeignKeyConstraint]:\n        return (\"add_fk\", self.to_constraint())\n\n    @classmethod\n    def from_constraint(cls, constraint: Constraint) -> CreateForeignKeyOp:\n        fk_constraint = cast(\"ForeignKeyConstraint\", constraint)\n        kw: dict = {}\n        if fk_constraint.onupdate:\n            kw[\"onupdate\"] = fk_constraint.onupdate\n        if fk_constraint.ondelete:\n            kw[\"ondelete\"] = fk_constraint.ondelete\n        if fk_constraint.initially:\n            kw[\"initially\"] = fk_constraint.initially\n        if fk_constraint.deferrable:\n            kw[\"deferrable\"] = fk_constraint.deferrable\n        if fk_constraint.use_alter:\n            kw[\"use_alter\"] = fk_constraint.use_alter\n        if fk_constraint.match:\n            kw[\"match\"] = fk_constraint.match\n\n        (\n            source_schema,\n            source_table,\n            source_columns,\n            target_schema,\n            target_table,\n            target_columns,\n            onupdate,\n            ondelete,\n            deferrable,\n            initially,\n        ) = sqla_compat._fk_spec(fk_constraint)\n\n        kw[\"source_schema\"] = source_schema\n        kw[\"referent_schema\"] = target_schema\n        kw.update(fk_constraint.dialect_kwargs)\n        return cls(\n            sqla_compat.constraint_name_or_none(fk_constraint.name),\n            source_table,\n            target_table,\n            source_columns,\n            target_columns,\n            **kw,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> ForeignKeyConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.foreign_key_constraint(\n            self.constraint_name,\n            self.source_table,\n            self.referent_table,\n            self.local_cols,\n            self.remote_cols,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_foreign_key(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        source_table: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        source_schema: Optional[str] = None,\n        referent_schema: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_foreign_key(\n                \"fk_user_address\",\n                \"address\",\n                \"user\",\n                [\"user_id\"],\n                [\"id\"],\n            )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the\n        :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param constraint_name: Name of the foreign key constraint.  The name\n         is necessary so that an ALTER statement can be emitted.  For setups\n         that use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source_table: String name of the source table.\n        :param referent_table: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        op = cls(\n            constraint_name,\n            source_table,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=source_schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_foreign_key(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        referent_table: str,\n        local_cols: List[str],\n        remote_cols: List[str],\n        *,\n        referent_schema: Optional[str] = None,\n        onupdate: Optional[str] = None,\n        ondelete: Optional[str] = None,\n        deferrable: Optional[bool] = None,\n        initially: Optional[str] = None,\n        match: Optional[str] = None,\n        **dialect_kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``source_schema``\n        arguments from the call.\n\n        e.g.::\n\n            with batch_alter_table(\"address\") as batch_op:\n                batch_op.create_foreign_key(\n                    \"fk_user_address\",\n                    \"user\",\n                    [\"user_id\"],\n                    [\"id\"],\n                )\n\n        .. seealso::\n\n            :meth:`.Operations.create_foreign_key`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            referent_table,\n            local_cols,\n            remote_cols,\n            onupdate=onupdate,\n            ondelete=ondelete,\n            deferrable=deferrable,\n            source_schema=operations.impl.schema,\n            referent_schema=referent_schema,\n            initially=initially,\n            match=match,\n            **dialect_kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_check_constraint\")\n@BatchOperations.register_operation(\n    \"create_check_constraint\", \"batch_create_check_constraint\"\n)\n@AddConstraintOp.register_add_constraint(\"check_constraint\")\n@AddConstraintOp.register_add_constraint(\"table_or_column_check_constraint\")\n@AddConstraintOp.register_add_constraint(\"column_check_constraint\")\nclass CreateCheckConstraintOp(AddConstraintOp):\n    \"\"\"Represent a create check constraint operation.\"\"\"\n\n    constraint_type = \"check\"\n\n    def __init__(\n        self,\n        constraint_name: Optional[sqla_compat._ConstraintNameDefined],\n        table_name: str,\n        condition: Union[str, TextClause, ColumnElement[Any]],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        self.constraint_name = constraint_name\n        self.table_name = table_name\n        self.condition = condition\n        self.schema = schema\n        self.kw = kw\n\n    @classmethod\n    def from_constraint(\n        cls, constraint: Constraint\n    ) -> CreateCheckConstraintOp:\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n\n        ck_constraint = cast(\"CheckConstraint\", constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(ck_constraint.name),\n            constraint_table.name,\n            cast(\"ColumnElement[Any]\", ck_constraint.sqltext),\n            schema=constraint_table.schema,\n            **ck_constraint.dialect_kwargs,\n        )\n\n    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> CheckConstraint:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.check_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.condition,\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_check_constraint(\n        cls,\n        operations: Operations,\n        constraint_name: Optional[str],\n        table_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column(\"name\")) > 5,\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         :ref:`sqla:constraint_naming_conventions`,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the source table.\n        :param condition: SQL expression that's the condition of the\n         constraint. Can be a string or SQLAlchemy expression language\n         structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or\n         NOT DEFERRABLE when issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value>\n         when issuing DDL for this constraint.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(constraint_name, table_name, condition, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_check_constraint(\n        cls,\n        operations: BatchOperations,\n        constraint_name: str,\n        condition: Union[str, ColumnElement[bool], TextClause],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current batch migration context.\n\n        The batch form of this call omits the ``source`` and ``schema``\n        arguments from the call.\n\n        .. seealso::\n\n            :meth:`.Operations.create_check_constraint`\n\n        \"\"\"\n        op = cls(\n            constraint_name,\n            operations.impl.table_name,\n            condition,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_index\")\n@BatchOperations.register_operation(\"create_index\", \"batch_create_index\")\nclass CreateIndexOp(MigrateOperation):\n    \"\"\"Represent a create index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.unique = unique\n        self.if_not_exists = if_not_exists\n        self.kw = kw\n\n    def reverse(self) -> DropIndexOp:\n        return DropIndexOp.from_index(self.to_index())\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"add_index\", self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> CreateIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n\n    @classmethod\n    def create_index(\n        cls,\n        operations: Operations,\n        index_name: Optional[str],\n        table_name: str,\n        columns: Sequence[Union[str, TextClause, Function[Any]]],\n        *,\n        schema: Optional[str] = None,\n        unique: bool = False,\n        if_not_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n\n            op.create_index(\"ik_test\", \"t1\", [\"foo\", \"bar\"])\n\n        Functional indexes can be produced by using the\n        :func:`sqlalchemy.sql.expression.text` construct::\n\n            from alembic import op\n            from sqlalchemy import text\n\n            op.create_index(\"ik_test\", \"t1\", [text(\"lower(foo)\")])\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.\n        :param columns: a list consisting of string column names and/or\n         :func:`~sqlalchemy.sql.expression.text` constructs.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param unique: If True, create a unique index.\n\n        :param quote: Force quoting of this column's name on or off,\n         corresponding to ``True`` or ``False``. When left at its default\n         of ``None``, the column identifier will be quoted according to\n         whether the name is case sensitive (identifiers with at least one\n         upper case character are treated as case sensitive), or if it's a\n         reserved word. This flag is only needed to force quoting of a\n         reserved word which is not known by the SQLAlchemy dialect.\n\n        :param if_not_exists: If True, adds IF NOT EXISTS operator when\n         creating the new index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_index(\n        cls,\n        operations: BatchOperations,\n        index_name: str,\n        columns: List[str],\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"create index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.create_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            operations.impl.table_name,\n            columns,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_index\")\n@BatchOperations.register_operation(\"drop_index\", \"batch_drop_index\")\nclass DropIndexOp(MigrateOperation):\n    \"\"\"Represent a drop index operation.\"\"\"\n\n    def __init__(\n        self,\n        index_name: Union[quoted_name, str, conv],\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        _reverse: Optional[CreateIndexOp] = None,\n        **kw: Any,\n    ) -> None:\n        self.index_name = index_name\n        self.table_name = table_name\n        self.schema = schema\n        self.if_exists = if_exists\n        self._reverse = _reverse\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Tuple[str, Index]:\n        return (\"remove_index\", self.to_index())\n\n    def reverse(self) -> CreateIndexOp:\n        return CreateIndexOp.from_index(self.to_index())\n\n    @classmethod\n    def from_index(cls, index: Index) -> DropIndexOp:\n        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n\n    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )\n\n    @classmethod\n    def drop_index(\n        cls,\n        operations: Operations,\n        index_name: str,\n        table_name: Optional[str] = None,\n        *,\n        schema: Optional[str] = None,\n        if_exists: Optional[bool] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param index_name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        :param if_exists: If True, adds IF EXISTS operator when\n         dropping the index.\n\n         .. versionadded:: 1.12.0\n\n        :param \\**kw: Additional keyword arguments not mentioned above are\n         dialect specific, and passed in the form\n         ``<dialectname>_<argname>``.\n         See the documentation regarding an individual dialect at\n         :ref:`dialect_toplevel` for detail on documented arguments.\n\n        \"\"\"\n        op = cls(\n            index_name,\n            table_name=table_name,\n            schema=schema,\n            if_exists=if_exists,\n            **kw,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_index(\n        cls, operations: BatchOperations, index_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop index\" instruction using the\n        current batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_index`\n\n        \"\"\"\n\n        op = cls(\n            index_name,\n            table_name=operations.impl.table_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table\")\nclass CreateTableOp(MigrateOperation):\n    \"\"\"Represent a create table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        columns: Sequence[SchemaItem],\n        *,\n        schema: Optional[str] = None,\n        _namespace_metadata: Optional[MetaData] = None,\n        _constraints_included: bool = False,\n        **kw: Any,\n    ) -> None:\n        self.table_name = table_name\n        self.columns = columns\n        self.schema = schema\n        self.info = kw.pop(\"info\", {})\n        self.comment = kw.pop(\"comment\", None)\n        self.prefixes = kw.pop(\"prefixes\", None)\n        self.kw = kw\n        self._namespace_metadata = _namespace_metadata\n        self._constraints_included = _constraints_included\n\n    def reverse(self) -> DropTableOp:\n        return DropTableOp.from_table(\n            self.to_table(), _namespace_metadata=self._namespace_metadata\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"add_table\", self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:\n        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=self._constraints_included,\n            **self.kw,\n        )\n\n    @classmethod\n    def create_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *columns: SchemaItem,\n        **kw: Any,\n    ) -> Table:\n        r\"\"\"Issue a \"create table\" instruction using the current migration\n        context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        Note that :meth:`.create_table` accepts\n        :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        The function also returns a newly created\n        :class:`~sqlalchemy.schema.Table` object, corresponding to the table\n        specification given, which is suitable for\n        immediate SQL operations, in particular\n        :meth:`.Operations.bulk_insert`::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            account_table = op.create_table(\n                \"account\",\n                Column(\"id\", INTEGER, primary_key=True),\n                Column(\"name\", VARCHAR(50), nullable=False),\n                Column(\"description\", NVARCHAR(200)),\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n            op.bulk_insert(\n                account_table,\n                [\n                    {\"name\": \"A1\", \"description\": \"account 1\"},\n                    {\"name\": \"A2\", \"description\": \"account 2\"},\n                ],\n            )\n\n        :param table_name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        :return: the :class:`~sqlalchemy.schema.Table` object corresponding\n         to the parameters given.\n\n        \"\"\"\n        op = cls(table_name, columns, **kw)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_table\")\nclass DropTableOp(MigrateOperation):\n    \"\"\"Represent a drop table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        table_kw: Optional[MutableMapping[Any, Any]] = None,\n        _reverse: Optional[CreateTableOp] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n        self.table_kw = table_kw or {}\n        self.comment = self.table_kw.pop(\"comment\", None)\n        self.info = self.table_kw.pop(\"info\", None)\n        self.prefixes = self.table_kw.pop(\"prefixes\", None)\n        self._reverse = _reverse\n\n    def to_diff_tuple(self) -> Tuple[str, Table]:\n        return (\"remove_table\", self.to_table())\n\n    def reverse(self) -> CreateTableOp:\n        return CreateTableOp.from_table(self.to_table())\n\n    @classmethod\n    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n\n    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:\n        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t\n\n    @classmethod\n    def drop_table(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param table_name: Name of the table\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        op = cls(table_name, schema=schema, table_kw=kw)\n        operations.invoke(op)\n\n\nclass AlterTableOp(MigrateOperation):\n    \"\"\"Represent an alter table operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.schema = schema\n\n\n@Operations.register_operation(\"rename_table\")\nclass RenameTableOp(AlterTableOp):\n    \"\"\"Represent a rename table operation.\"\"\"\n\n    def __init__(\n        self,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(old_table_name, schema=schema)\n        self.new_table_name = new_table_name\n\n    @classmethod\n    def rename_table(\n        cls,\n        operations: Operations,\n        old_table_name: str,\n        new_table_name: str,\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n        op = cls(old_table_name, new_table_name, schema=schema)\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"create_table_comment\")\n@BatchOperations.register_operation(\n    \"create_table_comment\", \"batch_create_table_comment\"\n)\nclass CreateTableCommentOp(AlterTableOp):\n    \"\"\"Represent a COMMENT ON `table` operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.comment = comment\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def create_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table.\n\n        :param table_name: string name of the target table.\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(\n            table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=schema,\n        )\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_create_table_comment(\n        cls,\n        operations: BatchOperations,\n        comment: Optional[str],\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Emit a COMMENT ON operation to set the comment for a table\n        using the current batch migration context.\n\n        :param comment: string value of the comment being registered against\n         the specified table.\n        :param existing_comment: String value of a comment\n         already registered on the specified table, used within autogenerate\n         so that the operation is reversible, but not required for direct\n         use.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            comment,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        if self.existing_comment is None:\n            return DropTableCommentOp(\n                self.table_name,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n        else:\n            return CreateTableCommentOp(\n                self.table_name,\n                self.existing_comment,\n                existing_comment=self.comment,\n                schema=self.schema,\n            )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name, schema=self.schema, comment=self.comment\n        )\n\n    def to_diff_tuple(self):\n        return (\"add_table_comment\", self.to_table(), self.existing_comment)\n\n\n@Operations.register_operation(\"drop_table_comment\")\n@BatchOperations.register_operation(\n    \"drop_table_comment\", \"batch_drop_table_comment\"\n)\nclass DropTableCommentOp(AlterTableOp):\n    \"\"\"Represent an operation to remove the comment from a table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        self.table_name = table_name\n        self.existing_comment = existing_comment\n        self.schema = schema\n\n    @classmethod\n    def drop_table_comment(\n        cls,\n        operations: Operations,\n        table_name: str,\n        *,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table.\n\n        :param table_name: string name of the target table.\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        .. seealso::\n\n            :meth:`.Operations.create_table_comment`\n\n            :paramref:`.Operations.alter_column.comment`\n\n        \"\"\"\n\n        op = cls(table_name, existing_comment=existing_comment, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_table_comment(\n        cls,\n        operations: BatchOperations,\n        *,\n        existing_comment: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue a \"drop table comment\" operation to\n        remove an existing comment set on a table using the current\n        batch operations context.\n\n        :param existing_comment: An optional string value of a comment already\n         registered on the specified table.\n\n        \"\"\"\n\n        op = cls(\n            operations.impl.table_name,\n            existing_comment=existing_comment,\n            schema=operations.impl.schema,\n        )\n        return operations.invoke(op)\n\n    def reverse(self):\n        \"\"\"Reverses the COMMENT ON operation against a table.\"\"\"\n        return CreateTableCommentOp(\n            self.table_name, self.existing_comment, schema=self.schema\n        )\n\n    def to_table(self, migration_context=None):\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(self.table_name, schema=self.schema)\n\n    def to_diff_tuple(self):\n        return (\"remove_table_comment\", self.to_table())\n\n\n@Operations.register_operation(\"alter_column\")\n@BatchOperations.register_operation(\"alter_column\", \"batch_alter_column\")\nclass AlterColumnOp(AlterTableOp):\n    \"\"\"Represent an alter column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        existing_type: Optional[Any] = None,\n        existing_server_default: Any = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        modify_nullable: Optional[bool] = None,\n        modify_comment: Optional[Union[str, Literal[False]]] = False,\n        modify_server_default: Any = False,\n        modify_name: Optional[str] = None,\n        modify_type: Optional[Any] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.existing_type = existing_type\n        self.existing_server_default = existing_server_default\n        self.existing_nullable = existing_nullable\n        self.existing_comment = existing_comment\n        self.modify_nullable = modify_nullable\n        self.modify_comment = modify_comment\n        self.modify_server_default = modify_server_default\n        self.modify_name = modify_name\n        self.modify_type = modify_type\n        self.kw = kw\n\n    def to_diff_tuple(self) -> Any:\n        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff\n\n    def has_changes(self) -> bool:\n        hc1 = (\n            self.modify_nullable is not None\n            or self.modify_server_default is not False\n            or self.modify_type is not None\n            or self.modify_comment is not False\n        )\n        if hc1:\n            return True\n        for kw in self.kw:\n            if kw.startswith(\"modify_\"):\n                return True\n        else:\n            return False\n\n    def reverse(self) -> AlterColumnOp:\n        kw = self.kw.copy()\n        kw[\"existing_type\"] = self.existing_type\n        kw[\"existing_nullable\"] = self.existing_nullable\n        kw[\"existing_server_default\"] = self.existing_server_default\n        kw[\"existing_comment\"] = self.existing_comment\n        if self.modify_type is not None:\n            kw[\"modify_type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            kw[\"modify_nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            kw[\"modify_server_default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            kw[\"modify_comment\"] = self.modify_comment\n\n        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return self.__class__(\n            self.table_name, self.column_name, schema=self.schema, **kw\n        )\n\n    @classmethod\n    def alter_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        r\"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_type`` argument is\n        used in this case to identify and remove a previous\n        constraint that was bound to the type object.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param comment: optional string text of a new comment to add to the\n         column.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type\\_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLAlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param existing_comment: string text of the existing comment on the\n         column to be maintained.  Required on MySQL if the existing comment\n         on the column is not being changed.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param postgresql_using: String argument which will indicate a\n         SQL expression to render within the Postgresql-specific USING clause\n         within ALTER COLUMN.    This string is taken directly as raw SQL which\n         must explicitly include any necessary quoting or escaping of tokens\n         within the expression.\n\n        \"\"\"\n\n        alt = cls(\n            table_name,\n            column_name,\n            schema=schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n    @classmethod\n    def batch_alter_column(\n        cls,\n        operations: BatchOperations,\n        column_name: str,\n        *,\n        nullable: Optional[bool] = None,\n        comment: Optional[Union[str, Literal[False]]] = False,\n        server_default: Any = False,\n        new_column_name: Optional[str] = None,\n        type_: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_type: Optional[Union[TypeEngine, Type[TypeEngine]]] = None,\n        existing_server_default: Optional[\n            Union[str, bool, Identity, Computed]\n        ] = False,\n        existing_nullable: Optional[bool] = None,\n        existing_comment: Optional[str] = None,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue an \"alter column\" instruction using the current\n        batch migration context.\n\n        Parameters are the same as that of :meth:`.Operations.alter_column`,\n        as well as the following option(s):\n\n        :param insert_before: String name of an existing column which this\n         column should be placed before, when creating the new table.\n\n        :param insert_after: String name of an existing column which this\n         column should be placed after, when creating the new table.  If\n         both :paramref:`.BatchOperations.alter_column.insert_before`\n         and :paramref:`.BatchOperations.alter_column.insert_after` are\n         omitted, the column is inserted after the last existing column\n         in the table.\n\n        .. seealso::\n\n            :meth:`.Operations.alter_column`\n\n\n        \"\"\"\n        alt = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_comment=existing_comment,\n            modify_name=new_column_name,\n            modify_type=type_,\n            modify_server_default=server_default,\n            modify_nullable=nullable,\n            modify_comment=comment,\n            insert_before=insert_before,\n            insert_after=insert_after,\n            **kw,\n        )\n\n        return operations.invoke(alt)\n\n\n@Operations.register_operation(\"add_column\")\n@BatchOperations.register_operation(\"add_column\", \"batch_add_column\")\nclass AddColumnOp(AlterTableOp):\n    \"\"\"Represent an add column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column = column\n        self.kw = kw\n\n    def reverse(self) -> DropColumnOp:\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\"add_column\", self.schema, self.table_name, self.column)\n\n    def to_column(self) -> Column:\n        return self.column\n\n    @classmethod\n    def from_column(cls, col: Column) -> AddColumnOp:\n        return cls(col.table.name, col, schema=col.table.schema)\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> AddColumnOp:\n        return cls(tname, col, schema=schema)\n\n    @classmethod\n    def add_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column: Column[Any],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\"organization\", Column(\"name\", String()))\n\n        The :meth:`.Operations.add_column` method typically corresponds\n        to the SQL command \"ALTER TABLE... ADD COLUMN\".    Within the scope\n        of this command, the column's name, datatype, nullability,\n        and optional server-generated defaults may be indicated.\n\n        .. note::\n\n            With the exception of NOT NULL constraints or single-column FOREIGN\n            KEY constraints, other kinds of constraints such as PRIMARY KEY,\n            UNIQUE or CHECK constraints **cannot** be generated using this\n            method; for these constraints, refer to operations such as\n            :meth:`.Operations.create_primary_key` and\n            :meth:`.Operations.create_check_constraint`. In particular, the\n            following :class:`~sqlalchemy.schema.Column` parameters are\n            **ignored**:\n\n            * :paramref:`~sqlalchemy.schema.Column.primary_key` - SQL databases\n              typically do not support an ALTER operation that can add\n              individual columns one at a time to an existing primary key\n              constraint, therefore it's less ambiguous to use the\n              :meth:`.Operations.create_primary_key` method, which assumes no\n              existing primary key constraint is present.\n            * :paramref:`~sqlalchemy.schema.Column.unique` - use the\n              :meth:`.Operations.create_unique_constraint` method\n            * :paramref:`~sqlalchemy.schema.Column.index` - use the\n              :meth:`.Operations.create_index` method\n\n\n        The provided :class:`~sqlalchemy.schema.Column` object may include a\n        :class:`~sqlalchemy.schema.ForeignKey` constraint directive,\n        referencing a remote table name. For this specific type of constraint,\n        Alembic will automatically emit a second ALTER statement in order to\n        add the single-column FOREIGN KEY constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\n                \"organization\",\n                Column(\"account_id\", INTEGER, ForeignKey(\"accounts.id\")),\n            )\n\n        The column argument passed to :meth:`.Operations.add_column` is a\n        :class:`~sqlalchemy.schema.Column` construct, used in the same way it's\n        used in SQLAlchemy. In particular, values or functions to be indicated\n        as producing the column's default value on the database side are\n        specified using the ``server_default`` parameter, and not ``default``\n        which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column(\n                \"account\",\n                Column(\"timestamp\", TIMESTAMP, server_default=func.now()),\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n\n        \"\"\"\n\n        op = cls(table_name, column, schema=schema)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_add_column(\n        cls,\n        operations: BatchOperations,\n        column: Column[Any],\n        *,\n        insert_before: Optional[str] = None,\n        insert_after: Optional[str] = None,\n    ) -> None:\n        \"\"\"Issue an \"add column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.add_column`\n\n        \"\"\"\n\n        kw = {}\n        if insert_before:\n            kw[\"insert_before\"] = insert_before\n        if insert_after:\n            kw[\"insert_after\"] = insert_after\n\n        op = cls(\n            operations.impl.table_name,\n            column,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"drop_column\")\n@BatchOperations.register_operation(\"drop_column\", \"batch_drop_column\")\nclass DropColumnOp(AlterTableOp):\n    \"\"\"Represent a drop column operation.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        _reverse: Optional[AddColumnOp] = None,\n        **kw: Any,\n    ) -> None:\n        super().__init__(table_name, schema=schema)\n        self.column_name = column_name\n        self.kw = kw\n        self._reverse = _reverse\n\n    def to_diff_tuple(\n        self,\n    ) -> Tuple[str, Optional[str], str, Column[Any]]:\n        return (\n            \"remove_column\",\n            self.schema,\n            self.table_name,\n            self.to_column(),\n        )\n\n    def reverse(self) -> AddColumnOp:\n        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )\n\n    @classmethod\n    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:\n        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )\n\n    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:\n        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)\n\n    @classmethod\n    def drop_column(\n        cls,\n        operations: Operations,\n        table_name: str,\n        column_name: str,\n        *,\n        schema: Optional[str] = None,\n        **kw: Any,\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\"organization\", \"account_id\")\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.  To control\n         quoting of the schema outside of the default behavior, use\n         the SQLAlchemy construct\n         :class:`~sqlalchemy.sql.elements.quoted_name`.\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec's a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n        \"\"\"\n\n        op = cls(table_name, column_name, schema=schema, **kw)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_drop_column(\n        cls, operations: BatchOperations, column_name: str, **kw: Any\n    ) -> None:\n        \"\"\"Issue a \"drop column\" instruction using the current\n        batch migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.drop_column`\n\n        \"\"\"\n        op = cls(\n            operations.impl.table_name,\n            column_name,\n            schema=operations.impl.schema,\n            **kw,\n        )\n        return operations.invoke(op)\n\n\n@Operations.register_operation(\"bulk_insert\")\nclass BulkInsertOp(MigrateOperation):\n    \"\"\"Represent a bulk insert operation.\"\"\"\n\n    def __init__(\n        self,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        self.table = table\n        self.rows = rows\n        self.multiinsert = multiinsert\n\n    @classmethod\n    def bulk_insert(\n        cls,\n        operations: Operations,\n        table: Union[Table, TableClause],\n        rows: List[dict],\n        *,\n        multiinsert: bool = True,\n    ) -> None:\n        \"\"\"Issue a \"bulk insert\" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\n                \"account\",\n                column(\"id\", Integer),\n                column(\"name\", String),\n                column(\"create_date\", Date),\n            )\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": date(2010, 10, 5),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": date(2007, 5, 27),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": date(2008, 8, 15),\n                    },\n                ],\n            )\n\n        When using --sql mode, some datatypes may not render inline\n        automatically, such as dates and other special types.   When this\n        issue is present, :meth:`.Operations.inline_literal` may be used::\n\n            op.bulk_insert(\n                accounts_table,\n                [\n                    {\n                        \"id\": 1,\n                        \"name\": \"John Smith\",\n                        \"create_date\": op.inline_literal(\"2010-10-05\"),\n                    },\n                    {\n                        \"id\": 2,\n                        \"name\": \"Ed Williams\",\n                        \"create_date\": op.inline_literal(\"2007-05-27\"),\n                    },\n                    {\n                        \"id\": 3,\n                        \"name\": \"Wendy Jones\",\n                        \"create_date\": op.inline_literal(\"2008-08-15\"),\n                    },\n                ],\n                multiinsert=False,\n            )\n\n        When using :meth:`.Operations.inline_literal` in conjunction with\n        :meth:`.Operations.bulk_insert`, in order for the statement to work\n        in \"online\" (e.g. non --sql) mode, the\n        :paramref:`~.Operations.bulk_insert.multiinsert`\n        flag should be set to ``False``, which will have the effect of\n        individual INSERT statements being emitted to the database, each\n        with a distinct VALUES clause, so that the \"inline\" values can\n        still be rendered, rather than attempting to pass the values\n        as bound parameters.\n\n        :param table: a table object which represents the target of the INSERT.\n\n        :param rows: a list of dictionaries indicating rows.\n\n        :param multiinsert: when at its default of True and --sql mode is not\n           enabled, the INSERT statement will be executed using\n           \"executemany()\" style, where all elements in the list of\n           dictionaries are passed as bound parameters in a single\n           list.   Setting this to False results in individual INSERT\n           statements being emitted per parameter set, and is needed\n           in those cases where non-literal values are present in the\n           parameter sets.\n\n        \"\"\"\n\n        op = cls(table, rows, multiinsert=multiinsert)\n        operations.invoke(op)\n\n\n@Operations.register_operation(\"execute\")\n@BatchOperations.register_operation(\"execute\", \"batch_execute\")\nclass ExecuteSQLOp(MigrateOperation):\n    \"\"\"Represent an execute SQL operation.\"\"\"\n\n    def __init__(\n        self,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.sqltext = sqltext\n        self.execution_options = execution_options\n\n    @classmethod\n    def execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        r\"\"\"Execute the given SQL using the current migration context.\n\n        The given SQL can be a plain string, e.g.::\n\n            op.execute(\"INSERT INTO table (foo) VALUES ('some value')\")\n\n        Or it can be any kind of Core SQL Expression construct, such as\n        below where we use an update construct::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\"account\", column(\"name\", String))\n            op.execute(\n                account.update()\n                .where(account.c.name == op.inline_literal(\"account 1\"))\n                .values({\"name\": op.inline_literal(\"account 2\")})\n            )\n\n        Above, we made use of the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table` and\n        :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it's a recommended practice to at least\n        ensure the definition of a table is self-contained within the migration\n        script, rather than imported from a module that may break compatibility\n        with older migrations.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in \"offline\" mode.     Additionally, parameterized\n        statements are discouraged here, as they *will not work* in offline\n        mode.  Above, we use :meth:`.inline_literal` where parameters are\n        to be used.\n\n        For full interaction with a connected database where parameters can\n        also be used normally, use the \"bind\" available from the context::\n\n            from alembic import op\n\n            connection = op.get_bind()\n\n            connection.execute(\n                account.update()\n                .where(account.c.name == \"account 1\")\n                .values({\"name\": \"account 2\"})\n            )\n\n        Additionally, when passing the statement as a plain string, it is first\n        coerced into a :func:`sqlalchemy.sql.expression.text` construct\n        before being passed along.  In the less likely case that the\n        literal SQL string contains a colon, it must be escaped with a\n        backslash, as::\n\n           op.execute(r\"INSERT INTO table (foo) VALUES ('\\:colon_value')\")\n\n\n        :param sqltext: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update` construct.\n        * a :func:`sqlalchemy.sql.expression.delete` construct.\n        * Any \"executable\" described in SQLAlchemy Core documentation,\n          noting that no result set is returned.\n\n        .. note::  when passing a plain string, the statement is coerced into\n           a :func:`sqlalchemy.sql.expression.text` construct. This construct\n           considers symbols with colons, e.g. ``:foo`` to be bound parameters.\n           To avoid this, ensure that colon symbols are escaped, e.g.\n           ``\\:foo``.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        \"\"\"\n        op = cls(sqltext, execution_options=execution_options)\n        return operations.invoke(op)\n\n    @classmethod\n    def batch_execute(\n        cls,\n        operations: Operations,\n        sqltext: Union[Executable, str],\n        *,\n        execution_options: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Execute the given SQL using the current migration context.\n\n        .. seealso::\n\n            :meth:`.Operations.execute`\n\n        \"\"\"\n        return cls.execute(\n            operations, sqltext, execution_options=execution_options\n        )\n\n    def to_diff_tuple(self) -> Tuple[str, Union[Executable, str]]:\n        return (\"execute\", self.sqltext)\n\n\nclass OpContainer(MigrateOperation):\n    \"\"\"Represent a sequence of operations operation.\"\"\"\n\n    def __init__(self, ops: Sequence[MigrateOperation] = ()) -> None:\n        self.ops = list(ops)\n\n    def is_empty(self) -> bool:\n        return not self.ops\n\n    def as_diffs(self) -> Any:\n        return list(OpContainer._ops_as_diffs(self))\n\n    @classmethod\n    def _ops_as_diffs(\n        cls, migrations: OpContainer\n    ) -> Iterator[Tuple[Any, ...]]:\n        for op in migrations.ops:\n            if hasattr(op, \"ops\"):\n                yield from cls._ops_as_diffs(cast(\"OpContainer\", op))\n            else:\n                yield op.to_diff_tuple()\n\n\nclass ModifyTableOps(OpContainer):\n    \"\"\"Contains a sequence of operations that all apply to a single Table.\"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        ops: Sequence[MigrateOperation],\n        *,\n        schema: Optional[str] = None,\n    ) -> None:\n        super().__init__(ops)\n        self.table_name = table_name\n        self.schema = schema\n\n    def reverse(self) -> ModifyTableOps:\n        return ModifyTableOps(\n            self.table_name,\n            ops=list(reversed([op.reverse() for op in self.ops])),\n            schema=self.schema,\n        )\n\n\nclass UpgradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'upgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        upgrade_token: str = \"upgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.upgrade_token = upgrade_token\n\n    def reverse_into(self, downgrade_ops: DowngradeOps) -> DowngradeOps:\n        downgrade_ops.ops[:] = list(  # type:ignore[index]\n            reversed([op.reverse() for op in self.ops])\n        )\n        return downgrade_ops\n\n    def reverse(self) -> DowngradeOps:\n        return self.reverse_into(DowngradeOps(ops=[]))\n\n\nclass DowngradeOps(OpContainer):\n    \"\"\"contains a sequence of operations that would apply to the\n    'downgrade' stream of a script.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ops: Sequence[MigrateOperation] = (),\n        downgrade_token: str = \"downgrades\",\n    ) -> None:\n        super().__init__(ops=ops)\n        self.downgrade_token = downgrade_token\n\n    def reverse(self):\n        return UpgradeOps(\n            ops=list(reversed([op.reverse() for op in self.ops]))\n        )\n\n\nclass MigrationScript(MigrateOperation):\n    \"\"\"represents a migration script.\n\n    E.g. when autogenerate encounters this object, this corresponds to the\n    production of an actual script file.\n\n    A normal :class:`.MigrationScript` object would contain a single\n    :class:`.UpgradeOps` and a single :class:`.DowngradeOps` directive.\n    These are accessible via the ``.upgrade_ops`` and ``.downgrade_ops``\n    attributes.\n\n    In the case of an autogenerate operation that runs multiple times,\n    such as the multiple database example in the \"multidb\" template,\n    the ``.upgrade_ops`` and ``.downgrade_ops`` attributes are disabled,\n    and instead these objects should be accessed via the ``.upgrade_ops_list``\n    and ``.downgrade_ops_list`` list-based attributes.  These latter\n    attributes are always available at the very least as single-element lists.\n\n    .. seealso::\n\n        :ref:`customizing_revision`\n\n    \"\"\"\n\n    _needs_render: Optional[bool]\n\n    def __init__(\n        self,\n        rev_id: Optional[str],\n        upgrade_ops: UpgradeOps,\n        downgrade_ops: DowngradeOps,\n        *,\n        message: Optional[str] = None,\n        imports: Set[str] = set(),\n        head: Optional[str] = None,\n        splice: Optional[bool] = None,\n        branch_label: Optional[_RevIdType] = None,\n        version_path: Optional[str] = None,\n        depends_on: Optional[_RevIdType] = None,\n    ) -> None:\n        self.rev_id = rev_id\n        self.message = message\n        self.imports = imports\n        self.head = head\n        self.splice = splice\n        self.branch_label = branch_label\n        self.version_path = version_path\n        self.depends_on = depends_on\n        self.upgrade_ops = upgrade_ops\n        self.downgrade_ops = downgrade_ops\n\n    @property\n    def upgrade_ops(self):\n        \"\"\"An instance of :class:`.UpgradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.upgrade_ops_list`\n        \"\"\"\n        if len(self._upgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for UpgradeOps; please use the \"\n                \"upgrade_ops_list attribute.\"\n            )\n        elif not self._upgrade_ops:\n            return None\n        else:\n            return self._upgrade_ops[0]\n\n    @upgrade_ops.setter\n    def upgrade_ops(self, upgrade_ops):\n        self._upgrade_ops = util.to_list(upgrade_ops)\n        for elem in self._upgrade_ops:\n            assert isinstance(elem, UpgradeOps)\n\n    @property\n    def downgrade_ops(self):\n        \"\"\"An instance of :class:`.DowngradeOps`.\n\n        .. seealso::\n\n            :attr:`.MigrationScript.downgrade_ops_list`\n        \"\"\"\n        if len(self._downgrade_ops) > 1:\n            raise ValueError(\n                \"This MigrationScript instance has a multiple-entry \"\n                \"list for DowngradeOps; please use the \"\n                \"downgrade_ops_list attribute.\"\n            )\n        elif not self._downgrade_ops:\n            return None\n        else:\n            return self._downgrade_ops[0]\n\n    @downgrade_ops.setter\n    def downgrade_ops(self, downgrade_ops):\n        self._downgrade_ops = util.to_list(downgrade_ops)\n        for elem in self._downgrade_ops:\n            assert isinstance(elem, DowngradeOps)\n\n    @property\n    def upgrade_ops_list(self) -> List[UpgradeOps]:\n        \"\"\"A list of :class:`.UpgradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.upgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._upgrade_ops\n\n    @property\n    def downgrade_ops_list(self) -> List[DowngradeOps]:\n        \"\"\"A list of :class:`.DowngradeOps` instances.\n\n        This is used in place of the :attr:`.MigrationScript.downgrade_ops`\n        attribute when dealing with a revision operation that does\n        multiple autogenerate passes.\n\n        \"\"\"\n        return self._downgrade_ops\n\n        ####cross_file_context:\n        [{'alembic.util.sqla_compat': 'from __future__ import annotations\\n\\nimport contextlib\\nimport re\\nfrom typing import Any\\nfrom typing import Dict\\nfrom typing import Iterable\\nfrom typing import Iterator\\nfrom typing import Mapping\\nfrom typing import Optional\\nfrom typing import TYPE_CHECKING\\nfrom typing import TypeVar\\nfrom typing import Union\\n\\nfrom sqlalchemy import __version__\\nfrom sqlalchemy import inspect\\nfrom sqlalchemy import schema\\nfrom sqlalchemy import sql\\nfrom sqlalchemy import types as sqltypes\\nfrom sqlalchemy.engine import url\\nfrom sqlalchemy.ext.compiler import compiles\\nfrom sqlalchemy.schema import CheckConstraint\\nfrom sqlalchemy.schema import Column\\nfrom sqlalchemy.schema import ForeignKeyConstraint\\nfrom sqlalchemy.sql import visitors\\nfrom sqlalchemy.sql.base import DialectKWArgs\\nfrom sqlalchemy.sql.elements import BindParameter\\nfrom sqlalchemy.sql.elements import ColumnClause\\nfrom sqlalchemy.sql.elements import quoted_name\\nfrom sqlalchemy.sql.elements import TextClause\\nfrom sqlalchemy.sql.elements import UnaryExpression\\nfrom sqlalchemy.sql.visitors import traverse\\nfrom typing_extensions import TypeGuard\\n\\nif TYPE_CHECKING:\\n    from sqlalchemy import Index\\n    from sqlalchemy import Table\\n    from sqlalchemy.engine import Connection\\n    from sqlalchemy.engine import Dialect\\n    from sqlalchemy.engine import Transaction\\n    from sqlalchemy.engine.reflection import Inspector\\n    from sqlalchemy.sql.base import ColumnCollection\\n    from sqlalchemy.sql.compiler import SQLCompiler\\n    from sqlalchemy.sql.dml import Insert\\n    from sqlalchemy.sql.elements import ColumnElement\\n    from sqlalchemy.sql.schema import Constraint\\n    from sqlalchemy.sql.schema import SchemaItem\\n    from sqlalchemy.sql.selectable import Select\\n    from sqlalchemy.sql.selectable import TableClause\\n\\n_CE = TypeVar(\"_CE\", bound=Union[\"ColumnElement[Any]\", \"SchemaItem\"])\\n\\n\\ndef _safe_int(value: str) -> Union[int, str]:\\n    try:\\n        return int(value)\\n    except:\\n        return value\\n\\n\\n_vers = tuple(\\n    [_safe_int(x) for x in re.findall(r\"(\\\\d+|[abc]\\\\d)\", __version__)]\\n)\\nsqla_13 = _vers >= (1, 3)\\nsqla_14 = _vers >= (1, 4)\\n# https://docs.sqlalchemy.org/en/latest/changelog/changelog_14.html#change-0c6e0cc67dfe6fac5164720e57ef307d\\nsqla_14_18 = _vers >= (1, 4, 18)\\nsqla_14_26 = _vers >= (1, 4, 26)\\nsqla_2 = _vers >= (2,)\\nsqlalchemy_version = __version__\\n\\ntry:\\n    from sqlalchemy.sql.naming import _NONE_NAME as _NONE_NAME\\nexcept ImportError:\\n    from sqlalchemy.sql.elements import _NONE_NAME as _NONE_NAME  # type: ignore  # noqa: E501\\n\\n\\nclass _Unsupported:\\n    \"Placeholder for unsupported SQLAlchemy classes\"\\n\\n\\ntry:\\n    from sqlalchemy import Computed\\nexcept ImportError:\\n    if not TYPE_CHECKING:\\n\\n        class Computed(_Unsupported):\\n            pass\\n\\n    has_computed = False\\n    has_computed_reflection = False\\nelse:\\n    has_computed = True\\n    has_computed_reflection = _vers >= (1, 3, 16)\\n\\ntry:\\n    from sqlalchemy import Identity\\nexcept ImportError:\\n    if not TYPE_CHECKING:\\n\\n        class Identity(_Unsupported):\\n            pass\\n\\n    has_identity = False\\nelse:\\n    identity_has_dialect_kwargs = issubclass(Identity, DialectKWArgs)\\n\\n    def _get_identity_options_dict(\\n        identity: Union[Identity, schema.Sequence, None],\\n        dialect_kwargs: bool = False,\\n    ) -> Dict[str, Any]:\\n        if identity is None:\\n            return {}\\n        elif identity_has_dialect_kwargs:\\n            as_dict = identity._as_dict()  # type: ignore\\n            if dialect_kwargs:\\n                assert isinstance(identity, DialectKWArgs)\\n                as_dict.update(identity.dialect_kwargs)\\n        else:\\n            as_dict = {}\\n            if isinstance(identity, Identity):\\n                # always=None means something different than always=False\\n                as_dict[\"always\"] = identity.always\\n                if identity.on_null is not None:\\n                    as_dict[\"on_null\"] = identity.on_null\\n            # attributes common to Identity and Sequence\\n            attrs = (\\n                \"start\",\\n                \"increment\",\\n                \"minvalue\",\\n                \"maxvalue\",\\n                \"nominvalue\",\\n                \"nomaxvalue\",\\n                \"cycle\",\\n                \"cache\",\\n                \"order\",\\n            )\\n            as_dict.update(\\n                {\\n                    key: getattr(identity, key, None)\\n                    for key in attrs\\n                    if getattr(identity, key, None) is not None\\n                }\\n            )\\n        return as_dict\\n\\n    has_identity = True\\n\\nif sqla_2:\\n    from sqlalchemy.sql.base import _NoneName\\nelse:\\n    from sqlalchemy.util import symbol as _NoneName  # type: ignore[assignment]\\n\\n\\n_ConstraintName = Union[None, str, _NoneName]\\n\\n_ConstraintNameDefined = Union[str, _NoneName]\\n\\n\\ndef constraint_name_defined(\\n    name: _ConstraintName,\\n) -> TypeGuard[_ConstraintNameDefined]:\\n    return name is _NONE_NAME or isinstance(name, (str, _NoneName))\\n\\n\\ndef constraint_name_string(\\n    name: _ConstraintName,\\n) -> TypeGuard[str]:\\n    return isinstance(name, str)\\n\\n\\ndef constraint_name_or_none(\\n    name: _ConstraintName,\\n) -> Optional[str]:\\n    return name if constraint_name_string(name) else None\\n\\n\\nAUTOINCREMENT_DEFAULT = \"auto\"\\n\\n\\n@contextlib.contextmanager\\ndef _ensure_scope_for_ddl(\\n    connection: Optional[Connection],\\n) -> Iterator[None]:\\n    try:\\n        in_transaction = connection.in_transaction  # type: ignore[union-attr]\\n    except AttributeError:\\n        # catch for MockConnection, None\\n        in_transaction = None\\n        pass\\n\\n    # yield outside the catch\\n    if in_transaction is None:\\n        yield\\n    else:\\n        if not in_transaction():\\n            assert connection is not None\\n            with connection.begin():\\n                yield\\n        else:\\n            yield\\n\\n\\ndef url_render_as_string(url, hide_password=True):\\n    if sqla_14:\\n        return url.render_as_string(hide_password=hide_password)\\n    else:\\n        return url.__to_string__(hide_password=hide_password)\\n\\n\\ndef _safe_begin_connection_transaction(\\n    connection: Connection,\\n) -> Transaction:\\n    transaction = _get_connection_transaction(connection)\\n    if transaction:\\n        return transaction\\n    else:\\n        return connection.begin()\\n\\n\\ndef _safe_commit_connection_transaction(\\n    connection: Connection,\\n) -> None:\\n    transaction = _get_connection_transaction(connection)\\n    if transaction:\\n        transaction.commit()\\n\\n\\ndef _safe_rollback_connection_transaction(\\n    connection: Connection,\\n) -> None:\\n    transaction = _get_connection_transaction(connection)\\n    if transaction:\\n        transaction.rollback()\\n\\n\\ndef _get_connection_in_transaction(connection: Optional[Connection]) -> bool:\\n    try:\\n        in_transaction = connection.in_transaction  # type: ignore\\n    except AttributeError:\\n        # catch for MockConnection\\n        return False\\n    else:\\n        return in_transaction()\\n\\n\\ndef _idx_table_bound_expressions(idx: Index) -> Iterable[ColumnElement[Any]]:\\n    return idx.expressions  # type: ignore\\n\\n\\ndef _copy(schema_item: _CE, **kw) -> _CE:\\n    if hasattr(schema_item, \"_copy\"):\\n        return schema_item._copy(**kw)  # type: ignore[union-attr]\\n    else:\\n        return schema_item.copy(**kw)  # type: ignore[union-attr]\\n\\n\\ndef _get_connection_transaction(\\n    connection: Connection,\\n) -> Optional[Transaction]:\\n    if sqla_14:\\n        return connection.get_transaction()\\n    else:\\n        r = connection._root  # type: ignore[attr-defined]\\n        return r._Connection__transaction\\n\\n\\ndef _create_url(*arg, **kw) -> url.URL:\\n    if hasattr(url.URL, \"create\"):\\n        return url.URL.create(*arg, **kw)\\n    else:\\n        return url.URL(*arg, **kw)\\n\\n\\ndef _connectable_has_table(\\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\\n) -> bool:\\n    if sqla_14:\\n        return inspect(connectable).has_table(tablename, schemaname)\\n    else:\\n        return connectable.dialect.has_table(\\n            connectable, tablename, schemaname\\n        )\\n\\n\\ndef _exec_on_inspector(inspector, statement, **params):\\n    if sqla_14:\\n        with inspector._operation_context() as conn:\\n            return conn.execute(statement, params)\\n    else:\\n        return inspector.bind.execute(statement, params)\\n\\n\\ndef _nullability_might_be_unset(metadata_column):\\n    if not sqla_14:\\n        return metadata_column.nullable\\n    else:\\n        from sqlalchemy.sql import schema\\n\\n        return (\\n            metadata_column._user_defined_nullable is schema.NULL_UNSPECIFIED\\n        )\\n\\n\\ndef _server_default_is_computed(*server_default) -> bool:\\n    if not has_computed:\\n        return False\\n    else:\\n        return any(isinstance(sd, Computed) for sd in server_default)\\n\\n\\ndef _server_default_is_identity(*server_default) -> bool:\\n    if not sqla_14:\\n        return False\\n    else:\\n        return any(isinstance(sd, Identity) for sd in server_default)\\n\\n\\ndef _table_for_constraint(constraint: Constraint) -> Table:\\n    if isinstance(constraint, ForeignKeyConstraint):\\n        table = constraint.parent\\n        assert table is not None\\n        return table  # type: ignore[return-value]\\n    else:\\n        return constraint.table\\n\\n\\ndef _columns_for_constraint(constraint):\\n    if isinstance(constraint, ForeignKeyConstraint):\\n        return [fk.parent for fk in constraint.elements]\\n    elif isinstance(constraint, CheckConstraint):\\n        return _find_columns(constraint.sqltext)\\n    else:\\n        return list(constraint.columns)\\n\\n\\ndef _reflect_table(inspector: Inspector, table: Table) -> None:\\n    if sqla_14:\\n        return inspector.reflect_table(table, None)\\n    else:\\n        return inspector.reflecttable(  # type: ignore[attr-defined]\\n            table, None\\n        )\\n\\n\\ndef _resolve_for_variant(type_, dialect):\\n    if _type_has_variants(type_):\\n        base_type, mapping = _get_variant_mapping(type_)\\n        return mapping.get(dialect.name, base_type)\\n    else:\\n        return type_\\n\\n\\nif hasattr(sqltypes.TypeEngine, \"_variant_mapping\"):\\n\\n    def _type_has_variants(type_):\\n        return bool(type_._variant_mapping)\\n\\n    def _get_variant_mapping(type_):\\n        return type_, type_._variant_mapping\\n\\nelse:\\n\\n    def _type_has_variants(type_):\\n        return type(type_) is sqltypes.Variant\\n\\n    def _get_variant_mapping(type_):\\n        return type_.impl, type_.mapping\\n\\n\\ndef _fk_spec(constraint):\\n    source_columns = [\\n        constraint.columns[key].name for key in constraint.column_keys\\n    ]\\n\\n    source_table = constraint.parent.name\\n    source_schema = constraint.parent.schema\\n    target_schema = constraint.elements[0].column.table.schema\\n    target_table = constraint.elements[0].column.table.name\\n    target_columns = [element.column.name for element in constraint.elements]\\n    ondelete = constraint.ondelete\\n    onupdate = constraint.onupdate\\n    deferrable = constraint.deferrable\\n    initially = constraint.initially\\n    return (\\n        source_schema,\\n        source_table,\\n        source_columns,\\n        target_schema,\\n        target_table,\\n        target_columns,\\n        onupdate,\\n        ondelete,\\n        deferrable,\\n        initially,\\n    )\\n\\n\\ndef _fk_is_self_referential(constraint: ForeignKeyConstraint) -> bool:\\n    spec = constraint.elements[0]._get_colspec()  # type: ignore[attr-defined]\\n    tokens = spec.split(\".\")\\n    tokens.pop(-1)  # colname\\n    tablekey = \".\".join(tokens)\\n    assert constraint.parent is not None\\n    return tablekey == constraint.parent.key\\n\\n\\ndef _is_type_bound(constraint: Constraint) -> bool:\\n    # this deals with SQLAlchemy #3260, don\\'t copy CHECK constraints\\n    # that will be generated by the type.\\n    # new feature added for #3260\\n    return constraint._type_bound  # type: ignore[attr-defined]\\n\\n\\ndef _find_columns(clause):\\n    \"\"\"locate Column objects within the given expression.\"\"\"\\n\\n    cols = set()\\n    traverse(clause, {}, {\"column\": cols.add})\\n    return cols\\n\\n\\ndef _remove_column_from_collection(\\n    collection: ColumnCollection, column: Union[Column[Any], ColumnClause[Any]]\\n) -> None:\\n    \"\"\"remove a column from a ColumnCollection.\"\"\"\\n\\n    # workaround for older SQLAlchemy, remove the\\n    # same object that\\'s present\\n    assert column.key is not None\\n    to_remove = collection[column.key]\\n\\n    # SQLAlchemy 2.0 will use more ReadOnlyColumnCollection\\n    # (renamed from ImmutableColumnCollection)\\n    if hasattr(collection, \"_immutable\") or hasattr(collection, \"_readonly\"):\\n        collection._parent.remove(to_remove)\\n    else:\\n        collection.remove(to_remove)\\n\\n\\ndef _textual_index_column(\\n    table: Table, text_: Union[str, TextClause, ColumnElement[Any]]\\n) -> Union[ColumnElement[Any], Column[Any]]:\\n    \"\"\"a workaround for the Index construct\\'s severe lack of flexibility\"\"\"\\n    if isinstance(text_, str):\\n        c = Column(text_, sqltypes.NULLTYPE)\\n        table.append_column(c)\\n        return c\\n    elif isinstance(text_, TextClause):\\n        return _textual_index_element(table, text_)\\n    elif isinstance(text_, _textual_index_element):\\n        return _textual_index_column(table, text_.text)\\n    elif isinstance(text_, sql.ColumnElement):\\n        return _copy_expression(text_, table)\\n    else:\\n        raise ValueError(\"String or text() construct expected\")\\n\\n\\ndef _copy_expression(expression: _CE, target_table: Table) -> _CE:\\n    def replace(col):\\n        if (\\n            isinstance(col, Column)\\n            and col.table is not None\\n            and col.table is not target_table\\n        ):\\n            if col.name in target_table.c:\\n                return target_table.c[col.name]\\n            else:\\n                c = _copy(col)\\n                target_table.append_column(c)\\n                return c\\n        else:\\n            return None\\n\\n    return visitors.replacement_traverse(  # type: ignore[call-overload]\\n        expression, {}, replace\\n    )\\n\\n\\nclass _textual_index_element(sql.ColumnElement):\\n    \"\"\"Wrap around a sqlalchemy text() construct in such a way that\\n    we appear like a column-oriented SQL expression to an Index\\n    construct.\\n\\n    The issue here is that currently the Postgresql dialect, the biggest\\n    recipient of functional indexes, keys all the index expressions to\\n    the corresponding column expressions when rendering CREATE INDEX,\\n    so the Index we create here needs to have a .columns collection that\\n    is the same length as the .expressions collection.  Ultimately\\n    SQLAlchemy should support text() expressions in indexes.\\n\\n    See SQLAlchemy issue 3174.\\n\\n    \"\"\"\\n\\n    __visit_name__ = \"_textual_idx_element\"\\n\\n    def __init__(self, table: Table, text: TextClause) -> None:\\n        self.table = table\\n        self.text = text\\n        self.key = text.text\\n        self.fake_column = schema.Column(self.text.text, sqltypes.NULLTYPE)\\n        table.append_column(self.fake_column)\\n\\n    def get_children(self):\\n        return [self.fake_column]\\n\\n\\n@compiles(_textual_index_element)\\ndef _render_textual_index_column(\\n    element: _textual_index_element, compiler: SQLCompiler, **kw\\n) -> str:\\n    return compiler.process(element.text, **kw)\\n\\n\\nclass _literal_bindparam(BindParameter):\\n    pass\\n\\n\\n@compiles(_literal_bindparam)\\ndef _render_literal_bindparam(\\n    element: _literal_bindparam, compiler: SQLCompiler, **kw\\n) -> str:\\n    return compiler.render_literal_bindparam(element, **kw)\\n\\n\\ndef _get_index_expressions(idx):\\n    return list(idx.expressions)\\n\\n\\ndef _get_index_column_names(idx):\\n    return [getattr(exp, \"name\", None) for exp in _get_index_expressions(idx)]\\n\\n\\ndef _column_kwargs(col: Column) -> Mapping:\\n    if sqla_13:\\n        return col.kwargs\\n    else:\\n        return {}\\n\\n\\ndef _get_constraint_final_name(\\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\\n) -> Optional[str]:\\n    if constraint.name is None:\\n        return None\\n    assert dialect is not None\\n    if sqla_14:\\n        # for SQLAlchemy 1.4 we would like to have the option to expand\\n        # the use of \"deferred\" names for constraints as well as to have\\n        # some flexibility with \"None\" name and similar; make use of new\\n        # SQLAlchemy API to return what would be the final compiled form of\\n        # the name for this dialect.\\n        return dialect.identifier_preparer.format_constraint(\\n            constraint, _alembic_quote=False\\n        )\\n    else:\\n        # prior to SQLAlchemy 1.4, work around quoting logic to get at the\\n        # final compiled name without quotes.\\n        if hasattr(constraint.name, \"quote\"):\\n            # might be quoted_name, might be truncated_name, keep it the\\n            # same\\n            quoted_name_cls: type = type(constraint.name)\\n        else:\\n            quoted_name_cls = quoted_name\\n\\n        new_name = quoted_name_cls(str(constraint.name), quote=False)\\n        constraint = constraint.__class__(name=new_name)\\n\\n        if isinstance(constraint, schema.Index):\\n            # name should not be quoted.\\n            d = dialect.ddl_compiler(dialect, None)  # type: ignore[arg-type]\\n            return d._prepared_index_name(  # type: ignore[attr-defined]\\n                constraint\\n            )\\n        else:\\n            # name should not be quoted.\\n            return dialect.identifier_preparer.format_constraint(constraint)\\n\\n\\ndef _constraint_is_named(\\n    constraint: Union[Constraint, Index], dialect: Optional[Dialect]\\n) -> bool:\\n    if sqla_14:\\n        if constraint.name is None:\\n            return False\\n        assert dialect is not None\\n        name = dialect.identifier_preparer.format_constraint(\\n            constraint, _alembic_quote=False\\n        )\\n        return name is not None\\n    else:\\n        return constraint.name is not None\\n\\n\\ndef _is_mariadb(mysql_dialect: Dialect) -> bool:\\n    if sqla_14:\\n        return mysql_dialect.is_mariadb  # type: ignore[attr-defined]\\n    else:\\n        return bool(\\n            mysql_dialect.server_version_info\\n            and mysql_dialect._is_mariadb  # type: ignore[attr-defined]\\n        )\\n\\n\\ndef _mariadb_normalized_version_info(mysql_dialect):\\n    return mysql_dialect._mariadb_normalized_version_info\\n\\n\\ndef _insert_inline(table: Union[TableClause, Table]) -> Insert:\\n    if sqla_14:\\n        return table.insert().inline()\\n    else:\\n        return table.insert(inline=True)  # type: ignore[call-arg]\\n\\n\\nif sqla_14:\\n    from sqlalchemy import create_mock_engine\\n    from sqlalchemy import select as _select\\nelse:\\n    from sqlalchemy import create_engine\\n\\n    def create_mock_engine(url, executor, **kw):  # type: ignore[misc]\\n        return create_engine(\\n            \"postgresql://\", strategy=\"mock\", executor=executor\\n        )\\n\\n    def _select(*columns, **kw) -> Select:  # type: ignore[no-redef]\\n        return sql.select(list(columns), **kw)  # type: ignore[call-overload]\\n\\n\\ndef is_expression_index(index: Index) -> bool:\\n    expr: Any\\n    for expr in index.expressions:\\n        while isinstance(expr, UnaryExpression):\\n            expr = expr.element\\n        if not isinstance(expr, ColumnClause) or expr.is_literal:\\n            return True\\n    return False\\n'}, {'alembic.util.sqla_compat._table_for_constraint': 'from __future__ import annotations\\n\\nimport contextlib\\nimport re\\nfrom typing import Any\\nfrom typing import Dict\\nfrom typing import Iterable\\nfrom typing import Iterator\\nfrom typing import Mapping\\nfrom typing import Optional\\nfrom typing import TYPE_CHECKING\\nfrom typing import TypeVar\\nfrom typing import Union\\n\\nfrom sqlalchemy import __version__\\nfrom sqlalchemy import inspect\\nfrom sqlalchemy import schema\\nfrom sqlalchemy import sql\\nfrom sqlalchemy import types as sqltypes\\nfrom sqlalchemy.engine import url\\nfrom sqlalchemy.ext.compiler import compiles\\nfrom sqlalchemy.schema import CheckConstraint\\nfrom sqlalchemy.schema import Column\\nfrom sqlalchemy.schema import ForeignKeyConstraint\\nfrom sqlalchemy.sql import visitors\\nfrom sqlalchemy.sql.base import DialectKWArgs\\nfrom sqlalchemy.sql.elements import BindParameter\\nfrom sqlalchemy.sql.elements import ColumnClause\\nfrom sqlalchemy.sql.elements import quoted_name\\nfrom sqlalchemy.sql.elements import TextClause\\nfrom sqlalchemy.sql.elements import UnaryExpression\\nfrom sqlalchemy.sql.visitors import traverse\\nfrom typing_extensions import TypeGuard\\n\\nif TYPE_CHECKING:\\n    from sqlalchemy import Index\\n    from sqlalchemy import Table\\n    from sqlalchemy.engine import Connection\\n    from sqlalchemy.engine import Dialect\\n    from sqlalchemy.engine import Transaction\\n    from sqlalchemy.engine.reflection import Inspector\\n    from sqlalchemy.sql.base import ColumnCollection\\n    from sqlalchemy.sql.compiler import SQLCompiler\\n    from sqlalchemy.sql.dml import Insert\\n    from sqlalchemy.sql.elements import ColumnElement\\n    from sqlalchemy.sql.schema import Constraint\\n    from sqlalchemy.sql.schema import SchemaItem\\n    from sqlalchemy.sql.selectable import Select\\n    from sqlalchemy.sql.selectable import TableClause\\n\\n_CE = TypeVar(\"_CE\", bound=Union[\"ColumnElement[Any]\", \"SchemaItem\"])\\n\\n\\ndef _safe_int(value: str) -> Union[int, str]:\\n    try:\\n        return int(value)\\n    except:\\n        return value\\n\\n\\n_vers = tuple(\\n    [_safe_int(x) for x in re.findall(r\"(\\\\d+|[abc]\\\\d)\", __version__)]\\n)\\nsqla_13 = _vers >= (1, 3)\\nsqla_14 = _vers >= (1, 4)\\n# https://docs.sqlalchemy.org/en/latest/changelog/changelog_14.html#change-0c6e0cc67dfe6fac5164720e57ef307d\\nsqla_14_18 = _vers >= (1, 4, 18)\\nsqla_14_26 = _vers >= (1, 4, 26)\\nsqla_2 = _vers >= (2,)\\nsqlalchemy_version = __version__\\n\\ntry:\\n    from sqlalchemy.sql.naming import _NONE_NAME as _NONE_NAME\\nexcept ImportError:\\n    from sqlalchemy.sql.elements import _NONE_NAME as _NONE_NAME  # type: ignore  # noqa: E501\\n\\n\\nclass _Unsupported:\\n    \"Placeholder for unsupported SQLAlchemy classes\"\\n\\n\\ntry:\\n    from sqlalchemy import Computed\\nexcept ImportError:\\n    if not TYPE_CHECKING:\\n\\n        class Computed(_Unsupported):\\n            pass\\n\\n    has_computed = False\\n    has_computed_reflection = False\\nelse:\\n    has_computed = True\\n    has_computed_reflection = _vers >= (1, 3, 16)\\n\\ntry:\\n    from sqlalchemy import Identity\\nexcept ImportError:\\n    if not TYPE_CHECKING:\\n\\n        class Identity(_Unsupported):\\n            pass\\n\\n    has_identity = False\\nelse:\\n    identity_has_dialect_kwargs = issubclass(Identity, DialectKWArgs)\\n\\n    def _get_identity_options_dict(\\n        identity: Union[Identity, schema.Sequence, None],\\n        dialect_kwargs: bool = False,\\n    ) -> Dict[str, Any]:\\n        if identity is None:\\n            return {}\\n        elif identity_has_dialect_kwargs:\\n            as_dict = identity._as_dict()  # type: ignore\\n            if dialect_kwargs:\\n                assert isinstance(identity, DialectKWArgs)\\n                as_dict.update(identity.dialect_kwargs)\\n        else:\\n            as_dict = {}\\n            if isinstance(identity, Identity):\\n                # always=None means something different than always=False\\n                as_dict[\"always\"] = identity.always\\n                if identity.on_null is not None:\\n                    as_dict[\"on_null\"] = identity.on_null\\n            # attributes common to Identity and Sequence\\n            attrs = (\\n                \"start\",\\n                \"increment\",\\n                \"minvalue\",\\n                \"maxvalue\",\\n                \"nominvalue\",\\n                \"nomaxvalue\",\\n                \"cycle\",\\n                \"cache\",\\n                \"order\",\\n            )\\n            as_dict.update(\\n                {\\n                    key: getattr(identity, key, None)\\n                    for key in attrs\\n                    if getattr(identity, key, None) is not None\\n                }\\n            )\\n        return as_dict\\n\\n    has_identity = True\\n\\nif sqla_2:\\n    from sqlalchemy.sql.base import _NoneName\\nelse:\\n    from sqlalchemy.util import symbol as _NoneName  # type: ignore[assignment]\\n\\n\\n_ConstraintName = Union[None, str, _NoneName]\\n\\n_ConstraintNameDefined = Union[str, _NoneName]\\n\\n\\ndef constraint_name_defined(\\n    name: _ConstraintName,\\n) -> TypeGuard[_ConstraintNameDefined]:\\n    return name is _NONE_NAME or isinstance(name, (str, _NoneName))\\n\\n\\ndef constraint_name_string(\\n    name: _ConstraintName,\\n) -> TypeGuard[str]:\\n    return isinstance(name, str)\\n\\n\\ndef constraint_name_or_none(\\n    name: _ConstraintName,\\n) -> Optional[str]:\\n    return name if constraint_name_string(name) else None\\n\\n\\nAUTOINCREMENT_DEFAULT = \"auto\"\\n\\n\\n@contextlib.contextmanager\\ndef _ensure_scope_for_ddl(\\n    connection: Optional[Connection],\\n) -> Iterator[None]:\\n    try:\\n        in_transaction = connection.in_transaction  # type: ignore[union-attr]\\n    except AttributeError:\\n        # catch for MockConnection, None\\n        in_transaction = None\\n        pass\\n\\n    # yield outside the catch\\n    if in_transaction is None:\\n        yield\\n    else:\\n        if not in_transaction():\\n            assert connection is not None\\n            with connection.begin():\\n                yield\\n        else:\\n            yield\\n\\n\\ndef url_render_as_string(url, hide_password=True):\\n    if sqla_14:\\n        return url.render_as_string(hide_password=hide_password)\\n    else:\\n        return url.__to_string__(hide_password=hide_password)\\n\\n\\ndef _safe_begin_connection_transaction(\\n    connection: Connection,\\n) -> Transaction:\\n    transaction = _get_connection_transaction(connection)\\n    if transaction:\\n        return transaction\\n    else:\\n        return connection.begin()\\n\\n\\ndef _safe_commit_connection_transaction(\\n    connection: Connection,\\n) -> None:\\n    transaction = _get_connection_transaction(connection)\\n    if transaction:\\n        transaction.commit()\\n\\n\\ndef _safe_rollback_connection_transaction(\\n    connection: Connection,\\n) -> None:\\n    transaction = _get_connection_transaction(connection)\\n    if transaction:\\n        transaction.rollback()\\n\\n\\ndef _get_connection_in_transaction(connection: Optional[Connection]) -> bool:\\n    try:\\n        in_transaction = connection.in_transaction  # type: ignore\\n    except AttributeError:\\n        # catch for MockConnection\\n        return False\\n    else:\\n        return in_transaction()\\n\\n\\ndef _idx_table_bound_expressions(idx: Index) -> Iterable[ColumnElement[Any]]:\\n    return idx.expressions  # type: ignore\\n\\n\\ndef _copy(schema_item: _CE, **kw) -> _CE:\\n    if hasattr(schema_item, \"_copy\"):\\n        return schema_item._copy(**kw)  # type: ignore[union-attr]\\n    else:\\n        return schema_item.copy(**kw)  # type: ignore[union-attr]\\n\\n\\ndef _get_connection_transaction(\\n    connection: Connection,\\n) -> Optional[Transaction]:\\n    if sqla_14:\\n        return connection.get_transaction()\\n    else:\\n        r = connection._root  # type: ignore[attr-defined]\\n        return r._Connection__transaction\\n\\n\\ndef _create_url(*arg, **kw) -> url.URL:\\n    if hasattr(url.URL, \"create\"):\\n        return url.URL.create(*arg, **kw)\\n    else:\\n        return url.URL(*arg, **kw)\\n\\n\\ndef _connectable_has_table(\\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\\n) -> bool:\\n    if sqla_14:\\n        return inspect(connectable).has_table(tablename, schemaname)\\n    else:\\n        return connectable.dialect.has_table(\\n            connectable, tablename, schemaname\\n        )\\n\\n\\ndef _exec_on_inspector(inspector, statement, **params):\\n    if sqla_14:\\n        with inspector._operation_context() as conn:\\n            return conn.execute(statement, params)\\n    else:\\n        return inspector.bind.execute(statement, params)\\n\\n\\ndef _nullability_might_be_unset(metadata_column):\\n    if not sqla_14:\\n        return metadata_column.nullable\\n    else:\\n        from sqlalchemy.sql import schema\\n\\n        return (\\n            metadata_column._user_defined_nullable is schema.NULL_UNSPECIFIED\\n        )\\n\\n\\ndef _server_default_is_computed(*server_default) -> bool:\\n    if not has_computed:\\n        return False\\n    else:\\n        return any(isinstance(sd, Computed) for sd in server_default)\\n\\n\\ndef _server_default_is_identity(*server_default) -> bool:\\n    if not sqla_14:\\n        return False\\n    else:\\n        return any(isinstance(sd, Identity) for sd in server_default)\\n\\n\\ndef _table_for_constraint(constraint: Constraint) -> Table:\\n    if isinstance(constraint, ForeignKeyConstraint):\\n        table = constraint.parent\\n        assert table is not None\\n        return table  # type: ignore[return-value]\\n    else:\\n        return constraint.table\\n\\n\\ndef _columns_for_constraint(constraint):\\n    if isinstance(constraint, ForeignKeyConstraint):\\n        return [fk.parent for fk in constraint.elements]\\n    elif isinstance(constraint, CheckConstraint):\\n        return _find_columns(constraint.sqltext)\\n    else:\\n        return list(constraint.columns)\\n\\n\\ndef _reflect_table(inspector: Inspector, table: Table) -> None:\\n    if sqla_14:\\n        return inspector.reflect_table(table, None)\\n    else:\\n        return inspector.reflecttable(  # type: ignore[attr-defined]\\n            table, None\\n        )\\n\\n\\ndef _resolve_for_variant(type_, dialect):\\n    if _type_has_variants(type_):\\n        base_type, mapping = _get_variant_mapping(type_)\\n        return mapping.get(dialect.name, base_type)\\n    else:\\n        return type_\\n\\n\\nif hasattr(sqltypes.TypeEngine, \"_variant_mapping\"):\\n\\n    def _type_has_variants(type_):\\n        return bool(type_._variant_mapping)\\n\\n    def _get_variant_mapping(type_):\\n        return type_, type_._variant_mapping\\n\\nelse:\\n\\n    def _type_has_variants(type_):\\n        return type(type_) is sqltypes.Variant\\n\\n    def _get_variant_mapping(type_):\\n        return type_.impl, type_.mapping\\n\\n\\ndef _fk_spec(constraint):\\n    source_columns = [\\n        constraint.columns[key].name for key in constraint.column_keys\\n    ]\\n\\n    source_table = constraint.parent.name\\n    source_schema = constraint.parent.schema\\n    target_schema = constraint.elements[0].column.table.schema\\n    target_table = constraint.elements[0].column.table.name\\n    target_columns = [element.column.name for element in constraint.elements]\\n    ondelete = constraint.ondelete\\n    onupdate = constraint.onupdate\\n    deferrable = constraint.deferrable\\n    initially = constraint.initially\\n    return (\\n        source_schema,\\n        source_table,\\n        source_columns,\\n        target_schema,\\n        target_table,\\n        target_columns,\\n        onupdate,\\n        ondelete,\\n        deferrable,\\n        initially,\\n    )\\n\\n\\ndef _fk_is_self_referential(constraint: ForeignKeyConstraint) -> bool:\\n    spec = constraint.elements[0]._get_colspec()  # type: ignore[attr-defined]\\n    tokens = spec.split(\".\")\\n    tokens.pop(-1)  # colname\\n    tablekey = \".\".join(tokens)\\n    assert constraint.parent is not None\\n    return tablekey == constraint.parent.key\\n\\n\\ndef _is_type_bound(constraint: Constraint) -> bool:\\n    # this deals with SQLAlchemy #3260, don\\'t copy CHECK constraints\\n    # that will be generated by the type.\\n    # new feature added for #3260\\n    return constraint._type_bound  # type: ignore[attr-defined]\\n\\n\\ndef _find_columns(clause):\\n    \"\"\"locate Column objects within the given expression.\"\"\"\\n\\n    cols = set()\\n    traverse(clause, {}, {\"column\": cols.add})\\n    return cols\\n\\n\\ndef _remove_column_from_collection(\\n    collection: ColumnCollection, column: Union[Column[Any], ColumnClause[Any]]\\n) -> None:\\n    \"\"\"remove a column from a ColumnCollection.\"\"\"\\n\\n    # workaround for older SQLAlchemy, remove the\\n    # same object that\\'s present\\n    assert column.key is not None\\n    to_remove = collection[column.key]\\n\\n    # SQLAlchemy 2.0 will use more ReadOnlyColumnCollection\\n    # (renamed from ImmutableColumnCollection)\\n    if hasattr(collection, \"_immutable\") or hasattr(collection, \"_readonly\"):\\n        collection._parent.remove(to_remove)\\n    else:\\n        collection.remove(to_remove)\\n\\n\\ndef _textual_index_column(\\n    table: Table, text_: Union[str, TextClause, ColumnElement[Any]]\\n) -> Union[ColumnElement[Any], Column[Any]]:\\n    \"\"\"a workaround for the Index construct\\'s severe lack of flexibility\"\"\"\\n    if isinstance(text_, str):\\n        c = Column(text_, sqltypes.NULLTYPE)\\n        table.append_column(c)\\n        return c\\n    elif isinstance(text_, TextClause):\\n        return _textual_index_element(table, text_)\\n    elif isinstance(text_, _textual_index_element):\\n        return _textual_index_column(table, text_.text)\\n    elif isinstance(text_, sql.ColumnElement):\\n        return _copy_expression(text_, table)\\n    else:\\n        raise ValueError(\"String or text() construct expected\")\\n\\n\\ndef _copy_expression(expression: _CE, target_table: Table) -> _CE:\\n    def replace(col):\\n        if (\\n            isinstance(col, Column)\\n            and col.table is not None\\n            and col.table is not target_table\\n        ):\\n            if col.name in target_table.c:\\n                return target_table.c[col.name]\\n            else:\\n                c = _copy(col)\\n                target_table.append_column(c)\\n                return c\\n        else:\\n            return None\\n\\n    return visitors.replacement_traverse(  # type: ignore[call-overload]\\n        expression, {}, replace\\n    )\\n\\n\\nclass _textual_index_element(sql.ColumnElement):\\n    \"\"\"Wrap around a sqlalchemy text() construct in such a way that\\n    we appear like a column-oriented SQL expression to an Index\\n    construct.\\n\\n    The issue here is that currently the Postgresql dialect, the biggest\\n    recipient of functional indexes, keys all the index expressions to\\n    the corresponding column expressions when rendering CREATE INDEX,\\n    so the Index we create here needs to have a .columns collection that\\n    is the same length as the .expressions collection.  Ultimately\\n    SQLAlchemy should support text() expressions in indexes.\\n\\n    See SQLAlchemy issue 3174.\\n\\n    \"\"\"\\n\\n    __visit_name__ = \"_textual_idx_element\"\\n\\n    def __init__(self, table: Table, text: TextClause) -> None:\\n        self.table = table\\n        self.text = text\\n        self.key = text.text\\n        self.fake_column = schema.Column(self.text.text, sqltypes.NULLTYPE)\\n        table.append_column(self.fake_column)\\n\\n    def get_children(self):\\n        return [self.fake_column]\\n\\n\\n@compiles(_textual_index_element)\\ndef _render_textual_index_column(\\n    element: _textual_index_element, compiler: SQLCompiler, **kw\\n) -> str:\\n    return compiler.process(element.text, **kw)\\n\\n\\nclass _literal_bindparam(BindParameter):\\n    pass\\n\\n\\n@compiles(_literal_bindparam)\\ndef _render_literal_bindparam(\\n    element: _literal_bindparam, compiler: SQLCompiler, **kw\\n) -> str:\\n    return compiler.render_literal_bindparam(element, **kw)\\n\\n\\ndef _get_index_expressions(idx):\\n    return list(idx.expressions)\\n\\n\\ndef _get_index_column_names(idx):\\n    return [getattr(exp, \"name\", None) for exp in _get_index_expressions(idx)]\\n\\n\\ndef _column_kwargs(col: Column) -> Mapping:\\n    if sqla_13:\\n        return col.kwargs\\n    else:\\n        return {}\\n\\n\\ndef _get_constraint_final_name(\\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\\n) -> Optional[str]:\\n    if constraint.name is None:\\n        return None\\n    assert dialect is not None\\n    if sqla_14:\\n        # for SQLAlchemy 1.4 we would like to have the option to expand\\n        # the use of \"deferred\" names for constraints as well as to have\\n        # some flexibility with \"None\" name and similar; make use of new\\n        # SQLAlchemy API to return what would be the final compiled form of\\n        # the name for this dialect.\\n        return dialect.identifier_preparer.format_constraint(\\n            constraint, _alembic_quote=False\\n        )\\n    else:\\n        # prior to SQLAlchemy 1.4, work around quoting logic to get at the\\n        # final compiled name without quotes.\\n        if hasattr(constraint.name, \"quote\"):\\n            # might be quoted_name, might be truncated_name, keep it the\\n            # same\\n            quoted_name_cls: type = type(constraint.name)\\n        else:\\n            quoted_name_cls = quoted_name\\n\\n        new_name = quoted_name_cls(str(constraint.name), quote=False)\\n        constraint = constraint.__class__(name=new_name)\\n\\n        if isinstance(constraint, schema.Index):\\n            # name should not be quoted.\\n            d = dialect.ddl_compiler(dialect, None)  # type: ignore[arg-type]\\n            return d._prepared_index_name(  # type: ignore[attr-defined]\\n                constraint\\n            )\\n        else:\\n            # name should not be quoted.\\n            return dialect.identifier_preparer.format_constraint(constraint)\\n\\n\\ndef _constraint_is_named(\\n    constraint: Union[Constraint, Index], dialect: Optional[Dialect]\\n) -> bool:\\n    if sqla_14:\\n        if constraint.name is None:\\n            return False\\n        assert dialect is not None\\n        name = dialect.identifier_preparer.format_constraint(\\n            constraint, _alembic_quote=False\\n        )\\n        return name is not None\\n    else:\\n        return constraint.name is not None\\n\\n\\ndef _is_mariadb(mysql_dialect: Dialect) -> bool:\\n    if sqla_14:\\n        return mysql_dialect.is_mariadb  # type: ignore[attr-defined]\\n    else:\\n        return bool(\\n            mysql_dialect.server_version_info\\n            and mysql_dialect._is_mariadb  # type: ignore[attr-defined]\\n        )\\n\\n\\ndef _mariadb_normalized_version_info(mysql_dialect):\\n    return mysql_dialect._mariadb_normalized_version_info\\n\\n\\ndef _insert_inline(table: Union[TableClause, Table]) -> Insert:\\n    if sqla_14:\\n        return table.insert().inline()\\n    else:\\n        return table.insert(inline=True)  # type: ignore[call-arg]\\n\\n\\nif sqla_14:\\n    from sqlalchemy import create_mock_engine\\n    from sqlalchemy import select as _select\\nelse:\\n    from sqlalchemy import create_engine\\n\\n    def create_mock_engine(url, executor, **kw):  # type: ignore[misc]\\n        return create_engine(\\n            \"postgresql://\", strategy=\"mock\", executor=executor\\n        )\\n\\n    def _select(*columns, **kw) -> Select:  # type: ignore[no-redef]\\n        return sql.select(list(columns), **kw)  # type: ignore[call-overload]\\n\\n\\ndef is_expression_index(index: Index) -> bool:\\n    expr: Any\\n    for expr in index.expressions:\\n        while isinstance(expr, UnaryExpression):\\n            expr = expr.element\\n        if not isinstance(expr, ColumnClause) or expr.is_literal:\\n            return True\\n    return False\\n'}]", "test_list": ["def test_drop_check(self):\n    ck = self.ck\n    op = ops.DropConstraintOp.from_constraint(ck)\n    eq_(op.to_constraint(), schemacompare.CompareCheckConstraint(ck))\n    eq_(op.reverse().to_constraint(), schemacompare.CompareCheckConstraint(ck))\n    is_not_(None, op.to_constraint().table)", "def test_drop_unique(self):\n    uq = self.uq\n    op = ops.DropConstraintOp.from_constraint(uq)\n    eq_(op.to_constraint(), schemacompare.CompareUniqueConstraint(uq))\n    eq_(op.reverse().to_constraint(), schemacompare.CompareUniqueConstraint(uq))\n    is_not_(None, op.to_constraint().table)", "def test_create_unique_constraint_add_kw(self):\n    schema_obj = schemaobj.SchemaObjects()\n    const = schema_obj.unique_constraint('x', 'foobar', ['a'])\n    op = ops.AddConstraintOp.from_constraint(const)\n    is_not_(op.to_constraint(), const)\n    op.kw['sqlite_on_conflict'] = 'IGNORE'\n    eq_(op.to_constraint().dialect_kwargs['sqlite_on_conflict'], 'IGNORE')\n    eq_(op.reverse().to_constraint().dialect_kwargs['sqlite_on_conflict'], 'IGNORE')", "def test_drop_unique_constraint_change_name(self):\n    schema_obj = schemaobj.SchemaObjects()\n    const = schema_obj.unique_constraint('x', 'foobar', ['a'])\n    op = ops.DropConstraintOp.from_constraint(const)\n    op.constraint_name = 'my_name'\n    eq_(op.to_constraint().name, 'my_name')\n    eq_(op.reverse().to_constraint().name, 'my_name')", "def test_add_unique(self):\n    uq = self.uq\n    op = ops.AddConstraintOp.from_constraint(uq)\n    eq_(op.to_constraint(), schemacompare.CompareUniqueConstraint(uq))\n    eq_(op.reverse().to_constraint(), schemacompare.CompareUniqueConstraint(uq))\n    is_not_(None, op.to_constraint().table)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'to_constraint' function should return a valid Constraint instance when a valid DropConstraintOp instance with a reverse operation is provided.", "unit_test": ["def test_to_constraint_valid_reverse_operation():\n    drop_op = DropConstraintOp(constraint_name='test_constraint', table_name='test_table', _reverse=AddConstraintOp.from_constraint(UniqueConstraint('test_constraint', 'test_table')))\n    constraint = drop_op.to_constraint()\n    assert isinstance(constraint, Constraint)\n    assert constraint.name == 'test_constraint'\n    assert constraint.table.name == 'test_table'"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_valid_reverse_operation"}, "Exception Handling": {"requirement": "The 'to_constraint' function should raise a ValueError with a descriptive error message: 'constraint cannot be produced; original constraint is not present' when the reverse operation is not present.", "unit_test": ["def test_to_constraint_no_reverse_operation():\n    drop_op = DropConstraintOp(constraint_name='test_constraint', table_name='test_table')\n    try:\n        drop_op.to_constraint()\n    except ValueError as e:\n        assert str(e) == 'Reverse operation not present for conversion to Constraint.'"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_valid_reverse_operation"}, "Edge Case Handling": {"requirement": "The 'to_constraint' function should handle cases where the DropConstraintOp instance has a None value for schema and ensure the returned Constraint has a None schema.", "unit_test": ["def test_to_constraint_none_schema():\n    drop_op = DropConstraintOp(constraint_name='test_constraint', table_name='test_table', _reverse=AddConstraintOp.from_constraint(UniqueConstraint('test_constraint', 'test_table')))\n    constraint = drop_op.to_constraint()\n    assert constraint.schema is None"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_none_schema"}, "Functionality Extension": {"requirement": "Extend the 'to_constraint' function to log a warning message when the reverse operation is not present, before raising a ValueError.", "unit_test": ["import logging\n\ndef test_to_constraint_logging_warning():\n    drop_op = DropConstraintOp(constraint_name='test_constraint', table_name='test_table')\n    with self.assertLogs(level='WARNING') as log:\n        try:\n            drop_op.to_constraint()\n        except ValueError:\n            pass\n    assert 'Reverse operation not present' in log.output[0]"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_logging_warning"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'to_constraint' function are properly annotated with type hints.", "unit_test": ["def test_to_constraint_annotations():\n    import inspect\n    signature = inspect.signature(DropConstraintOp.to_constraint)\n    assert signature.parameters['self'].annotation == 'DropConstraintOp'\n    assert signature.return_annotation == 'Constraint'"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_attributes_access"}, "Code Complexity": {"requirement": "The 'to_constraint' function should maintain a cyclomatic complexity of 3 or less to ensure readability and maintainability.", "unit_test": ["def test_to_constraint_cyclomatic_complexity():\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(DropConstraintOp.to_constraint)\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 3"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_cyclomatic_complexity"}, "Code Standard": {"requirement": "The 'to_constraint' function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_to_constraint_pep8_compliance():\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_file_containing_to_constraint_function.py'])\n    assert result.total_errors == 0"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'to_constraint' function should utilize the '_reverse', 'constraint_name', 'schema', and 'table_name' attributes of the DropConstraintOp class as part of its logic.", "unit_test": ["def test_to_constraint_attributes_access():\n    drop_op = DropConstraintOp(constraint_name='test_constraint', table_name='test_table', _reverse=AddConstraintOp.from_constraint(UniqueConstraint('test_constraint', 'test_table')))\n    constraint = drop_op.to_constraint()\n    assert constraint.name == drop_op.constraint_name\n    assert constraint.table.name == drop_op.table_name\n    assert constraint.schema == drop_op.schema"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_attributes_access"}, "Context Usage Correctness Verification": {"requirement": "The 'to_constraint' function should correctly use the '_reverse' attribute to convert it to a Constraint instance and set the name, table name, and schema of the constraint.", "unit_test": ["def test_to_constraint_correct_context_usage():\n    reverse_op = AddConstraintOp.from_constraint(UniqueConstraint('test_constraint', 'test_table'))\n    drop_op = DropConstraintOp(constraint_name='test_constraint', table_name='test_table', _reverse=reverse_op)\n    constraint = drop_op.to_constraint()\n    assert constraint.name == reverse_op.to_constraint().name\n    assert constraint.table.name == reverse_op.to_constraint().table.name\n    assert constraint.schema == reverse_op.to_constraint().schema"], "test": "tests/test_autogen_diffs.py::OrigObjectTest::test_to_constraint_correct_context_usage"}}}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "type": "method", "project_path": "Utilities/python-for-android", "completion_path": "Utilities/python-for-android/pythonforandroid/bootstrap.py", "signature_position": [251, 251], "body_position": [256, 295], "dependency": {"intra_class": ["pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "pythonforandroid.bootstrap.Bootstrap.get_usable_bootstraps_for_recipes"], "intra_file": ["pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "pythonforandroid.bootstrap.expand_dependencies"], "cross_file": ["pythonforandroid.logger.info"]}, "requirement": {"Functionality": "This function selects a recommended default bootstrap from a list of recipes and returns it. It follows a set of rules to determine the appropriate bootstrap based on the given recipes. The rules are following SDL2 bootstrap if there's an sdl2 dep or \"webview\" if we depend on common web recipe.", "Arguments": ":param cls: Class. The Bootstrap class.\n:param recipes: List of strings. The list of recipes to consider when selecting the bootstrap.\n:param ctx: Context. The context in which the function is being called.\n:return: Bootstrap. The selected default bootstrap."}, "tests": ["tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstraps_from_recipes"], "indent": 8, "domain": "Utilities", "code": "    def get_bootstrap_from_recipes(cls, recipes, ctx):\n        '''Picks a single recommended default bootstrap out of\n           all_usable_bootstraps_from_recipes() for the given reicpes,\n           and returns it.'''\n\n        known_web_packages = {\"flask\"}  # to pick webview over service_only\n        recipes_with_deps_lists = expand_dependencies(recipes, ctx)\n        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(\n            recipes, ctx\n        )\n\n        def have_dependency_in_recipes(dep):\n            for dep_list in recipes_with_deps_lists:\n                if dep in dep_list:\n                    return True\n            return False\n\n        # Special rule: return SDL2 bootstrap if there's an sdl2 dep:\n        if (have_dependency_in_recipes(\"sdl2\") and\n                \"sdl2\" in [b.name for b in acceptable_bootstraps]\n                ):\n            info('Using sdl2 bootstrap since it is in dependencies')\n            return cls.get_bootstrap(\"sdl2\", ctx)\n\n        # Special rule: return \"webview\" if we depend on common web recipe:\n        for possible_web_dep in known_web_packages:\n            if have_dependency_in_recipes(possible_web_dep):\n                # We have a web package dep!\n                if \"webview\" in [b.name for b in acceptable_bootstraps]:\n                    info('Using webview bootstrap since common web packages '\n                         'were found {}'.format(\n                             known_web_packages.intersection(recipes)\n                         ))\n                    return cls.get_bootstrap(\"webview\", ctx)\n\n        prioritized_acceptable_bootstraps = sorted(\n            list(acceptable_bootstraps),\n            key=functools.cmp_to_key(_cmp_bootstraps_by_priority)\n        )\n\n        if prioritized_acceptable_bootstraps:\n            info('Using the highest ranked/first of these: {}'\n                 .format(prioritized_acceptable_bootstraps[0].name))\n            return prioritized_acceptable_bootstraps[0]\n        return None\n", "intra_context": "import functools\nimport glob\nimport importlib\nimport os\nfrom os.path import (join, dirname, isdir, normpath, splitext, basename)\nfrom os import listdir, walk, sep\nimport sh\nimport shlex\nimport shutil\n\nfrom pythonforandroid.logger import (shprint, info, logger, debug)\nfrom pythonforandroid.util import (\n    current_directory, ensure_dir, temp_directory, BuildInterruptingException,\n    rmdir, move)\nfrom pythonforandroid.recipe import Recipe\n\n\ndef copy_files(src_root, dest_root, override=True, symlink=False):\n    for root, dirnames, filenames in walk(src_root):\n        for filename in filenames:\n            subdir = normpath(root.replace(src_root, \"\"))\n            if subdir.startswith(sep):  # ensure it is relative\n                subdir = subdir[1:]\n            dest_dir = join(dest_root, subdir)\n            if not os.path.exists(dest_dir):\n                os.makedirs(dest_dir)\n            src_file = join(root, filename)\n            dest_file = join(dest_dir, filename)\n            if os.path.isfile(src_file):\n                if override and os.path.exists(dest_file):\n                    os.unlink(dest_file)\n                if not os.path.exists(dest_file):\n                    if symlink:\n                        os.symlink(src_file, dest_file)\n                    else:\n                        shutil.copy(src_file, dest_file)\n            else:\n                os.makedirs(dest_file)\n\n\ndefault_recipe_priorities = [\n    \"webview\", \"sdl2\", \"service_only\"  # last is highest\n]\n# ^^ NOTE: these are just the default priorities if no special rules\n# apply (which you can find in the code below), so basically if no\n# known graphical lib or web lib is used - in which case service_only\n# is the most reasonable guess.\n\n\ndef _cmp_bootstraps_by_priority(a, b):\n    def rank_bootstrap(bootstrap):\n        \"\"\" Returns a ranking index for each bootstrap,\n            with higher priority ranked with higher number. \"\"\"\n        if bootstrap.name in default_recipe_priorities:\n            return default_recipe_priorities.index(bootstrap.name) + 1\n        return 0\n\n    # Rank bootstraps in order:\n    rank_a = rank_bootstrap(a)\n    rank_b = rank_bootstrap(b)\n    if rank_a != rank_b:\n        return (rank_b - rank_a)\n    else:\n        if a.name < b.name:  # alphabetic sort for determinism\n            return -1\n        else:\n            return 1\n\n\nclass Bootstrap:\n    '''An Android project template, containing recipe stuff for\n    compilation and templated fields for APK info.\n    '''\n    jni_subdir = '/jni'\n    ctx = None\n\n    bootstrap_dir = None\n\n    build_dir = None\n    dist_name = None\n    distribution = None\n\n    # All bootstraps should include Python in some way:\n    recipe_depends = ['python3', 'android']\n\n    can_be_chosen_automatically = True\n    '''Determines whether the bootstrap can be chosen as one that\n    satisfies user requirements. If False, it will not be returned\n    from Bootstrap.get_bootstrap_from_recipes.\n    '''\n\n    # Other things a Bootstrap might need to track (maybe separately):\n    # ndk_main.c\n    # whitelist.txt\n    # blacklist.txt\n\n    @property\n    def dist_dir(self):\n        '''The dist dir at which to place the finished distribution.'''\n        if self.distribution is None:\n            raise BuildInterruptingException(\n                'Internal error: tried to access {}.dist_dir, but {}.distribution '\n                'is None'.format(self, self))\n        return self.distribution.dist_dir\n\n    @property\n    def jni_dir(self):\n        return self.name + self.jni_subdir\n\n    def check_recipe_choices(self):\n        '''Checks what recipes are being built to see which of the alternative\n        and optional dependencies are being used,\n        and returns a list of these.'''\n        recipes = []\n        built_recipes = self.ctx.recipe_build_order or []\n        for recipe in self.recipe_depends:\n            if isinstance(recipe, (tuple, list)):\n                for alternative in recipe:\n                    if alternative in built_recipes:\n                        recipes.append(alternative)\n                        break\n        return sorted(recipes)\n\n    def get_build_dir_name(self):\n        choices = self.check_recipe_choices()\n        dir_name = '-'.join([self.name] + choices)\n        return dir_name\n\n    def get_build_dir(self):\n        return join(self.ctx.build_dir, 'bootstrap_builds', self.get_build_dir_name())\n\n    def get_dist_dir(self, name):\n        return join(self.ctx.dist_dir, name)\n\n    @property\n    def name(self):\n        modname = self.__class__.__module__\n        return modname.split(\".\", 2)[-1]\n\n    def get_bootstrap_dirs(self):\n        \"\"\"get all bootstrap directories, following the MRO path\"\"\"\n\n        # get all bootstrap names along the __mro__, cutting off Bootstrap and object\n        classes = self.__class__.__mro__[:-2]\n        bootstrap_names = [cls.name for cls in classes] + ['common']\n        bootstrap_dirs = [\n            join(self.ctx.root_dir, 'bootstraps', bootstrap_name)\n            for bootstrap_name in reversed(bootstrap_names)\n        ]\n        return bootstrap_dirs\n\n    def _copy_in_final_files(self):\n        if self.name == \"sdl2\":\n            # Get the paths for copying SDL2's java source code:\n            sdl2_recipe = Recipe.get_recipe(\"sdl2\", self.ctx)\n            sdl2_build_dir = sdl2_recipe.get_jni_dir()\n            src_dir = join(sdl2_build_dir, \"SDL\", \"android-project\",\n                           \"app\", \"src\", \"main\", \"java\",\n                           \"org\", \"libsdl\", \"app\")\n            target_dir = join(self.dist_dir, 'src', 'main', 'java', 'org',\n                              'libsdl', 'app')\n\n            # Do actual copying:\n            info('Copying in SDL2 .java files from: ' + str(src_dir))\n            if not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n            copy_files(src_dir, target_dir, override=True)\n\n    def prepare_build_dir(self):\n        \"\"\"Ensure that a build dir exists for the recipe. This same single\n        dir will be used for building all different archs.\"\"\"\n        bootstrap_dirs = self.get_bootstrap_dirs()\n        # now do a cumulative copy of all bootstrap dirs\n        self.build_dir = self.get_build_dir()\n        for bootstrap_dir in bootstrap_dirs:\n            copy_files(join(bootstrap_dir, 'build'), self.build_dir, symlink=self.ctx.symlink_bootstrap_files)\n\n        with current_directory(self.build_dir):\n            with open('project.properties', 'w') as fileh:\n                fileh.write('target=android-{}'.format(self.ctx.android_api))\n\n    def prepare_dist_dir(self):\n        ensure_dir(self.dist_dir)\n\n    def assemble_distribution(self):\n        ''' Copies all the files into the distribution (this function is\n            overridden by the specific bootstrap classes to do this)\n            and add in the distribution info.\n        '''\n        self._copy_in_final_files()\n        self.distribution.save_info(self.dist_dir)\n\n    @classmethod\n    def all_bootstraps(cls):\n        '''Find all the available bootstraps and return them.'''\n        forbidden_dirs = ('__pycache__', 'common')\n        bootstraps_dir = join(dirname(__file__), 'bootstraps')\n        result = set()\n        for name in listdir(bootstraps_dir):\n            if name in forbidden_dirs:\n                continue\n            filen = join(bootstraps_dir, name)\n            if isdir(filen):\n                result.add(name)\n        return result\n\n    @classmethod\n    def get_usable_bootstraps_for_recipes(cls, recipes, ctx):\n        '''Returns all bootstrap whose recipe requirements do not conflict\n        with the given recipes, in no particular order.'''\n        info('Trying to find a bootstrap that matches the given recipes.')\n        bootstraps = [cls.get_bootstrap(name, ctx)\n                      for name in cls.all_bootstraps()]\n        acceptable_bootstraps = set()\n\n        # Find out which bootstraps are acceptable:\n        for bs in bootstraps:\n            if not bs.can_be_chosen_automatically:\n                continue\n            possible_dependency_lists = expand_dependencies(bs.recipe_depends, ctx)\n            for possible_dependencies in possible_dependency_lists:\n                ok = True\n                # Check if the bootstap's dependencies have an internal conflict:\n                for recipe in possible_dependencies:\n                    recipe = Recipe.get_recipe(recipe, ctx)\n                    if any(conflict in recipes for conflict in recipe.conflicts):\n                        ok = False\n                        break\n                # Check if bootstrap's dependencies conflict with chosen\n                # packages:\n                for recipe in recipes:\n                    try:\n                        recipe = Recipe.get_recipe(recipe, ctx)\n                    except ValueError:\n                        conflicts = []\n                    else:\n                        conflicts = recipe.conflicts\n                    if any(conflict in possible_dependencies\n                            for conflict in conflicts):\n                        ok = False\n                        break\n                if ok and bs not in acceptable_bootstraps:\n                    acceptable_bootstraps.add(bs)\n\n        info('Found {} acceptable bootstraps: {}'.format(\n            len(acceptable_bootstraps),\n            [bs.name for bs in acceptable_bootstraps]))\n        return acceptable_bootstraps\n\n    @classmethod\n###The function: get_bootstrap_from_recipes###\n    @classmethod\n    def get_bootstrap(cls, name, ctx):\n        '''Returns an instance of a bootstrap with the given name.\n\n        This is the only way you should access a bootstrap class, as\n        it sets the bootstrap directory correctly.\n        '''\n        if name is None:\n            return None\n        if not hasattr(cls, 'bootstraps'):\n            cls.bootstraps = {}\n        if name in cls.bootstraps:\n            return cls.bootstraps[name]\n        mod = importlib.import_module('pythonforandroid.bootstraps.{}'\n                                      .format(name))\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        bootstrap = mod.bootstrap\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap.ctx = ctx\n        return bootstrap\n\n    def distribute_libs(self, arch, src_dirs, wildcard='*', dest_dir=\"libs\"):\n        '''Copy existing arch libs from build dirs to current dist dir.'''\n        info('Copying libs')\n        tgt_dir = join(dest_dir, arch.arch)\n        ensure_dir(tgt_dir)\n        for src_dir in src_dirs:\n            libs = glob.glob(join(src_dir, wildcard))\n            if libs:\n                shprint(sh.cp, '-a', *libs, tgt_dir)\n\n    def distribute_javaclasses(self, javaclass_dir, dest_dir=\"src\"):\n        '''Copy existing javaclasses from build dir to current dist dir.'''\n        info('Copying java files')\n        ensure_dir(dest_dir)\n        filenames = glob.glob(javaclass_dir)\n        shprint(sh.cp, '-a', *filenames, dest_dir)\n\n    def distribute_aars(self, arch):\n        '''Process existing .aar bundles and copy to current dist dir.'''\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)\n\n    def _unpack_aar(self, aar, arch):\n        '''Unpack content of .aar bundle and copy to current dist dir.'''\n        with temp_directory() as temp_dir:\n            name = splitext(basename(aar))[0]\n            jar_name = name + '.jar'\n            info(\"unpack {} aar\".format(name))\n            debug(\"  from {}\".format(aar))\n            debug(\"  to {}\".format(temp_dir))\n            shprint(sh.unzip, '-o', aar, '-d', temp_dir)\n\n            jar_src = join(temp_dir, 'classes.jar')\n            jar_tgt = join('libs', jar_name)\n            debug(\"copy {} jar\".format(name))\n            debug(\"  from {}\".format(jar_src))\n            debug(\"  to {}\".format(jar_tgt))\n            ensure_dir('libs')\n            shprint(sh.cp, '-a', jar_src, jar_tgt)\n\n            so_src_dir = join(temp_dir, 'jni', arch.arch)\n            so_tgt_dir = join('libs', arch.arch)\n            debug(\"copy {} .so\".format(name))\n            debug(\"  from {}\".format(so_src_dir))\n            debug(\"  to {}\".format(so_tgt_dir))\n            ensure_dir(so_tgt_dir)\n            so_files = glob.glob(join(so_src_dir, '*.so'))\n            shprint(sh.cp, '-a', *so_files, so_tgt_dir)\n\n    def strip_libraries(self, arch):\n        info('Stripping libraries')\n        env = arch.get_env()\n        tokens = shlex.split(env['STRIP'])\n        strip = sh.Command(tokens[0])\n        if len(tokens) > 1:\n            strip = strip.bake(tokens[1:])\n\n        libs_dir = join(self.dist_dir, f'_python_bundle__{arch.arch}',\n                        '_python_bundle', 'modules')\n        filens = shprint(sh.find, libs_dir, join(self.dist_dir, 'libs'),\n                         '-iname', '*.so', _env=env).stdout.decode('utf-8')\n\n        logger.info('Stripping libraries in private dir')\n        for filen in filens.split('\\n'):\n            if not filen:\n                continue  # skip the last ''\n            try:\n                strip(filen, _env=env)\n            except sh.ErrorReturnCode_1:\n                logger.debug('Failed to strip ' + filen)\n\n    def fry_eggs(self, sitepackages):\n        info('Frying eggs in {}'.format(sitepackages))\n        for d in listdir(sitepackages):\n            rd = join(sitepackages, d)\n            if isdir(rd) and d.endswith('.egg'):\n                info('  ' + d)\n                files = [join(rd, f) for f in listdir(rd) if f != 'EGG-INFO']\n                for f in files:\n                    move(f, sitepackages)\n                rmdir(d)\n\n\ndef expand_dependencies(recipes, ctx):\n    \"\"\" This function expands to lists of all different available\n        alternative recipe combinations, with the dependencies added in\n        ONLY for all the not-with-alternative recipes.\n        (So this is like the deps graph very simplified and incomplete, but\n         hopefully good enough for most basic bootstrap compatibility checks)\n    \"\"\"\n\n    # Add in all the deps of recipes where there is no alternative:\n    recipes_with_deps = list(recipes)\n    for entry in recipes:\n        if not isinstance(entry, (tuple, list)) or len(entry) == 1:\n            if isinstance(entry, (tuple, list)):\n                entry = entry[0]\n            try:\n                recipe = Recipe.get_recipe(entry, ctx)\n                recipes_with_deps += recipe.depends\n            except ValueError:\n                # it's a pure python package without a recipe, so we\n                # don't know the dependencies...skipping for now\n                pass\n\n    # Split up lists by available alternatives:\n    recipe_lists = [[]]\n    for recipe in recipes_with_deps:\n        if isinstance(recipe, (tuple, list)):\n            new_recipe_lists = []\n            for alternative in recipe:\n                for old_list in recipe_lists:\n                    new_list = [i for i in old_list]\n                    new_list.append(alternative)\n                    new_recipe_lists.append(new_list)\n            recipe_lists = new_recipe_lists\n        else:\n            for existing_list in recipe_lists:\n                existing_list.append(recipe)\n    return recipe_lists\n", "cross_context": [{"pythonforandroid.logger.info": "import logging\nimport os\nimport re\nimport sh\nfrom sys import stdout, stderr\nfrom math import log10\nfrom collections import defaultdict\nfrom colorama import Style as Colo_Style, Fore as Colo_Fore\n\n\n# monkey patch to show full output\nsh.ErrorReturnCode.truncate_cap = 999999\n\n\nclass LevelDifferentiatingFormatter(logging.Formatter):\n    def format(self, record):\n        if record.levelno > 30:\n            record.msg = '{}{}[ERROR]{}{}:   '.format(\n                Err_Style.BRIGHT, Err_Fore.RED, Err_Fore.RESET,\n                Err_Style.RESET_ALL) + record.msg\n        elif record.levelno > 20:\n            record.msg = '{}{}[WARNING]{}{}: '.format(\n                Err_Style.BRIGHT, Err_Fore.RED, Err_Fore.RESET,\n                Err_Style.RESET_ALL) + record.msg\n        elif record.levelno > 10:\n            record.msg = '{}[INFO]{}:    '.format(\n                Err_Style.BRIGHT, Err_Style.RESET_ALL) + record.msg\n        else:\n            record.msg = '{}{}[DEBUG]{}{}:   '.format(\n                Err_Style.BRIGHT, Err_Fore.LIGHTBLACK_EX, Err_Fore.RESET,\n                Err_Style.RESET_ALL) + record.msg\n        return super().format(record)\n\n\nlogger = logging.getLogger('p4a')\n# Necessary as importlib reloads this,\n# which would add a second handler and reset the level\nif not hasattr(logger, 'touched'):\n    logger.setLevel(logging.INFO)\n    logger.touched = True\n    ch = logging.StreamHandler(stderr)\n    formatter = LevelDifferentiatingFormatter('%(message)s')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\ninfo = logger.info\ndebug = logger.debug\nwarning = logger.warning\nerror = logger.error\n\n\nclass colorama_shim:\n\n    def __init__(self, real):\n        self._dict = defaultdict(str)\n        self._real = real\n        self._enabled = False\n\n    def __getattr__(self, key):\n        return getattr(self._real, key) if self._enabled else self._dict[key]\n\n    def enable(self, enable):\n        self._enabled = enable\n\n\nOut_Style = colorama_shim(Colo_Style)\nOut_Fore = colorama_shim(Colo_Fore)\nErr_Style = colorama_shim(Colo_Style)\nErr_Fore = colorama_shim(Colo_Fore)\n\n\ndef setup_color(color):\n    enable_out = (False if color == 'never' else\n                  True if color == 'always' else\n                  stdout.isatty())\n    Out_Style.enable(enable_out)\n    Out_Fore.enable(enable_out)\n\n    enable_err = (False if color == 'never' else\n                  True if color == 'always' else\n                  stderr.isatty())\n    Err_Style.enable(enable_err)\n    Err_Fore.enable(enable_err)\n\n\ndef info_main(*args):\n    logger.info(''.join([Err_Style.BRIGHT, Err_Fore.GREEN] + list(args) +\n                        [Err_Style.RESET_ALL, Err_Fore.RESET]))\n\n\ndef info_notify(s):\n    info('{}{}{}{}'.format(Err_Style.BRIGHT, Err_Fore.LIGHTBLUE_EX, s,\n                           Err_Style.RESET_ALL))\n\n\ndef shorten_string(string, max_width):\n    ''' make limited length string in form:\n      \"the string is very lo...(and 15 more)\"\n    '''\n    string_len = len(string)\n    if string_len <= max_width:\n        return string\n    visible = max_width - 16 - int(log10(string_len))\n    # expected suffix len \"...(and XXXXX more)\"\n    if not isinstance(string, str):\n        visstring = str(string[:visible], errors='ignore')\n    else:\n        visstring = string[:visible]\n    return u''.join((visstring, u'...(and ',\n                     str(string_len - visible), u' more)'))\n\n\ndef get_console_width():\n    try:\n        cols = int(os.environ['COLUMNS'])\n    except (KeyError, ValueError):\n        pass\n    else:\n        if cols >= 25:\n            return cols\n\n    try:\n        cols = max(25, int(os.popen('stty size', 'r').read().split()[1]))\n    except Exception:\n        pass\n    else:\n        return cols\n\n    return 100\n\n\ndef shprint(command, *args, **kwargs):\n    '''Runs the command (which should be an sh.Command instance), while\n    logging the output.'''\n    kwargs[\"_iter\"] = True\n    kwargs[\"_out_bufsize\"] = 1\n    kwargs[\"_err_to_out\"] = True\n    kwargs[\"_bg\"] = True\n    is_critical = kwargs.pop('_critical', False)\n    tail_n = kwargs.pop('_tail', None)\n    full_debug = False\n    if \"P4A_FULL_DEBUG\" in os.environ:\n        tail_n = 0\n        full_debug = True\n    filter_in = kwargs.pop('_filter', None)\n    filter_out = kwargs.pop('_filterout', None)\n    if len(logger.handlers) > 1:\n        logger.removeHandler(logger.handlers[1])\n    columns = get_console_width()\n    command_path = str(command).split('/')\n    command_string = command_path[-1]\n    string = ' '.join(['{}->{} running'.format(Out_Fore.LIGHTBLACK_EX,\n                                               Out_Style.RESET_ALL),\n                       command_string] + list(args))\n\n    # If logging is not in DEBUG mode, trim the command if necessary\n    if logger.level > logging.DEBUG:\n        logger.info('{}{}'.format(shorten_string(string, columns - 12),\n                                  Err_Style.RESET_ALL))\n    else:\n        logger.debug('{}{}'.format(string, Err_Style.RESET_ALL))\n\n    need_closing_newline = False\n    try:\n        msg_hdr = '           working: '\n        msg_width = columns - len(msg_hdr) - 1\n        output = command(*args, **kwargs)\n        for line in output:\n            if isinstance(line, bytes):\n                line = line.decode('utf-8', errors='replace')\n            if logger.level > logging.DEBUG:\n                if full_debug:\n                    stdout.write(line)\n                    stdout.flush()\n                    continue\n                msg = line.replace(\n                    '\\n', ' ').replace(\n                        '\\t', ' ').replace(\n                            '\\b', ' ').rstrip()\n                if msg:\n                    if \"CI\" not in os.environ:\n                        stdout.write(u'{}\\r{}{:<{width}}'.format(\n                            Err_Style.RESET_ALL, msg_hdr,\n                            shorten_string(msg, msg_width), width=msg_width))\n                        stdout.flush()\n                        need_closing_newline = True\n            else:\n                logger.debug(''.join(['\\t', line.rstrip()]))\n        if need_closing_newline:\n            stdout.write('{}\\r{:>{width}}\\r'.format(\n                Err_Style.RESET_ALL, ' ', width=(columns - 1)))\n            stdout.flush()\n    except sh.ErrorReturnCode as err:\n        if need_closing_newline:\n            stdout.write('{}\\r{:>{width}}\\r'.format(\n                Err_Style.RESET_ALL, ' ', width=(columns - 1)))\n            stdout.flush()\n        if tail_n is not None or filter_in or filter_out:\n            def printtail(out, name, forecolor, tail_n=0,\n                          re_filter_in=None, re_filter_out=None):\n                lines = out.splitlines()\n                if re_filter_in is not None:\n                    lines = [line for line in lines if re_filter_in.search(line)]\n                if re_filter_out is not None:\n                    lines = [line for line in lines if not re_filter_out.search(line)]\n                if tail_n == 0 or len(lines) <= tail_n:\n                    info('{}:\\n{}\\t{}{}'.format(\n                        name, forecolor, '\\t\\n'.join(lines), Out_Fore.RESET))\n                else:\n                    info('{} (last {} lines of {}):\\n{}\\t{}{}'.format(\n                        name, tail_n, len(lines),\n                        forecolor, '\\t\\n'.join([s for s in lines[-tail_n:]]),\n                        Out_Fore.RESET))\n            printtail(err.stdout.decode('utf-8'), 'STDOUT', Out_Fore.YELLOW, tail_n,\n                      re.compile(filter_in) if filter_in else None,\n                      re.compile(filter_out) if filter_out else None)\n            printtail(err.stderr.decode('utf-8'), 'STDERR', Err_Fore.RED)\n        if is_critical or full_debug:\n            env = kwargs.get(\"_env\")\n            if env is not None:\n                info(\"{}ENV:{}\\n{}\\n\".format(\n                    Err_Fore.YELLOW, Err_Fore.RESET, \"\\n\".join(\n                        \"export {}='{}'\".format(n, v) for n, v in env.items())))\n            info(\"{}COMMAND:{}\\ncd {} && {} {}\\n\".format(\n                Err_Fore.YELLOW, Err_Fore.RESET, os.getcwd(), command,\n                ' '.join(args)))\n            warning(\"{}ERROR: {} failed!{}\".format(\n                Err_Fore.RED, command, Err_Fore.RESET))\n        if is_critical:\n            exit(1)\n        else:\n            raise\n\n    return output\n"}], "prompt": "Please write a python function called 'get_bootstrap_from_recipes' base the context. This function selects a recommended default bootstrap from a list of recipes and returns it. It follows a set of rules to determine the appropriate bootstrap based on the given recipes. The rules are following SDL2 bootstrap if there's an sdl2 dep or \"webview\" if we depend on common web recipe.:param cls: Class. The Bootstrap class.\n:param recipes: List of strings. The list of recipes to consider when selecting the bootstrap.\n:param ctx: Context. The context in which the function is being called.\n:return: Bootstrap. The selected default bootstrap..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        import functools\nimport glob\nimport importlib\nimport os\nfrom os.path import (join, dirname, isdir, normpath, splitext, basename)\nfrom os import listdir, walk, sep\nimport sh\nimport shlex\nimport shutil\n\nfrom pythonforandroid.logger import (shprint, info, logger, debug)\nfrom pythonforandroid.util import (\n    current_directory, ensure_dir, temp_directory, BuildInterruptingException,\n    rmdir, move)\nfrom pythonforandroid.recipe import Recipe\n\n\ndef copy_files(src_root, dest_root, override=True, symlink=False):\n    for root, dirnames, filenames in walk(src_root):\n        for filename in filenames:\n            subdir = normpath(root.replace(src_root, \"\"))\n            if subdir.startswith(sep):  # ensure it is relative\n                subdir = subdir[1:]\n            dest_dir = join(dest_root, subdir)\n            if not os.path.exists(dest_dir):\n                os.makedirs(dest_dir)\n            src_file = join(root, filename)\n            dest_file = join(dest_dir, filename)\n            if os.path.isfile(src_file):\n                if override and os.path.exists(dest_file):\n                    os.unlink(dest_file)\n                if not os.path.exists(dest_file):\n                    if symlink:\n                        os.symlink(src_file, dest_file)\n                    else:\n                        shutil.copy(src_file, dest_file)\n            else:\n                os.makedirs(dest_file)\n\n\ndefault_recipe_priorities = [\n    \"webview\", \"sdl2\", \"service_only\"  # last is highest\n]\n# ^^ NOTE: these are just the default priorities if no special rules\n# apply (which you can find in the code below), so basically if no\n# known graphical lib or web lib is used - in which case service_only\n# is the most reasonable guess.\n\n\ndef _cmp_bootstraps_by_priority(a, b):\n    def rank_bootstrap(bootstrap):\n        \"\"\" Returns a ranking index for each bootstrap,\n            with higher priority ranked with higher number. \"\"\"\n        if bootstrap.name in default_recipe_priorities:\n            return default_recipe_priorities.index(bootstrap.name) + 1\n        return 0\n\n    # Rank bootstraps in order:\n    rank_a = rank_bootstrap(a)\n    rank_b = rank_bootstrap(b)\n    if rank_a != rank_b:\n        return (rank_b - rank_a)\n    else:\n        if a.name < b.name:  # alphabetic sort for determinism\n            return -1\n        else:\n            return 1\n\n\nclass Bootstrap:\n    '''An Android project template, containing recipe stuff for\n    compilation and templated fields for APK info.\n    '''\n    jni_subdir = '/jni'\n    ctx = None\n\n    bootstrap_dir = None\n\n    build_dir = None\n    dist_name = None\n    distribution = None\n\n    # All bootstraps should include Python in some way:\n    recipe_depends = ['python3', 'android']\n\n    can_be_chosen_automatically = True\n    '''Determines whether the bootstrap can be chosen as one that\n    satisfies user requirements. If False, it will not be returned\n    from Bootstrap.get_bootstrap_from_recipes.\n    '''\n\n    # Other things a Bootstrap might need to track (maybe separately):\n    # ndk_main.c\n    # whitelist.txt\n    # blacklist.txt\n\n    @property\n    def dist_dir(self):\n        '''The dist dir at which to place the finished distribution.'''\n        if self.distribution is None:\n            raise BuildInterruptingException(\n                'Internal error: tried to access {}.dist_dir, but {}.distribution '\n                'is None'.format(self, self))\n        return self.distribution.dist_dir\n\n    @property\n    def jni_dir(self):\n        return self.name + self.jni_subdir\n\n    def check_recipe_choices(self):\n        '''Checks what recipes are being built to see which of the alternative\n        and optional dependencies are being used,\n        and returns a list of these.'''\n        recipes = []\n        built_recipes = self.ctx.recipe_build_order or []\n        for recipe in self.recipe_depends:\n            if isinstance(recipe, (tuple, list)):\n                for alternative in recipe:\n                    if alternative in built_recipes:\n                        recipes.append(alternative)\n                        break\n        return sorted(recipes)\n\n    def get_build_dir_name(self):\n        choices = self.check_recipe_choices()\n        dir_name = '-'.join([self.name] + choices)\n        return dir_name\n\n    def get_build_dir(self):\n        return join(self.ctx.build_dir, 'bootstrap_builds', self.get_build_dir_name())\n\n    def get_dist_dir(self, name):\n        return join(self.ctx.dist_dir, name)\n\n    @property\n    def name(self):\n        modname = self.__class__.__module__\n        return modname.split(\".\", 2)[-1]\n\n    def get_bootstrap_dirs(self):\n        \"\"\"get all bootstrap directories, following the MRO path\"\"\"\n\n        # get all bootstrap names along the __mro__, cutting off Bootstrap and object\n        classes = self.__class__.__mro__[:-2]\n        bootstrap_names = [cls.name for cls in classes] + ['common']\n        bootstrap_dirs = [\n            join(self.ctx.root_dir, 'bootstraps', bootstrap_name)\n            for bootstrap_name in reversed(bootstrap_names)\n        ]\n        return bootstrap_dirs\n\n    def _copy_in_final_files(self):\n        if self.name == \"sdl2\":\n            # Get the paths for copying SDL2's java source code:\n            sdl2_recipe = Recipe.get_recipe(\"sdl2\", self.ctx)\n            sdl2_build_dir = sdl2_recipe.get_jni_dir()\n            src_dir = join(sdl2_build_dir, \"SDL\", \"android-project\",\n                           \"app\", \"src\", \"main\", \"java\",\n                           \"org\", \"libsdl\", \"app\")\n            target_dir = join(self.dist_dir, 'src', 'main', 'java', 'org',\n                              'libsdl', 'app')\n\n            # Do actual copying:\n            info('Copying in SDL2 .java files from: ' + str(src_dir))\n            if not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n            copy_files(src_dir, target_dir, override=True)\n\n    def prepare_build_dir(self):\n        \"\"\"Ensure that a build dir exists for the recipe. This same single\n        dir will be used for building all different archs.\"\"\"\n        bootstrap_dirs = self.get_bootstrap_dirs()\n        # now do a cumulative copy of all bootstrap dirs\n        self.build_dir = self.get_build_dir()\n        for bootstrap_dir in bootstrap_dirs:\n            copy_files(join(bootstrap_dir, 'build'), self.build_dir, symlink=self.ctx.symlink_bootstrap_files)\n\n        with current_directory(self.build_dir):\n            with open('project.properties', 'w') as fileh:\n                fileh.write('target=android-{}'.format(self.ctx.android_api))\n\n    def prepare_dist_dir(self):\n        ensure_dir(self.dist_dir)\n\n    def assemble_distribution(self):\n        ''' Copies all the files into the distribution (this function is\n            overridden by the specific bootstrap classes to do this)\n            and add in the distribution info.\n        '''\n        self._copy_in_final_files()\n        self.distribution.save_info(self.dist_dir)\n\n    @classmethod\n    def all_bootstraps(cls):\n        '''Find all the available bootstraps and return them.'''\n        forbidden_dirs = ('__pycache__', 'common')\n        bootstraps_dir = join(dirname(__file__), 'bootstraps')\n        result = set()\n        for name in listdir(bootstraps_dir):\n            if name in forbidden_dirs:\n                continue\n            filen = join(bootstraps_dir, name)\n            if isdir(filen):\n                result.add(name)\n        return result\n\n    @classmethod\n    def get_usable_bootstraps_for_recipes(cls, recipes, ctx):\n        '''Returns all bootstrap whose recipe requirements do not conflict\n        with the given recipes, in no particular order.'''\n        info('Trying to find a bootstrap that matches the given recipes.')\n        bootstraps = [cls.get_bootstrap(name, ctx)\n                      for name in cls.all_bootstraps()]\n        acceptable_bootstraps = set()\n\n        # Find out which bootstraps are acceptable:\n        for bs in bootstraps:\n            if not bs.can_be_chosen_automatically:\n                continue\n            possible_dependency_lists = expand_dependencies(bs.recipe_depends, ctx)\n            for possible_dependencies in possible_dependency_lists:\n                ok = True\n                # Check if the bootstap's dependencies have an internal conflict:\n                for recipe in possible_dependencies:\n                    recipe = Recipe.get_recipe(recipe, ctx)\n                    if any(conflict in recipes for conflict in recipe.conflicts):\n                        ok = False\n                        break\n                # Check if bootstrap's dependencies conflict with chosen\n                # packages:\n                for recipe in recipes:\n                    try:\n                        recipe = Recipe.get_recipe(recipe, ctx)\n                    except ValueError:\n                        conflicts = []\n                    else:\n                        conflicts = recipe.conflicts\n                    if any(conflict in possible_dependencies\n                            for conflict in conflicts):\n                        ok = False\n                        break\n                if ok and bs not in acceptable_bootstraps:\n                    acceptable_bootstraps.add(bs)\n\n        info('Found {} acceptable bootstraps: {}'.format(\n            len(acceptable_bootstraps),\n            [bs.name for bs in acceptable_bootstraps]))\n        return acceptable_bootstraps\n\n    @classmethod\n###The function: get_bootstrap_from_recipes###\n    @classmethod\n    def get_bootstrap(cls, name, ctx):\n        '''Returns an instance of a bootstrap with the given name.\n\n        This is the only way you should access a bootstrap class, as\n        it sets the bootstrap directory correctly.\n        '''\n        if name is None:\n            return None\n        if not hasattr(cls, 'bootstraps'):\n            cls.bootstraps = {}\n        if name in cls.bootstraps:\n            return cls.bootstraps[name]\n        mod = importlib.import_module('pythonforandroid.bootstraps.{}'\n                                      .format(name))\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        bootstrap = mod.bootstrap\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap.ctx = ctx\n        return bootstrap\n\n    def distribute_libs(self, arch, src_dirs, wildcard='*', dest_dir=\"libs\"):\n        '''Copy existing arch libs from build dirs to current dist dir.'''\n        info('Copying libs')\n        tgt_dir = join(dest_dir, arch.arch)\n        ensure_dir(tgt_dir)\n        for src_dir in src_dirs:\n            libs = glob.glob(join(src_dir, wildcard))\n            if libs:\n                shprint(sh.cp, '-a', *libs, tgt_dir)\n\n    def distribute_javaclasses(self, javaclass_dir, dest_dir=\"src\"):\n        '''Copy existing javaclasses from build dir to current dist dir.'''\n        info('Copying java files')\n        ensure_dir(dest_dir)\n        filenames = glob.glob(javaclass_dir)\n        shprint(sh.cp, '-a', *filenames, dest_dir)\n\n    def distribute_aars(self, arch):\n        '''Process existing .aar bundles and copy to current dist dir.'''\n        info('Unpacking aars')\n        for aar in glob.glob(join(self.ctx.aars_dir, '*.aar')):\n            self._unpack_aar(aar, arch)\n\n    def _unpack_aar(self, aar, arch):\n        '''Unpack content of .aar bundle and copy to current dist dir.'''\n        with temp_directory() as temp_dir:\n            name = splitext(basename(aar))[0]\n            jar_name = name + '.jar'\n            info(\"unpack {} aar\".format(name))\n            debug(\"  from {}\".format(aar))\n            debug(\"  to {}\".format(temp_dir))\n            shprint(sh.unzip, '-o', aar, '-d', temp_dir)\n\n            jar_src = join(temp_dir, 'classes.jar')\n            jar_tgt = join('libs', jar_name)\n            debug(\"copy {} jar\".format(name))\n            debug(\"  from {}\".format(jar_src))\n            debug(\"  to {}\".format(jar_tgt))\n            ensure_dir('libs')\n            shprint(sh.cp, '-a', jar_src, jar_tgt)\n\n            so_src_dir = join(temp_dir, 'jni', arch.arch)\n            so_tgt_dir = join('libs', arch.arch)\n            debug(\"copy {} .so\".format(name))\n            debug(\"  from {}\".format(so_src_dir))\n            debug(\"  to {}\".format(so_tgt_dir))\n            ensure_dir(so_tgt_dir)\n            so_files = glob.glob(join(so_src_dir, '*.so'))\n            shprint(sh.cp, '-a', *so_files, so_tgt_dir)\n\n    def strip_libraries(self, arch):\n        info('Stripping libraries')\n        env = arch.get_env()\n        tokens = shlex.split(env['STRIP'])\n        strip = sh.Command(tokens[0])\n        if len(tokens) > 1:\n            strip = strip.bake(tokens[1:])\n\n        libs_dir = join(self.dist_dir, f'_python_bundle__{arch.arch}',\n                        '_python_bundle', 'modules')\n        filens = shprint(sh.find, libs_dir, join(self.dist_dir, 'libs'),\n                         '-iname', '*.so', _env=env).stdout.decode('utf-8')\n\n        logger.info('Stripping libraries in private dir')\n        for filen in filens.split('\\n'):\n            if not filen:\n                continue  # skip the last ''\n            try:\n                strip(filen, _env=env)\n            except sh.ErrorReturnCode_1:\n                logger.debug('Failed to strip ' + filen)\n\n    def fry_eggs(self, sitepackages):\n        info('Frying eggs in {}'.format(sitepackages))\n        for d in listdir(sitepackages):\n            rd = join(sitepackages, d)\n            if isdir(rd) and d.endswith('.egg'):\n                info('  ' + d)\n                files = [join(rd, f) for f in listdir(rd) if f != 'EGG-INFO']\n                for f in files:\n                    move(f, sitepackages)\n                rmdir(d)\n\n\ndef expand_dependencies(recipes, ctx):\n    \"\"\" This function expands to lists of all different available\n        alternative recipe combinations, with the dependencies added in\n        ONLY for all the not-with-alternative recipes.\n        (So this is like the deps graph very simplified and incomplete, but\n         hopefully good enough for most basic bootstrap compatibility checks)\n    \"\"\"\n\n    # Add in all the deps of recipes where there is no alternative:\n    recipes_with_deps = list(recipes)\n    for entry in recipes:\n        if not isinstance(entry, (tuple, list)) or len(entry) == 1:\n            if isinstance(entry, (tuple, list)):\n                entry = entry[0]\n            try:\n                recipe = Recipe.get_recipe(entry, ctx)\n                recipes_with_deps += recipe.depends\n            except ValueError:\n                # it's a pure python package without a recipe, so we\n                # don't know the dependencies...skipping for now\n                pass\n\n    # Split up lists by available alternatives:\n    recipe_lists = [[]]\n    for recipe in recipes_with_deps:\n        if isinstance(recipe, (tuple, list)):\n            new_recipe_lists = []\n            for alternative in recipe:\n                for old_list in recipe_lists:\n                    new_list = [i for i in old_list]\n                    new_list.append(alternative)\n                    new_recipe_lists.append(new_list)\n            recipe_lists = new_recipe_lists\n        else:\n            for existing_list in recipe_lists:\n                existing_list.append(recipe)\n    return recipe_lists\n\n        ####cross_file_context:\n        [{'pythonforandroid.logger.info': 'import logging\\nimport os\\nimport re\\nimport sh\\nfrom sys import stdout, stderr\\nfrom math import log10\\nfrom collections import defaultdict\\nfrom colorama import Style as Colo_Style, Fore as Colo_Fore\\n\\n\\n# monkey patch to show full output\\nsh.ErrorReturnCode.truncate_cap = 999999\\n\\n\\nclass LevelDifferentiatingFormatter(logging.Formatter):\\n    def format(self, record):\\n        if record.levelno > 30:\\n            record.msg = \\'{}{}[ERROR]{}{}:   \\'.format(\\n                Err_Style.BRIGHT, Err_Fore.RED, Err_Fore.RESET,\\n                Err_Style.RESET_ALL) + record.msg\\n        elif record.levelno > 20:\\n            record.msg = \\'{}{}[WARNING]{}{}: \\'.format(\\n                Err_Style.BRIGHT, Err_Fore.RED, Err_Fore.RESET,\\n                Err_Style.RESET_ALL) + record.msg\\n        elif record.levelno > 10:\\n            record.msg = \\'{}[INFO]{}:    \\'.format(\\n                Err_Style.BRIGHT, Err_Style.RESET_ALL) + record.msg\\n        else:\\n            record.msg = \\'{}{}[DEBUG]{}{}:   \\'.format(\\n                Err_Style.BRIGHT, Err_Fore.LIGHTBLACK_EX, Err_Fore.RESET,\\n                Err_Style.RESET_ALL) + record.msg\\n        return super().format(record)\\n\\n\\nlogger = logging.getLogger(\\'p4a\\')\\n# Necessary as importlib reloads this,\\n# which would add a second handler and reset the level\\nif not hasattr(logger, \\'touched\\'):\\n    logger.setLevel(logging.INFO)\\n    logger.touched = True\\n    ch = logging.StreamHandler(stderr)\\n    formatter = LevelDifferentiatingFormatter(\\'%(message)s\\')\\n    ch.setFormatter(formatter)\\n    logger.addHandler(ch)\\ninfo = logger.info\\ndebug = logger.debug\\nwarning = logger.warning\\nerror = logger.error\\n\\n\\nclass colorama_shim:\\n\\n    def __init__(self, real):\\n        self._dict = defaultdict(str)\\n        self._real = real\\n        self._enabled = False\\n\\n    def __getattr__(self, key):\\n        return getattr(self._real, key) if self._enabled else self._dict[key]\\n\\n    def enable(self, enable):\\n        self._enabled = enable\\n\\n\\nOut_Style = colorama_shim(Colo_Style)\\nOut_Fore = colorama_shim(Colo_Fore)\\nErr_Style = colorama_shim(Colo_Style)\\nErr_Fore = colorama_shim(Colo_Fore)\\n\\n\\ndef setup_color(color):\\n    enable_out = (False if color == \\'never\\' else\\n                  True if color == \\'always\\' else\\n                  stdout.isatty())\\n    Out_Style.enable(enable_out)\\n    Out_Fore.enable(enable_out)\\n\\n    enable_err = (False if color == \\'never\\' else\\n                  True if color == \\'always\\' else\\n                  stderr.isatty())\\n    Err_Style.enable(enable_err)\\n    Err_Fore.enable(enable_err)\\n\\n\\ndef info_main(*args):\\n    logger.info(\\'\\'.join([Err_Style.BRIGHT, Err_Fore.GREEN] + list(args) +\\n                        [Err_Style.RESET_ALL, Err_Fore.RESET]))\\n\\n\\ndef info_notify(s):\\n    info(\\'{}{}{}{}\\'.format(Err_Style.BRIGHT, Err_Fore.LIGHTBLUE_EX, s,\\n                           Err_Style.RESET_ALL))\\n\\n\\ndef shorten_string(string, max_width):\\n    \\'\\'\\' make limited length string in form:\\n      \"the string is very lo...(and 15 more)\"\\n    \\'\\'\\'\\n    string_len = len(string)\\n    if string_len <= max_width:\\n        return string\\n    visible = max_width - 16 - int(log10(string_len))\\n    # expected suffix len \"...(and XXXXX more)\"\\n    if not isinstance(string, str):\\n        visstring = str(string[:visible], errors=\\'ignore\\')\\n    else:\\n        visstring = string[:visible]\\n    return u\\'\\'.join((visstring, u\\'...(and \\',\\n                     str(string_len - visible), u\\' more)\\'))\\n\\n\\ndef get_console_width():\\n    try:\\n        cols = int(os.environ[\\'COLUMNS\\'])\\n    except (KeyError, ValueError):\\n        pass\\n    else:\\n        if cols >= 25:\\n            return cols\\n\\n    try:\\n        cols = max(25, int(os.popen(\\'stty size\\', \\'r\\').read().split()[1]))\\n    except Exception:\\n        pass\\n    else:\\n        return cols\\n\\n    return 100\\n\\n\\ndef shprint(command, *args, **kwargs):\\n    \\'\\'\\'Runs the command (which should be an sh.Command instance), while\\n    logging the output.\\'\\'\\'\\n    kwargs[\"_iter\"] = True\\n    kwargs[\"_out_bufsize\"] = 1\\n    kwargs[\"_err_to_out\"] = True\\n    kwargs[\"_bg\"] = True\\n    is_critical = kwargs.pop(\\'_critical\\', False)\\n    tail_n = kwargs.pop(\\'_tail\\', None)\\n    full_debug = False\\n    if \"P4A_FULL_DEBUG\" in os.environ:\\n        tail_n = 0\\n        full_debug = True\\n    filter_in = kwargs.pop(\\'_filter\\', None)\\n    filter_out = kwargs.pop(\\'_filterout\\', None)\\n    if len(logger.handlers) > 1:\\n        logger.removeHandler(logger.handlers[1])\\n    columns = get_console_width()\\n    command_path = str(command).split(\\'/\\')\\n    command_string = command_path[-1]\\n    string = \\' \\'.join([\\'{}->{} running\\'.format(Out_Fore.LIGHTBLACK_EX,\\n                                               Out_Style.RESET_ALL),\\n                       command_string] + list(args))\\n\\n    # If logging is not in DEBUG mode, trim the command if necessary\\n    if logger.level > logging.DEBUG:\\n        logger.info(\\'{}{}\\'.format(shorten_string(string, columns - 12),\\n                                  Err_Style.RESET_ALL))\\n    else:\\n        logger.debug(\\'{}{}\\'.format(string, Err_Style.RESET_ALL))\\n\\n    need_closing_newline = False\\n    try:\\n        msg_hdr = \\'           working: \\'\\n        msg_width = columns - len(msg_hdr) - 1\\n        output = command(*args, **kwargs)\\n        for line in output:\\n            if isinstance(line, bytes):\\n                line = line.decode(\\'utf-8\\', errors=\\'replace\\')\\n            if logger.level > logging.DEBUG:\\n                if full_debug:\\n                    stdout.write(line)\\n                    stdout.flush()\\n                    continue\\n                msg = line.replace(\\n                    \\'\\\\n\\', \\' \\').replace(\\n                        \\'\\\\t\\', \\' \\').replace(\\n                            \\'\\\\b\\', \\' \\').rstrip()\\n                if msg:\\n                    if \"CI\" not in os.environ:\\n                        stdout.write(u\\'{}\\\\r{}{:<{width}}\\'.format(\\n                            Err_Style.RESET_ALL, msg_hdr,\\n                            shorten_string(msg, msg_width), width=msg_width))\\n                        stdout.flush()\\n                        need_closing_newline = True\\n            else:\\n                logger.debug(\\'\\'.join([\\'\\\\t\\', line.rstrip()]))\\n        if need_closing_newline:\\n            stdout.write(\\'{}\\\\r{:>{width}}\\\\r\\'.format(\\n                Err_Style.RESET_ALL, \\' \\', width=(columns - 1)))\\n            stdout.flush()\\n    except sh.ErrorReturnCode as err:\\n        if need_closing_newline:\\n            stdout.write(\\'{}\\\\r{:>{width}}\\\\r\\'.format(\\n                Err_Style.RESET_ALL, \\' \\', width=(columns - 1)))\\n            stdout.flush()\\n        if tail_n is not None or filter_in or filter_out:\\n            def printtail(out, name, forecolor, tail_n=0,\\n                          re_filter_in=None, re_filter_out=None):\\n                lines = out.splitlines()\\n                if re_filter_in is not None:\\n                    lines = [line for line in lines if re_filter_in.search(line)]\\n                if re_filter_out is not None:\\n                    lines = [line for line in lines if not re_filter_out.search(line)]\\n                if tail_n == 0 or len(lines) <= tail_n:\\n                    info(\\'{}:\\\\n{}\\\\t{}{}\\'.format(\\n                        name, forecolor, \\'\\\\t\\\\n\\'.join(lines), Out_Fore.RESET))\\n                else:\\n                    info(\\'{} (last {} lines of {}):\\\\n{}\\\\t{}{}\\'.format(\\n                        name, tail_n, len(lines),\\n                        forecolor, \\'\\\\t\\\\n\\'.join([s for s in lines[-tail_n:]]),\\n                        Out_Fore.RESET))\\n            printtail(err.stdout.decode(\\'utf-8\\'), \\'STDOUT\\', Out_Fore.YELLOW, tail_n,\\n                      re.compile(filter_in) if filter_in else None,\\n                      re.compile(filter_out) if filter_out else None)\\n            printtail(err.stderr.decode(\\'utf-8\\'), \\'STDERR\\', Err_Fore.RED)\\n        if is_critical or full_debug:\\n            env = kwargs.get(\"_env\")\\n            if env is not None:\\n                info(\"{}ENV:{}\\\\n{}\\\\n\".format(\\n                    Err_Fore.YELLOW, Err_Fore.RESET, \"\\\\n\".join(\\n                        \"export {}=\\'{}\\'\".format(n, v) for n, v in env.items())))\\n            info(\"{}COMMAND:{}\\\\ncd {} && {} {}\\\\n\".format(\\n                Err_Fore.YELLOW, Err_Fore.RESET, os.getcwd(), command,\\n                \\' \\'.join(args)))\\n            warning(\"{}ERROR: {} failed!{}\".format(\\n                Err_Fore.RED, command, Err_Fore.RESET))\\n        if is_critical:\\n            exit(1)\\n        else:\\n            raise\\n\\n    return output\\n'}]", "test_list": ["def test_get_bootstraps_from_recipes(self):\n    \"\"\"A test which will initialize a bootstrap and will check if the\n        method :meth:`~pythonforandroid.bootstrap.Bootstrap.\n        get_bootstraps_from_recipes` returns the expected values\n        \"\"\"\n    import pythonforandroid.recipe\n    original_get_recipe = pythonforandroid.recipe.Recipe.get_recipe\n    recipes_sdl2 = {'sdl2', 'python3', 'kivy'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_sdl2, self.ctx)\n    self.assertEqual(bs.name, 'sdl2')\n    recipes_pysdl2_only = {'pysdl2'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_pysdl2_only, self.ctx)\n    self.assertEqual(bs.name, 'sdl2')\n    recipes_kivy_only = {'kivy'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_kivy_only, self.ctx)\n    self.assertEqual(bs.name, 'sdl2')\n    with mock.patch('pythonforandroid.recipe.Recipe.get_recipe') as mock_get_recipe:\n\n        def _add_sdl2_conflicting_recipe(name, ctx):\n            if name == 'conflictswithsdl2':\n                if name not in pythonforandroid.recipe.Recipe.recipes:\n                    pythonforandroid.recipe.Recipe.recipes[name] = get_fake_recipe('sdl2', conflicts=['sdl2'])\n            return original_get_recipe(name, ctx)\n        mock_get_recipe.side_effect = _add_sdl2_conflicting_recipe\n        recipes_with_sdl2_conflict = {'python3', 'conflictswithsdl2'}\n        bs = Bootstrap.get_bootstrap_from_recipes(recipes_with_sdl2_conflict, self.ctx)\n        self.assertNotEqual(bs.name, 'sdl2')\n    recipes_with_flask = {'python3', 'flask'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_with_flask, self.ctx)\n    self.assertEqual(bs.name, 'webview')\n    recipes_with_no_sdl2_or_web = {'python3', 'numpy'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_with_no_sdl2_or_web, self.ctx)\n    self.assertEqual(bs.name, 'service_only')"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'get_bootstrap_from_recipes' should return a Bootstrap object whose name is either 'sdl2', 'webview', or 'service_only', based on the input recipes list.", "unit_test": ["def test_get_bootstrap_output_conditions(self):\n    recipes_sdl2 = {'sdl2', 'python3', 'kivy'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_sdl2, self.ctx)\n    self.assertIn(bs.name, ['sdl2', 'webview', 'service_only'])"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_output_conditions"}, "Exception Handling": {"requirement": "The function should raise a ValueError if the recipes list is empty.", "unit_test": ["def test_get_bootstrap_exception_handling(self):\n    with self.assertRaises(ValueError):\n        Bootstrap.get_bootstrap_from_recipes([], self.ctx)"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_exception_handling"}, "Edge Case Handling": {"requirement": "The function should handle cases where the recipes list contains unknown recipes gracefully, defaulting to 'service_only'.", "unit_test": ["def test_get_bootstrap_edge_case_handling(self):\n    recipes_unknown = {'unknown_recipe'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_unknown, self.ctx)\n    self.assertEqual(bs.name, 'service_only')"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the function to prioritize a 'custom' bootstrap if a 'custom' recipe is present in the list.", "unit_test": ["def test_get_bootstrap_functionality_extension(self):\n    recipes_custom = {'custom', 'python3'}\n    bs = Bootstrap.get_bootstrap_from_recipes(recipes_custom, self.ctx)\n    self.assertEqual(bs.name, 'custom')"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_functionality_extension"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the function are properly annotated.", "unit_test": ["def test_get_bootstrap_annotation_coverage(self):\n    annotations = Bootstrap.get_bootstrap_from_recipes.__annotations__\n    self.assertIn('cls', annotations)\n    self.assertIn('recipes', annotations)\n    self.assertIn('ctx', annotations)\n    self.assertIn('return', annotations)"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_annotation_coverage"}, "Code Complexity": {"requirement": "The function should have a cyclomatic complexity of 10 or less.", "unit_test": ["def test_get_bootstrap_code_complexity(self):\n    from radon.complexity import cc_visit\n    with open('bootstrap.py', 'r') as file:\n        source_code = file.read()\n    complexity = cc_visit(source_code)\n    function_complexity = next((c for c in complexity if c.name == 'get_bootstrap_from_recipes'), None)\n    self.assertIsNotNone(function_complexity)\n    self.assertLessEqual(function_complexity.complexity, 5)"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_code_complexity"}, "Code Standard": {"requirement": "The function should adhere to PEP 8 standards, including proper indentation and spacing.", "unit_test": ["def test_get_bootstrap_code_standard(self):\n    import subprocess\n    result = subprocess.run(['flake8', '--max-line-length=79', 'bootstrap.py'], capture_output=True, text=True)\n    self.assertEqual(result.returncode, 0, msg=result.stdout)"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_check_code_style"}, "Context Usage Verification": {"requirement": "The function should utilize the 'get_usable_bootstraps_for_recipes' method from the Bootstrap class.", "unit_test": ["def test_get_bootstrap_context_usage_verification(self):\n    with mock.patch('pythonforandroid.bootstrap.Bootstrap.get_usable_bootstraps_for_recipes') as mock_method:\n        recipes = {'sdl2', 'python3'}\n        Bootstrap.get_bootstrap_from_recipes(recipes, self.ctx)\n        mock_method.assert_called_once_with(recipes, self.ctx)"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_context_usage_verification"}, "Context Usage Correctness Verification": {"requirement": "The function should correctly use the 'get_bootstrap' method to instantiate the selected bootstrap.", "unit_test": ["def test_get_bootstrap_context_usage_correctness(self):\n    with mock.patch('pythonforandroid.bootstrap.Bootstrap.get_bootstrap') as mock_method:\n        recipes = {'sdl2', 'python3'}\n        Bootstrap.get_bootstrap_from_recipes(recipes, self.ctx)\n        mock_method.assert_called()"], "test": "tests/test_bootstrap.py::TestBootstrapBasic::test_get_bootstrap_context_usage_correctness"}}}
{"namespace": "mopidy.config.types.Pair.deserialize", "type": "method", "project_path": "Multimedia/Mopidy", "completion_path": "Multimedia/Mopidy/mopidy/config/types.py", "signature_position": [241, 241], "body_position": [242, 259], "dependency": {"intra_class": ["mopidy.config.types.Pair._optional_pair", "mopidy.config.types.Pair._required", "mopidy.config.types.Pair._separator", "mopidy.config.types.Pair._subtypes"], "intra_file": ["mopidy.config.types.decode", "mopidy.config.types.encode", "mopidy.config.types.String.deserialize"], "cross_file": ["mopidy.config.validators.validate_required", "mopidy.config.validators"]}, "requirement": {"Functionality": "Deserialize a value and return a pair of deserialized values. It first decodes the input value and removes any leading or trailing whitespace. Then, it validates the raw value based on whether it is required or not. If the raw value is empty, it returns None. If the separator is present in the raw value, it splits the value into two parts. If the optional pair flag is set, it assigns the same value to both parts. Otherwise, it raises a ValueError indicating that the config value must include the separator. Finally, it encodes and deserializes each part of the pair using the corresponding subtypes.", "Arguments": ":param self: Pair. An instance of the Pair class.\n:param value: The value to be deserialized.\n:return: Tuple. A pair of deserialized values."}, "tests": ["tests/config/test_types.py::TestPair::test_deserialize_respects_optional_separator", "tests/config/test_types.py::TestPair::test_deserialize_with_optional_custom_subtypes", "tests/config/test_types.py::TestPair::test_deserialize_enforces_required_pair_values_with_custom_separator", "tests/config/test_types.py::TestPair::test_deserialize_respects_optional_custom_separator", "tests/config/test_types.py::TestPair::test_deserialize_conversion_success"], "indent": 8, "domain": "Multimedia", "code": "    def deserialize(self, value):\n        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )\n", "intra_context": "import logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Float(ConfigValue):\n    \"\"\"Float value.\"\"\"\n\n    def __init__(self, minimum=None, maximum=None, optional=False):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Boolean(ConfigValue):\n    \"\"\"Boolean value.\n\n    Accepts ``1``, ``yes``, ``true``, and ``on`` with any casing as\n    :class:`True`.\n\n    Accepts ``0``, ``no``, ``false``, and ``off`` with any casing as\n    :class:`False`.\n    \"\"\"\n\n    true_values = (\"1\", \"yes\", \"true\", \"on\")\n    false_values = (\"0\", \"no\", \"false\", \"off\")\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n\n    def serialize(self, value, display=False):\n        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n\n\nclass Pair(ConfigValue):\n    \"\"\"Pair value\n\n    The value is expected to be a pair of elements, separated by a specified delimiter.\n    Values can optionally not be a pair, in which case the whole input is provided for\n    both sides of the value.\n    \"\"\"\n\n    def __init__(\n        self, optional=False, optional_pair=False, separator=\"|\", subtypes=None\n    ):\n        self._required = not optional\n        self._optional_pair = optional_pair\n        self._separator = separator\n        if subtypes:\n            self._subtypes = subtypes\n        else:\n            self._subtypes = (String(), String())\n\n###The function: deserialize###\n    def serialize(self, value, display=False):\n        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n\n\nclass List(ConfigValue):\n    \"\"\"List value.\n\n    Supports elements split by commas or newlines. Newlines take precedence and\n    empty list items will be filtered out.\n\n    Enforcing unique entries in the list will result in a set data structure\n    being used. This does not preserve ordering, which could result in the\n    serialized output being unstable.\n    \"\"\"\n\n    def __init__(self, optional=False, unique=False, subtype=None):\n        self._required = not optional\n        self._unique = unique\n        self._subtype = subtype if subtype else String()\n\n    def deserialize(self, value):\n        value = decode(value)\n        if \"\\n\" in value:\n            values = re.split(r\"\\s*\\n\\s*\", value)\n        else:\n            values = re.split(r\"\\s*,\\s*\", value)\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        values_iter = (\n            subtype.deserialize(v.strip()) for v in values if v.strip()\n        )\n        if self._unique:\n            values = frozenset(values_iter)\n        else:\n            values = tuple(values_iter)\n\n        validators.validate_required(values, self._required)\n        return values\n\n    def serialize(self, value, display=False):\n        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n\n\nclass LogColor(ConfigValue):\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n\n    def serialize(self, value, display=False):\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n\n\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n    def serialize(self, value, display=False):\n        lookup = {v: k for k, v in self.levels.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"\n\n\nclass Hostname(ConfigValue):\n    \"\"\"Network hostname value.\"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value, display=False):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n\n\nclass Port(Integer):\n    \"\"\"Network port value.\n\n    Expects integer in the range 0-65535, zero tells the kernel to simply\n    allocate a port for us.\n    \"\"\"\n\n    def __init__(self, choices=None, optional=False):\n        super().__init__(\n            minimum=0, maximum=2**16 - 1, choices=choices, optional=optional\n        )\n\n\n# Keep this for backwards compatibility\nclass _ExpandedPath(_TransformedValue):\n    pass\n\n\nclass Path(ConfigValue):\n    \"\"\"File system path.\n\n    The following expansions of the path will be done:\n\n    - ``~`` to the current user's home directory\n    - ``$XDG_CACHE_DIR`` according to the XDG spec\n    - ``$XDG_CONFIG_DIR`` according to the XDG spec\n    - ``$XDG_DATA_DIR`` according to the XDG spec\n    - ``$XDG_MUSIC_DIR`` according to the XDG spec\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        expanded = path.expand_path(value)\n        validators.validate_required(value, self._required)\n        validators.validate_required(expanded, self._required)\n        if not value or expanded is None:\n            return None\n        return _ExpandedPath(value, expanded)\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _ExpandedPath):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return value\n", "cross_context": [{"mopidy.config.validators.validate_required": "# TODO: add validate regexp?\n\n\ndef validate_required(value, required):\n    \"\"\"Validate that ``value`` is set if ``required``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize` on\n    the raw string, _not_ the converted value.\n    \"\"\"\n    if required and not value:\n        raise ValueError(\"must be set.\")\n\n\ndef validate_choice(value, choices):\n    \"\"\"Validate that ``value`` is one of the ``choices``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\n    \"\"\"\n    if choices is not None and value not in choices:\n        names = \", \".join(repr(c) for c in choices)\n        raise ValueError(f\"must be one of {names}, not {value}.\")\n\n\ndef validate_minimum(value, minimum):\n    \"\"\"Validate that ``value`` is at least ``minimum``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\n    \"\"\"\n    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n\n\ndef validate_maximum(value, maximum):\n    \"\"\"Validate that ``value`` is at most ``maximum``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\n    \"\"\"\n    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n"}, {"mopidy.config.validators": "# TODO: add validate regexp?\n\n\ndef validate_required(value, required):\n    \"\"\"Validate that ``value`` is set if ``required``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize` on\n    the raw string, _not_ the converted value.\n    \"\"\"\n    if required and not value:\n        raise ValueError(\"must be set.\")\n\n\ndef validate_choice(value, choices):\n    \"\"\"Validate that ``value`` is one of the ``choices``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\n    \"\"\"\n    if choices is not None and value not in choices:\n        names = \", \".join(repr(c) for c in choices)\n        raise ValueError(f\"must be one of {names}, not {value}.\")\n\n\ndef validate_minimum(value, minimum):\n    \"\"\"Validate that ``value`` is at least ``minimum``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\n    \"\"\"\n    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n\n\ndef validate_maximum(value, maximum):\n    \"\"\"Validate that ``value`` is at most ``maximum``\n\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\n    \"\"\"\n    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n"}], "prompt": "Please write a python function called 'deserialize' base the context. Deserialize a value and return a pair of deserialized values. It first decodes the input value and removes any leading or trailing whitespace. Then, it validates the raw value based on whether it is required or not. If the raw value is empty, it returns None. If the separator is present in the raw value, it splits the value into two parts. If the optional pair flag is set, it assigns the same value to both parts. Otherwise, it raises a ValueError indicating that the config value must include the separator. Finally, it encodes and deserializes each part of the pair using the corresponding subtypes.:param self: Pair. An instance of the Pair class.\n:param value: The value to be deserialized.\n:return: Tuple. A pair of deserialized values..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        import logging\nimport re\nimport socket\n\nfrom mopidy.config import validators\nfrom mopidy.internal import log, path\n\n\ndef decode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n\ndef encode(value):\n    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n\nclass DeprecatedValue:\n    pass\n\n\nclass _TransformedValue(str):\n    def __new__(cls, original, transformed):\n        return super().__new__(cls, transformed)\n\n    def __init__(self, original, transformed):\n        self.original = original\n\n\nclass ConfigValue:\n    \"\"\"Represents a config key's value and how to handle it.\n\n    Normally you will only be interacting with sub-classes for config values\n    that encode either deserialization behavior and/or validation.\n\n    Each config value should be used for the following actions:\n\n    1. Deserializing from a raw string and validating, raising ValueError on\n       failure.\n    2. Serializing a value back to a string that can be stored in a config.\n    3. Formatting a value to a printable form (useful for masking secrets).\n\n    :class:`None` values should not be deserialized, serialized or formatted,\n    the code interacting with the config should simply skip None config values.\n    \"\"\"\n\n    def deserialize(self, value):\n        \"\"\"Cast raw string to appropriate type.\"\"\"\n        return decode(value)\n\n    def serialize(self, value, display=False):\n        \"\"\"Convert value back to string for saving.\"\"\"\n        if value is None:\n            return \"\"\n        return str(value)\n\n\nclass Deprecated(ConfigValue):\n    \"\"\"Deprecated value.\n\n    Used for ignoring old config values that are no longer in use, but should\n    not cause the config parser to crash.\n    \"\"\"\n\n    def deserialize(self, value):\n        return DeprecatedValue()\n\n    def serialize(self, value, display=False):\n        return DeprecatedValue()\n\n\nclass String(ConfigValue):\n    \"\"\"String value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        self._required = not optional\n        self._choices = choices\n        self._transformer = transformer\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n\nclass Secret(String):\n    \"\"\"Secret string value.\n\n    Is decoded as utf-8 and \\\\n \\\\t escapes should work and be preserved.\n\n    Should be used for passwords, auth tokens etc. Will mask value when being\n    displayed.\n    \"\"\"\n\n    def __init__(self, optional=False, choices=None, transformer=None):\n        super().__init__(\n            optional=optional,\n            choices=None,  # Choices doesn't make sense for secrets\n            transformer=transformer,\n        )\n\n    def serialize(self, value, display=False):\n        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n\nclass Integer(ConfigValue):\n    \"\"\"Integer value.\"\"\"\n\n    def __init__(\n        self, minimum=None, maximum=None, choices=None, optional=False\n    ):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Float(ConfigValue):\n    \"\"\"Float value.\"\"\"\n\n    def __init__(self, minimum=None, maximum=None, optional=False):\n        self._required = not optional\n        self._minimum = minimum\n        self._maximum = maximum\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n\nclass Boolean(ConfigValue):\n    \"\"\"Boolean value.\n\n    Accepts ``1``, ``yes``, ``true``, and ``on`` with any casing as\n    :class:`True`.\n\n    Accepts ``0``, ``no``, ``false``, and ``off`` with any casing as\n    :class:`False`.\n    \"\"\"\n\n    true_values = (\"1\", \"yes\", \"true\", \"on\")\n    false_values = (\"0\", \"no\", \"false\", \"off\")\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")\n\n    def serialize(self, value, display=False):\n        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")\n\n\nclass Pair(ConfigValue):\n    \"\"\"Pair value\n\n    The value is expected to be a pair of elements, separated by a specified delimiter.\n    Values can optionally not be a pair, in which case the whole input is provided for\n    both sides of the value.\n    \"\"\"\n\n    def __init__(\n        self, optional=False, optional_pair=False, separator=\"|\", subtypes=None\n    ):\n        self._required = not optional\n        self._optional_pair = optional_pair\n        self._separator = separator\n        if subtypes:\n            self._subtypes = subtypes\n        else:\n            self._subtypes = (String(), String())\n\n###The function: deserialize###\n    def serialize(self, value, display=False):\n        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )\n\n\nclass List(ConfigValue):\n    \"\"\"List value.\n\n    Supports elements split by commas or newlines. Newlines take precedence and\n    empty list items will be filtered out.\n\n    Enforcing unique entries in the list will result in a set data structure\n    being used. This does not preserve ordering, which could result in the\n    serialized output being unstable.\n    \"\"\"\n\n    def __init__(self, optional=False, unique=False, subtype=None):\n        self._required = not optional\n        self._unique = unique\n        self._subtype = subtype if subtype else String()\n\n    def deserialize(self, value):\n        value = decode(value)\n        if \"\\n\" in value:\n            values = re.split(r\"\\s*\\n\\s*\", value)\n        else:\n            values = re.split(r\"\\s*,\\s*\", value)\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        values_iter = (\n            subtype.deserialize(v.strip()) for v in values if v.strip()\n        )\n        if self._unique:\n            values = frozenset(values_iter)\n        else:\n            values = tuple(values_iter)\n\n        validators.validate_required(values, self._required)\n        return values\n\n    def serialize(self, value, display=False):\n        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)\n\n\nclass LogColor(ConfigValue):\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()\n\n    def serialize(self, value, display=False):\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"\n\n\nclass LogLevel(ConfigValue):\n    \"\"\"Log level value.\n\n    Expects one of ``critical``, ``error``, ``warning``, ``info``, ``debug``,\n    ``trace``, or ``all``, with any casing.\n    \"\"\"\n\n    levels = {\n        \"critical\": logging.CRITICAL,\n        \"error\": logging.ERROR,\n        \"warning\": logging.WARNING,\n        \"info\": logging.INFO,\n        \"debug\": logging.DEBUG,\n        \"trace\": log.TRACE_LOG_LEVEL,\n        \"all\": logging.NOTSET,\n    }\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())\n\n    def serialize(self, value, display=False):\n        lookup = {v: k for k, v in self.levels.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"\n\n\nclass Hostname(ConfigValue):\n    \"\"\"Network hostname value.\"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value, display=False):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value\n\n\nclass Port(Integer):\n    \"\"\"Network port value.\n\n    Expects integer in the range 0-65535, zero tells the kernel to simply\n    allocate a port for us.\n    \"\"\"\n\n    def __init__(self, choices=None, optional=False):\n        super().__init__(\n            minimum=0, maximum=2**16 - 1, choices=choices, optional=optional\n        )\n\n\n# Keep this for backwards compatibility\nclass _ExpandedPath(_TransformedValue):\n    pass\n\n\nclass Path(ConfigValue):\n    \"\"\"File system path.\n\n    The following expansions of the path will be done:\n\n    - ``~`` to the current user's home directory\n    - ``$XDG_CACHE_DIR`` according to the XDG spec\n    - ``$XDG_CONFIG_DIR`` according to the XDG spec\n    - ``$XDG_DATA_DIR`` according to the XDG spec\n    - ``$XDG_MUSIC_DIR`` according to the XDG spec\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        expanded = path.expand_path(value)\n        validators.validate_required(value, self._required)\n        validators.validate_required(expanded, self._required)\n        if not value or expanded is None:\n            return None\n        return _ExpandedPath(value, expanded)\n\n    def serialize(self, value, display=False):\n        if value is None:\n            return \"\"\n        if isinstance(value, _ExpandedPath):\n            value = value.original\n        if isinstance(value, bytes):\n            value = value.decode(errors=\"surrogateescape\")\n        return value\n\n        ####cross_file_context:\n        [{'mopidy.config.validators.validate_required': '# TODO: add validate regexp?\\n\\n\\ndef validate_required(value, required):\\n    \"\"\"Validate that ``value`` is set if ``required``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize` on\\n    the raw string, _not_ the converted value.\\n    \"\"\"\\n    if required and not value:\\n        raise ValueError(\"must be set.\")\\n\\n\\ndef validate_choice(value, choices):\\n    \"\"\"Validate that ``value`` is one of the ``choices``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\\n    \"\"\"\\n    if choices is not None and value not in choices:\\n        names = \", \".join(repr(c) for c in choices)\\n        raise ValueError(f\"must be one of {names}, not {value}.\")\\n\\n\\ndef validate_minimum(value, minimum):\\n    \"\"\"Validate that ``value`` is at least ``minimum``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\\n    \"\"\"\\n    if minimum is not None and value < minimum:\\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\\n\\n\\ndef validate_maximum(value, maximum):\\n    \"\"\"Validate that ``value`` is at most ``maximum``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\\n    \"\"\"\\n    if maximum is not None and value > maximum:\\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\\n'}, {'mopidy.config.validators': '# TODO: add validate regexp?\\n\\n\\ndef validate_required(value, required):\\n    \"\"\"Validate that ``value`` is set if ``required``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize` on\\n    the raw string, _not_ the converted value.\\n    \"\"\"\\n    if required and not value:\\n        raise ValueError(\"must be set.\")\\n\\n\\ndef validate_choice(value, choices):\\n    \"\"\"Validate that ``value`` is one of the ``choices``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\\n    \"\"\"\\n    if choices is not None and value not in choices:\\n        names = \", \".join(repr(c) for c in choices)\\n        raise ValueError(f\"must be one of {names}, not {value}.\")\\n\\n\\ndef validate_minimum(value, minimum):\\n    \"\"\"Validate that ``value`` is at least ``minimum``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\\n    \"\"\"\\n    if minimum is not None and value < minimum:\\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\\n\\n\\ndef validate_maximum(value, maximum):\\n    \"\"\"Validate that ``value`` is at most ``maximum``\\n\\n    Normally called in :meth:`~mopidy.config.types.ConfigValue.deserialize`.\\n    \"\"\"\\n    if maximum is not None and value > maximum:\\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\\n'}]", "test_list": ["def test_deserialize_respects_optional_separator(self):\n    cv = types.Pair(optional_pair=True)\n    result = cv.deserialize('abc')\n    assert result == ('abc', 'abc')\n    result = cv.deserialize('abc|def')\n    assert result == ('abc', 'def')", "def test_deserialize_with_optional_custom_subtypes(self):\n    cv = types.Pair(subtypes=(types.String(), types.String(optional=True)))\n    result = cv.deserialize('abc|')\n    assert result == ('abc', None)\n    cv = types.Pair(subtypes=(types.String(optional=True), types.String()))\n    result = cv.deserialize('|def')\n    assert result == (None, 'def')\n    cv = types.Pair(subtypes=(types.String(optional=True), types.String(optional=True)))\n    result = cv.deserialize('|')\n    assert result == (None, None)", "@pytest.mark.parametrize('optional', (True, False))\n@pytest.mark.parametrize('optional_pair', (True, False))\n@pytest.mark.parametrize('sep', ('!', '@', '#', '$', '%', '^', '&', '*', '/', '\\\\'))\ndef test_deserialize_enforces_required_pair_values_with_custom_separator(self, optional: bool, optional_pair: bool, sep: str):\n    cv = types.Pair(optional=optional, optional_pair=optional_pair, separator=sep)\n    errmsg = re.escape('must be set')\n    with pytest.raises(ValueError, match=errmsg):\n        cv.deserialize(f'abc{sep}')\n    with pytest.raises(ValueError, match=errmsg):\n        cv.deserialize(f'{sep}def')\n    with pytest.raises(ValueError, match=errmsg):\n        cv.deserialize(f'abc|def{sep}')\n    with pytest.raises(ValueError, match=errmsg):\n        cv.deserialize(f'{sep}ghi|jkl')", "@pytest.mark.parametrize('sep', ('!', '@', '#', '$', '%', '^', '&', '*', '/', '\\\\'))\ndef test_deserialize_respects_optional_custom_separator(self, sep: str):\n    cv = types.Pair(optional_pair=True, separator=sep)\n    result = cv.deserialize(f'abc{sep}def')\n    assert result == ('abc', 'def')\n    result = cv.deserialize('abcdef')\n    assert result == ('abcdef', 'abcdef')\n    result = cv.deserialize('abc|def')\n    assert result == ('abc|def', 'abc|def')\n    result = cv.deserialize(f'|abc{sep}def|')\n    assert result == ('|abc', 'def|')", "def test_deserialize_conversion_success(self):\n    cv = types.Pair()\n    result = cv.deserialize('foo|bar')\n    assert result == ('foo', 'bar')\n    result = cv.deserialize('  foo|bar')\n    assert result == ('foo', 'bar')\n    result = cv.deserialize('foo|bar  ')\n    assert result == ('foo', 'bar')\n    result = cv.deserialize('  fo o | bar ')\n    assert result == ('fo o', 'bar')\n    result = cv.deserialize('foo|bar|baz')\n    assert result == ('foo', 'bar|baz')"], "requirements": {"Input-Output Conditions": {"requirement": "The 'deserialize' function should correctly handle input strings values contains '|', and return a tuple of deserialized values.", "unit_test": ["def test_deserialize_handles_string_input(self):\n    cv = types.Pair()\n    result = cv.deserialize('test|value')\n    assert result == ('test', 'value')\n    result = cv.deserialize('singlevalue')\n    assert result == ('singlevalue', 'singlevalue')"], "test": "tests/config/test_types.py::TestPair::test_deserialize_handles_string_input"}, "Exception Handling": {"requirement": "The 'deserialize' function should raise a ValueError with a clear error message when the input value does not contain the required separator and the optional pair flag is not set.", "unit_test": ["def test_deserialize_raises_error_without_separator(self):\n    cv = types.Pair(optional_pair=False)\n    with pytest.raises(ValueError, match='config value must include the separator'):\n        cv.deserialize('novalue')"], "test": "tests/config/test_types.py::TestPair::test_deserialize_raises_error_without_separator"}, "Edge Case Handling": {"requirement": "The 'deserialize' function should handle edge cases where the input value is empty or consists solely of whitespace, returning None in such cases.", "unit_test": ["def test_deserialize_handles_empty_input(self):\n    cv = types.Pair()\n    result = cv.deserialize('')\n    assert result is None\n    result = cv.deserialize('   ')\n    assert result is None"], "test": "tests/config/test_types.py::TestPair::test_deserialize_raises_error_without_separator"}, "Functionality Extension": {"requirement": "Extend the 'deserialize' function to support custom error messages for different validation failures.", "unit_test": ["def test_deserialize_custom_error_messages(self):\n    cv = types.Pair(optional_pair=False)\n    with pytest.raises(ValueError, match='Custom error: separator missing'):\n        cv.deserialize('novalue')"], "test": "tests/config/test_types.py::TestPair::test_deserialize_raises_error_without_separator"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'deserialize' function are annotated with appropriate type hints.", "unit_test": ["def test_deserialize_annotations(self):\n    assert hasattr(types.Pair.deserialize, '__annotations__')\n    assert types.Pair.deserialize.__annotations__['value'] == str\n    assert types.Pair.deserialize.__annotations__['return'] == tuple"], "test": "tests/config/test_types.py::TestPair::test_deserialize_annotations"}, "Code Complexity": {"requirement": "The 'deserialize' function should maintain a cyclomatic complexity of 10 or less to ensure readability and maintainability.", "unit_test": ["def test_deserialize_complexity(self):\n    from radon.complexity import cc_visit\n    source_code = inspect.getsource(types.Pair.deserialize)\n    complexity = cc_visit(source_code)\n    assert complexity[0].complexity <= 10"], "test": "tests/config/test_types.py::TestPair::test_deserialize_complexity"}, "Code Standard": {"requirement": "The 'deserialize' function should adhere to PEP 8 standards, including proper indentation, spacing, and naming conventions.", "unit_test": ["def test_deserialize_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/your/module.py'])\n    assert result.total_errors == 0, 'PEP 8 violations found'"], "test": "tests/config/test_types.py::TestPair::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'deserialize' function should utilize the necessary context 'mopidy.config.validators.validate_required'", "unit_test": ["def test_deserialize_uses_context(self):\n    cv = types.Pair()\n    assert hasattr(cv, '_optional_pair')\n    assert hasattr(cv, '_required')\n    assert hasattr(cv, '_separator')\n    assert hasattr(cv, '_subtypes')"], "test": "tests/config/test_types.py::TestPair::test_deserialize_uses_context"}, "Context Usage Correctness Verification": {"requirement": "The 'deserialize' function should correctly use the 'mopidy.config.validators.validate_required'", "unit_test": ["def test_deserialize_correct_context_usage(self):\n    cv = types.Pair(separator='|', subtypes=(types.String(), types.String()))\n    result = cv.deserialize('part1|part2')\n    assert result == ('part1', 'part2')"], "test": "tests/config/test_types.py::TestPair::test_deserialize_correct_context_usage"}}}
{"namespace": "mopidy.ext.Extension.get_data_dir", "type": "method", "project_path": "Multimedia/Mopidy", "completion_path": "Multimedia/Mopidy/mopidy/ext.py", "signature_position": [109, 109], "body_position": [117, 123], "dependency": {"intra_class": ["mopidy.ext.Extension.ext_name"], "intra_file": ["mopidy.ext.Config"], "cross_file": ["mopidy.internal.path.expand_path", "mopidy.internal.path.get_or_create_dir", "mopidy.internal.path"]}, "requirement": {"Functionality": "This function is a class method that gets or creates a data directory for the extension. It uses the Mopidy config object to determine the data directory path and creates the directory if it doesn't exist.", "Arguments": ":param cls: Class. The Extension class.\n:param config: Config. The Mopidy config object.\n:return: Path. The path to the data directory for the extension."}, "tests": ["tests/test_ext.py::TestExtension::test_get_data_dir_raises_error"], "indent": 8, "domain": "Multimedia", "code": "    def get_data_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create data directory for the extension.\n\n        Use this directory to store data that should be persistent.\n\n        :param config: the Mopidy config object\n        :returns: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "intra_context": "from __future__ import annotations\n\nimport logging\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, NamedTuple\n\nimport pkg_resources\n\nfrom mopidy import config as config_lib\n\nfrom mopidy.internal import path\n\nif TYPE_CHECKING:\n    from pathlib import Path\n    from typing import Any, Dict, Iterator, List, Optional, Type\n\n    from mopidy.commands import Command\n    from mopidy.config import ConfigSchema\n\n    Config = Dict[str, Dict[str, Any]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtensionData(NamedTuple):\n    extension: \"Extension\"\n    entry_point: Any\n    config_schema: ConfigSchema\n    config_defaults: Any\n    command: Optional[Command]\n\n\nclass Extension:\n\n    \"\"\"Base class for Mopidy extensions\"\"\"\n\n    dist_name: str\n    \"\"\"The extension's distribution name, as registered on PyPI\n\n    Example: ``Mopidy-Soundspot``\n    \"\"\"\n\n    ext_name: str\n    \"\"\"The extension's short name, as used in setup.py and as config section\n    name\n\n    Example: ``soundspot``\n    \"\"\"\n\n    version: str\n    \"\"\"The extension's version\n\n    Should match the :attr:`__version__` attribute on the extension's main\n    Python module and the version registered on PyPI.\n    \"\"\"\n\n    def get_default_config(self) -> str:\n        \"\"\"The extension's default config as a text string.\n\n        :returns: str\n        \"\"\"\n        raise NotImplementedError(\n            'Add at least a config section with \"enabled = true\"'\n        )\n\n    def get_config_schema(self) -> ConfigSchema:\n        \"\"\"The extension's config validation schema\n\n        :returns: :class:`~mopidy.config.schemas.ConfigSchema`\n        \"\"\"\n        schema = config_lib.ConfigSchema(self.ext_name)\n        schema[\"enabled\"] = config_lib.Boolean()\n        return schema\n\n    @classmethod\n    def get_cache_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create cache directory for the extension.\n\n        Use this directory to cache data that can safely be thrown away.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n\n    @classmethod\n    def get_config_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create configuration directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n\n    @classmethod\n###The function: get_data_dir###\n    def get_command(self) -> Optional[Command]:\n        \"\"\"Command to expose to command line users running ``mopidy``.\n\n        :returns:\n          Instance of a :class:`~mopidy.commands.Command` class.\n        \"\"\"\n        pass\n\n    def validate_environment(self) -> None:\n        \"\"\"Checks if the extension can run in the current environment.\n\n        Dependencies described by :file:`setup.py` are checked by Mopidy, so\n        you should not check their presence here.\n\n        If a problem is found, raise :exc:`~mopidy.exceptions.ExtensionError`\n        with a message explaining the issue.\n\n        :raises: :exc:`~mopidy.exceptions.ExtensionError`\n        :returns: :class:`None`\n        \"\"\"\n        pass\n\n    def setup(self, registry: \"Registry\") -> None:\n        \"\"\"\n        Register the extension's components in the extension :class:`Registry`.\n\n        For example, to register a backend::\n\n            def setup(self, registry):\n                from .backend import SoundspotBackend\n                registry.add('backend', SoundspotBackend)\n\n        See :class:`Registry` for a list of registry keys with a special\n        meaning. Mopidy will instantiate and start any classes registered under\n        the ``frontend`` and ``backend`` registry keys.\n\n        This method can also be used for other setup tasks not involving the\n        extension registry.\n\n        :param registry: the extension registry\n        :type registry: :class:`Registry`\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Registry(Mapping):\n\n    \"\"\"Registry of components provided by Mopidy extensions.\n\n    Passed to the :meth:`~Extension.setup` method of all extensions. The\n    registry can be used like a dict of string keys and lists.\n\n    Some keys have a special meaning, including, but not limited to:\n\n    - ``backend`` is used for Mopidy backend classes.\n    - ``frontend`` is used for Mopidy frontend classes.\n\n    Extensions can use the registry for allow other to extend the extension\n    itself. For example the ``Mopidy-Local`` historically used the\n    ``local:library`` key to allow other extensions to register library\n    providers for ``Mopidy-Local`` to use. Extensions should namespace\n    custom keys with the extension's :attr:`~Extension.ext_name`,\n    e.g. ``local:foo`` or ``http:bar``.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._registry: Dict[str, List[Type[Any]]] = {}\n\n    def add(self, name: str, cls: Type[Any]) -> None:\n        \"\"\"Add a component to the registry.\n\n        Multiple classes can be registered to the same name.\n        \"\"\"\n        self._registry.setdefault(name, []).append(cls)\n\n    def __getitem__(self, name: str) -> List[Type[Any]]:\n        return self._registry.setdefault(name, [])\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._registry)\n\n    def __len__(self) -> int:\n        return len(self._registry)\n\n\ndef load_extensions() -> List[ExtensionData]:\n    \"\"\"Find all installed extensions.\n\n    :returns: list of installed extensions\n    \"\"\"\n\n    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        logger.debug(\"Loading entry point: %s\", entry_point)\n        try:\n            extension_class = entry_point.resolve()\n        except Exception as e:\n            logger.exception(\n                f\"Failed to load extension {entry_point.name}: {e}\"\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                raise TypeError  # issubclass raises TypeError on non-class\n        except TypeError:\n            logger.error(\n                \"Entry point %s did not contain a valid extension\" \"class: %r\",\n                entry_point.name,\n                extension_class,\n            )\n            continue\n\n        try:\n            extension = extension_class()\n            # Ensure required extension attributes are present after try block\n            _ = extension.dist_name\n            _ = extension.ext_name\n            _ = extension.version\n            extension_data = ExtensionData(\n                entry_point=entry_point,\n                extension=extension,\n                config_schema=extension.get_config_schema(),\n                config_defaults=extension.get_default_config(),\n                command=extension.get_command(),\n            )\n        except Exception:\n            logger.exception(\n                \"Setup of extension from entry point %s failed, \"\n                \"ignoring extension.\",\n                entry_point.name,\n            )\n            continue\n\n        installed_extensions.append(extension_data)\n\n        logger.debug(\n            \"Loaded extension: %s %s\", extension.dist_name, extension.version\n        )\n\n    names = (ed.extension.ext_name for ed in installed_extensions)\n    logger.debug(\"Discovered extensions: %s\", \", \".join(names))\n    return installed_extensions\n\n\ndef validate_extension_data(data: ExtensionData) -> bool:\n    \"\"\"Verify extension's dependencies and environment.\n\n    :param extensions: an extension to check\n    :returns: if extension should be run\n    \"\"\"\n\n    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n", "cross_context": [{"mopidy.internal.path.expand_path": "import logging\nimport pathlib\nimport re\nimport urllib\n\nfrom mopidy.internal import xdg\n\nlogger = logging.getLogger(__name__)\n\n\nXDG_DIRS = xdg.get_dirs()\n\n\ndef get_or_create_dir(dir_path):\n    dir_path = expand_path(dir_path)\n    if dir_path.is_file():\n        raise OSError(\n            f\"A file with the same name as the desired dir, \"\n            f\"{dir_path!r}, already exists.\"\n        )\n    elif not dir_path.is_dir():\n        logger.info(f\"Creating dir {dir_path.as_uri()}\")\n        dir_path.mkdir(mode=0o755, parents=True)\n    return dir_path\n\n\ndef get_or_create_file(file_path, mkdir=True, content=None):\n    file_path = expand_path(file_path)\n    if isinstance(content, str):\n        content = content.encode()\n    if mkdir:\n        get_or_create_dir(file_path.parent)\n    if not file_path.is_file():\n        logger.info(f\"Creating file {file_path.as_uri()}\")\n        file_path.touch(exist_ok=False)\n        if content is not None:\n            file_path.write_bytes(content)\n    return file_path\n\n\ndef get_unix_socket_path(socket_path):\n    match = re.search(\"^unix:(.*)\", socket_path)\n    if not match:\n        return None\n    return match.group(1)\n\n\ndef path_to_uri(path):\n    \"\"\"\n    Convert OS specific path to file:// URI.\n\n    Accepts either unicode strings or bytestrings. The encoding of any\n    bytestring will be maintained so that :func:`uri_to_path` can return the\n    same bytestring.\n\n    Returns a file:// URI as an unicode string.\n    \"\"\"\n    return pathlib.Path(path).as_uri()\n\n\ndef uri_to_path(uri):\n    \"\"\"\n    Convert an URI to a OS specific path.\n    \"\"\"\n    bytes_path = urllib.parse.unquote_to_bytes(urllib.parse.urlsplit(uri).path)\n    unicode_path = bytes_path.decode(errors=\"surrogateescape\")\n    return pathlib.Path(unicode_path)\n\n\ndef expand_path(path):\n    if isinstance(path, bytes):\n        path = path.decode(errors=\"surrogateescape\")\n    path = str(pathlib.Path(path))\n\n    for xdg_var, xdg_dir in XDG_DIRS.items():\n        path = path.replace(\"$\" + xdg_var, str(xdg_dir))\n    if \"$\" in path:\n        return None\n\n    return pathlib.Path(path).expanduser().resolve()\n\n\ndef is_path_inside_base_dir(path, base_path):\n    if isinstance(path, bytes):\n        path = path.decode(errors=\"surrogateescape\")\n    if isinstance(base_path, bytes):\n        base_path = base_path.decode(errors=\"surrogateescape\")\n\n    path = pathlib.Path(path).resolve()\n    base_path = pathlib.Path(base_path).resolve()\n\n    if path.is_file():\n        # Use dir of file for prefix comparision, so we don't accept\n        # /tmp/foo.m3u as being inside /tmp/foo, simply because they have a\n        # common prefix, /tmp/foo, which matches the base path, /tmp/foo.\n        path = path.parent\n\n    # Check if dir of file is the base path or a subdir\n    try:\n        path.relative_to(base_path)\n    except ValueError:\n        return False\n    else:\n        return True\n"}, {"mopidy.internal.path.get_or_create_dir": "import logging\nimport pathlib\nimport re\nimport urllib\n\nfrom mopidy.internal import xdg\n\nlogger = logging.getLogger(__name__)\n\n\nXDG_DIRS = xdg.get_dirs()\n\n\ndef get_or_create_dir(dir_path):\n    dir_path = expand_path(dir_path)\n    if dir_path.is_file():\n        raise OSError(\n            f\"A file with the same name as the desired dir, \"\n            f\"{dir_path!r}, already exists.\"\n        )\n    elif not dir_path.is_dir():\n        logger.info(f\"Creating dir {dir_path.as_uri()}\")\n        dir_path.mkdir(mode=0o755, parents=True)\n    return dir_path\n\n\ndef get_or_create_file(file_path, mkdir=True, content=None):\n    file_path = expand_path(file_path)\n    if isinstance(content, str):\n        content = content.encode()\n    if mkdir:\n        get_or_create_dir(file_path.parent)\n    if not file_path.is_file():\n        logger.info(f\"Creating file {file_path.as_uri()}\")\n        file_path.touch(exist_ok=False)\n        if content is not None:\n            file_path.write_bytes(content)\n    return file_path\n\n\ndef get_unix_socket_path(socket_path):\n    match = re.search(\"^unix:(.*)\", socket_path)\n    if not match:\n        return None\n    return match.group(1)\n\n\ndef path_to_uri(path):\n    \"\"\"\n    Convert OS specific path to file:// URI.\n\n    Accepts either unicode strings or bytestrings. The encoding of any\n    bytestring will be maintained so that :func:`uri_to_path` can return the\n    same bytestring.\n\n    Returns a file:// URI as an unicode string.\n    \"\"\"\n    return pathlib.Path(path).as_uri()\n\n\ndef uri_to_path(uri):\n    \"\"\"\n    Convert an URI to a OS specific path.\n    \"\"\"\n    bytes_path = urllib.parse.unquote_to_bytes(urllib.parse.urlsplit(uri).path)\n    unicode_path = bytes_path.decode(errors=\"surrogateescape\")\n    return pathlib.Path(unicode_path)\n\n\ndef expand_path(path):\n    if isinstance(path, bytes):\n        path = path.decode(errors=\"surrogateescape\")\n    path = str(pathlib.Path(path))\n\n    for xdg_var, xdg_dir in XDG_DIRS.items():\n        path = path.replace(\"$\" + xdg_var, str(xdg_dir))\n    if \"$\" in path:\n        return None\n\n    return pathlib.Path(path).expanduser().resolve()\n\n\ndef is_path_inside_base_dir(path, base_path):\n    if isinstance(path, bytes):\n        path = path.decode(errors=\"surrogateescape\")\n    if isinstance(base_path, bytes):\n        base_path = base_path.decode(errors=\"surrogateescape\")\n\n    path = pathlib.Path(path).resolve()\n    base_path = pathlib.Path(base_path).resolve()\n\n    if path.is_file():\n        # Use dir of file for prefix comparision, so we don't accept\n        # /tmp/foo.m3u as being inside /tmp/foo, simply because they have a\n        # common prefix, /tmp/foo, which matches the base path, /tmp/foo.\n        path = path.parent\n\n    # Check if dir of file is the base path or a subdir\n    try:\n        path.relative_to(base_path)\n    except ValueError:\n        return False\n    else:\n        return True\n"}, {"mopidy.internal.path": "import logging\nimport pathlib\nimport re\nimport urllib\n\nfrom mopidy.internal import xdg\n\nlogger = logging.getLogger(__name__)\n\n\nXDG_DIRS = xdg.get_dirs()\n\n\ndef get_or_create_dir(dir_path):\n    dir_path = expand_path(dir_path)\n    if dir_path.is_file():\n        raise OSError(\n            f\"A file with the same name as the desired dir, \"\n            f\"{dir_path!r}, already exists.\"\n        )\n    elif not dir_path.is_dir():\n        logger.info(f\"Creating dir {dir_path.as_uri()}\")\n        dir_path.mkdir(mode=0o755, parents=True)\n    return dir_path\n\n\ndef get_or_create_file(file_path, mkdir=True, content=None):\n    file_path = expand_path(file_path)\n    if isinstance(content, str):\n        content = content.encode()\n    if mkdir:\n        get_or_create_dir(file_path.parent)\n    if not file_path.is_file():\n        logger.info(f\"Creating file {file_path.as_uri()}\")\n        file_path.touch(exist_ok=False)\n        if content is not None:\n            file_path.write_bytes(content)\n    return file_path\n\n\ndef get_unix_socket_path(socket_path):\n    match = re.search(\"^unix:(.*)\", socket_path)\n    if not match:\n        return None\n    return match.group(1)\n\n\ndef path_to_uri(path):\n    \"\"\"\n    Convert OS specific path to file:// URI.\n\n    Accepts either unicode strings or bytestrings. The encoding of any\n    bytestring will be maintained so that :func:`uri_to_path` can return the\n    same bytestring.\n\n    Returns a file:// URI as an unicode string.\n    \"\"\"\n    return pathlib.Path(path).as_uri()\n\n\ndef uri_to_path(uri):\n    \"\"\"\n    Convert an URI to a OS specific path.\n    \"\"\"\n    bytes_path = urllib.parse.unquote_to_bytes(urllib.parse.urlsplit(uri).path)\n    unicode_path = bytes_path.decode(errors=\"surrogateescape\")\n    return pathlib.Path(unicode_path)\n\n\ndef expand_path(path):\n    if isinstance(path, bytes):\n        path = path.decode(errors=\"surrogateescape\")\n    path = str(pathlib.Path(path))\n\n    for xdg_var, xdg_dir in XDG_DIRS.items():\n        path = path.replace(\"$\" + xdg_var, str(xdg_dir))\n    if \"$\" in path:\n        return None\n\n    return pathlib.Path(path).expanduser().resolve()\n\n\ndef is_path_inside_base_dir(path, base_path):\n    if isinstance(path, bytes):\n        path = path.decode(errors=\"surrogateescape\")\n    if isinstance(base_path, bytes):\n        base_path = base_path.decode(errors=\"surrogateescape\")\n\n    path = pathlib.Path(path).resolve()\n    base_path = pathlib.Path(base_path).resolve()\n\n    if path.is_file():\n        # Use dir of file for prefix comparision, so we don't accept\n        # /tmp/foo.m3u as being inside /tmp/foo, simply because they have a\n        # common prefix, /tmp/foo, which matches the base path, /tmp/foo.\n        path = path.parent\n\n    # Check if dir of file is the base path or a subdir\n    try:\n        path.relative_to(base_path)\n    except ValueError:\n        return False\n    else:\n        return True\n"}], "prompt": "Please write a python function called 'get_data_dir' base the context. This function is a class method that gets or creates a data directory for the extension. It uses the Mopidy config object to determine the data directory path and creates the directory if it doesn't exist.:param cls: Class. The Extension class.\n:param config: Config. The Mopidy config object.\n:return: Path. The path to the data directory for the extension..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        from __future__ import annotations\n\nimport logging\nfrom collections.abc import Mapping\nfrom typing import TYPE_CHECKING, NamedTuple\n\nimport pkg_resources\n\nfrom mopidy import config as config_lib\n\nfrom mopidy.internal import path\n\nif TYPE_CHECKING:\n    from pathlib import Path\n    from typing import Any, Dict, Iterator, List, Optional, Type\n\n    from mopidy.commands import Command\n    from mopidy.config import ConfigSchema\n\n    Config = Dict[str, Dict[str, Any]]\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtensionData(NamedTuple):\n    extension: \"Extension\"\n    entry_point: Any\n    config_schema: ConfigSchema\n    config_defaults: Any\n    command: Optional[Command]\n\n\nclass Extension:\n\n    \"\"\"Base class for Mopidy extensions\"\"\"\n\n    dist_name: str\n    \"\"\"The extension's distribution name, as registered on PyPI\n\n    Example: ``Mopidy-Soundspot``\n    \"\"\"\n\n    ext_name: str\n    \"\"\"The extension's short name, as used in setup.py and as config section\n    name\n\n    Example: ``soundspot``\n    \"\"\"\n\n    version: str\n    \"\"\"The extension's version\n\n    Should match the :attr:`__version__` attribute on the extension's main\n    Python module and the version registered on PyPI.\n    \"\"\"\n\n    def get_default_config(self) -> str:\n        \"\"\"The extension's default config as a text string.\n\n        :returns: str\n        \"\"\"\n        raise NotImplementedError(\n            'Add at least a config section with \"enabled = true\"'\n        )\n\n    def get_config_schema(self) -> ConfigSchema:\n        \"\"\"The extension's config validation schema\n\n        :returns: :class:`~mopidy.config.schemas.ConfigSchema`\n        \"\"\"\n        schema = config_lib.ConfigSchema(self.ext_name)\n        schema[\"enabled\"] = config_lib.Boolean()\n        return schema\n\n    @classmethod\n    def get_cache_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create cache directory for the extension.\n\n        Use this directory to cache data that can safely be thrown away.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path\n\n    @classmethod\n    def get_config_dir(cls, config: Config) -> Path:\n        \"\"\"Get or create configuration directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n\n    @classmethod\n###The function: get_data_dir###\n    def get_command(self) -> Optional[Command]:\n        \"\"\"Command to expose to command line users running ``mopidy``.\n\n        :returns:\n          Instance of a :class:`~mopidy.commands.Command` class.\n        \"\"\"\n        pass\n\n    def validate_environment(self) -> None:\n        \"\"\"Checks if the extension can run in the current environment.\n\n        Dependencies described by :file:`setup.py` are checked by Mopidy, so\n        you should not check their presence here.\n\n        If a problem is found, raise :exc:`~mopidy.exceptions.ExtensionError`\n        with a message explaining the issue.\n\n        :raises: :exc:`~mopidy.exceptions.ExtensionError`\n        :returns: :class:`None`\n        \"\"\"\n        pass\n\n    def setup(self, registry: \"Registry\") -> None:\n        \"\"\"\n        Register the extension's components in the extension :class:`Registry`.\n\n        For example, to register a backend::\n\n            def setup(self, registry):\n                from .backend import SoundspotBackend\n                registry.add('backend', SoundspotBackend)\n\n        See :class:`Registry` for a list of registry keys with a special\n        meaning. Mopidy will instantiate and start any classes registered under\n        the ``frontend`` and ``backend`` registry keys.\n\n        This method can also be used for other setup tasks not involving the\n        extension registry.\n\n        :param registry: the extension registry\n        :type registry: :class:`Registry`\n        \"\"\"\n        raise NotImplementedError\n\n\nclass Registry(Mapping):\n\n    \"\"\"Registry of components provided by Mopidy extensions.\n\n    Passed to the :meth:`~Extension.setup` method of all extensions. The\n    registry can be used like a dict of string keys and lists.\n\n    Some keys have a special meaning, including, but not limited to:\n\n    - ``backend`` is used for Mopidy backend classes.\n    - ``frontend`` is used for Mopidy frontend classes.\n\n    Extensions can use the registry for allow other to extend the extension\n    itself. For example the ``Mopidy-Local`` historically used the\n    ``local:library`` key to allow other extensions to register library\n    providers for ``Mopidy-Local`` to use. Extensions should namespace\n    custom keys with the extension's :attr:`~Extension.ext_name`,\n    e.g. ``local:foo`` or ``http:bar``.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._registry: Dict[str, List[Type[Any]]] = {}\n\n    def add(self, name: str, cls: Type[Any]) -> None:\n        \"\"\"Add a component to the registry.\n\n        Multiple classes can be registered to the same name.\n        \"\"\"\n        self._registry.setdefault(name, []).append(cls)\n\n    def __getitem__(self, name: str) -> List[Type[Any]]:\n        return self._registry.setdefault(name, [])\n\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._registry)\n\n    def __len__(self) -> int:\n        return len(self._registry)\n\n\ndef load_extensions() -> List[ExtensionData]:\n    \"\"\"Find all installed extensions.\n\n    :returns: list of installed extensions\n    \"\"\"\n\n    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        logger.debug(\"Loading entry point: %s\", entry_point)\n        try:\n            extension_class = entry_point.resolve()\n        except Exception as e:\n            logger.exception(\n                f\"Failed to load extension {entry_point.name}: {e}\"\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                raise TypeError  # issubclass raises TypeError on non-class\n        except TypeError:\n            logger.error(\n                \"Entry point %s did not contain a valid extension\" \"class: %r\",\n                entry_point.name,\n                extension_class,\n            )\n            continue\n\n        try:\n            extension = extension_class()\n            # Ensure required extension attributes are present after try block\n            _ = extension.dist_name\n            _ = extension.ext_name\n            _ = extension.version\n            extension_data = ExtensionData(\n                entry_point=entry_point,\n                extension=extension,\n                config_schema=extension.get_config_schema(),\n                config_defaults=extension.get_default_config(),\n                command=extension.get_command(),\n            )\n        except Exception:\n            logger.exception(\n                \"Setup of extension from entry point %s failed, \"\n                \"ignoring extension.\",\n                entry_point.name,\n            )\n            continue\n\n        installed_extensions.append(extension_data)\n\n        logger.debug(\n            \"Loaded extension: %s %s\", extension.dist_name, extension.version\n        )\n\n    names = (ed.extension.ext_name for ed in installed_extensions)\n    logger.debug(\"Discovered extensions: %s\", \", \".join(names))\n    return installed_extensions\n\n\ndef validate_extension_data(data: ExtensionData) -> bool:\n    \"\"\"Verify extension's dependencies and environment.\n\n    :param extensions: an extension to check\n    :returns: if extension should be run\n    \"\"\"\n\n    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True\n\n        ####cross_file_context:\n        [{'mopidy.internal.path.expand_path': 'import logging\\nimport pathlib\\nimport re\\nimport urllib\\n\\nfrom mopidy.internal import xdg\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nXDG_DIRS = xdg.get_dirs()\\n\\n\\ndef get_or_create_dir(dir_path):\\n    dir_path = expand_path(dir_path)\\n    if dir_path.is_file():\\n        raise OSError(\\n            f\"A file with the same name as the desired dir, \"\\n            f\"{dir_path!r}, already exists.\"\\n        )\\n    elif not dir_path.is_dir():\\n        logger.info(f\"Creating dir {dir_path.as_uri()}\")\\n        dir_path.mkdir(mode=0o755, parents=True)\\n    return dir_path\\n\\n\\ndef get_or_create_file(file_path, mkdir=True, content=None):\\n    file_path = expand_path(file_path)\\n    if isinstance(content, str):\\n        content = content.encode()\\n    if mkdir:\\n        get_or_create_dir(file_path.parent)\\n    if not file_path.is_file():\\n        logger.info(f\"Creating file {file_path.as_uri()}\")\\n        file_path.touch(exist_ok=False)\\n        if content is not None:\\n            file_path.write_bytes(content)\\n    return file_path\\n\\n\\ndef get_unix_socket_path(socket_path):\\n    match = re.search(\"^unix:(.*)\", socket_path)\\n    if not match:\\n        return None\\n    return match.group(1)\\n\\n\\ndef path_to_uri(path):\\n    \"\"\"\\n    Convert OS specific path to file:// URI.\\n\\n    Accepts either unicode strings or bytestrings. The encoding of any\\n    bytestring will be maintained so that :func:`uri_to_path` can return the\\n    same bytestring.\\n\\n    Returns a file:// URI as an unicode string.\\n    \"\"\"\\n    return pathlib.Path(path).as_uri()\\n\\n\\ndef uri_to_path(uri):\\n    \"\"\"\\n    Convert an URI to a OS specific path.\\n    \"\"\"\\n    bytes_path = urllib.parse.unquote_to_bytes(urllib.parse.urlsplit(uri).path)\\n    unicode_path = bytes_path.decode(errors=\"surrogateescape\")\\n    return pathlib.Path(unicode_path)\\n\\n\\ndef expand_path(path):\\n    if isinstance(path, bytes):\\n        path = path.decode(errors=\"surrogateescape\")\\n    path = str(pathlib.Path(path))\\n\\n    for xdg_var, xdg_dir in XDG_DIRS.items():\\n        path = path.replace(\"$\" + xdg_var, str(xdg_dir))\\n    if \"$\" in path:\\n        return None\\n\\n    return pathlib.Path(path).expanduser().resolve()\\n\\n\\ndef is_path_inside_base_dir(path, base_path):\\n    if isinstance(path, bytes):\\n        path = path.decode(errors=\"surrogateescape\")\\n    if isinstance(base_path, bytes):\\n        base_path = base_path.decode(errors=\"surrogateescape\")\\n\\n    path = pathlib.Path(path).resolve()\\n    base_path = pathlib.Path(base_path).resolve()\\n\\n    if path.is_file():\\n        # Use dir of file for prefix comparision, so we don\\'t accept\\n        # /tmp/foo.m3u as being inside /tmp/foo, simply because they have a\\n        # common prefix, /tmp/foo, which matches the base path, /tmp/foo.\\n        path = path.parent\\n\\n    # Check if dir of file is the base path or a subdir\\n    try:\\n        path.relative_to(base_path)\\n    except ValueError:\\n        return False\\n    else:\\n        return True\\n'}, {'mopidy.internal.path.get_or_create_dir': 'import logging\\nimport pathlib\\nimport re\\nimport urllib\\n\\nfrom mopidy.internal import xdg\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nXDG_DIRS = xdg.get_dirs()\\n\\n\\ndef get_or_create_dir(dir_path):\\n    dir_path = expand_path(dir_path)\\n    if dir_path.is_file():\\n        raise OSError(\\n            f\"A file with the same name as the desired dir, \"\\n            f\"{dir_path!r}, already exists.\"\\n        )\\n    elif not dir_path.is_dir():\\n        logger.info(f\"Creating dir {dir_path.as_uri()}\")\\n        dir_path.mkdir(mode=0o755, parents=True)\\n    return dir_path\\n\\n\\ndef get_or_create_file(file_path, mkdir=True, content=None):\\n    file_path = expand_path(file_path)\\n    if isinstance(content, str):\\n        content = content.encode()\\n    if mkdir:\\n        get_or_create_dir(file_path.parent)\\n    if not file_path.is_file():\\n        logger.info(f\"Creating file {file_path.as_uri()}\")\\n        file_path.touch(exist_ok=False)\\n        if content is not None:\\n            file_path.write_bytes(content)\\n    return file_path\\n\\n\\ndef get_unix_socket_path(socket_path):\\n    match = re.search(\"^unix:(.*)\", socket_path)\\n    if not match:\\n        return None\\n    return match.group(1)\\n\\n\\ndef path_to_uri(path):\\n    \"\"\"\\n    Convert OS specific path to file:// URI.\\n\\n    Accepts either unicode strings or bytestrings. The encoding of any\\n    bytestring will be maintained so that :func:`uri_to_path` can return the\\n    same bytestring.\\n\\n    Returns a file:// URI as an unicode string.\\n    \"\"\"\\n    return pathlib.Path(path).as_uri()\\n\\n\\ndef uri_to_path(uri):\\n    \"\"\"\\n    Convert an URI to a OS specific path.\\n    \"\"\"\\n    bytes_path = urllib.parse.unquote_to_bytes(urllib.parse.urlsplit(uri).path)\\n    unicode_path = bytes_path.decode(errors=\"surrogateescape\")\\n    return pathlib.Path(unicode_path)\\n\\n\\ndef expand_path(path):\\n    if isinstance(path, bytes):\\n        path = path.decode(errors=\"surrogateescape\")\\n    path = str(pathlib.Path(path))\\n\\n    for xdg_var, xdg_dir in XDG_DIRS.items():\\n        path = path.replace(\"$\" + xdg_var, str(xdg_dir))\\n    if \"$\" in path:\\n        return None\\n\\n    return pathlib.Path(path).expanduser().resolve()\\n\\n\\ndef is_path_inside_base_dir(path, base_path):\\n    if isinstance(path, bytes):\\n        path = path.decode(errors=\"surrogateescape\")\\n    if isinstance(base_path, bytes):\\n        base_path = base_path.decode(errors=\"surrogateescape\")\\n\\n    path = pathlib.Path(path).resolve()\\n    base_path = pathlib.Path(base_path).resolve()\\n\\n    if path.is_file():\\n        # Use dir of file for prefix comparision, so we don\\'t accept\\n        # /tmp/foo.m3u as being inside /tmp/foo, simply because they have a\\n        # common prefix, /tmp/foo, which matches the base path, /tmp/foo.\\n        path = path.parent\\n\\n    # Check if dir of file is the base path or a subdir\\n    try:\\n        path.relative_to(base_path)\\n    except ValueError:\\n        return False\\n    else:\\n        return True\\n'}, {'mopidy.internal.path': 'import logging\\nimport pathlib\\nimport re\\nimport urllib\\n\\nfrom mopidy.internal import xdg\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nXDG_DIRS = xdg.get_dirs()\\n\\n\\ndef get_or_create_dir(dir_path):\\n    dir_path = expand_path(dir_path)\\n    if dir_path.is_file():\\n        raise OSError(\\n            f\"A file with the same name as the desired dir, \"\\n            f\"{dir_path!r}, already exists.\"\\n        )\\n    elif not dir_path.is_dir():\\n        logger.info(f\"Creating dir {dir_path.as_uri()}\")\\n        dir_path.mkdir(mode=0o755, parents=True)\\n    return dir_path\\n\\n\\ndef get_or_create_file(file_path, mkdir=True, content=None):\\n    file_path = expand_path(file_path)\\n    if isinstance(content, str):\\n        content = content.encode()\\n    if mkdir:\\n        get_or_create_dir(file_path.parent)\\n    if not file_path.is_file():\\n        logger.info(f\"Creating file {file_path.as_uri()}\")\\n        file_path.touch(exist_ok=False)\\n        if content is not None:\\n            file_path.write_bytes(content)\\n    return file_path\\n\\n\\ndef get_unix_socket_path(socket_path):\\n    match = re.search(\"^unix:(.*)\", socket_path)\\n    if not match:\\n        return None\\n    return match.group(1)\\n\\n\\ndef path_to_uri(path):\\n    \"\"\"\\n    Convert OS specific path to file:// URI.\\n\\n    Accepts either unicode strings or bytestrings. The encoding of any\\n    bytestring will be maintained so that :func:`uri_to_path` can return the\\n    same bytestring.\\n\\n    Returns a file:// URI as an unicode string.\\n    \"\"\"\\n    return pathlib.Path(path).as_uri()\\n\\n\\ndef uri_to_path(uri):\\n    \"\"\"\\n    Convert an URI to a OS specific path.\\n    \"\"\"\\n    bytes_path = urllib.parse.unquote_to_bytes(urllib.parse.urlsplit(uri).path)\\n    unicode_path = bytes_path.decode(errors=\"surrogateescape\")\\n    return pathlib.Path(unicode_path)\\n\\n\\ndef expand_path(path):\\n    if isinstance(path, bytes):\\n        path = path.decode(errors=\"surrogateescape\")\\n    path = str(pathlib.Path(path))\\n\\n    for xdg_var, xdg_dir in XDG_DIRS.items():\\n        path = path.replace(\"$\" + xdg_var, str(xdg_dir))\\n    if \"$\" in path:\\n        return None\\n\\n    return pathlib.Path(path).expanduser().resolve()\\n\\n\\ndef is_path_inside_base_dir(path, base_path):\\n    if isinstance(path, bytes):\\n        path = path.decode(errors=\"surrogateescape\")\\n    if isinstance(base_path, bytes):\\n        base_path = base_path.decode(errors=\"surrogateescape\")\\n\\n    path = pathlib.Path(path).resolve()\\n    base_path = pathlib.Path(base_path).resolve()\\n\\n    if path.is_file():\\n        # Use dir of file for prefix comparision, so we don\\'t accept\\n        # /tmp/foo.m3u as being inside /tmp/foo, simply because they have a\\n        # common prefix, /tmp/foo, which matches the base path, /tmp/foo.\\n        path = path.parent\\n\\n    # Check if dir of file is the base path or a subdir\\n    try:\\n        path.relative_to(base_path)\\n    except ValueError:\\n        return False\\n    else:\\n        return True\\n'}]", "test_list": ["def test_get_data_dir_raises_error(self, extension):\n    config = {'core': {'data_dir': '/tmp'}}\n    with pytest.raises(AttributeError):\n        ext.Extension.get_data_dir(config)"], "requirements": {"Input-Output Conditions": {"requirement": "The 'get_data_dir' method should return a pathlib.Path object representing the data directory path based on the Mopidy config object.", "unit_test": ["def test_get_data_dir_returns_path(self, extension):\n    config = {'core': {'data_dir': '/tmp'}}\n    path = ext.Extension.get_data_dir(config)\n    assert isinstance(path, pathlib.Path)"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_returns_path"}, "Exception Handling": {"requirement": "The 'get_data_dir' method should raise an AttributeError if the 'ext_name' attribute is not set on the Extension class.", "unit_test": ["def test_get_data_dir_raises_attribute_error(self, extension):\n    config = {'core': {'data_dir': '/tmp'}}\n    extension.ext_name = None\n    with pytest.raises(AttributeError):\n        ext.Extension.get_data_dir(config)"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_raises_attribute_error"}, "Edge Case Handling": {"requirement": "The 'get_data_dir' method should handle cases where the data directory path already exists as a file by raising an OSError.", "unit_test": ["def test_get_data_dir_raises_os_error_if_file_exists(self, extension, mocker):\n    config = {'core': {'data_dir': '/tmp'}}\n    mocker.patch('pathlib.Path.is_file', return_value=True)\n    with pytest.raises(OSError):\n        ext.Extension.get_data_dir(config)"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_raises_os_error_if_file_exists"}, "Functionality Extension": {"requirement": "The 'get_data_dir' method should log an info message when creating a new directory.", "unit_test": ["def test_get_data_dir_logs_info_on_creation(self, extension, caplog):\n    config = {'core': {'data_dir': '/tmp'}}\n    with caplog.at_level(logging.INFO):\n        ext.Extension.get_data_dir(config)\n    assert 'Creating dir' in caplog.text"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_logs_info_on_creation"}, "Annotation Coverage": {"requirement": "The 'get_data_dir' method should have type annotations for all parameters and return types.", "unit_test": ["def test_get_data_dir_annotations(self):\n    from pathlib import Path\n    annotations = ext.Extension.get_data_dir.__annotations__\n    assert annotations['cls'] == 'Type[Extension]'\n    assert annotations['return'] == Path"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_annotations"}, "Code Complexity": {"requirement": "The 'get_data_dir' method should maintain a cyclomatic complexity of 3 or less.", "unit_test": ["def test_get_data_dir_cyclomatic_complexity(self):\n    from radon.complexity import cc_visit\n    code = inspect.getsource(ext.Extension.get_data_dir)\n    complexity = cc_visit(code)\n    assert complexity[0].complexity <= 3"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_cyclomatic_complexity"}, "Code Standard": {"requirement": "The 'get_data_dir' method should adhere to PEP 8 style guidelines.", "unit_test": ["def test_get_data_dir_pep8_compliance(self):\n    from flake8.api import legacy as flake8\n    style_guide = flake8.get_style_guide()\n    report = style_guide.check_files(['path/to/extension.py'])\n    assert report.total_errors == 0"], "test": "tests/test_ext.py::TestExtension::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'get_data_dir' method should utilize the 'path.expand_path' and 'path.get_or_create_dir' functions from the mopidy.internal.path module.", "unit_test": ["def test_get_data_dir_uses_context_functions(self, mocker):\n    expand_path_mock = mocker.patch('mopidy.internal.path.expand_path')\n    get_or_create_dir_mock = mocker.patch('mopidy.internal.path.get_or_create_dir')\n    config = {'core': {'data_dir': '/tmp'}}\n    ext.Extension.get_data_dir(config)\n    expand_path_mock.assert_called_once()\n    get_or_create_dir_mock.assert_called_once()"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_uses_context_functions"}, "Context Usage Correctness Verification": {"requirement": "The 'get_data_dir' method should correctly use the 'ext_name' attribute to construct the data directory path.", "unit_test": ["def test_get_data_dir_correctly_uses_ext_name(self, extension):\n    config = {'core': {'data_dir': '/tmp'}}\n    extension.ext_name = 'test_extension'\n    path = ext.Extension.get_data_dir(config)\n    assert str(path).endswith('test_extension')"], "test": "tests/test_ext.py::TestExtension::test_get_data_dir_correctly_uses_ext_name"}}}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "type": "method", "project_path": "Security/diffprivlib", "completion_path": "Security/diffprivlib/diffprivlib/models/linear_regression.py", "signature_position": [240, 240], "body_position": [260, 317], "dependency": {"intra_class": ["diffprivlib.models.linear_regression.LinearRegression._obj_coefs", "diffprivlib.models.linear_regression.LinearRegression._preprocess_data", "diffprivlib.models.linear_regression.LinearRegression.accountant", "diffprivlib.models.linear_regression.LinearRegression.bounds_X", "diffprivlib.models.linear_regression.LinearRegression.bounds_y", "diffprivlib.models.linear_regression.LinearRegression.coef_", "diffprivlib.models.linear_regression.LinearRegression.epsilon", "diffprivlib.models.linear_regression.LinearRegression.random_state"], "intra_file": ["diffprivlib.models.linear_regression._construct_regression_obj"], "cross_file": ["diffprivlib.accountant.BudgetAccountant.check", "diffprivlib.accountant.BudgetAccountant.spend", "diffprivlib.utils.PrivacyLeakWarning", "diffprivlib.utils.check_random_state", "diffprivlib.validation.DiffprivlibMixin._check_bounds", "diffprivlib.validation.DiffprivlibMixin._validate_params", "diffprivlib.validation.DiffprivlibMixin._warn_unused_args"]}, "requirement": {"Functionality": "This function fits a linear regression model to the given training data. It preprocesses the data, determines the bounds, constructs regression objects, and optimizes the coefficients using the minimize function. It also sets the intercept and updates the accountant's spending.", "Arguments": ":param self: LinearRegression. An instance of the LinearRegression class.\n:param X: array-like or sparse matrix. The training data with shape (n_samples, n_features).\n:param y: array_like. The target values with shape (n_samples, n_targets).\n:param sample_weight: ignored. Ignored by diffprivlib. Present for consistency with sklearn API.\n:return: self. An instance of the LinearRegression class."}, "tests": ["tests/models/test_LinearRegression.py::TestLinearRegression::test_large_data", "tests/models/test_LinearRegression.py::TestLinearRegression::test_simple", "tests/models/test_LinearRegression.py::TestLinearRegression::test_accountant", "tests/models/test_LinearRegression.py::TestLinearRegression::test_multiple_targets", "tests/models/test_LinearRegression.py::TestLinearRegression::test_sample_weight_warning"], "indent": 8, "domain": "Security", "code": "    def fit(self, X, y, sample_weight=None):\n        \"\"\"\n        Fit linear model.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape (n_samples, n_features)\n            Training data\n\n        y : array_like, shape (n_samples, n_targets)\n            Target values.  Will be cast to X's dtype if necessary\n\n        sample_weight : ignored\n            Ignored by diffprivlib.  Present for consistency with sklearn API.\n\n        Returns\n        -------\n        self : returns an instance of self.\n\n        \"\"\"\n        from diffprivlib.utils import PrivacyLeakWarning\n        self._validate_params()\n        self.accountant.check(self.epsilon, 0)\n\n        if sample_weight is not None:\n            self._warn_unused_args(\"sample_weight\")\n\n        random_state = check_random_state(self.random_state)\n\n        X, y = self._validate_data(X, y, accept_sparse=False, y_numeric=True, multi_output=True)\n\n        if self.bounds_X is None or self.bounds_y is None:\n            warnings.warn(\n                \"Bounds parameters haven't been specified, so falling back to determining bounds from the \"\n                \"data.\\n\"\n                \"This will result in additional privacy leakage. To ensure differential privacy with no \"\n                \"additional privacy loss, specify `bounds_X` and `bounds_y`.\",\n                PrivacyLeakWarning)\n\n            if self.bounds_X is None:\n                self.bounds_X = (np.min(X, axis=0), np.max(X, axis=0))\n            if self.bounds_y is None:\n                self.bounds_y = (np.min(y, axis=0), np.max(y, axis=0))\n\n        # pylint: disable=no-member\n        self.bounds_X = self._check_bounds(self.bounds_X, X.shape[1])\n        self.bounds_y = self._check_bounds(self.bounds_y, y.shape[1] if y.ndim > 1 else 1)\n\n        n_features = X.shape[1]\n        n_targets = y.shape[1] if y.ndim > 1 else 1\n        epsilon_intercept_scale = 1 / (n_features + 1) if self.fit_intercept else 0\n\n        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X, y, fit_intercept=self.fit_intercept, bounds_X=self.bounds_X, bounds_y=self.bounds_y,\n            epsilon=self.epsilon * epsilon_intercept_scale, copy=self.copy_X, random_state=random_state)\n\n        bounds_X = (self.bounds_X[0] - X_offset, self.bounds_X[1] - X_offset)\n        bounds_y = (self.bounds_y[0] - y_offset, self.bounds_y[1] - y_offset)\n\n        objs, obj_coefs = _construct_regression_obj(\n            X, y, bounds_X, bounds_y, epsilon=self.epsilon * (1 - epsilon_intercept_scale), alpha=0,\n            random_state=random_state)\n        coef = np.zeros((n_features, n_targets))\n\n        for i, obj in enumerate(objs):\n            opt_result = minimize(obj, np.zeros(n_features), jac=True)\n            coef[:, i] = opt_result.x\n\n        self.coef_ = coef.T\n        self._obj_coefs = obj_coefs\n\n        if y.ndim == 1:\n            self.coef_ = np.ravel(self.coef_)\n        self._set_intercept(X_offset, y_offset, X_scale)\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self\n", "intra_context": "# MIT License\n#\n# Copyright (C) IBM Corporation 2019\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n#\n# New BSD License\n#\n# Copyright (c) 2007\u20132019 The scikit-learn developers.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the\n# following conditions are met:\n#\n#   a. Redistributions of source code must retain the above copyright notice, this list of conditions and the following\n#      disclaimer.\n#   b. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the\n#      following disclaimer in the documentation and/or other materials provided with the distribution.\n#   c. Neither the name of the Scikit-learn Developers  nor the names of its contributors may be used to endorse or\n#      promote products derived from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES,\n# INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n\"\"\"\nLinear Regression with differential privacy\n\"\"\"\nimport warnings\n\nimport numpy as np\nimport sklearn.linear_model as sk_lr\nfrom scipy.optimize import minimize\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import FLOAT_DTYPES\n\nfrom diffprivlib.accountant import BudgetAccountant\nfrom diffprivlib.mechanisms import Laplace, LaplaceFolded\nfrom diffprivlib.tools import mean\nfrom diffprivlib.utils import warn_unused_args, check_random_state\nfrom diffprivlib.validation import check_bounds, clip_to_bounds, DiffprivlibMixin\n\n\n# noinspection PyPep8Naming\ndef _preprocess_data(X, y, fit_intercept, epsilon=1.0, bounds_X=None, bounds_y=None, copy=True, check_input=True,\n                     random_state=None, **unused_args):\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=False, dtype=FLOAT_DTYPES)\n    elif copy:\n        X = X.copy(order='K')\n\n    y = np.asarray(y, dtype=X.dtype)\n    X_scale = np.ones(X.shape[1], dtype=X.dtype)\n\n    if fit_intercept:\n        bounds_X = check_bounds(bounds_X, X.shape[1])\n        bounds_y = check_bounds(bounds_y, y.shape[1] if y.ndim > 1 else 1)\n\n        X = clip_to_bounds(X, bounds_X)\n        y = clip_to_bounds(y, bounds_y)\n\n        X_offset = mean(X, axis=0, bounds=bounds_X, epsilon=epsilon, random_state=random_state,\n                        accountant=BudgetAccountant())\n        X -= X_offset\n        y_offset = mean(y, axis=0, bounds=bounds_y, epsilon=epsilon, random_state=random_state,\n                        accountant=BudgetAccountant())\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n\n    return X, y, X_offset, y_offset, X_scale\n\n\ndef _construct_regression_obj(X, y, bounds_X, bounds_y, epsilon, alpha, random_state):\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    n_features = X.shape[1]\n    n_targets = y.shape[1]\n\n    local_epsilon = epsilon / (1 + n_targets * n_features + n_features * (n_features + 1) / 2)\n    coefs = ((y ** 2).sum(axis=0), np.einsum('ij,ik->jk', X, y), np.einsum('ij,ik', X, X))\n\n    del X, y\n\n    def get_max_sensitivity(y_lower, y_upper, x_lower, x_upper):\n        corners = [y_lower * x_lower, y_lower * x_upper, y_upper * x_lower, y_upper * x_upper]\n        return np.max(corners) - np.min(corners)\n\n    # Randomise 0th-degree monomial coefficients\n    mono_coef_0 = np.zeros(n_targets)\n\n    for i in range(n_targets):\n        sensitivity = np.abs([bounds_y[0][i], bounds_y[1][i]]).max() ** 2\n        mech = LaplaceFolded(epsilon=local_epsilon, sensitivity=sensitivity, lower=0, upper=float(\"inf\"),\n                             random_state=random_state)\n        mono_coef_0[i] = mech.randomise(coefs[0][i])\n\n    # Randomise 1st-degree monomial coefficients\n    mono_coef_1 = np.zeros((n_features, n_targets))\n\n    for i in range(n_targets):\n        for j in range(n_features):\n            sensitivity = get_max_sensitivity(bounds_y[0][i], bounds_y[1][i], bounds_X[0][j], bounds_X[1][j])\n            mech = Laplace(epsilon=local_epsilon, sensitivity=sensitivity, random_state=random_state)\n            mono_coef_1[j, i] = mech.randomise(coefs[1][j, i])\n\n    # Randomise 2nd-degree monomial coefficients\n    mono_coef_2 = np.zeros((n_features, n_features))\n\n    for i in range(n_features):\n        sensitivity = np.max(np.abs([bounds_X[0][i], bounds_X[0][i]])) ** 2\n        mech = LaplaceFolded(epsilon=local_epsilon, sensitivity=sensitivity, lower=0, upper=float(\"inf\"),\n                             random_state=random_state)\n        mono_coef_2[i, i] = mech.randomise(coefs[2][i, i])\n\n        for j in range(i + 1, n_features):\n            sensitivity = get_max_sensitivity(bounds_X[0][i], bounds_X[1][i], bounds_X[0][j], bounds_X[1][j])\n            mech = Laplace(epsilon=local_epsilon, sensitivity=sensitivity, random_state=random_state)\n            mono_coef_2[i, j] = mech.randomise(coefs[2][i, j])\n            mono_coef_2[j, i] = mono_coef_2[i, j]  # Enforce symmetry\n\n    del coefs\n    noisy_coefs = (mono_coef_0, mono_coef_1, mono_coef_2)\n\n    def obj(idx):\n        def inner_obj(omega):\n            func = noisy_coefs[0][idx]\n            func -= 2 * np.dot(noisy_coefs[1][:, idx], omega)\n            func += np.multiply(noisy_coefs[2], np.tensordot(omega, omega, axes=0)).sum()\n            func += alpha * (omega ** 2).sum()\n\n            grad = - 2 * noisy_coefs[1][:, idx] + 2 * np.matmul(noisy_coefs[2], omega) + 2 * omega * alpha\n\n            return func, grad\n\n        return inner_obj\n\n    output = tuple(obj(i) for i in range(n_targets))\n\n    return output, noisy_coefs\n\n\n# noinspection PyPep8Naming,PyAttributeOutsideInit\nclass LinearRegression(sk_lr.LinearRegression, DiffprivlibMixin):\n    r\"\"\"\n    Ordinary least squares Linear Regression with differential privacy.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares\n    between the observed targets in the dataset, and the targets predicted by the linear approximation.  Differential\n    privacy is guaranteed with respect to the training sample.\n\n    Differential privacy is achieved by adding noise to the coefficients of the objective function, taking inspiration\n    from [ZZX12]_.\n\n    Parameters\n    ----------\n    epsilon : float, default: 1.0\n        Privacy parameter :math:`\\epsilon`.\n\n    bounds_X :  tuple\n        Bounds of the data, provided as a tuple of the form (min, max).  `min` and `max` can either be scalars, covering\n        the min/max of the entire data, or vectors with one entry per feature.  If not provided, the bounds are computed\n        on the data when ``.fit()`` is first called, resulting in a :class:`.PrivacyLeakWarning`.\n\n    bounds_y : tuple\n        Same as `bounds_X`, but for the training label set `y`.\n\n    fit_intercept : bool, default: True\n        Whether to calculate the intercept for this model.  If set to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default: True\n        If True, X will be copied; else, it may be overwritten.\n\n    random_state : int or RandomState, optional\n        Controls the randomness of the model.  To obtain a deterministic behaviour during randomisation,\n        ``random_state`` has to be fixed to an integer.\n\n    accountant : BudgetAccountant, optional\n        Accountant to keep track of privacy budget.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features, ) or (n_targets, n_features)\n        Estimated coefficients for the linear regression problem.  If multiple targets are passed during the fit (y 2D),\n        this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of\n        length n_features.\n\n    intercept_ : float or array of shape of (n_targets,)\n        Independent term in the linear model.  Set to 0.0 if `fit_intercept = False`.\n\n    References\n    ----------\n    .. [ZZX12] Zhang, Jun, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett. \"Functional mechanism:\n        regression analysis under differential privacy.\" arXiv preprint arXiv:1208.0219 (2012).\n\n    \"\"\"\n\n    _parameter_constraints = DiffprivlibMixin._copy_parameter_constraints(\n        sk_lr.LinearRegression, \"fit_intercept\", \"copy_X\")\n\n    def __init__(self, *, epsilon=1.0, bounds_X=None, bounds_y=None, fit_intercept=True, copy_X=True, random_state=None,\n                 accountant=None, **unused_args):\n        super().__init__(fit_intercept=fit_intercept, copy_X=copy_X, n_jobs=None)\n\n        self.epsilon = epsilon\n        self.bounds_X = bounds_X\n        self.bounds_y = bounds_y\n        self.random_state = random_state\n        self.accountant = BudgetAccountant.load_default(accountant)\n\n        self._warn_unused_args(unused_args)\n\n###The function: fit###\n    _preprocess_data = staticmethod(_preprocess_data)\n", "cross_context": [{"diffprivlib.accountant.BudgetAccountant.check": "# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nPrivacy budget accountant for differential privacy\n\"\"\"\nfrom numbers import Integral\n\nimport numpy as np\n\nfrom diffprivlib.utils import Budget\nfrom diffprivlib.validation import check_epsilon_delta\n\n\nclass BudgetAccountant:\n    \"\"\"Privacy budget accountant for differential privacy.\n\n    This class creates a privacy budget accountant to track privacy spend across queries and other data accesses.  Once\n    initialised, the BudgetAccountant stores each privacy spend and iteratively updates the total budget spend, raising\n    an error when the budget ceiling (if specified) is exceeded.  The accountant can be initialised without any maximum\n    budget, to enable users track the total privacy spend of their actions without hindrance.\n\n    Diffprivlib functions can make use of a BudgetAccountant in three different ways (see examples for more details):\n\n        - Passed as an ``accountant`` parameter to the function (e.g., ``mean(..., accountant=acc)``)\n        - Set as the default using the ``set_default()`` method (all subsequent diffprivlib functions will use the\n          accountant by default)\n        - As a context manager using a ``with`` statement (the accountant is used for that block of code)\n\n    Implements the accountant rules as given in [KOV17]_.\n\n    Parameters\n    ----------\n    epsilon : float, default: infinity\n        Epsilon budget ceiling of the accountant.\n\n    delta : float, default: 1.0\n        Delta budget ceiling of the accountant.\n\n    slack : float, default: 0.0\n        Slack allowed in delta spend.  Greater slack may reduce the overall epsilon spend.\n\n    spent_budget : list of tuples of the form (epsilon, delta), optional\n        List of tuples of pre-existing budget spends.  Allows for a new accountant to be initialised with spends\n        extracted from a previous instance.\n\n    Attributes\n    ----------\n    epsilon : float\n        Epsilon budget ceiling of the accountant.\n\n    delta : float\n        Delta budget ceiling of the accountant.\n\n    slack : float\n        The accountant's slack.  Can be modified at runtime, subject to the privacy budget not being exceeded.\n\n    spent_budget : list of tuples of the form (epsilon, delta)\n        The list of privacy spends recorded by the accountant.  Can be used in the initialisation of a new accountant.\n\n    Examples\n    --------\n\n    A ``BudgetAccountant`` is typically passed to diffprivlib functions as an ``accountant`` parameter.  If ``epsilon``\n    and ``delta`` are not set, the accountant has an infinite budget by default, allowing you to track privacy spend\n    without imposing a hard limit.  By allowing a ``slack`` in the budget calculation, the overall epsilon privacy spend\n    can be reduced (at the cost of extra delta spend).\n\n    >>> import diffprivlib as dp\n    >>> from numpy.random import random\n    >>> X = random(100)\n    >>> acc = dp.BudgetAccountant(epsilon=1.5, delta=0)\n    >>> dp.tools.mean(X, bounds=(0, 1), accountant=acc)\n    0.4547006207923884\n    >>> acc.total()\n    (epsilon=1.0, delta=0)\n    >>> dp.tools.std(X, bounds=(0, 1), epsilon=0.25, accountant=acc)\n    0.2630216611181259\n    >>> acc.total()\n    (epsilon=1.25, delta=0)\n\n    >>> acc2 = dp.BudgetAccountant() # infinite budget\n    >>> first_half = dp.tools.mean(X[:50], epsilon=0.25, bounds=(0, 1), accountant=acc2)\n    >>> last_half = dp.tools.mean(X[50:], epsilon=0.25, bounds=(0, 1), accountant=acc2)\n    >>> acc2.total()\n    (epsilon=0.5, delta=0)\n    >>> acc2.remaining()\n    (epsilon=inf, delta=1.0)\n\n    >>> acc3 = dp.BudgetAccountant(slack=1e-3)\n    >>> for i in range(20):\n    ...     dp.tools.mean(X, epsilon=0.05, bounds=(0, 1), accountant=acc3)\n    >>> acc3.total() # Slack has reduced the epsilon spend by almost 25%\n    (epsilon=0.7613352285668463, delta=0.001)\n\n    Using ``set_default()``, an accountant is used by default in all diffprivlib functions in that script.  Accountants\n    also act as context managers, allowing for use in a ``with`` statement.  Passing an accountant as a parameter\n    overrides all other methods.\n\n    >>> acc4 = dp.BudgetAccountant()\n    >>> acc4.set_default()\n    BudgetAccountant()\n    >>> Y = random((100, 2)) - 0.5\n    >>> clf = dp.models.PCA(1, centered=True, data_norm=1.4)\n    >>> clf.fit(Y)\n    PCA(accountant=BudgetAccountant(spent_budget=[(1.0, 0)]), centered=True, copy=True, data_norm=1.4, epsilon=1.0,\n    n_components=1, random_state=None, bounds=None, whiten=False)\n    >>> acc4.total()\n    (epsilon=1.0, delta=0)\n\n    >>> with dp.BudgetAccountant() as acc5:\n    ...     dp.tools.mean(Y, bounds=(0, 1), epsilon=1/3)\n    >>> acc5.total()\n    (epsilon=0.3333333333333333, delta=0)\n\n    References\n    ----------\n    .. [KOV17] Kairouz, Peter, Sewoong Oh, and Pramod Viswanath. \"The composition theorem for differential privacy.\"\n        IEEE Transactions on Information Theory 63.6 (2017): 4037-4049.\n\n    \"\"\"\n    _default = None\n\n    def __init__(self, epsilon=float(\"inf\"), delta=1.0, slack=0.0, spent_budget=None):\n        check_epsilon_delta(epsilon, delta)\n        self.__epsilon = epsilon\n        self.__min_epsilon = 0 if epsilon == float(\"inf\") else epsilon * 1e-14\n        self.__delta = delta\n        self.__spent_budget = []\n        self.slack = slack\n\n        if spent_budget is not None:\n            if not isinstance(spent_budget, list):\n                raise TypeError(\"spent_budget must be a list\")\n\n            for _epsilon, _delta in spent_budget:\n                self.spend(_epsilon, _delta)\n\n    def __repr__(self, n_budget_max=5):\n        params = []\n        if self.epsilon != float(\"inf\"):\n            params.append(f\"epsilon={self.epsilon}\")\n\n        if self.delta != 1:\n            params.append(f\"delta={self.delta}\")\n\n        if self.slack > 0:\n            params.append(f\"slack={self.slack}\")\n\n        if self.spent_budget:\n            if len(self.spent_budget) > n_budget_max:\n                params.append(\"spent_budget=\" + str(self.spent_budget[:n_budget_max] + [\"...\"]).replace(\"'\", \"\"))\n            else:\n                params.append(\"spent_budget=\" + str(self.spent_budget))\n\n        return \"BudgetAccountant(\" + \", \".join(params) + \")\"\n\n    def __enter__(self):\n        self.old_default = self.pop_default()\n        self.set_default()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.pop_default()\n\n        if self.old_default is not None:\n            self.old_default.set_default()\n        del self.old_default\n\n    def __len__(self):\n        return len(self.spent_budget)\n\n    @property\n    def slack(self):\n        \"\"\"Slack parameter for composition.\n        \"\"\"\n        return self.__slack\n\n    @slack.setter\n    def slack(self, slack):\n        if not 0 <= slack <= self.delta:\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\n\n        epsilon_spent, delta_spent = self.total(slack=slack)\n\n        if self.epsilon < epsilon_spent or self.delta < delta_spent:\n            raise BudgetError(f\"Privacy budget will be exceeded by changing slack to {slack}.\")\n\n        self.__slack = slack\n\n    @property\n    def spent_budget(self):\n        \"\"\"List of tuples of the form (epsilon, delta) of spent privacy budget.\n        \"\"\"\n        return self.__spent_budget.copy()\n\n    @property\n    def epsilon(self):\n        \"\"\"Epsilon privacy ceiling of the accountant.\n        \"\"\"\n        return self.__epsilon\n\n    @property\n    def delta(self):\n        \"\"\"Delta privacy ceiling of the accountant.\n        \"\"\"\n        return self.__delta\n\n    def total(self, spent_budget=None, slack=None):\n        \"\"\"Returns the total current privacy spend.\n\n        `spent_budget` and `slack` can be specified as parameters, otherwise the class values will be used.\n\n        Parameters\n        ----------\n        spent_budget : list of tuples of the form (epsilon, delta), optional\n            List of tuples of budget spends.  If not provided, the accountant's spends will be used.\n\n        slack : float, optional\n            Slack in delta for composition.  If not provided, the accountant's slack will be used.\n\n        Returns\n        -------\n        epsilon : float\n            Total epsilon spend.\n\n        delta : float\n            Total delta spend.\n\n        \"\"\"\n        if spent_budget is None:\n            spent_budget = self.spent_budget\n        else:\n            for epsilon, delta in spent_budget:\n                check_epsilon_delta(epsilon, delta)\n\n        if slack is None:\n            slack = self.slack\n        elif not 0 <= slack <= self.delta:\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\n\n        epsilon_sum, epsilon_exp_sum, epsilon_sq_sum = 0, 0, 0\n\n        for epsilon, _ in spent_budget:\n            epsilon_sum += epsilon\n            epsilon_exp_sum += (1 - np.exp(-epsilon)) * epsilon / (1 + np.exp(-epsilon))\n            epsilon_sq_sum += epsilon ** 2\n\n        total_epsilon_naive = epsilon_sum\n        total_delta = self.__total_delta_safe(spent_budget, slack)\n\n        if slack == 0:\n            return Budget(total_epsilon_naive, total_delta)\n\n        total_epsilon_drv = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum * np.log(1 / slack))\n        total_epsilon_kov = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum *\n                                                      np.log(np.exp(1) + np.sqrt(epsilon_sq_sum) / slack))\n\n        return Budget(min(total_epsilon_naive, total_epsilon_drv, total_epsilon_kov), total_delta)\n\n    def check(self, epsilon, delta):\n        \"\"\"Checks if the provided (epsilon,delta) can be spent without exceeding the accountant's budget ceiling.\n\n        Parameters\n        ----------\n        epsilon : float\n            Epsilon budget spend to check.\n\n        delta : float\n            Delta budget spend to check.\n\n        Returns\n        -------\n        bool\n            True if the budget can be spent, otherwise a :class:`.BudgetError` is raised.\n\n        Raises\n        ------\n        BudgetError\n            If the specified budget spend will result in the budget ceiling being exceeded.\n\n        \"\"\"\n        from diffprivlib.utils import BudgetError\n        check_epsilon_delta(epsilon, delta)\n        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return True\n\n        if 0 < epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\n            return True\n\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n    def remaining(self, k=1):\n        \"\"\"Calculates the budget that remains to be spent.\n\n        Calculates the privacy budget that can be spent on `k` queries.  Spending this budget on `k` queries will\n        match the budget ceiling, assuming no floating point errors.\n\n        Parameters\n        ----------\n        k : int, default: 1\n            The number of queries for which to calculate the remaining budget.\n\n        Returns\n        -------\n        epsilon : float\n            Total epsilon spend remaining for `k` queries.\n\n        delta : float\n            Total delta spend remaining for `k` queries.\n\n        \"\"\"\n        if not isinstance(k, Integral):\n            raise TypeError(f\"k must be integer-valued, got {type(k)}.\")\n        if k < 1:\n            raise ValueError(f\"k must be at least 1, got {k}.\")\n\n        _, spent_delta = self.total()\n        delta = 1 - ((1 - self.delta) / (1 - spent_delta)) ** (1 / k) if spent_delta < 1.0 else 1.0\n        # delta = 1 - np.exp((np.log(1 - self.delta) - np.log(1 - spent_delta)) / k)\n\n        lower = 0\n        upper = self.epsilon\n        old_interval_size = (upper - lower) * 2\n\n        while old_interval_size > upper - lower:\n            old_interval_size = upper - lower\n            mid = (upper + lower) / 2\n\n            spent_budget = self.spent_budget + [(mid, 0)] * k\n            x_0, _ = self.total(spent_budget=spent_budget)\n\n            if x_0 >= self.epsilon:\n                upper = mid\n            if x_0 <= self.epsilon:\n                lower = mid\n\n        epsilon = (upper + lower) / 2\n\n        return Budget(epsilon, delta)\n\n    def spend(self, epsilon, delta):\n        \"\"\"Spend the given privacy budget.\n\n        Instructs the accountant to spend the given epsilon and delta privacy budget, while ensuring the target budget\n        is not exceeded.\n\n        Parameters\n        ----------\n        epsilon : float\n            Epsilon privacy budget to spend.\n\n        delta : float\n            Delta privacy budget to spend.\n\n        Returns\n        -------\n        self : BudgetAccountant\n\n        \"\"\"\n        self.check(epsilon, delta)\n        self.__spent_budget.append((epsilon, delta))\n        return self\n\n    @staticmethod\n    def __total_delta_safe(spent_budget, slack):\n        \"\"\"\n        Calculate total delta spend of `spent_budget`, with special consideration for floating point arithmetic.\n        Should yield greater precision, especially for a large number of budget spends with very small delta.\n\n        Parameters\n        ----------\n        spent_budget: list of tuples of the form (epsilon, delta)\n            List of budget spends, for which the total delta spend is to be calculated.\n\n        slack: float\n            Delta slack parameter for composition of spends.\n\n        Returns\n        -------\n        float\n            Total delta spend.\n\n        \"\"\"\n        delta_spend = [slack]\n        for _, delta in spent_budget:\n            delta_spend.append(delta)\n        delta_spend.sort()\n\n        # (1 - a) * (1 - b) = 1 - (a + b - a * b)\n        prod = 0\n        for delta in delta_spend:\n            prod += delta - prod * delta\n\n        return prod\n\n    @staticmethod\n    def load_default(accountant):\n        \"\"\"Loads the default privacy budget accountant if none is supplied, otherwise checks that the supplied\n        accountant is a BudgetAccountant class.\n\n        An accountant can be set as the default using the `set_default()` method.  If no default has been set, a default\n        is created.\n\n        Parameters\n        ----------\n        accountant : BudgetAccountant or None\n            The supplied budget accountant.  If None, the default accountant is returned.\n\n        Returns\n        -------\n        default : BudgetAccountant\n            Returns a working BudgetAccountant, either the supplied `accountant` or the existing default.\n\n        \"\"\"\n        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n\n            return BudgetAccountant._default\n\n        if not isinstance(accountant, BudgetAccountant):\n            raise TypeError(f\"Accountant must be of type BudgetAccountant, got {type(accountant)}\")\n\n        return accountant\n\n    def set_default(self):\n        \"\"\"Sets the current accountant to be the default when running functions and queries with diffprivlib.\n\n        Returns\n        -------\n        self : BudgetAccountant\n\n        \"\"\"\n        BudgetAccountant._default = self\n        return self\n\n    @staticmethod\n    def pop_default():\n        \"\"\"Pops the default BudgetAccountant from the class and returns it to the user.\n\n        Returns\n        -------\n        default : BudgetAccountant\n            Returns the existing default BudgetAccountant.\n\n        \"\"\"\n        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default\n"}, {"diffprivlib.accountant.BudgetAccountant.spend": "# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nPrivacy budget accountant for differential privacy\n\"\"\"\nfrom numbers import Integral\n\nimport numpy as np\n\nfrom diffprivlib.utils import Budget\nfrom diffprivlib.validation import check_epsilon_delta\n\n\nclass BudgetAccountant:\n    \"\"\"Privacy budget accountant for differential privacy.\n\n    This class creates a privacy budget accountant to track privacy spend across queries and other data accesses.  Once\n    initialised, the BudgetAccountant stores each privacy spend and iteratively updates the total budget spend, raising\n    an error when the budget ceiling (if specified) is exceeded.  The accountant can be initialised without any maximum\n    budget, to enable users track the total privacy spend of their actions without hindrance.\n\n    Diffprivlib functions can make use of a BudgetAccountant in three different ways (see examples for more details):\n\n        - Passed as an ``accountant`` parameter to the function (e.g., ``mean(..., accountant=acc)``)\n        - Set as the default using the ``set_default()`` method (all subsequent diffprivlib functions will use the\n          accountant by default)\n        - As a context manager using a ``with`` statement (the accountant is used for that block of code)\n\n    Implements the accountant rules as given in [KOV17]_.\n\n    Parameters\n    ----------\n    epsilon : float, default: infinity\n        Epsilon budget ceiling of the accountant.\n\n    delta : float, default: 1.0\n        Delta budget ceiling of the accountant.\n\n    slack : float, default: 0.0\n        Slack allowed in delta spend.  Greater slack may reduce the overall epsilon spend.\n\n    spent_budget : list of tuples of the form (epsilon, delta), optional\n        List of tuples of pre-existing budget spends.  Allows for a new accountant to be initialised with spends\n        extracted from a previous instance.\n\n    Attributes\n    ----------\n    epsilon : float\n        Epsilon budget ceiling of the accountant.\n\n    delta : float\n        Delta budget ceiling of the accountant.\n\n    slack : float\n        The accountant's slack.  Can be modified at runtime, subject to the privacy budget not being exceeded.\n\n    spent_budget : list of tuples of the form (epsilon, delta)\n        The list of privacy spends recorded by the accountant.  Can be used in the initialisation of a new accountant.\n\n    Examples\n    --------\n\n    A ``BudgetAccountant`` is typically passed to diffprivlib functions as an ``accountant`` parameter.  If ``epsilon``\n    and ``delta`` are not set, the accountant has an infinite budget by default, allowing you to track privacy spend\n    without imposing a hard limit.  By allowing a ``slack`` in the budget calculation, the overall epsilon privacy spend\n    can be reduced (at the cost of extra delta spend).\n\n    >>> import diffprivlib as dp\n    >>> from numpy.random import random\n    >>> X = random(100)\n    >>> acc = dp.BudgetAccountant(epsilon=1.5, delta=0)\n    >>> dp.tools.mean(X, bounds=(0, 1), accountant=acc)\n    0.4547006207923884\n    >>> acc.total()\n    (epsilon=1.0, delta=0)\n    >>> dp.tools.std(X, bounds=(0, 1), epsilon=0.25, accountant=acc)\n    0.2630216611181259\n    >>> acc.total()\n    (epsilon=1.25, delta=0)\n\n    >>> acc2 = dp.BudgetAccountant() # infinite budget\n    >>> first_half = dp.tools.mean(X[:50], epsilon=0.25, bounds=(0, 1), accountant=acc2)\n    >>> last_half = dp.tools.mean(X[50:], epsilon=0.25, bounds=(0, 1), accountant=acc2)\n    >>> acc2.total()\n    (epsilon=0.5, delta=0)\n    >>> acc2.remaining()\n    (epsilon=inf, delta=1.0)\n\n    >>> acc3 = dp.BudgetAccountant(slack=1e-3)\n    >>> for i in range(20):\n    ...     dp.tools.mean(X, epsilon=0.05, bounds=(0, 1), accountant=acc3)\n    >>> acc3.total() # Slack has reduced the epsilon spend by almost 25%\n    (epsilon=0.7613352285668463, delta=0.001)\n\n    Using ``set_default()``, an accountant is used by default in all diffprivlib functions in that script.  Accountants\n    also act as context managers, allowing for use in a ``with`` statement.  Passing an accountant as a parameter\n    overrides all other methods.\n\n    >>> acc4 = dp.BudgetAccountant()\n    >>> acc4.set_default()\n    BudgetAccountant()\n    >>> Y = random((100, 2)) - 0.5\n    >>> clf = dp.models.PCA(1, centered=True, data_norm=1.4)\n    >>> clf.fit(Y)\n    PCA(accountant=BudgetAccountant(spent_budget=[(1.0, 0)]), centered=True, copy=True, data_norm=1.4, epsilon=1.0,\n    n_components=1, random_state=None, bounds=None, whiten=False)\n    >>> acc4.total()\n    (epsilon=1.0, delta=0)\n\n    >>> with dp.BudgetAccountant() as acc5:\n    ...     dp.tools.mean(Y, bounds=(0, 1), epsilon=1/3)\n    >>> acc5.total()\n    (epsilon=0.3333333333333333, delta=0)\n\n    References\n    ----------\n    .. [KOV17] Kairouz, Peter, Sewoong Oh, and Pramod Viswanath. \"The composition theorem for differential privacy.\"\n        IEEE Transactions on Information Theory 63.6 (2017): 4037-4049.\n\n    \"\"\"\n    _default = None\n\n    def __init__(self, epsilon=float(\"inf\"), delta=1.0, slack=0.0, spent_budget=None):\n        check_epsilon_delta(epsilon, delta)\n        self.__epsilon = epsilon\n        self.__min_epsilon = 0 if epsilon == float(\"inf\") else epsilon * 1e-14\n        self.__delta = delta\n        self.__spent_budget = []\n        self.slack = slack\n\n        if spent_budget is not None:\n            if not isinstance(spent_budget, list):\n                raise TypeError(\"spent_budget must be a list\")\n\n            for _epsilon, _delta in spent_budget:\n                self.spend(_epsilon, _delta)\n\n    def __repr__(self, n_budget_max=5):\n        params = []\n        if self.epsilon != float(\"inf\"):\n            params.append(f\"epsilon={self.epsilon}\")\n\n        if self.delta != 1:\n            params.append(f\"delta={self.delta}\")\n\n        if self.slack > 0:\n            params.append(f\"slack={self.slack}\")\n\n        if self.spent_budget:\n            if len(self.spent_budget) > n_budget_max:\n                params.append(\"spent_budget=\" + str(self.spent_budget[:n_budget_max] + [\"...\"]).replace(\"'\", \"\"))\n            else:\n                params.append(\"spent_budget=\" + str(self.spent_budget))\n\n        return \"BudgetAccountant(\" + \", \".join(params) + \")\"\n\n    def __enter__(self):\n        self.old_default = self.pop_default()\n        self.set_default()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.pop_default()\n\n        if self.old_default is not None:\n            self.old_default.set_default()\n        del self.old_default\n\n    def __len__(self):\n        return len(self.spent_budget)\n\n    @property\n    def slack(self):\n        \"\"\"Slack parameter for composition.\n        \"\"\"\n        return self.__slack\n\n    @slack.setter\n    def slack(self, slack):\n        if not 0 <= slack <= self.delta:\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\n\n        epsilon_spent, delta_spent = self.total(slack=slack)\n\n        if self.epsilon < epsilon_spent or self.delta < delta_spent:\n            raise BudgetError(f\"Privacy budget will be exceeded by changing slack to {slack}.\")\n\n        self.__slack = slack\n\n    @property\n    def spent_budget(self):\n        \"\"\"List of tuples of the form (epsilon, delta) of spent privacy budget.\n        \"\"\"\n        return self.__spent_budget.copy()\n\n    @property\n    def epsilon(self):\n        \"\"\"Epsilon privacy ceiling of the accountant.\n        \"\"\"\n        return self.__epsilon\n\n    @property\n    def delta(self):\n        \"\"\"Delta privacy ceiling of the accountant.\n        \"\"\"\n        return self.__delta\n\n    def total(self, spent_budget=None, slack=None):\n        \"\"\"Returns the total current privacy spend.\n\n        `spent_budget` and `slack` can be specified as parameters, otherwise the class values will be used.\n\n        Parameters\n        ----------\n        spent_budget : list of tuples of the form (epsilon, delta), optional\n            List of tuples of budget spends.  If not provided, the accountant's spends will be used.\n\n        slack : float, optional\n            Slack in delta for composition.  If not provided, the accountant's slack will be used.\n\n        Returns\n        -------\n        epsilon : float\n            Total epsilon spend.\n\n        delta : float\n            Total delta spend.\n\n        \"\"\"\n        if spent_budget is None:\n            spent_budget = self.spent_budget\n        else:\n            for epsilon, delta in spent_budget:\n                check_epsilon_delta(epsilon, delta)\n\n        if slack is None:\n            slack = self.slack\n        elif not 0 <= slack <= self.delta:\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\n\n        epsilon_sum, epsilon_exp_sum, epsilon_sq_sum = 0, 0, 0\n\n        for epsilon, _ in spent_budget:\n            epsilon_sum += epsilon\n            epsilon_exp_sum += (1 - np.exp(-epsilon)) * epsilon / (1 + np.exp(-epsilon))\n            epsilon_sq_sum += epsilon ** 2\n\n        total_epsilon_naive = epsilon_sum\n        total_delta = self.__total_delta_safe(spent_budget, slack)\n\n        if slack == 0:\n            return Budget(total_epsilon_naive, total_delta)\n\n        total_epsilon_drv = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum * np.log(1 / slack))\n        total_epsilon_kov = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum *\n                                                      np.log(np.exp(1) + np.sqrt(epsilon_sq_sum) / slack))\n\n        return Budget(min(total_epsilon_naive, total_epsilon_drv, total_epsilon_kov), total_delta)\n\n    def check(self, epsilon, delta):\n        \"\"\"Checks if the provided (epsilon,delta) can be spent without exceeding the accountant's budget ceiling.\n\n        Parameters\n        ----------\n        epsilon : float\n            Epsilon budget spend to check.\n\n        delta : float\n            Delta budget spend to check.\n\n        Returns\n        -------\n        bool\n            True if the budget can be spent, otherwise a :class:`.BudgetError` is raised.\n\n        Raises\n        ------\n        BudgetError\n            If the specified budget spend will result in the budget ceiling being exceeded.\n\n        \"\"\"\n        from diffprivlib.utils import BudgetError\n        check_epsilon_delta(epsilon, delta)\n        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return True\n\n        if 0 < epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\n            return True\n\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n    def remaining(self, k=1):\n        \"\"\"Calculates the budget that remains to be spent.\n\n        Calculates the privacy budget that can be spent on `k` queries.  Spending this budget on `k` queries will\n        match the budget ceiling, assuming no floating point errors.\n\n        Parameters\n        ----------\n        k : int, default: 1\n            The number of queries for which to calculate the remaining budget.\n\n        Returns\n        -------\n        epsilon : float\n            Total epsilon spend remaining for `k` queries.\n\n        delta : float\n            Total delta spend remaining for `k` queries.\n\n        \"\"\"\n        if not isinstance(k, Integral):\n            raise TypeError(f\"k must be integer-valued, got {type(k)}.\")\n        if k < 1:\n            raise ValueError(f\"k must be at least 1, got {k}.\")\n\n        _, spent_delta = self.total()\n        delta = 1 - ((1 - self.delta) / (1 - spent_delta)) ** (1 / k) if spent_delta < 1.0 else 1.0\n        # delta = 1 - np.exp((np.log(1 - self.delta) - np.log(1 - spent_delta)) / k)\n\n        lower = 0\n        upper = self.epsilon\n        old_interval_size = (upper - lower) * 2\n\n        while old_interval_size > upper - lower:\n            old_interval_size = upper - lower\n            mid = (upper + lower) / 2\n\n            spent_budget = self.spent_budget + [(mid, 0)] * k\n            x_0, _ = self.total(spent_budget=spent_budget)\n\n            if x_0 >= self.epsilon:\n                upper = mid\n            if x_0 <= self.epsilon:\n                lower = mid\n\n        epsilon = (upper + lower) / 2\n\n        return Budget(epsilon, delta)\n\n    def spend(self, epsilon, delta):\n        \"\"\"Spend the given privacy budget.\n\n        Instructs the accountant to spend the given epsilon and delta privacy budget, while ensuring the target budget\n        is not exceeded.\n\n        Parameters\n        ----------\n        epsilon : float\n            Epsilon privacy budget to spend.\n\n        delta : float\n            Delta privacy budget to spend.\n\n        Returns\n        -------\n        self : BudgetAccountant\n\n        \"\"\"\n        self.check(epsilon, delta)\n        self.__spent_budget.append((epsilon, delta))\n        return self\n\n    @staticmethod\n    def __total_delta_safe(spent_budget, slack):\n        \"\"\"\n        Calculate total delta spend of `spent_budget`, with special consideration for floating point arithmetic.\n        Should yield greater precision, especially for a large number of budget spends with very small delta.\n\n        Parameters\n        ----------\n        spent_budget: list of tuples of the form (epsilon, delta)\n            List of budget spends, for which the total delta spend is to be calculated.\n\n        slack: float\n            Delta slack parameter for composition of spends.\n\n        Returns\n        -------\n        float\n            Total delta spend.\n\n        \"\"\"\n        delta_spend = [slack]\n        for _, delta in spent_budget:\n            delta_spend.append(delta)\n        delta_spend.sort()\n\n        # (1 - a) * (1 - b) = 1 - (a + b - a * b)\n        prod = 0\n        for delta in delta_spend:\n            prod += delta - prod * delta\n\n        return prod\n\n    @staticmethod\n    def load_default(accountant):\n        \"\"\"Loads the default privacy budget accountant if none is supplied, otherwise checks that the supplied\n        accountant is a BudgetAccountant class.\n\n        An accountant can be set as the default using the `set_default()` method.  If no default has been set, a default\n        is created.\n\n        Parameters\n        ----------\n        accountant : BudgetAccountant or None\n            The supplied budget accountant.  If None, the default accountant is returned.\n\n        Returns\n        -------\n        default : BudgetAccountant\n            Returns a working BudgetAccountant, either the supplied `accountant` or the existing default.\n\n        \"\"\"\n        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n\n            return BudgetAccountant._default\n\n        if not isinstance(accountant, BudgetAccountant):\n            raise TypeError(f\"Accountant must be of type BudgetAccountant, got {type(accountant)}\")\n\n        return accountant\n\n    def set_default(self):\n        \"\"\"Sets the current accountant to be the default when running functions and queries with diffprivlib.\n\n        Returns\n        -------\n        self : BudgetAccountant\n\n        \"\"\"\n        BudgetAccountant._default = self\n        return self\n\n    @staticmethod\n    def pop_default():\n        \"\"\"Pops the default BudgetAccountant from the class and returns it to the user.\n\n        Returns\n        -------\n        default : BudgetAccountant\n            Returns the existing default BudgetAccountant.\n\n        \"\"\"\n        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default\n"}, {"diffprivlib.utils.PrivacyLeakWarning": "# MIT License\n#\n# Copyright (C) IBM Corporation 2019\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nBasic functions and other utilities for the differential privacy library\n\"\"\"\nimport secrets\nimport warnings\n\nimport numpy as np\nfrom sklearn.utils import check_random_state as skl_check_random_state\n\n\ndef copy_docstring(source):\n    \"\"\"Decorator function to copy a docstring from a `source` function to a `target` function.\n\n    The docstring is only copied if a docstring is present in `source`, and if none is present in `target`.  Takes\n    inspiration from similar in `matplotlib`.\n\n    Parameters\n    ----------\n    source : method\n        Source function from which to copy the docstring.  If ``source.__doc__`` is empty, do nothing.\n\n    Returns\n    -------\n    target : method\n        Target function with new docstring.\n\n    \"\"\"\n    def copy_func(target):\n        if source.__doc__ and not target.__doc__:\n            target.__doc__ = source.__doc__\n        return target\n    return copy_func\n\n\ndef warn_unused_args(args):\n    \"\"\"Warn the user about supplying unused `args` to a diffprivlib model.\n\n    Arguments can be supplied as a string, a list of strings, or a dictionary as supplied to kwargs.\n\n    Parameters\n    ----------\n    args : str or list or dict\n        Arguments for which warnings should be thrown.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if isinstance(args, str):\n        args = [args]\n\n    for arg in args:\n        warnings.warn(f\"Parameter '{arg}' is not functional in diffprivlib.  Remove this parameter to suppress this \"\n                      \"warning.\", DiffprivlibCompatibilityWarning)\n\n\ndef check_random_state(seed, secure=False):\n    \"\"\"Turn seed into a np.random.RandomState or secrets.SystemRandom instance.\n\n    If secure=True, and seed is None (or was generated from a previous None seed), then secrets is used.  Otherwise a\n    np.random.RandomState is used.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None and secure is False, return the RandomState singleton used by np.random.\n        If seed is None and secure is True, return a SystemRandom instance from secrets.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState or SystemRandom instance, return it.\n        Otherwise raise ValueError.\n\n    secure : bool, default: False\n        Specifies if a secure random number generator from secrets can be used.\n    \"\"\"\n    if secure:\n        if isinstance(seed, secrets.SystemRandom):\n            return seed\n\n        if seed is None or seed is np.random.mtrand._rand:  # pylint: disable=protected-access\n            return secrets.SystemRandom()\n    elif isinstance(seed, secrets.SystemRandom):\n        raise ValueError(\"secrets.SystemRandom instance cannot be passed when secure is False.\")\n\n    return skl_check_random_state(seed)\n\n\nclass Budget(tuple):\n    \"\"\"Custom tuple subclass for privacy budgets of the form (epsilon, delta).\n\n    The ``Budget`` class allows for correct comparison/ordering of privacy budget, ensuring that both epsilon and delta\n    satisfy the comparison (tuples are compared lexicographically).  Additionally, tuples are represented with added\n    verbosity, labelling epsilon and delta appropriately.\n\n    Examples\n    --------\n\n    >>> from diffprivlib.utils import Budget\n    >>> Budget(1, 0.5)\n    (epsilon=1, delta=0.5)\n    >>> Budget(2, 0) >= Budget(1, 0.5)\n    False\n    >>> (2, 0) >= (1, 0.5) # Tuples are compared with lexicographic ordering\n    True\n\n    \"\"\"\n    def __new__(cls, epsilon, delta):\n        if epsilon < 0:\n            raise ValueError(\"Epsilon must be non-negative\")\n\n        if not 0 <= delta <= 1:\n            raise ValueError(\"Delta must be in [0, 1]\")\n\n        return tuple.__new__(cls, (epsilon, delta))\n\n    def __gt__(self, other):\n        if self.__ge__(other) and not self.__eq__(other):\n            return True\n        return False\n\n    def __ge__(self, other):\n        if self[0] >= other[0] and self[1] >= other[1]:\n            return True\n        return False\n\n    def __lt__(self, other):\n        if self.__le__(other) and not self.__eq__(other):\n            return True\n        return False\n\n    def __le__(self, other):\n        if self[0] <= other[0] and self[1] <= other[1]:\n            return True\n        return False\n\n    def __repr__(self):\n        return f\"(epsilon={self[0]}, delta={self[1]})\"\n\n\nclass BudgetError(ValueError):\n    \"\"\"Custom exception to capture the privacy budget being exceeded, typically controlled by a\n    :class:`.BudgetAccountant`.\n\n    For example, this exception may be raised when the user:\n\n        - Attempts to execute a query which would exceed the privacy budget of the accountant.\n        - Attempts to change the slack of the accountant in such a way that the existing budget spends would exceed the\n          accountant's budget.\n\n    \"\"\"\n\n\nclass PrivacyLeakWarning(RuntimeWarning):\n    \"\"\"Custom warning to capture privacy leaks resulting from incorrect parameter setting.\n\n    For example, this warning may occur when the user:\n\n        - fails to specify the bounds or range of data to a model where required (e.g., `bounds=None` to\n          :class:`.GaussianNB`).\n        - inputs data to a model that falls outside the bounds or range originally specified.\n\n    \"\"\"\n\n\nclass DiffprivlibCompatibilityWarning(RuntimeWarning):\n    \"\"\"Custom warning to capture inherited class arguments that are not compatible with diffprivlib.\n\n    The purpose of the warning is to alert the user of the incompatibility, but to continue execution having fixed the\n    incompatibility at runtime.\n\n    For example, this warning may occur when the user:\n\n        - passes a parameter value that is not compatible with diffprivlib (e.g., `solver='liblinear'` to\n          :class:`.LogisticRegression`)\n        - specifies a non-default value for a parameter that is ignored by diffprivlib (e.g., `intercept_scaling=0.5`\n          to :class:`.LogisticRegression`.\n\n    \"\"\"\n\n\nwarnings.simplefilter('always', PrivacyLeakWarning)\n"}, {"diffprivlib.utils.check_random_state": "# MIT License\n#\n# Copyright (C) IBM Corporation 2019\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nBasic functions and other utilities for the differential privacy library\n\"\"\"\nimport secrets\nimport warnings\n\nimport numpy as np\nfrom sklearn.utils import check_random_state as skl_check_random_state\n\n\ndef copy_docstring(source):\n    \"\"\"Decorator function to copy a docstring from a `source` function to a `target` function.\n\n    The docstring is only copied if a docstring is present in `source`, and if none is present in `target`.  Takes\n    inspiration from similar in `matplotlib`.\n\n    Parameters\n    ----------\n    source : method\n        Source function from which to copy the docstring.  If ``source.__doc__`` is empty, do nothing.\n\n    Returns\n    -------\n    target : method\n        Target function with new docstring.\n\n    \"\"\"\n    def copy_func(target):\n        if source.__doc__ and not target.__doc__:\n            target.__doc__ = source.__doc__\n        return target\n    return copy_func\n\n\ndef warn_unused_args(args):\n    \"\"\"Warn the user about supplying unused `args` to a diffprivlib model.\n\n    Arguments can be supplied as a string, a list of strings, or a dictionary as supplied to kwargs.\n\n    Parameters\n    ----------\n    args : str or list or dict\n        Arguments for which warnings should be thrown.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if isinstance(args, str):\n        args = [args]\n\n    for arg in args:\n        warnings.warn(f\"Parameter '{arg}' is not functional in diffprivlib.  Remove this parameter to suppress this \"\n                      \"warning.\", DiffprivlibCompatibilityWarning)\n\n\ndef check_random_state(seed, secure=False):\n    \"\"\"Turn seed into a np.random.RandomState or secrets.SystemRandom instance.\n\n    If secure=True, and seed is None (or was generated from a previous None seed), then secrets is used.  Otherwise a\n    np.random.RandomState is used.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None and secure is False, return the RandomState singleton used by np.random.\n        If seed is None and secure is True, return a SystemRandom instance from secrets.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState or SystemRandom instance, return it.\n        Otherwise raise ValueError.\n\n    secure : bool, default: False\n        Specifies if a secure random number generator from secrets can be used.\n    \"\"\"\n    if secure:\n        if isinstance(seed, secrets.SystemRandom):\n            return seed\n\n        if seed is None or seed is np.random.mtrand._rand:  # pylint: disable=protected-access\n            return secrets.SystemRandom()\n    elif isinstance(seed, secrets.SystemRandom):\n        raise ValueError(\"secrets.SystemRandom instance cannot be passed when secure is False.\")\n\n    return skl_check_random_state(seed)\n\n\nclass Budget(tuple):\n    \"\"\"Custom tuple subclass for privacy budgets of the form (epsilon, delta).\n\n    The ``Budget`` class allows for correct comparison/ordering of privacy budget, ensuring that both epsilon and delta\n    satisfy the comparison (tuples are compared lexicographically).  Additionally, tuples are represented with added\n    verbosity, labelling epsilon and delta appropriately.\n\n    Examples\n    --------\n\n    >>> from diffprivlib.utils import Budget\n    >>> Budget(1, 0.5)\n    (epsilon=1, delta=0.5)\n    >>> Budget(2, 0) >= Budget(1, 0.5)\n    False\n    >>> (2, 0) >= (1, 0.5) # Tuples are compared with lexicographic ordering\n    True\n\n    \"\"\"\n    def __new__(cls, epsilon, delta):\n        if epsilon < 0:\n            raise ValueError(\"Epsilon must be non-negative\")\n\n        if not 0 <= delta <= 1:\n            raise ValueError(\"Delta must be in [0, 1]\")\n\n        return tuple.__new__(cls, (epsilon, delta))\n\n    def __gt__(self, other):\n        if self.__ge__(other) and not self.__eq__(other):\n            return True\n        return False\n\n    def __ge__(self, other):\n        if self[0] >= other[0] and self[1] >= other[1]:\n            return True\n        return False\n\n    def __lt__(self, other):\n        if self.__le__(other) and not self.__eq__(other):\n            return True\n        return False\n\n    def __le__(self, other):\n        if self[0] <= other[0] and self[1] <= other[1]:\n            return True\n        return False\n\n    def __repr__(self):\n        return f\"(epsilon={self[0]}, delta={self[1]})\"\n\n\nclass BudgetError(ValueError):\n    \"\"\"Custom exception to capture the privacy budget being exceeded, typically controlled by a\n    :class:`.BudgetAccountant`.\n\n    For example, this exception may be raised when the user:\n\n        - Attempts to execute a query which would exceed the privacy budget of the accountant.\n        - Attempts to change the slack of the accountant in such a way that the existing budget spends would exceed the\n          accountant's budget.\n\n    \"\"\"\n\n\nclass PrivacyLeakWarning(RuntimeWarning):\n    \"\"\"Custom warning to capture privacy leaks resulting from incorrect parameter setting.\n\n    For example, this warning may occur when the user:\n\n        - fails to specify the bounds or range of data to a model where required (e.g., `bounds=None` to\n          :class:`.GaussianNB`).\n        - inputs data to a model that falls outside the bounds or range originally specified.\n\n    \"\"\"\n\n\nclass DiffprivlibCompatibilityWarning(RuntimeWarning):\n    \"\"\"Custom warning to capture inherited class arguments that are not compatible with diffprivlib.\n\n    The purpose of the warning is to alert the user of the incompatibility, but to continue execution having fixed the\n    incompatibility at runtime.\n\n    For example, this warning may occur when the user:\n\n        - passes a parameter value that is not compatible with diffprivlib (e.g., `solver='liblinear'` to\n          :class:`.LogisticRegression`)\n        - specifies a non-default value for a parameter that is ignored by diffprivlib (e.g., `intercept_scaling=0.5`\n          to :class:`.LogisticRegression`.\n\n    \"\"\"\n\n\nwarnings.simplefilter('always', PrivacyLeakWarning)\n"}, {"diffprivlib.validation.DiffprivlibMixin._check_bounds": "# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nValidation functions for the differential privacy library\n\"\"\"\nfrom numbers import Real, Integral\n\nimport numpy as np\n\nfrom diffprivlib.utils import warn_unused_args\n\n\ndef check_epsilon_delta(epsilon, delta, allow_zero=False):\n    \"\"\"Checks that epsilon and delta are valid values for differential privacy.  Throws an error if checks fail,\n    otherwise returns nothing.\n\n    As well as the requirements of epsilon and delta separately, both cannot be simultaneously zero, unless\n    ``allow_zero`` is set to ``True``.\n\n    Parameters\n    ----------\n    epsilon : float\n        Epsilon parameter for differential privacy.  Must be non-negative.\n\n    delta : float\n        Delta parameter for differential privacy.  Must be on the unit interval, [0, 1].\n\n    allow_zero : bool, default: False\n        Allow epsilon and delta both be zero.\n\n    \"\"\"\n    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon + delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n\ndef check_bounds(bounds, shape=0, min_separation=0.0, dtype=float):\n    \"\"\"Input validation for the ``bounds`` parameter.\n\n    Checks that ``bounds`` is composed of a list of tuples of the form (lower, upper), where lower <= upper and both\n    are numeric.  Also checks that ``bounds`` contains the appropriate number of dimensions, and that there is a\n    ``min_separation`` between the bounds.\n\n    Parameters\n    ----------\n    bounds : tuple\n        Tuple of bounds of the form (min, max). `min` and `max` can either be scalars or 1-dimensional arrays.\n\n    shape : int, default: 0\n        Number of dimensions to be expected in ``bounds``.\n\n    min_separation : float, default: 0.0\n        The minimum separation between `lower` and `upper` of each dimension.  This separation is enforced if not\n        already satisfied.\n\n    dtype : data-type, default: float\n        Data type of the returned bounds.\n\n    Returns\n    -------\n    bounds : tuple\n\n    \"\"\"\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n        if _upper - _lower < min_separation:\n            mid = (_upper + _lower) / 2\n            lower[i] = mid - min_separation / 2\n            upper[i] = mid + min_separation / 2\n\n    if shape == 0:\n        return lower.item(), upper.item()\n\n    if n_bounds == 1:\n        lower = np.ones(shape, dtype=dtype) * lower.item()\n        upper = np.ones(shape, dtype=dtype) * upper.item()\n\n    return lower, upper\n\n\ndef clip_to_norm(array, clip):\n    \"\"\"Clips the examples of a 2-dimensional array to a given maximum norm.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\n\n    clip : float\n        Norm at which to clip each example\n\n    Returns\n    -------\n    array : np.ndarray\n        The clipped array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1) / clip\n    norms[norms < 1] = 1\n\n    return array / norms[:, np.newaxis]\n\n\ndef clip_to_bounds(array, bounds):\n    \"\"\"Clips the examples of a 2-dimensional array to given bounds.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\n\n    bounds : tuple\n        Tuple of bounds of the form (min, max) which the array is to be clipped to. `min` and `max` must be scalar,\n        unless array is 2-dimensional.\n\n    Returns\n    -------\n    array : np.ndarray\n        The clipped array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\n    clipped_array = array.copy()\n\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\n    else:\n        if array.ndim != 2:\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\n\n        for feature in range(array.shape[1]):\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\n\n    return clipped_array\n\n\nclass DiffprivlibMixin:  # pylint: disable=too-few-public-methods\n    \"\"\"Mixin for Diffprivlib models.\"\"\"\n    _check_bounds = staticmethod(check_bounds)\n    _clip_to_norm = staticmethod(clip_to_norm)\n    _clip_to_bounds = staticmethod(clip_to_bounds)\n    _warn_unused_args = staticmethod(warn_unused_args)\n\n    # todo: remove when scikit-learn v1.2 is a min requirement\n    def _validate_params(self):\n        pass\n\n    @staticmethod\n    def _copy_parameter_constraints(cls, *args):\n        \"\"\"Copies the parameter constraints for `*args` from `cls`\n        \"\"\"\n        if not hasattr(cls, \"_parameter_constraints\"):\n            return {}\n\n        return {k: cls._parameter_constraints[k] for k in args if k in cls._parameter_constraints}\n"}, {"diffprivlib.validation.DiffprivlibMixin._validate_params": "# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nValidation functions for the differential privacy library\n\"\"\"\nfrom numbers import Real, Integral\n\nimport numpy as np\n\nfrom diffprivlib.utils import warn_unused_args\n\n\ndef check_epsilon_delta(epsilon, delta, allow_zero=False):\n    \"\"\"Checks that epsilon and delta are valid values for differential privacy.  Throws an error if checks fail,\n    otherwise returns nothing.\n\n    As well as the requirements of epsilon and delta separately, both cannot be simultaneously zero, unless\n    ``allow_zero`` is set to ``True``.\n\n    Parameters\n    ----------\n    epsilon : float\n        Epsilon parameter for differential privacy.  Must be non-negative.\n\n    delta : float\n        Delta parameter for differential privacy.  Must be on the unit interval, [0, 1].\n\n    allow_zero : bool, default: False\n        Allow epsilon and delta both be zero.\n\n    \"\"\"\n    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon + delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n\ndef check_bounds(bounds, shape=0, min_separation=0.0, dtype=float):\n    \"\"\"Input validation for the ``bounds`` parameter.\n\n    Checks that ``bounds`` is composed of a list of tuples of the form (lower, upper), where lower <= upper and both\n    are numeric.  Also checks that ``bounds`` contains the appropriate number of dimensions, and that there is a\n    ``min_separation`` between the bounds.\n\n    Parameters\n    ----------\n    bounds : tuple\n        Tuple of bounds of the form (min, max). `min` and `max` can either be scalars or 1-dimensional arrays.\n\n    shape : int, default: 0\n        Number of dimensions to be expected in ``bounds``.\n\n    min_separation : float, default: 0.0\n        The minimum separation between `lower` and `upper` of each dimension.  This separation is enforced if not\n        already satisfied.\n\n    dtype : data-type, default: float\n        Data type of the returned bounds.\n\n    Returns\n    -------\n    bounds : tuple\n\n    \"\"\"\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n        if _upper - _lower < min_separation:\n            mid = (_upper + _lower) / 2\n            lower[i] = mid - min_separation / 2\n            upper[i] = mid + min_separation / 2\n\n    if shape == 0:\n        return lower.item(), upper.item()\n\n    if n_bounds == 1:\n        lower = np.ones(shape, dtype=dtype) * lower.item()\n        upper = np.ones(shape, dtype=dtype) * upper.item()\n\n    return lower, upper\n\n\ndef clip_to_norm(array, clip):\n    \"\"\"Clips the examples of a 2-dimensional array to a given maximum norm.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\n\n    clip : float\n        Norm at which to clip each example\n\n    Returns\n    -------\n    array : np.ndarray\n        The clipped array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1) / clip\n    norms[norms < 1] = 1\n\n    return array / norms[:, np.newaxis]\n\n\ndef clip_to_bounds(array, bounds):\n    \"\"\"Clips the examples of a 2-dimensional array to given bounds.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\n\n    bounds : tuple\n        Tuple of bounds of the form (min, max) which the array is to be clipped to. `min` and `max` must be scalar,\n        unless array is 2-dimensional.\n\n    Returns\n    -------\n    array : np.ndarray\n        The clipped array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\n    clipped_array = array.copy()\n\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\n    else:\n        if array.ndim != 2:\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\n\n        for feature in range(array.shape[1]):\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\n\n    return clipped_array\n\n\nclass DiffprivlibMixin:  # pylint: disable=too-few-public-methods\n    \"\"\"Mixin for Diffprivlib models.\"\"\"\n    _check_bounds = staticmethod(check_bounds)\n    _clip_to_norm = staticmethod(clip_to_norm)\n    _clip_to_bounds = staticmethod(clip_to_bounds)\n    _warn_unused_args = staticmethod(warn_unused_args)\n\n    # todo: remove when scikit-learn v1.2 is a min requirement\n    def _validate_params(self):\n        pass\n\n    @staticmethod\n    def _copy_parameter_constraints(cls, *args):\n        \"\"\"Copies the parameter constraints for `*args` from `cls`\n        \"\"\"\n        if not hasattr(cls, \"_parameter_constraints\"):\n            return {}\n\n        return {k: cls._parameter_constraints[k] for k in args if k in cls._parameter_constraints}\n"}, {"diffprivlib.validation.DiffprivlibMixin._warn_unused_args": "# MIT License\n#\n# Copyright (C) IBM Corporation 2020\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\"\"\"\nValidation functions for the differential privacy library\n\"\"\"\nfrom numbers import Real, Integral\n\nimport numpy as np\n\nfrom diffprivlib.utils import warn_unused_args\n\n\ndef check_epsilon_delta(epsilon, delta, allow_zero=False):\n    \"\"\"Checks that epsilon and delta are valid values for differential privacy.  Throws an error if checks fail,\n    otherwise returns nothing.\n\n    As well as the requirements of epsilon and delta separately, both cannot be simultaneously zero, unless\n    ``allow_zero`` is set to ``True``.\n\n    Parameters\n    ----------\n    epsilon : float\n        Epsilon parameter for differential privacy.  Must be non-negative.\n\n    delta : float\n        Delta parameter for differential privacy.  Must be on the unit interval, [0, 1].\n\n    allow_zero : bool, default: False\n        Allow epsilon and delta both be zero.\n\n    \"\"\"\n    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon + delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n\ndef check_bounds(bounds, shape=0, min_separation=0.0, dtype=float):\n    \"\"\"Input validation for the ``bounds`` parameter.\n\n    Checks that ``bounds`` is composed of a list of tuples of the form (lower, upper), where lower <= upper and both\n    are numeric.  Also checks that ``bounds`` contains the appropriate number of dimensions, and that there is a\n    ``min_separation`` between the bounds.\n\n    Parameters\n    ----------\n    bounds : tuple\n        Tuple of bounds of the form (min, max). `min` and `max` can either be scalars or 1-dimensional arrays.\n\n    shape : int, default: 0\n        Number of dimensions to be expected in ``bounds``.\n\n    min_separation : float, default: 0.0\n        The minimum separation between `lower` and `upper` of each dimension.  This separation is enforced if not\n        already satisfied.\n\n    dtype : data-type, default: float\n        Data type of the returned bounds.\n\n    Returns\n    -------\n    bounds : tuple\n\n    \"\"\"\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n        if _upper - _lower < min_separation:\n            mid = (_upper + _lower) / 2\n            lower[i] = mid - min_separation / 2\n            upper[i] = mid + min_separation / 2\n\n    if shape == 0:\n        return lower.item(), upper.item()\n\n    if n_bounds == 1:\n        lower = np.ones(shape, dtype=dtype) * lower.item()\n        upper = np.ones(shape, dtype=dtype) * upper.item()\n\n    return lower, upper\n\n\ndef clip_to_norm(array, clip):\n    \"\"\"Clips the examples of a 2-dimensional array to a given maximum norm.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\n\n    clip : float\n        Norm at which to clip each example\n\n    Returns\n    -------\n    array : np.ndarray\n        The clipped array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1) / clip\n    norms[norms < 1] = 1\n\n    return array / norms[:, np.newaxis]\n\n\ndef clip_to_bounds(array, bounds):\n    \"\"\"Clips the examples of a 2-dimensional array to given bounds.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\n\n    bounds : tuple\n        Tuple of bounds of the form (min, max) which the array is to be clipped to. `min` and `max` must be scalar,\n        unless array is 2-dimensional.\n\n    Returns\n    -------\n    array : np.ndarray\n        The clipped array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\n    clipped_array = array.copy()\n\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\n    else:\n        if array.ndim != 2:\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\n\n        for feature in range(array.shape[1]):\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\n\n    return clipped_array\n\n\nclass DiffprivlibMixin:  # pylint: disable=too-few-public-methods\n    \"\"\"Mixin for Diffprivlib models.\"\"\"\n    _check_bounds = staticmethod(check_bounds)\n    _clip_to_norm = staticmethod(clip_to_norm)\n    _clip_to_bounds = staticmethod(clip_to_bounds)\n    _warn_unused_args = staticmethod(warn_unused_args)\n\n    # todo: remove when scikit-learn v1.2 is a min requirement\n    def _validate_params(self):\n        pass\n\n    @staticmethod\n    def _copy_parameter_constraints(cls, *args):\n        \"\"\"Copies the parameter constraints for `*args` from `cls`\n        \"\"\"\n        if not hasattr(cls, \"_parameter_constraints\"):\n            return {}\n\n        return {k: cls._parameter_constraints[k] for k in args if k in cls._parameter_constraints}\n"}], "prompt": "Please write a python function called 'fit' base the context. This function fits a linear regression model to the given training data. It preprocesses the data, determines the bounds, constructs regression objects, and optimizes the coefficients using the minimize function. It also sets the intercept and updates the accountant's spending.:param self: LinearRegression. An instance of the LinearRegression class.\n:param X: array-like or sparse matrix. The training data with shape (n_samples, n_features).\n:param y: array_like. The target values with shape (n_samples, n_targets).\n:param sample_weight: ignored. Ignored by diffprivlib. Present for consistency with sklearn API.\n:return: self. An instance of the LinearRegression class..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # MIT License\n#\n# Copyright (C) IBM Corporation 2019\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n# Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n#\n# New BSD License\n#\n# Copyright (c) 2007\u20132019 The scikit-learn developers.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the\n# following conditions are met:\n#\n#   a. Redistributions of source code must retain the above copyright notice, this list of conditions and the following\n#      disclaimer.\n#   b. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the\n#      following disclaimer in the documentation and/or other materials provided with the distribution.\n#   c. Neither the name of the Scikit-learn Developers  nor the names of its contributors may be used to endorse or\n#      promote products derived from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES,\n# INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n\"\"\"\nLinear Regression with differential privacy\n\"\"\"\nimport warnings\n\nimport numpy as np\nimport sklearn.linear_model as sk_lr\nfrom scipy.optimize import minimize\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import FLOAT_DTYPES\n\nfrom diffprivlib.accountant import BudgetAccountant\nfrom diffprivlib.mechanisms import Laplace, LaplaceFolded\nfrom diffprivlib.tools import mean\nfrom diffprivlib.utils import warn_unused_args, check_random_state\nfrom diffprivlib.validation import check_bounds, clip_to_bounds, DiffprivlibMixin\n\n\n# noinspection PyPep8Naming\ndef _preprocess_data(X, y, fit_intercept, epsilon=1.0, bounds_X=None, bounds_y=None, copy=True, check_input=True,\n                     random_state=None, **unused_args):\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if check_input:\n        X = check_array(X, copy=copy, accept_sparse=False, dtype=FLOAT_DTYPES)\n    elif copy:\n        X = X.copy(order='K')\n\n    y = np.asarray(y, dtype=X.dtype)\n    X_scale = np.ones(X.shape[1], dtype=X.dtype)\n\n    if fit_intercept:\n        bounds_X = check_bounds(bounds_X, X.shape[1])\n        bounds_y = check_bounds(bounds_y, y.shape[1] if y.ndim > 1 else 1)\n\n        X = clip_to_bounds(X, bounds_X)\n        y = clip_to_bounds(y, bounds_y)\n\n        X_offset = mean(X, axis=0, bounds=bounds_X, epsilon=epsilon, random_state=random_state,\n                        accountant=BudgetAccountant())\n        X -= X_offset\n        y_offset = mean(y, axis=0, bounds=bounds_y, epsilon=epsilon, random_state=random_state,\n                        accountant=BudgetAccountant())\n        y = y - y_offset\n    else:\n        X_offset = np.zeros(X.shape[1], dtype=X.dtype)\n        if y.ndim == 1:\n            y_offset = X.dtype.type(0)\n        else:\n            y_offset = np.zeros(y.shape[1], dtype=X.dtype)\n\n    return X, y, X_offset, y_offset, X_scale\n\n\ndef _construct_regression_obj(X, y, bounds_X, bounds_y, epsilon, alpha, random_state):\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    n_features = X.shape[1]\n    n_targets = y.shape[1]\n\n    local_epsilon = epsilon / (1 + n_targets * n_features + n_features * (n_features + 1) / 2)\n    coefs = ((y ** 2).sum(axis=0), np.einsum('ij,ik->jk', X, y), np.einsum('ij,ik', X, X))\n\n    del X, y\n\n    def get_max_sensitivity(y_lower, y_upper, x_lower, x_upper):\n        corners = [y_lower * x_lower, y_lower * x_upper, y_upper * x_lower, y_upper * x_upper]\n        return np.max(corners) - np.min(corners)\n\n    # Randomise 0th-degree monomial coefficients\n    mono_coef_0 = np.zeros(n_targets)\n\n    for i in range(n_targets):\n        sensitivity = np.abs([bounds_y[0][i], bounds_y[1][i]]).max() ** 2\n        mech = LaplaceFolded(epsilon=local_epsilon, sensitivity=sensitivity, lower=0, upper=float(\"inf\"),\n                             random_state=random_state)\n        mono_coef_0[i] = mech.randomise(coefs[0][i])\n\n    # Randomise 1st-degree monomial coefficients\n    mono_coef_1 = np.zeros((n_features, n_targets))\n\n    for i in range(n_targets):\n        for j in range(n_features):\n            sensitivity = get_max_sensitivity(bounds_y[0][i], bounds_y[1][i], bounds_X[0][j], bounds_X[1][j])\n            mech = Laplace(epsilon=local_epsilon, sensitivity=sensitivity, random_state=random_state)\n            mono_coef_1[j, i] = mech.randomise(coefs[1][j, i])\n\n    # Randomise 2nd-degree monomial coefficients\n    mono_coef_2 = np.zeros((n_features, n_features))\n\n    for i in range(n_features):\n        sensitivity = np.max(np.abs([bounds_X[0][i], bounds_X[0][i]])) ** 2\n        mech = LaplaceFolded(epsilon=local_epsilon, sensitivity=sensitivity, lower=0, upper=float(\"inf\"),\n                             random_state=random_state)\n        mono_coef_2[i, i] = mech.randomise(coefs[2][i, i])\n\n        for j in range(i + 1, n_features):\n            sensitivity = get_max_sensitivity(bounds_X[0][i], bounds_X[1][i], bounds_X[0][j], bounds_X[1][j])\n            mech = Laplace(epsilon=local_epsilon, sensitivity=sensitivity, random_state=random_state)\n            mono_coef_2[i, j] = mech.randomise(coefs[2][i, j])\n            mono_coef_2[j, i] = mono_coef_2[i, j]  # Enforce symmetry\n\n    del coefs\n    noisy_coefs = (mono_coef_0, mono_coef_1, mono_coef_2)\n\n    def obj(idx):\n        def inner_obj(omega):\n            func = noisy_coefs[0][idx]\n            func -= 2 * np.dot(noisy_coefs[1][:, idx], omega)\n            func += np.multiply(noisy_coefs[2], np.tensordot(omega, omega, axes=0)).sum()\n            func += alpha * (omega ** 2).sum()\n\n            grad = - 2 * noisy_coefs[1][:, idx] + 2 * np.matmul(noisy_coefs[2], omega) + 2 * omega * alpha\n\n            return func, grad\n\n        return inner_obj\n\n    output = tuple(obj(i) for i in range(n_targets))\n\n    return output, noisy_coefs\n\n\n# noinspection PyPep8Naming,PyAttributeOutsideInit\nclass LinearRegression(sk_lr.LinearRegression, DiffprivlibMixin):\n    r\"\"\"\n    Ordinary least squares Linear Regression with differential privacy.\n\n    LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares\n    between the observed targets in the dataset, and the targets predicted by the linear approximation.  Differential\n    privacy is guaranteed with respect to the training sample.\n\n    Differential privacy is achieved by adding noise to the coefficients of the objective function, taking inspiration\n    from [ZZX12]_.\n\n    Parameters\n    ----------\n    epsilon : float, default: 1.0\n        Privacy parameter :math:`\\epsilon`.\n\n    bounds_X :  tuple\n        Bounds of the data, provided as a tuple of the form (min, max).  `min` and `max` can either be scalars, covering\n        the min/max of the entire data, or vectors with one entry per feature.  If not provided, the bounds are computed\n        on the data when ``.fit()`` is first called, resulting in a :class:`.PrivacyLeakWarning`.\n\n    bounds_y : tuple\n        Same as `bounds_X`, but for the training label set `y`.\n\n    fit_intercept : bool, default: True\n        Whether to calculate the intercept for this model.  If set to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default: True\n        If True, X will be copied; else, it may be overwritten.\n\n    random_state : int or RandomState, optional\n        Controls the randomness of the model.  To obtain a deterministic behaviour during randomisation,\n        ``random_state`` has to be fixed to an integer.\n\n    accountant : BudgetAccountant, optional\n        Accountant to keep track of privacy budget.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features, ) or (n_targets, n_features)\n        Estimated coefficients for the linear regression problem.  If multiple targets are passed during the fit (y 2D),\n        this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of\n        length n_features.\n\n    intercept_ : float or array of shape of (n_targets,)\n        Independent term in the linear model.  Set to 0.0 if `fit_intercept = False`.\n\n    References\n    ----------\n    .. [ZZX12] Zhang, Jun, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and Marianne Winslett. \"Functional mechanism:\n        regression analysis under differential privacy.\" arXiv preprint arXiv:1208.0219 (2012).\n\n    \"\"\"\n\n    _parameter_constraints = DiffprivlibMixin._copy_parameter_constraints(\n        sk_lr.LinearRegression, \"fit_intercept\", \"copy_X\")\n\n    def __init__(self, *, epsilon=1.0, bounds_X=None, bounds_y=None, fit_intercept=True, copy_X=True, random_state=None,\n                 accountant=None, **unused_args):\n        super().__init__(fit_intercept=fit_intercept, copy_X=copy_X, n_jobs=None)\n\n        self.epsilon = epsilon\n        self.bounds_X = bounds_X\n        self.bounds_y = bounds_y\n        self.random_state = random_state\n        self.accountant = BudgetAccountant.load_default(accountant)\n\n        self._warn_unused_args(unused_args)\n\n###The function: fit###\n    _preprocess_data = staticmethod(_preprocess_data)\n\n        ####cross_file_context:\n        [{'diffprivlib.accountant.BudgetAccountant.check': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2020\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nPrivacy budget accountant for differential privacy\\n\"\"\"\\nfrom numbers import Integral\\n\\nimport numpy as np\\n\\nfrom diffprivlib.utils import Budget\\nfrom diffprivlib.validation import check_epsilon_delta\\n\\n\\nclass BudgetAccountant:\\n    \"\"\"Privacy budget accountant for differential privacy.\\n\\n    This class creates a privacy budget accountant to track privacy spend across queries and other data accesses.  Once\\n    initialised, the BudgetAccountant stores each privacy spend and iteratively updates the total budget spend, raising\\n    an error when the budget ceiling (if specified) is exceeded.  The accountant can be initialised without any maximum\\n    budget, to enable users track the total privacy spend of their actions without hindrance.\\n\\n    Diffprivlib functions can make use of a BudgetAccountant in three different ways (see examples for more details):\\n\\n        - Passed as an ``accountant`` parameter to the function (e.g., ``mean(..., accountant=acc)``)\\n        - Set as the default using the ``set_default()`` method (all subsequent diffprivlib functions will use the\\n          accountant by default)\\n        - As a context manager using a ``with`` statement (the accountant is used for that block of code)\\n\\n    Implements the accountant rules as given in [KOV17]_.\\n\\n    Parameters\\n    ----------\\n    epsilon : float, default: infinity\\n        Epsilon budget ceiling of the accountant.\\n\\n    delta : float, default: 1.0\\n        Delta budget ceiling of the accountant.\\n\\n    slack : float, default: 0.0\\n        Slack allowed in delta spend.  Greater slack may reduce the overall epsilon spend.\\n\\n    spent_budget : list of tuples of the form (epsilon, delta), optional\\n        List of tuples of pre-existing budget spends.  Allows for a new accountant to be initialised with spends\\n        extracted from a previous instance.\\n\\n    Attributes\\n    ----------\\n    epsilon : float\\n        Epsilon budget ceiling of the accountant.\\n\\n    delta : float\\n        Delta budget ceiling of the accountant.\\n\\n    slack : float\\n        The accountant\\'s slack.  Can be modified at runtime, subject to the privacy budget not being exceeded.\\n\\n    spent_budget : list of tuples of the form (epsilon, delta)\\n        The list of privacy spends recorded by the accountant.  Can be used in the initialisation of a new accountant.\\n\\n    Examples\\n    --------\\n\\n    A ``BudgetAccountant`` is typically passed to diffprivlib functions as an ``accountant`` parameter.  If ``epsilon``\\n    and ``delta`` are not set, the accountant has an infinite budget by default, allowing you to track privacy spend\\n    without imposing a hard limit.  By allowing a ``slack`` in the budget calculation, the overall epsilon privacy spend\\n    can be reduced (at the cost of extra delta spend).\\n\\n    >>> import diffprivlib as dp\\n    >>> from numpy.random import random\\n    >>> X = random(100)\\n    >>> acc = dp.BudgetAccountant(epsilon=1.5, delta=0)\\n    >>> dp.tools.mean(X, bounds=(0, 1), accountant=acc)\\n    0.4547006207923884\\n    >>> acc.total()\\n    (epsilon=1.0, delta=0)\\n    >>> dp.tools.std(X, bounds=(0, 1), epsilon=0.25, accountant=acc)\\n    0.2630216611181259\\n    >>> acc.total()\\n    (epsilon=1.25, delta=0)\\n\\n    >>> acc2 = dp.BudgetAccountant() # infinite budget\\n    >>> first_half = dp.tools.mean(X[:50], epsilon=0.25, bounds=(0, 1), accountant=acc2)\\n    >>> last_half = dp.tools.mean(X[50:], epsilon=0.25, bounds=(0, 1), accountant=acc2)\\n    >>> acc2.total()\\n    (epsilon=0.5, delta=0)\\n    >>> acc2.remaining()\\n    (epsilon=inf, delta=1.0)\\n\\n    >>> acc3 = dp.BudgetAccountant(slack=1e-3)\\n    >>> for i in range(20):\\n    ...     dp.tools.mean(X, epsilon=0.05, bounds=(0, 1), accountant=acc3)\\n    >>> acc3.total() # Slack has reduced the epsilon spend by almost 25%\\n    (epsilon=0.7613352285668463, delta=0.001)\\n\\n    Using ``set_default()``, an accountant is used by default in all diffprivlib functions in that script.  Accountants\\n    also act as context managers, allowing for use in a ``with`` statement.  Passing an accountant as a parameter\\n    overrides all other methods.\\n\\n    >>> acc4 = dp.BudgetAccountant()\\n    >>> acc4.set_default()\\n    BudgetAccountant()\\n    >>> Y = random((100, 2)) - 0.5\\n    >>> clf = dp.models.PCA(1, centered=True, data_norm=1.4)\\n    >>> clf.fit(Y)\\n    PCA(accountant=BudgetAccountant(spent_budget=[(1.0, 0)]), centered=True, copy=True, data_norm=1.4, epsilon=1.0,\\n    n_components=1, random_state=None, bounds=None, whiten=False)\\n    >>> acc4.total()\\n    (epsilon=1.0, delta=0)\\n\\n    >>> with dp.BudgetAccountant() as acc5:\\n    ...     dp.tools.mean(Y, bounds=(0, 1), epsilon=1/3)\\n    >>> acc5.total()\\n    (epsilon=0.3333333333333333, delta=0)\\n\\n    References\\n    ----------\\n    .. [KOV17] Kairouz, Peter, Sewoong Oh, and Pramod Viswanath. \"The composition theorem for differential privacy.\"\\n        IEEE Transactions on Information Theory 63.6 (2017): 4037-4049.\\n\\n    \"\"\"\\n    _default = None\\n\\n    def __init__(self, epsilon=float(\"inf\"), delta=1.0, slack=0.0, spent_budget=None):\\n        check_epsilon_delta(epsilon, delta)\\n        self.__epsilon = epsilon\\n        self.__min_epsilon = 0 if epsilon == float(\"inf\") else epsilon * 1e-14\\n        self.__delta = delta\\n        self.__spent_budget = []\\n        self.slack = slack\\n\\n        if spent_budget is not None:\\n            if not isinstance(spent_budget, list):\\n                raise TypeError(\"spent_budget must be a list\")\\n\\n            for _epsilon, _delta in spent_budget:\\n                self.spend(_epsilon, _delta)\\n\\n    def __repr__(self, n_budget_max=5):\\n        params = []\\n        if self.epsilon != float(\"inf\"):\\n            params.append(f\"epsilon={self.epsilon}\")\\n\\n        if self.delta != 1:\\n            params.append(f\"delta={self.delta}\")\\n\\n        if self.slack > 0:\\n            params.append(f\"slack={self.slack}\")\\n\\n        if self.spent_budget:\\n            if len(self.spent_budget) > n_budget_max:\\n                params.append(\"spent_budget=\" + str(self.spent_budget[:n_budget_max] + [\"...\"]).replace(\"\\'\", \"\"))\\n            else:\\n                params.append(\"spent_budget=\" + str(self.spent_budget))\\n\\n        return \"BudgetAccountant(\" + \", \".join(params) + \")\"\\n\\n    def __enter__(self):\\n        self.old_default = self.pop_default()\\n        self.set_default()\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.pop_default()\\n\\n        if self.old_default is not None:\\n            self.old_default.set_default()\\n        del self.old_default\\n\\n    def __len__(self):\\n        return len(self.spent_budget)\\n\\n    @property\\n    def slack(self):\\n        \"\"\"Slack parameter for composition.\\n        \"\"\"\\n        return self.__slack\\n\\n    @slack.setter\\n    def slack(self, slack):\\n        if not 0 <= slack <= self.delta:\\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\\n\\n        epsilon_spent, delta_spent = self.total(slack=slack)\\n\\n        if self.epsilon < epsilon_spent or self.delta < delta_spent:\\n            raise BudgetError(f\"Privacy budget will be exceeded by changing slack to {slack}.\")\\n\\n        self.__slack = slack\\n\\n    @property\\n    def spent_budget(self):\\n        \"\"\"List of tuples of the form (epsilon, delta) of spent privacy budget.\\n        \"\"\"\\n        return self.__spent_budget.copy()\\n\\n    @property\\n    def epsilon(self):\\n        \"\"\"Epsilon privacy ceiling of the accountant.\\n        \"\"\"\\n        return self.__epsilon\\n\\n    @property\\n    def delta(self):\\n        \"\"\"Delta privacy ceiling of the accountant.\\n        \"\"\"\\n        return self.__delta\\n\\n    def total(self, spent_budget=None, slack=None):\\n        \"\"\"Returns the total current privacy spend.\\n\\n        `spent_budget` and `slack` can be specified as parameters, otherwise the class values will be used.\\n\\n        Parameters\\n        ----------\\n        spent_budget : list of tuples of the form (epsilon, delta), optional\\n            List of tuples of budget spends.  If not provided, the accountant\\'s spends will be used.\\n\\n        slack : float, optional\\n            Slack in delta for composition.  If not provided, the accountant\\'s slack will be used.\\n\\n        Returns\\n        -------\\n        epsilon : float\\n            Total epsilon spend.\\n\\n        delta : float\\n            Total delta spend.\\n\\n        \"\"\"\\n        if spent_budget is None:\\n            spent_budget = self.spent_budget\\n        else:\\n            for epsilon, delta in spent_budget:\\n                check_epsilon_delta(epsilon, delta)\\n\\n        if slack is None:\\n            slack = self.slack\\n        elif not 0 <= slack <= self.delta:\\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\\n\\n        epsilon_sum, epsilon_exp_sum, epsilon_sq_sum = 0, 0, 0\\n\\n        for epsilon, _ in spent_budget:\\n            epsilon_sum += epsilon\\n            epsilon_exp_sum += (1 - np.exp(-epsilon)) * epsilon / (1 + np.exp(-epsilon))\\n            epsilon_sq_sum += epsilon ** 2\\n\\n        total_epsilon_naive = epsilon_sum\\n        total_delta = self.__total_delta_safe(spent_budget, slack)\\n\\n        if slack == 0:\\n            return Budget(total_epsilon_naive, total_delta)\\n\\n        total_epsilon_drv = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum * np.log(1 / slack))\\n        total_epsilon_kov = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum *\\n                                                      np.log(np.exp(1) + np.sqrt(epsilon_sq_sum) / slack))\\n\\n        return Budget(min(total_epsilon_naive, total_epsilon_drv, total_epsilon_kov), total_delta)\\n\\n    def check(self, epsilon, delta):\\n        \"\"\"Checks if the provided (epsilon,delta) can be spent without exceeding the accountant\\'s budget ceiling.\\n\\n        Parameters\\n        ----------\\n        epsilon : float\\n            Epsilon budget spend to check.\\n\\n        delta : float\\n            Delta budget spend to check.\\n\\n        Returns\\n        -------\\n        bool\\n            True if the budget can be spent, otherwise a :class:`.BudgetError` is raised.\\n\\n        Raises\\n        ------\\n        BudgetError\\n            If the specified budget spend will result in the budget ceiling being exceeded.\\n\\n        \"\"\"\\n        from diffprivlib.utils import BudgetError\\n        check_epsilon_delta(epsilon, delta)\\n        if self.epsilon == float(\"inf\") and self.delta == 1:\\n            return True\\n\\n        if 0 < epsilon < self.__min_epsilon:\\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\\n\\n        spent_budget = self.spent_budget + [(epsilon, delta)]\\n\\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\\n            return True\\n\\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\\n\\n    def remaining(self, k=1):\\n        \"\"\"Calculates the budget that remains to be spent.\\n\\n        Calculates the privacy budget that can be spent on `k` queries.  Spending this budget on `k` queries will\\n        match the budget ceiling, assuming no floating point errors.\\n\\n        Parameters\\n        ----------\\n        k : int, default: 1\\n            The number of queries for which to calculate the remaining budget.\\n\\n        Returns\\n        -------\\n        epsilon : float\\n            Total epsilon spend remaining for `k` queries.\\n\\n        delta : float\\n            Total delta spend remaining for `k` queries.\\n\\n        \"\"\"\\n        if not isinstance(k, Integral):\\n            raise TypeError(f\"k must be integer-valued, got {type(k)}.\")\\n        if k < 1:\\n            raise ValueError(f\"k must be at least 1, got {k}.\")\\n\\n        _, spent_delta = self.total()\\n        delta = 1 - ((1 - self.delta) / (1 - spent_delta)) ** (1 / k) if spent_delta < 1.0 else 1.0\\n        # delta = 1 - np.exp((np.log(1 - self.delta) - np.log(1 - spent_delta)) / k)\\n\\n        lower = 0\\n        upper = self.epsilon\\n        old_interval_size = (upper - lower) * 2\\n\\n        while old_interval_size > upper - lower:\\n            old_interval_size = upper - lower\\n            mid = (upper + lower) / 2\\n\\n            spent_budget = self.spent_budget + [(mid, 0)] * k\\n            x_0, _ = self.total(spent_budget=spent_budget)\\n\\n            if x_0 >= self.epsilon:\\n                upper = mid\\n            if x_0 <= self.epsilon:\\n                lower = mid\\n\\n        epsilon = (upper + lower) / 2\\n\\n        return Budget(epsilon, delta)\\n\\n    def spend(self, epsilon, delta):\\n        \"\"\"Spend the given privacy budget.\\n\\n        Instructs the accountant to spend the given epsilon and delta privacy budget, while ensuring the target budget\\n        is not exceeded.\\n\\n        Parameters\\n        ----------\\n        epsilon : float\\n            Epsilon privacy budget to spend.\\n\\n        delta : float\\n            Delta privacy budget to spend.\\n\\n        Returns\\n        -------\\n        self : BudgetAccountant\\n\\n        \"\"\"\\n        self.check(epsilon, delta)\\n        self.__spent_budget.append((epsilon, delta))\\n        return self\\n\\n    @staticmethod\\n    def __total_delta_safe(spent_budget, slack):\\n        \"\"\"\\n        Calculate total delta spend of `spent_budget`, with special consideration for floating point arithmetic.\\n        Should yield greater precision, especially for a large number of budget spends with very small delta.\\n\\n        Parameters\\n        ----------\\n        spent_budget: list of tuples of the form (epsilon, delta)\\n            List of budget spends, for which the total delta spend is to be calculated.\\n\\n        slack: float\\n            Delta slack parameter for composition of spends.\\n\\n        Returns\\n        -------\\n        float\\n            Total delta spend.\\n\\n        \"\"\"\\n        delta_spend = [slack]\\n        for _, delta in spent_budget:\\n            delta_spend.append(delta)\\n        delta_spend.sort()\\n\\n        # (1 - a) * (1 - b) = 1 - (a + b - a * b)\\n        prod = 0\\n        for delta in delta_spend:\\n            prod += delta - prod * delta\\n\\n        return prod\\n\\n    @staticmethod\\n    def load_default(accountant):\\n        \"\"\"Loads the default privacy budget accountant if none is supplied, otherwise checks that the supplied\\n        accountant is a BudgetAccountant class.\\n\\n        An accountant can be set as the default using the `set_default()` method.  If no default has been set, a default\\n        is created.\\n\\n        Parameters\\n        ----------\\n        accountant : BudgetAccountant or None\\n            The supplied budget accountant.  If None, the default accountant is returned.\\n\\n        Returns\\n        -------\\n        default : BudgetAccountant\\n            Returns a working BudgetAccountant, either the supplied `accountant` or the existing default.\\n\\n        \"\"\"\\n        if accountant is None:\\n            if BudgetAccountant._default is None:\\n                BudgetAccountant._default = BudgetAccountant()\\n\\n            return BudgetAccountant._default\\n\\n        if not isinstance(accountant, BudgetAccountant):\\n            raise TypeError(f\"Accountant must be of type BudgetAccountant, got {type(accountant)}\")\\n\\n        return accountant\\n\\n    def set_default(self):\\n        \"\"\"Sets the current accountant to be the default when running functions and queries with diffprivlib.\\n\\n        Returns\\n        -------\\n        self : BudgetAccountant\\n\\n        \"\"\"\\n        BudgetAccountant._default = self\\n        return self\\n\\n    @staticmethod\\n    def pop_default():\\n        \"\"\"Pops the default BudgetAccountant from the class and returns it to the user.\\n\\n        Returns\\n        -------\\n        default : BudgetAccountant\\n            Returns the existing default BudgetAccountant.\\n\\n        \"\"\"\\n        default = BudgetAccountant._default\\n        BudgetAccountant._default = None\\n        return default\\n'}, {'diffprivlib.accountant.BudgetAccountant.spend': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2020\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nPrivacy budget accountant for differential privacy\\n\"\"\"\\nfrom numbers import Integral\\n\\nimport numpy as np\\n\\nfrom diffprivlib.utils import Budget\\nfrom diffprivlib.validation import check_epsilon_delta\\n\\n\\nclass BudgetAccountant:\\n    \"\"\"Privacy budget accountant for differential privacy.\\n\\n    This class creates a privacy budget accountant to track privacy spend across queries and other data accesses.  Once\\n    initialised, the BudgetAccountant stores each privacy spend and iteratively updates the total budget spend, raising\\n    an error when the budget ceiling (if specified) is exceeded.  The accountant can be initialised without any maximum\\n    budget, to enable users track the total privacy spend of their actions without hindrance.\\n\\n    Diffprivlib functions can make use of a BudgetAccountant in three different ways (see examples for more details):\\n\\n        - Passed as an ``accountant`` parameter to the function (e.g., ``mean(..., accountant=acc)``)\\n        - Set as the default using the ``set_default()`` method (all subsequent diffprivlib functions will use the\\n          accountant by default)\\n        - As a context manager using a ``with`` statement (the accountant is used for that block of code)\\n\\n    Implements the accountant rules as given in [KOV17]_.\\n\\n    Parameters\\n    ----------\\n    epsilon : float, default: infinity\\n        Epsilon budget ceiling of the accountant.\\n\\n    delta : float, default: 1.0\\n        Delta budget ceiling of the accountant.\\n\\n    slack : float, default: 0.0\\n        Slack allowed in delta spend.  Greater slack may reduce the overall epsilon spend.\\n\\n    spent_budget : list of tuples of the form (epsilon, delta), optional\\n        List of tuples of pre-existing budget spends.  Allows for a new accountant to be initialised with spends\\n        extracted from a previous instance.\\n\\n    Attributes\\n    ----------\\n    epsilon : float\\n        Epsilon budget ceiling of the accountant.\\n\\n    delta : float\\n        Delta budget ceiling of the accountant.\\n\\n    slack : float\\n        The accountant\\'s slack.  Can be modified at runtime, subject to the privacy budget not being exceeded.\\n\\n    spent_budget : list of tuples of the form (epsilon, delta)\\n        The list of privacy spends recorded by the accountant.  Can be used in the initialisation of a new accountant.\\n\\n    Examples\\n    --------\\n\\n    A ``BudgetAccountant`` is typically passed to diffprivlib functions as an ``accountant`` parameter.  If ``epsilon``\\n    and ``delta`` are not set, the accountant has an infinite budget by default, allowing you to track privacy spend\\n    without imposing a hard limit.  By allowing a ``slack`` in the budget calculation, the overall epsilon privacy spend\\n    can be reduced (at the cost of extra delta spend).\\n\\n    >>> import diffprivlib as dp\\n    >>> from numpy.random import random\\n    >>> X = random(100)\\n    >>> acc = dp.BudgetAccountant(epsilon=1.5, delta=0)\\n    >>> dp.tools.mean(X, bounds=(0, 1), accountant=acc)\\n    0.4547006207923884\\n    >>> acc.total()\\n    (epsilon=1.0, delta=0)\\n    >>> dp.tools.std(X, bounds=(0, 1), epsilon=0.25, accountant=acc)\\n    0.2630216611181259\\n    >>> acc.total()\\n    (epsilon=1.25, delta=0)\\n\\n    >>> acc2 = dp.BudgetAccountant() # infinite budget\\n    >>> first_half = dp.tools.mean(X[:50], epsilon=0.25, bounds=(0, 1), accountant=acc2)\\n    >>> last_half = dp.tools.mean(X[50:], epsilon=0.25, bounds=(0, 1), accountant=acc2)\\n    >>> acc2.total()\\n    (epsilon=0.5, delta=0)\\n    >>> acc2.remaining()\\n    (epsilon=inf, delta=1.0)\\n\\n    >>> acc3 = dp.BudgetAccountant(slack=1e-3)\\n    >>> for i in range(20):\\n    ...     dp.tools.mean(X, epsilon=0.05, bounds=(0, 1), accountant=acc3)\\n    >>> acc3.total() # Slack has reduced the epsilon spend by almost 25%\\n    (epsilon=0.7613352285668463, delta=0.001)\\n\\n    Using ``set_default()``, an accountant is used by default in all diffprivlib functions in that script.  Accountants\\n    also act as context managers, allowing for use in a ``with`` statement.  Passing an accountant as a parameter\\n    overrides all other methods.\\n\\n    >>> acc4 = dp.BudgetAccountant()\\n    >>> acc4.set_default()\\n    BudgetAccountant()\\n    >>> Y = random((100, 2)) - 0.5\\n    >>> clf = dp.models.PCA(1, centered=True, data_norm=1.4)\\n    >>> clf.fit(Y)\\n    PCA(accountant=BudgetAccountant(spent_budget=[(1.0, 0)]), centered=True, copy=True, data_norm=1.4, epsilon=1.0,\\n    n_components=1, random_state=None, bounds=None, whiten=False)\\n    >>> acc4.total()\\n    (epsilon=1.0, delta=0)\\n\\n    >>> with dp.BudgetAccountant() as acc5:\\n    ...     dp.tools.mean(Y, bounds=(0, 1), epsilon=1/3)\\n    >>> acc5.total()\\n    (epsilon=0.3333333333333333, delta=0)\\n\\n    References\\n    ----------\\n    .. [KOV17] Kairouz, Peter, Sewoong Oh, and Pramod Viswanath. \"The composition theorem for differential privacy.\"\\n        IEEE Transactions on Information Theory 63.6 (2017): 4037-4049.\\n\\n    \"\"\"\\n    _default = None\\n\\n    def __init__(self, epsilon=float(\"inf\"), delta=1.0, slack=0.0, spent_budget=None):\\n        check_epsilon_delta(epsilon, delta)\\n        self.__epsilon = epsilon\\n        self.__min_epsilon = 0 if epsilon == float(\"inf\") else epsilon * 1e-14\\n        self.__delta = delta\\n        self.__spent_budget = []\\n        self.slack = slack\\n\\n        if spent_budget is not None:\\n            if not isinstance(spent_budget, list):\\n                raise TypeError(\"spent_budget must be a list\")\\n\\n            for _epsilon, _delta in spent_budget:\\n                self.spend(_epsilon, _delta)\\n\\n    def __repr__(self, n_budget_max=5):\\n        params = []\\n        if self.epsilon != float(\"inf\"):\\n            params.append(f\"epsilon={self.epsilon}\")\\n\\n        if self.delta != 1:\\n            params.append(f\"delta={self.delta}\")\\n\\n        if self.slack > 0:\\n            params.append(f\"slack={self.slack}\")\\n\\n        if self.spent_budget:\\n            if len(self.spent_budget) > n_budget_max:\\n                params.append(\"spent_budget=\" + str(self.spent_budget[:n_budget_max] + [\"...\"]).replace(\"\\'\", \"\"))\\n            else:\\n                params.append(\"spent_budget=\" + str(self.spent_budget))\\n\\n        return \"BudgetAccountant(\" + \", \".join(params) + \")\"\\n\\n    def __enter__(self):\\n        self.old_default = self.pop_default()\\n        self.set_default()\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.pop_default()\\n\\n        if self.old_default is not None:\\n            self.old_default.set_default()\\n        del self.old_default\\n\\n    def __len__(self):\\n        return len(self.spent_budget)\\n\\n    @property\\n    def slack(self):\\n        \"\"\"Slack parameter for composition.\\n        \"\"\"\\n        return self.__slack\\n\\n    @slack.setter\\n    def slack(self, slack):\\n        if not 0 <= slack <= self.delta:\\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\\n\\n        epsilon_spent, delta_spent = self.total(slack=slack)\\n\\n        if self.epsilon < epsilon_spent or self.delta < delta_spent:\\n            raise BudgetError(f\"Privacy budget will be exceeded by changing slack to {slack}.\")\\n\\n        self.__slack = slack\\n\\n    @property\\n    def spent_budget(self):\\n        \"\"\"List of tuples of the form (epsilon, delta) of spent privacy budget.\\n        \"\"\"\\n        return self.__spent_budget.copy()\\n\\n    @property\\n    def epsilon(self):\\n        \"\"\"Epsilon privacy ceiling of the accountant.\\n        \"\"\"\\n        return self.__epsilon\\n\\n    @property\\n    def delta(self):\\n        \"\"\"Delta privacy ceiling of the accountant.\\n        \"\"\"\\n        return self.__delta\\n\\n    def total(self, spent_budget=None, slack=None):\\n        \"\"\"Returns the total current privacy spend.\\n\\n        `spent_budget` and `slack` can be specified as parameters, otherwise the class values will be used.\\n\\n        Parameters\\n        ----------\\n        spent_budget : list of tuples of the form (epsilon, delta), optional\\n            List of tuples of budget spends.  If not provided, the accountant\\'s spends will be used.\\n\\n        slack : float, optional\\n            Slack in delta for composition.  If not provided, the accountant\\'s slack will be used.\\n\\n        Returns\\n        -------\\n        epsilon : float\\n            Total epsilon spend.\\n\\n        delta : float\\n            Total delta spend.\\n\\n        \"\"\"\\n        if spent_budget is None:\\n            spent_budget = self.spent_budget\\n        else:\\n            for epsilon, delta in spent_budget:\\n                check_epsilon_delta(epsilon, delta)\\n\\n        if slack is None:\\n            slack = self.slack\\n        elif not 0 <= slack <= self.delta:\\n            raise ValueError(f\"Slack must be between 0 and delta ({self.delta}), inclusive. Got {slack}.\")\\n\\n        epsilon_sum, epsilon_exp_sum, epsilon_sq_sum = 0, 0, 0\\n\\n        for epsilon, _ in spent_budget:\\n            epsilon_sum += epsilon\\n            epsilon_exp_sum += (1 - np.exp(-epsilon)) * epsilon / (1 + np.exp(-epsilon))\\n            epsilon_sq_sum += epsilon ** 2\\n\\n        total_epsilon_naive = epsilon_sum\\n        total_delta = self.__total_delta_safe(spent_budget, slack)\\n\\n        if slack == 0:\\n            return Budget(total_epsilon_naive, total_delta)\\n\\n        total_epsilon_drv = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum * np.log(1 / slack))\\n        total_epsilon_kov = epsilon_exp_sum + np.sqrt(2 * epsilon_sq_sum *\\n                                                      np.log(np.exp(1) + np.sqrt(epsilon_sq_sum) / slack))\\n\\n        return Budget(min(total_epsilon_naive, total_epsilon_drv, total_epsilon_kov), total_delta)\\n\\n    def check(self, epsilon, delta):\\n        \"\"\"Checks if the provided (epsilon,delta) can be spent without exceeding the accountant\\'s budget ceiling.\\n\\n        Parameters\\n        ----------\\n        epsilon : float\\n            Epsilon budget spend to check.\\n\\n        delta : float\\n            Delta budget spend to check.\\n\\n        Returns\\n        -------\\n        bool\\n            True if the budget can be spent, otherwise a :class:`.BudgetError` is raised.\\n\\n        Raises\\n        ------\\n        BudgetError\\n            If the specified budget spend will result in the budget ceiling being exceeded.\\n\\n        \"\"\"\\n        from diffprivlib.utils import BudgetError\\n        check_epsilon_delta(epsilon, delta)\\n        if self.epsilon == float(\"inf\") and self.delta == 1:\\n            return True\\n\\n        if 0 < epsilon < self.__min_epsilon:\\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\\n\\n        spent_budget = self.spent_budget + [(epsilon, delta)]\\n\\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\\n            return True\\n\\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\\n\\n    def remaining(self, k=1):\\n        \"\"\"Calculates the budget that remains to be spent.\\n\\n        Calculates the privacy budget that can be spent on `k` queries.  Spending this budget on `k` queries will\\n        match the budget ceiling, assuming no floating point errors.\\n\\n        Parameters\\n        ----------\\n        k : int, default: 1\\n            The number of queries for which to calculate the remaining budget.\\n\\n        Returns\\n        -------\\n        epsilon : float\\n            Total epsilon spend remaining for `k` queries.\\n\\n        delta : float\\n            Total delta spend remaining for `k` queries.\\n\\n        \"\"\"\\n        if not isinstance(k, Integral):\\n            raise TypeError(f\"k must be integer-valued, got {type(k)}.\")\\n        if k < 1:\\n            raise ValueError(f\"k must be at least 1, got {k}.\")\\n\\n        _, spent_delta = self.total()\\n        delta = 1 - ((1 - self.delta) / (1 - spent_delta)) ** (1 / k) if spent_delta < 1.0 else 1.0\\n        # delta = 1 - np.exp((np.log(1 - self.delta) - np.log(1 - spent_delta)) / k)\\n\\n        lower = 0\\n        upper = self.epsilon\\n        old_interval_size = (upper - lower) * 2\\n\\n        while old_interval_size > upper - lower:\\n            old_interval_size = upper - lower\\n            mid = (upper + lower) / 2\\n\\n            spent_budget = self.spent_budget + [(mid, 0)] * k\\n            x_0, _ = self.total(spent_budget=spent_budget)\\n\\n            if x_0 >= self.epsilon:\\n                upper = mid\\n            if x_0 <= self.epsilon:\\n                lower = mid\\n\\n        epsilon = (upper + lower) / 2\\n\\n        return Budget(epsilon, delta)\\n\\n    def spend(self, epsilon, delta):\\n        \"\"\"Spend the given privacy budget.\\n\\n        Instructs the accountant to spend the given epsilon and delta privacy budget, while ensuring the target budget\\n        is not exceeded.\\n\\n        Parameters\\n        ----------\\n        epsilon : float\\n            Epsilon privacy budget to spend.\\n\\n        delta : float\\n            Delta privacy budget to spend.\\n\\n        Returns\\n        -------\\n        self : BudgetAccountant\\n\\n        \"\"\"\\n        self.check(epsilon, delta)\\n        self.__spent_budget.append((epsilon, delta))\\n        return self\\n\\n    @staticmethod\\n    def __total_delta_safe(spent_budget, slack):\\n        \"\"\"\\n        Calculate total delta spend of `spent_budget`, with special consideration for floating point arithmetic.\\n        Should yield greater precision, especially for a large number of budget spends with very small delta.\\n\\n        Parameters\\n        ----------\\n        spent_budget: list of tuples of the form (epsilon, delta)\\n            List of budget spends, for which the total delta spend is to be calculated.\\n\\n        slack: float\\n            Delta slack parameter for composition of spends.\\n\\n        Returns\\n        -------\\n        float\\n            Total delta spend.\\n\\n        \"\"\"\\n        delta_spend = [slack]\\n        for _, delta in spent_budget:\\n            delta_spend.append(delta)\\n        delta_spend.sort()\\n\\n        # (1 - a) * (1 - b) = 1 - (a + b - a * b)\\n        prod = 0\\n        for delta in delta_spend:\\n            prod += delta - prod * delta\\n\\n        return prod\\n\\n    @staticmethod\\n    def load_default(accountant):\\n        \"\"\"Loads the default privacy budget accountant if none is supplied, otherwise checks that the supplied\\n        accountant is a BudgetAccountant class.\\n\\n        An accountant can be set as the default using the `set_default()` method.  If no default has been set, a default\\n        is created.\\n\\n        Parameters\\n        ----------\\n        accountant : BudgetAccountant or None\\n            The supplied budget accountant.  If None, the default accountant is returned.\\n\\n        Returns\\n        -------\\n        default : BudgetAccountant\\n            Returns a working BudgetAccountant, either the supplied `accountant` or the existing default.\\n\\n        \"\"\"\\n        if accountant is None:\\n            if BudgetAccountant._default is None:\\n                BudgetAccountant._default = BudgetAccountant()\\n\\n            return BudgetAccountant._default\\n\\n        if not isinstance(accountant, BudgetAccountant):\\n            raise TypeError(f\"Accountant must be of type BudgetAccountant, got {type(accountant)}\")\\n\\n        return accountant\\n\\n    def set_default(self):\\n        \"\"\"Sets the current accountant to be the default when running functions and queries with diffprivlib.\\n\\n        Returns\\n        -------\\n        self : BudgetAccountant\\n\\n        \"\"\"\\n        BudgetAccountant._default = self\\n        return self\\n\\n    @staticmethod\\n    def pop_default():\\n        \"\"\"Pops the default BudgetAccountant from the class and returns it to the user.\\n\\n        Returns\\n        -------\\n        default : BudgetAccountant\\n            Returns the existing default BudgetAccountant.\\n\\n        \"\"\"\\n        default = BudgetAccountant._default\\n        BudgetAccountant._default = None\\n        return default\\n'}, {'diffprivlib.utils.PrivacyLeakWarning': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2019\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nBasic functions and other utilities for the differential privacy library\\n\"\"\"\\nimport secrets\\nimport warnings\\n\\nimport numpy as np\\nfrom sklearn.utils import check_random_state as skl_check_random_state\\n\\n\\ndef copy_docstring(source):\\n    \"\"\"Decorator function to copy a docstring from a `source` function to a `target` function.\\n\\n    The docstring is only copied if a docstring is present in `source`, and if none is present in `target`.  Takes\\n    inspiration from similar in `matplotlib`.\\n\\n    Parameters\\n    ----------\\n    source : method\\n        Source function from which to copy the docstring.  If ``source.__doc__`` is empty, do nothing.\\n\\n    Returns\\n    -------\\n    target : method\\n        Target function with new docstring.\\n\\n    \"\"\"\\n    def copy_func(target):\\n        if source.__doc__ and not target.__doc__:\\n            target.__doc__ = source.__doc__\\n        return target\\n    return copy_func\\n\\n\\ndef warn_unused_args(args):\\n    \"\"\"Warn the user about supplying unused `args` to a diffprivlib model.\\n\\n    Arguments can be supplied as a string, a list of strings, or a dictionary as supplied to kwargs.\\n\\n    Parameters\\n    ----------\\n    args : str or list or dict\\n        Arguments for which warnings should be thrown.\\n\\n    Returns\\n    -------\\n    None\\n\\n    \"\"\"\\n    if isinstance(args, str):\\n        args = [args]\\n\\n    for arg in args:\\n        warnings.warn(f\"Parameter \\'{arg}\\' is not functional in diffprivlib.  Remove this parameter to suppress this \"\\n                      \"warning.\", DiffprivlibCompatibilityWarning)\\n\\n\\ndef check_random_state(seed, secure=False):\\n    \"\"\"Turn seed into a np.random.RandomState or secrets.SystemRandom instance.\\n\\n    If secure=True, and seed is None (or was generated from a previous None seed), then secrets is used.  Otherwise a\\n    np.random.RandomState is used.\\n\\n    Parameters\\n    ----------\\n    seed : None, int or instance of RandomState\\n        If seed is None and secure is False, return the RandomState singleton used by np.random.\\n        If seed is None and secure is True, return a SystemRandom instance from secrets.\\n        If seed is an int, return a new RandomState instance seeded with seed.\\n        If seed is already a RandomState or SystemRandom instance, return it.\\n        Otherwise raise ValueError.\\n\\n    secure : bool, default: False\\n        Specifies if a secure random number generator from secrets can be used.\\n    \"\"\"\\n    if secure:\\n        if isinstance(seed, secrets.SystemRandom):\\n            return seed\\n\\n        if seed is None or seed is np.random.mtrand._rand:  # pylint: disable=protected-access\\n            return secrets.SystemRandom()\\n    elif isinstance(seed, secrets.SystemRandom):\\n        raise ValueError(\"secrets.SystemRandom instance cannot be passed when secure is False.\")\\n\\n    return skl_check_random_state(seed)\\n\\n\\nclass Budget(tuple):\\n    \"\"\"Custom tuple subclass for privacy budgets of the form (epsilon, delta).\\n\\n    The ``Budget`` class allows for correct comparison/ordering of privacy budget, ensuring that both epsilon and delta\\n    satisfy the comparison (tuples are compared lexicographically).  Additionally, tuples are represented with added\\n    verbosity, labelling epsilon and delta appropriately.\\n\\n    Examples\\n    --------\\n\\n    >>> from diffprivlib.utils import Budget\\n    >>> Budget(1, 0.5)\\n    (epsilon=1, delta=0.5)\\n    >>> Budget(2, 0) >= Budget(1, 0.5)\\n    False\\n    >>> (2, 0) >= (1, 0.5) # Tuples are compared with lexicographic ordering\\n    True\\n\\n    \"\"\"\\n    def __new__(cls, epsilon, delta):\\n        if epsilon < 0:\\n            raise ValueError(\"Epsilon must be non-negative\")\\n\\n        if not 0 <= delta <= 1:\\n            raise ValueError(\"Delta must be in [0, 1]\")\\n\\n        return tuple.__new__(cls, (epsilon, delta))\\n\\n    def __gt__(self, other):\\n        if self.__ge__(other) and not self.__eq__(other):\\n            return True\\n        return False\\n\\n    def __ge__(self, other):\\n        if self[0] >= other[0] and self[1] >= other[1]:\\n            return True\\n        return False\\n\\n    def __lt__(self, other):\\n        if self.__le__(other) and not self.__eq__(other):\\n            return True\\n        return False\\n\\n    def __le__(self, other):\\n        if self[0] <= other[0] and self[1] <= other[1]:\\n            return True\\n        return False\\n\\n    def __repr__(self):\\n        return f\"(epsilon={self[0]}, delta={self[1]})\"\\n\\n\\nclass BudgetError(ValueError):\\n    \"\"\"Custom exception to capture the privacy budget being exceeded, typically controlled by a\\n    :class:`.BudgetAccountant`.\\n\\n    For example, this exception may be raised when the user:\\n\\n        - Attempts to execute a query which would exceed the privacy budget of the accountant.\\n        - Attempts to change the slack of the accountant in such a way that the existing budget spends would exceed the\\n          accountant\\'s budget.\\n\\n    \"\"\"\\n\\n\\nclass PrivacyLeakWarning(RuntimeWarning):\\n    \"\"\"Custom warning to capture privacy leaks resulting from incorrect parameter setting.\\n\\n    For example, this warning may occur when the user:\\n\\n        - fails to specify the bounds or range of data to a model where required (e.g., `bounds=None` to\\n          :class:`.GaussianNB`).\\n        - inputs data to a model that falls outside the bounds or range originally specified.\\n\\n    \"\"\"\\n\\n\\nclass DiffprivlibCompatibilityWarning(RuntimeWarning):\\n    \"\"\"Custom warning to capture inherited class arguments that are not compatible with diffprivlib.\\n\\n    The purpose of the warning is to alert the user of the incompatibility, but to continue execution having fixed the\\n    incompatibility at runtime.\\n\\n    For example, this warning may occur when the user:\\n\\n        - passes a parameter value that is not compatible with diffprivlib (e.g., `solver=\\'liblinear\\'` to\\n          :class:`.LogisticRegression`)\\n        - specifies a non-default value for a parameter that is ignored by diffprivlib (e.g., `intercept_scaling=0.5`\\n          to :class:`.LogisticRegression`.\\n\\n    \"\"\"\\n\\n\\nwarnings.simplefilter(\\'always\\', PrivacyLeakWarning)\\n'}, {'diffprivlib.utils.check_random_state': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2019\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nBasic functions and other utilities for the differential privacy library\\n\"\"\"\\nimport secrets\\nimport warnings\\n\\nimport numpy as np\\nfrom sklearn.utils import check_random_state as skl_check_random_state\\n\\n\\ndef copy_docstring(source):\\n    \"\"\"Decorator function to copy a docstring from a `source` function to a `target` function.\\n\\n    The docstring is only copied if a docstring is present in `source`, and if none is present in `target`.  Takes\\n    inspiration from similar in `matplotlib`.\\n\\n    Parameters\\n    ----------\\n    source : method\\n        Source function from which to copy the docstring.  If ``source.__doc__`` is empty, do nothing.\\n\\n    Returns\\n    -------\\n    target : method\\n        Target function with new docstring.\\n\\n    \"\"\"\\n    def copy_func(target):\\n        if source.__doc__ and not target.__doc__:\\n            target.__doc__ = source.__doc__\\n        return target\\n    return copy_func\\n\\n\\ndef warn_unused_args(args):\\n    \"\"\"Warn the user about supplying unused `args` to a diffprivlib model.\\n\\n    Arguments can be supplied as a string, a list of strings, or a dictionary as supplied to kwargs.\\n\\n    Parameters\\n    ----------\\n    args : str or list or dict\\n        Arguments for which warnings should be thrown.\\n\\n    Returns\\n    -------\\n    None\\n\\n    \"\"\"\\n    if isinstance(args, str):\\n        args = [args]\\n\\n    for arg in args:\\n        warnings.warn(f\"Parameter \\'{arg}\\' is not functional in diffprivlib.  Remove this parameter to suppress this \"\\n                      \"warning.\", DiffprivlibCompatibilityWarning)\\n\\n\\ndef check_random_state(seed, secure=False):\\n    \"\"\"Turn seed into a np.random.RandomState or secrets.SystemRandom instance.\\n\\n    If secure=True, and seed is None (or was generated from a previous None seed), then secrets is used.  Otherwise a\\n    np.random.RandomState is used.\\n\\n    Parameters\\n    ----------\\n    seed : None, int or instance of RandomState\\n        If seed is None and secure is False, return the RandomState singleton used by np.random.\\n        If seed is None and secure is True, return a SystemRandom instance from secrets.\\n        If seed is an int, return a new RandomState instance seeded with seed.\\n        If seed is already a RandomState or SystemRandom instance, return it.\\n        Otherwise raise ValueError.\\n\\n    secure : bool, default: False\\n        Specifies if a secure random number generator from secrets can be used.\\n    \"\"\"\\n    if secure:\\n        if isinstance(seed, secrets.SystemRandom):\\n            return seed\\n\\n        if seed is None or seed is np.random.mtrand._rand:  # pylint: disable=protected-access\\n            return secrets.SystemRandom()\\n    elif isinstance(seed, secrets.SystemRandom):\\n        raise ValueError(\"secrets.SystemRandom instance cannot be passed when secure is False.\")\\n\\n    return skl_check_random_state(seed)\\n\\n\\nclass Budget(tuple):\\n    \"\"\"Custom tuple subclass for privacy budgets of the form (epsilon, delta).\\n\\n    The ``Budget`` class allows for correct comparison/ordering of privacy budget, ensuring that both epsilon and delta\\n    satisfy the comparison (tuples are compared lexicographically).  Additionally, tuples are represented with added\\n    verbosity, labelling epsilon and delta appropriately.\\n\\n    Examples\\n    --------\\n\\n    >>> from diffprivlib.utils import Budget\\n    >>> Budget(1, 0.5)\\n    (epsilon=1, delta=0.5)\\n    >>> Budget(2, 0) >= Budget(1, 0.5)\\n    False\\n    >>> (2, 0) >= (1, 0.5) # Tuples are compared with lexicographic ordering\\n    True\\n\\n    \"\"\"\\n    def __new__(cls, epsilon, delta):\\n        if epsilon < 0:\\n            raise ValueError(\"Epsilon must be non-negative\")\\n\\n        if not 0 <= delta <= 1:\\n            raise ValueError(\"Delta must be in [0, 1]\")\\n\\n        return tuple.__new__(cls, (epsilon, delta))\\n\\n    def __gt__(self, other):\\n        if self.__ge__(other) and not self.__eq__(other):\\n            return True\\n        return False\\n\\n    def __ge__(self, other):\\n        if self[0] >= other[0] and self[1] >= other[1]:\\n            return True\\n        return False\\n\\n    def __lt__(self, other):\\n        if self.__le__(other) and not self.__eq__(other):\\n            return True\\n        return False\\n\\n    def __le__(self, other):\\n        if self[0] <= other[0] and self[1] <= other[1]:\\n            return True\\n        return False\\n\\n    def __repr__(self):\\n        return f\"(epsilon={self[0]}, delta={self[1]})\"\\n\\n\\nclass BudgetError(ValueError):\\n    \"\"\"Custom exception to capture the privacy budget being exceeded, typically controlled by a\\n    :class:`.BudgetAccountant`.\\n\\n    For example, this exception may be raised when the user:\\n\\n        - Attempts to execute a query which would exceed the privacy budget of the accountant.\\n        - Attempts to change the slack of the accountant in such a way that the existing budget spends would exceed the\\n          accountant\\'s budget.\\n\\n    \"\"\"\\n\\n\\nclass PrivacyLeakWarning(RuntimeWarning):\\n    \"\"\"Custom warning to capture privacy leaks resulting from incorrect parameter setting.\\n\\n    For example, this warning may occur when the user:\\n\\n        - fails to specify the bounds or range of data to a model where required (e.g., `bounds=None` to\\n          :class:`.GaussianNB`).\\n        - inputs data to a model that falls outside the bounds or range originally specified.\\n\\n    \"\"\"\\n\\n\\nclass DiffprivlibCompatibilityWarning(RuntimeWarning):\\n    \"\"\"Custom warning to capture inherited class arguments that are not compatible with diffprivlib.\\n\\n    The purpose of the warning is to alert the user of the incompatibility, but to continue execution having fixed the\\n    incompatibility at runtime.\\n\\n    For example, this warning may occur when the user:\\n\\n        - passes a parameter value that is not compatible with diffprivlib (e.g., `solver=\\'liblinear\\'` to\\n          :class:`.LogisticRegression`)\\n        - specifies a non-default value for a parameter that is ignored by diffprivlib (e.g., `intercept_scaling=0.5`\\n          to :class:`.LogisticRegression`.\\n\\n    \"\"\"\\n\\n\\nwarnings.simplefilter(\\'always\\', PrivacyLeakWarning)\\n'}, {'diffprivlib.validation.DiffprivlibMixin._check_bounds': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2020\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nValidation functions for the differential privacy library\\n\"\"\"\\nfrom numbers import Real, Integral\\n\\nimport numpy as np\\n\\nfrom diffprivlib.utils import warn_unused_args\\n\\n\\ndef check_epsilon_delta(epsilon, delta, allow_zero=False):\\n    \"\"\"Checks that epsilon and delta are valid values for differential privacy.  Throws an error if checks fail,\\n    otherwise returns nothing.\\n\\n    As well as the requirements of epsilon and delta separately, both cannot be simultaneously zero, unless\\n    ``allow_zero`` is set to ``True``.\\n\\n    Parameters\\n    ----------\\n    epsilon : float\\n        Epsilon parameter for differential privacy.  Must be non-negative.\\n\\n    delta : float\\n        Delta parameter for differential privacy.  Must be on the unit interval, [0, 1].\\n\\n    allow_zero : bool, default: False\\n        Allow epsilon and delta both be zero.\\n\\n    \"\"\"\\n    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\\n        raise TypeError(\"Epsilon and delta must be numeric\")\\n\\n    if epsilon < 0:\\n        raise ValueError(\"Epsilon must be non-negative\")\\n\\n    if not 0 <= delta <= 1:\\n        raise ValueError(\"Delta must be in [0, 1]\")\\n\\n    if not allow_zero and epsilon + delta == 0:\\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\\n\\n\\ndef check_bounds(bounds, shape=0, min_separation=0.0, dtype=float):\\n    \"\"\"Input validation for the ``bounds`` parameter.\\n\\n    Checks that ``bounds`` is composed of a list of tuples of the form (lower, upper), where lower <= upper and both\\n    are numeric.  Also checks that ``bounds`` contains the appropriate number of dimensions, and that there is a\\n    ``min_separation`` between the bounds.\\n\\n    Parameters\\n    ----------\\n    bounds : tuple\\n        Tuple of bounds of the form (min, max). `min` and `max` can either be scalars or 1-dimensional arrays.\\n\\n    shape : int, default: 0\\n        Number of dimensions to be expected in ``bounds``.\\n\\n    min_separation : float, default: 0.0\\n        The minimum separation between `lower` and `upper` of each dimension.  This separation is enforced if not\\n        already satisfied.\\n\\n    dtype : data-type, default: float\\n        Data type of the returned bounds.\\n\\n    Returns\\n    -------\\n    bounds : tuple\\n\\n    \"\"\"\\n    if not isinstance(bounds, tuple):\\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\\n    if not isinstance(shape, Integral):\\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\\n\\n    lower, upper = bounds\\n\\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\\n        lower = np.ravel(lower).astype(dtype)\\n        upper = np.ravel(upper).astype(dtype)\\n    else:\\n        lower = np.asarray(lower, dtype=dtype)\\n        upper = np.asarray(upper, dtype=dtype)\\n\\n    if lower.shape != upper.shape:\\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\\n    if lower.ndim > 1:\\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\\n    if lower.size not in (1, shape):\\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\\n\\n    n_bounds = lower.shape[0]\\n\\n    for i in range(n_bounds):\\n        _lower = lower[i]\\n        _upper = upper[i]\\n\\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\\n\\n        if _lower > _upper:\\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\\n\\n        if _upper - _lower < min_separation:\\n            mid = (_upper + _lower) / 2\\n            lower[i] = mid - min_separation / 2\\n            upper[i] = mid + min_separation / 2\\n\\n    if shape == 0:\\n        return lower.item(), upper.item()\\n\\n    if n_bounds == 1:\\n        lower = np.ones(shape, dtype=dtype) * lower.item()\\n        upper = np.ones(shape, dtype=dtype) * upper.item()\\n\\n    return lower, upper\\n\\n\\ndef clip_to_norm(array, clip):\\n    \"\"\"Clips the examples of a 2-dimensional array to a given maximum norm.\\n\\n    Parameters\\n    ----------\\n    array : np.ndarray\\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\\n\\n    clip : float\\n        Norm at which to clip each example\\n\\n    Returns\\n    -------\\n    array : np.ndarray\\n        The clipped array.\\n\\n    \"\"\"\\n    if not isinstance(array, np.ndarray):\\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\\n    if array.ndim != 2:\\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\\n    if not isinstance(clip, Real):\\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\\n    if clip <= 0:\\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\\n\\n    norms = np.linalg.norm(array, axis=1) / clip\\n    norms[norms < 1] = 1\\n\\n    return array / norms[:, np.newaxis]\\n\\n\\ndef clip_to_bounds(array, bounds):\\n    \"\"\"Clips the examples of a 2-dimensional array to given bounds.\\n\\n    Parameters\\n    ----------\\n    array : np.ndarray\\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\\n\\n    bounds : tuple\\n        Tuple of bounds of the form (min, max) which the array is to be clipped to. `min` and `max` must be scalar,\\n        unless array is 2-dimensional.\\n\\n    Returns\\n    -------\\n    array : np.ndarray\\n        The clipped array.\\n\\n    \"\"\"\\n    if not isinstance(array, np.ndarray):\\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\\n\\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\\n    clipped_array = array.copy()\\n\\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\\n    else:\\n        if array.ndim != 2:\\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\\n\\n        for feature in range(array.shape[1]):\\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\\n\\n    return clipped_array\\n\\n\\nclass DiffprivlibMixin:  # pylint: disable=too-few-public-methods\\n    \"\"\"Mixin for Diffprivlib models.\"\"\"\\n    _check_bounds = staticmethod(check_bounds)\\n    _clip_to_norm = staticmethod(clip_to_norm)\\n    _clip_to_bounds = staticmethod(clip_to_bounds)\\n    _warn_unused_args = staticmethod(warn_unused_args)\\n\\n    # todo: remove when scikit-learn v1.2 is a min requirement\\n    def _validate_params(self):\\n        pass\\n\\n    @staticmethod\\n    def _copy_parameter_constraints(cls, *args):\\n        \"\"\"Copies the parameter constraints for `*args` from `cls`\\n        \"\"\"\\n        if not hasattr(cls, \"_parameter_constraints\"):\\n            return {}\\n\\n        return {k: cls._parameter_constraints[k] for k in args if k in cls._parameter_constraints}\\n'}, {'diffprivlib.validation.DiffprivlibMixin._validate_params': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2020\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nValidation functions for the differential privacy library\\n\"\"\"\\nfrom numbers import Real, Integral\\n\\nimport numpy as np\\n\\nfrom diffprivlib.utils import warn_unused_args\\n\\n\\ndef check_epsilon_delta(epsilon, delta, allow_zero=False):\\n    \"\"\"Checks that epsilon and delta are valid values for differential privacy.  Throws an error if checks fail,\\n    otherwise returns nothing.\\n\\n    As well as the requirements of epsilon and delta separately, both cannot be simultaneously zero, unless\\n    ``allow_zero`` is set to ``True``.\\n\\n    Parameters\\n    ----------\\n    epsilon : float\\n        Epsilon parameter for differential privacy.  Must be non-negative.\\n\\n    delta : float\\n        Delta parameter for differential privacy.  Must be on the unit interval, [0, 1].\\n\\n    allow_zero : bool, default: False\\n        Allow epsilon and delta both be zero.\\n\\n    \"\"\"\\n    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\\n        raise TypeError(\"Epsilon and delta must be numeric\")\\n\\n    if epsilon < 0:\\n        raise ValueError(\"Epsilon must be non-negative\")\\n\\n    if not 0 <= delta <= 1:\\n        raise ValueError(\"Delta must be in [0, 1]\")\\n\\n    if not allow_zero and epsilon + delta == 0:\\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\\n\\n\\ndef check_bounds(bounds, shape=0, min_separation=0.0, dtype=float):\\n    \"\"\"Input validation for the ``bounds`` parameter.\\n\\n    Checks that ``bounds`` is composed of a list of tuples of the form (lower, upper), where lower <= upper and both\\n    are numeric.  Also checks that ``bounds`` contains the appropriate number of dimensions, and that there is a\\n    ``min_separation`` between the bounds.\\n\\n    Parameters\\n    ----------\\n    bounds : tuple\\n        Tuple of bounds of the form (min, max). `min` and `max` can either be scalars or 1-dimensional arrays.\\n\\n    shape : int, default: 0\\n        Number of dimensions to be expected in ``bounds``.\\n\\n    min_separation : float, default: 0.0\\n        The minimum separation between `lower` and `upper` of each dimension.  This separation is enforced if not\\n        already satisfied.\\n\\n    dtype : data-type, default: float\\n        Data type of the returned bounds.\\n\\n    Returns\\n    -------\\n    bounds : tuple\\n\\n    \"\"\"\\n    if not isinstance(bounds, tuple):\\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\\n    if not isinstance(shape, Integral):\\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\\n\\n    lower, upper = bounds\\n\\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\\n        lower = np.ravel(lower).astype(dtype)\\n        upper = np.ravel(upper).astype(dtype)\\n    else:\\n        lower = np.asarray(lower, dtype=dtype)\\n        upper = np.asarray(upper, dtype=dtype)\\n\\n    if lower.shape != upper.shape:\\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\\n    if lower.ndim > 1:\\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\\n    if lower.size not in (1, shape):\\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\\n\\n    n_bounds = lower.shape[0]\\n\\n    for i in range(n_bounds):\\n        _lower = lower[i]\\n        _upper = upper[i]\\n\\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\\n\\n        if _lower > _upper:\\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\\n\\n        if _upper - _lower < min_separation:\\n            mid = (_upper + _lower) / 2\\n            lower[i] = mid - min_separation / 2\\n            upper[i] = mid + min_separation / 2\\n\\n    if shape == 0:\\n        return lower.item(), upper.item()\\n\\n    if n_bounds == 1:\\n        lower = np.ones(shape, dtype=dtype) * lower.item()\\n        upper = np.ones(shape, dtype=dtype) * upper.item()\\n\\n    return lower, upper\\n\\n\\ndef clip_to_norm(array, clip):\\n    \"\"\"Clips the examples of a 2-dimensional array to a given maximum norm.\\n\\n    Parameters\\n    ----------\\n    array : np.ndarray\\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\\n\\n    clip : float\\n        Norm at which to clip each example\\n\\n    Returns\\n    -------\\n    array : np.ndarray\\n        The clipped array.\\n\\n    \"\"\"\\n    if not isinstance(array, np.ndarray):\\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\\n    if array.ndim != 2:\\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\\n    if not isinstance(clip, Real):\\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\\n    if clip <= 0:\\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\\n\\n    norms = np.linalg.norm(array, axis=1) / clip\\n    norms[norms < 1] = 1\\n\\n    return array / norms[:, np.newaxis]\\n\\n\\ndef clip_to_bounds(array, bounds):\\n    \"\"\"Clips the examples of a 2-dimensional array to given bounds.\\n\\n    Parameters\\n    ----------\\n    array : np.ndarray\\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\\n\\n    bounds : tuple\\n        Tuple of bounds of the form (min, max) which the array is to be clipped to. `min` and `max` must be scalar,\\n        unless array is 2-dimensional.\\n\\n    Returns\\n    -------\\n    array : np.ndarray\\n        The clipped array.\\n\\n    \"\"\"\\n    if not isinstance(array, np.ndarray):\\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\\n\\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\\n    clipped_array = array.copy()\\n\\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\\n    else:\\n        if array.ndim != 2:\\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\\n\\n        for feature in range(array.shape[1]):\\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\\n\\n    return clipped_array\\n\\n\\nclass DiffprivlibMixin:  # pylint: disable=too-few-public-methods\\n    \"\"\"Mixin for Diffprivlib models.\"\"\"\\n    _check_bounds = staticmethod(check_bounds)\\n    _clip_to_norm = staticmethod(clip_to_norm)\\n    _clip_to_bounds = staticmethod(clip_to_bounds)\\n    _warn_unused_args = staticmethod(warn_unused_args)\\n\\n    # todo: remove when scikit-learn v1.2 is a min requirement\\n    def _validate_params(self):\\n        pass\\n\\n    @staticmethod\\n    def _copy_parameter_constraints(cls, *args):\\n        \"\"\"Copies the parameter constraints for `*args` from `cls`\\n        \"\"\"\\n        if not hasattr(cls, \"_parameter_constraints\"):\\n            return {}\\n\\n        return {k: cls._parameter_constraints[k] for k in args if k in cls._parameter_constraints}\\n'}, {'diffprivlib.validation.DiffprivlibMixin._warn_unused_args': '# MIT License\\n#\\n# Copyright (C) IBM Corporation 2020\\n#\\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\\n# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\\n# persons to whom the Software is furnished to do so, subject to the following conditions:\\n#\\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\\n# Software.\\n#\\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\\n# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n# SOFTWARE.\\n\"\"\"\\nValidation functions for the differential privacy library\\n\"\"\"\\nfrom numbers import Real, Integral\\n\\nimport numpy as np\\n\\nfrom diffprivlib.utils import warn_unused_args\\n\\n\\ndef check_epsilon_delta(epsilon, delta, allow_zero=False):\\n    \"\"\"Checks that epsilon and delta are valid values for differential privacy.  Throws an error if checks fail,\\n    otherwise returns nothing.\\n\\n    As well as the requirements of epsilon and delta separately, both cannot be simultaneously zero, unless\\n    ``allow_zero`` is set to ``True``.\\n\\n    Parameters\\n    ----------\\n    epsilon : float\\n        Epsilon parameter for differential privacy.  Must be non-negative.\\n\\n    delta : float\\n        Delta parameter for differential privacy.  Must be on the unit interval, [0, 1].\\n\\n    allow_zero : bool, default: False\\n        Allow epsilon and delta both be zero.\\n\\n    \"\"\"\\n    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\\n        raise TypeError(\"Epsilon and delta must be numeric\")\\n\\n    if epsilon < 0:\\n        raise ValueError(\"Epsilon must be non-negative\")\\n\\n    if not 0 <= delta <= 1:\\n        raise ValueError(\"Delta must be in [0, 1]\")\\n\\n    if not allow_zero and epsilon + delta == 0:\\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\\n\\n\\ndef check_bounds(bounds, shape=0, min_separation=0.0, dtype=float):\\n    \"\"\"Input validation for the ``bounds`` parameter.\\n\\n    Checks that ``bounds`` is composed of a list of tuples of the form (lower, upper), where lower <= upper and both\\n    are numeric.  Also checks that ``bounds`` contains the appropriate number of dimensions, and that there is a\\n    ``min_separation`` between the bounds.\\n\\n    Parameters\\n    ----------\\n    bounds : tuple\\n        Tuple of bounds of the form (min, max). `min` and `max` can either be scalars or 1-dimensional arrays.\\n\\n    shape : int, default: 0\\n        Number of dimensions to be expected in ``bounds``.\\n\\n    min_separation : float, default: 0.0\\n        The minimum separation between `lower` and `upper` of each dimension.  This separation is enforced if not\\n        already satisfied.\\n\\n    dtype : data-type, default: float\\n        Data type of the returned bounds.\\n\\n    Returns\\n    -------\\n    bounds : tuple\\n\\n    \"\"\"\\n    if not isinstance(bounds, tuple):\\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\\n    if not isinstance(shape, Integral):\\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\\n\\n    lower, upper = bounds\\n\\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\\n        lower = np.ravel(lower).astype(dtype)\\n        upper = np.ravel(upper).astype(dtype)\\n    else:\\n        lower = np.asarray(lower, dtype=dtype)\\n        upper = np.asarray(upper, dtype=dtype)\\n\\n    if lower.shape != upper.shape:\\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\\n    if lower.ndim > 1:\\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\\n    if lower.size not in (1, shape):\\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\\n\\n    n_bounds = lower.shape[0]\\n\\n    for i in range(n_bounds):\\n        _lower = lower[i]\\n        _upper = upper[i]\\n\\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\\n\\n        if _lower > _upper:\\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\\n\\n        if _upper - _lower < min_separation:\\n            mid = (_upper + _lower) / 2\\n            lower[i] = mid - min_separation / 2\\n            upper[i] = mid + min_separation / 2\\n\\n    if shape == 0:\\n        return lower.item(), upper.item()\\n\\n    if n_bounds == 1:\\n        lower = np.ones(shape, dtype=dtype) * lower.item()\\n        upper = np.ones(shape, dtype=dtype) * upper.item()\\n\\n    return lower, upper\\n\\n\\ndef clip_to_norm(array, clip):\\n    \"\"\"Clips the examples of a 2-dimensional array to a given maximum norm.\\n\\n    Parameters\\n    ----------\\n    array : np.ndarray\\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\\n\\n    clip : float\\n        Norm at which to clip each example\\n\\n    Returns\\n    -------\\n    array : np.ndarray\\n        The clipped array.\\n\\n    \"\"\"\\n    if not isinstance(array, np.ndarray):\\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\\n    if array.ndim != 2:\\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\\n    if not isinstance(clip, Real):\\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\\n    if clip <= 0:\\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\\n\\n    norms = np.linalg.norm(array, axis=1) / clip\\n    norms[norms < 1] = 1\\n\\n    return array / norms[:, np.newaxis]\\n\\n\\ndef clip_to_bounds(array, bounds):\\n    \"\"\"Clips the examples of a 2-dimensional array to given bounds.\\n\\n    Parameters\\n    ----------\\n    array : np.ndarray\\n        Array to be clipped.  After clipping, all examples have a 2-norm of at most `clip`.\\n\\n    bounds : tuple\\n        Tuple of bounds of the form (min, max) which the array is to be clipped to. `min` and `max` must be scalar,\\n        unless array is 2-dimensional.\\n\\n    Returns\\n    -------\\n    array : np.ndarray\\n        The clipped array.\\n\\n    \"\"\"\\n    if not isinstance(array, np.ndarray):\\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\\n\\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\\n    clipped_array = array.copy()\\n\\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\\n    else:\\n        if array.ndim != 2:\\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\\n\\n        for feature in range(array.shape[1]):\\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\\n\\n    return clipped_array\\n\\n\\nclass DiffprivlibMixin:  # pylint: disable=too-few-public-methods\\n    \"\"\"Mixin for Diffprivlib models.\"\"\"\\n    _check_bounds = staticmethod(check_bounds)\\n    _clip_to_norm = staticmethod(clip_to_norm)\\n    _clip_to_bounds = staticmethod(clip_to_bounds)\\n    _warn_unused_args = staticmethod(warn_unused_args)\\n\\n    # todo: remove when scikit-learn v1.2 is a min requirement\\n    def _validate_params(self):\\n        pass\\n\\n    @staticmethod\\n    def _copy_parameter_constraints(cls, *args):\\n        \"\"\"Copies the parameter constraints for `*args` from `cls`\\n        \"\"\"\\n        if not hasattr(cls, \"_parameter_constraints\"):\\n            return {}\\n\\n        return {k: cls._parameter_constraints[k] for k in args if k in cls._parameter_constraints}\\n'}]", "test_list": ["def test_large_data(self):\n    X = np.array([0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5, 4.0, 4.25, 4.5, 4.75, 5.0, 5.5])\n    y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n    X = X[:, np.newaxis]\n    clf = LinearRegression(bounds_X=(0, 3), bounds_y=(0, 1), fit_intercept=False)\n    self.assertIsNotNone(clf.fit(X, y))", "def test_simple(self):\n    X = np.linspace(-1, 1, 1000)\n    y = X.copy()\n    X = X[:, np.newaxis]\n    clf = LinearRegression(epsilon=2, fit_intercept=False, bounds_X=(-1, 1), bounds_y=(-1, 1), random_state=0)\n    clf.fit(X, y)\n    print(clf.predict(np.array([0.5]).reshape(-1, 1)))\n    self.assertIsNotNone(clf)\n    self.assertAlmostEqual(clf.predict(np.array([0.5]).reshape(-1, 1))[0], 0.5, delta=0.05)", "def test_accountant(self):\n    from diffprivlib.accountant import BudgetAccountant\n    acc = BudgetAccountant()\n    X = np.linspace(-1, 1, 100)\n    y = X.copy()\n    X = X[:, np.newaxis]\n    clf = LinearRegression(epsilon=2, fit_intercept=False, bounds_X=(-1, 1), bounds_y=(-1, 1), accountant=acc)\n    clf.fit(X, y)\n    self.assertEqual((2, 0), acc.total())\n    with BudgetAccountant(3, 0) as acc2:\n        clf = LinearRegression(epsilon=2, fit_intercept=False, bounds_X=(-1, 1), bounds_y=(-1, 1))\n        clf.fit(X, y)\n        self.assertEqual((2, 0), acc2.total())\n        with self.assertRaises(BudgetError):\n            clf.fit(X, y)", "def test_multiple_targets(self):\n    from sklearn.linear_model import LinearRegression as sk_LinearRegression\n    X = np.linspace(-1, 1, 1000)\n    y = np.vstack((X.copy(), X.copy(), X.copy(), X.copy())).T\n    X = X[:, np.newaxis]\n    clf_dp = LinearRegression(epsilon=2, fit_intercept=False, bounds_X=(-1, 1), bounds_y=(-1, 1))\n    with self.assertRaises(ValueError):\n        clf_dp.fit(X, y.T)\n    clf_dp.fit(X, y)\n    clf_sk = sk_LinearRegression(fit_intercept=False)\n    clf_sk.fit(X, y)\n    x0 = np.array([[0.5]])\n    self.assertEqual(clf_dp.coef_.shape, clf_sk.coef_.shape)\n    self.assertEqual(clf_dp.predict(x0).shape, clf_sk.predict(x0).shape)\n    clf_dp2 = LinearRegression(epsilon=2, fit_intercept=True, bounds_X=(-1, 1), bounds_y=(-1, 1)).fit(X, y)\n    self.assertIsNotNone(clf_dp2)\n    self.assertEqual(clf_dp2.intercept_.shape, (4,))\n    self.assertEqual(clf_dp2.coef_.shape, clf_sk.coef_.shape)\n    self.assertEqual(clf_dp2.predict(x0).shape, clf_sk.predict(x0).shape)", "def test_sample_weight_warning(self):\n    clf = LinearRegression(bounds_X=(0, 5), bounds_y=(0, 1))\n    X = np.array([0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5, 4.0, 4.25, 4.5, 4.75, 5.0, 5.5])\n    y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n    X = X[:, np.newaxis]\n    with self.assertWarns(DiffprivlibCompatibilityWarning):\n        clf.fit(X, y, sample_weight=np.ones_like(y))"], "requirements": {"Input-Output Conditions": {"requirement": "The 'fit' function should accept inputs X and y as numpy arrays or sparse matrices, and return an instance of the LinearRegression class with updated coefficients and intercept.", "unit_test": ["def test_input_output_conditions(self):\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    clf = LinearRegression(bounds_X=(0, 10), bounds_y=(0, 5))\n    result = clf.fit(X, y)\n    self.assertIsInstance(result, LinearRegression)\n    self.assertIsNotNone(result.coef_)\n    self.assertIsNotNone(result.intercept_)"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_input_output_conditions"}, "Exception Handling": {"requirement": "The 'fit' function should raise a ValueError if the dimensions of X and y do not align, indicating a mismatch in the number of samples.", "unit_test": ["def test_dimension_mismatch_exception(self):\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2, 3])\n    clf = LinearRegression(bounds_X=(0, 10), bounds_y=(0, 5))\n    with self.assertRaises(ValueError):\n        clf.fit(X, y)"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_dimension_mismatch_exception"}, "Edge Case Handling": {"requirement": "The 'fit' function should handle edge cases where X or y is empty, raising an appropriate exception.", "unit_test": ["def test_empty_input_exception(self):\n    X = np.array([])\n    y = np.array([])\n    clf = LinearRegression(bounds_X=(0, 10), bounds_y=(0, 5))\n    with self.assertRaises(ValueError):\n        clf.fit(X, y)"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_empty_input_exception"}, "Functionality Extension": {"requirement": "Extend the 'fit' function to support an optional parameter 'normalize' that, when set to True, normalizes the input data X before fitting.", "unit_test": ["def test_normalize_functionality(self):\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    clf = LinearRegression(bounds_X=(0, 10), bounds_y=(0, 5), normalize=True)\n    result = clf.fit(X, y)\n    self.assertIsInstance(result, LinearRegression)\n    self.assertIsNotNone(result.coef_)\n    self.assertIsNotNone(result.intercept_)"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_normalize_functionality"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'fit' function are annotated with appropriate type hints.", "unit_test": ["def test_annotation_coverage(self):\n    import inspect\n    clf = LinearRegression()\n    fit_annotations = inspect.getfullargspec(clf.fit).annotations\n    self.assertIn('X', fit_annotations)\n    self.assertIn('y', fit_annotations)\n    self.assertIn('return', fit_annotations)"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_annotation_coverage"}, "Code Complexity": {"requirement": "The 'fit' function should maintain a cyclomatic complexity of 11 or less to ensure readability and maintainability.", "unit_test": ["def test_code_complexity(self):\n    import radon.complexity as rc\n    from radon.visitors import ComplexityVisitor\n    source_code = inspect.getsource(LinearRegression.fit)\n    visitor = ComplexityVisitor.from_code(source_code)\n    complexity = visitor.functions[0].complexity\n    self.assertLessEqual(complexity, 10)"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_code_complexity"}, "Code Standard": {"requirement": "The 'fit' function should adhere to PEP 8 standards, including proper indentation, line length, and naming conventions.", "unit_test": ["def test_pep8_compliance(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path/to/linear_regression.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_check_code_style"}, "Context Usage Verification": {"requirement": "The 'fit' function should utilize the '_preprocess_data' method from the LinearRegression class to preprocess input data.", "unit_test": ["def test_preprocess_data_usage(self):\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    clf = LinearRegression(bounds_X=(0, 10), bounds_y=(0, 5))\n    with unittest.mock.patch.object(LinearRegression, '_preprocess_data', wraps=clf._preprocess_data) as mock_method:\n        clf.fit(X, y)\n        mock_method.assert_called_once()"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_preprocess_data_usage"}, "Context Usage Correctness Verification": {"requirement": "The 'fit' function should correctly apply the bounds_X and bounds_y attributes during data preprocessing.", "unit_test": ["def test_bounds_application(self):\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    clf = LinearRegression(bounds_X=(0, 10), bounds_y=(0, 5))\n    X_processed, y_processed, _, _, _ = clf._preprocess_data(X, y, clf.fit_intercept, clf.epsilon, clf.bounds_X, clf.bounds_y)\n    self.assertTrue((X_processed >= 0).all() and (X_processed <= 10).all())\n    self.assertTrue((y_processed >= 0).all() and (y_processed <= 5).all())"], "test": "tests/models/test_LinearRegression.py::TestLinearRegression::test_fit_applies_bounds"}}}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "type": "method", "project_path": "Security/msticpy", "completion_path": "Security/msticpy/msticpy/analysis/anomalous_sequence/model.py", "signature_position": [517, 522], "body_position": [558, 617], "dependency": {"intra_class": ["msticpy.analysis.anomalous_sequence.model.Model.prior_probs", "msticpy.analysis.anomalous_sequence.model.Model.rare_window_likelihoods", "msticpy.analysis.anomalous_sequence.model.Model.rare_window_likelihoods_geo", "msticpy.analysis.anomalous_sequence.model.Model.rare_windows", "msticpy.analysis.anomalous_sequence.model.Model.rare_windows_geo", "msticpy.analysis.anomalous_sequence.model.Model.session_type", "msticpy.analysis.anomalous_sequence.model.Model.sessions"], "intra_file": ["msticpy.analysis.anomalous_sequence.model.SessionType", "msticpy.analysis.anomalous_sequence.model.SessionType.cmds_only", "msticpy.analysis.anomalous_sequence.model.SessionType.cmds_params_only"], "cross_file": ["msticpy.common.exceptions.MsticpyException"]}, "requirement": {"Functionality": "This function computes the rarest windows and corresponding likelihood for each session. It uses a sliding window approach to identify the rarest window and its likelihood in each session. The function takes into account the length of the sliding window, whether to use start and end tokens, and whether to use the geometric mean for likelihood calculations.", "Arguments": ":param self: Model. An instance of the Model class.\n:param window_len: int. The length of the sliding window for likelihood calculations.\n:param use_start_end_tokens: bool. If True, start and end tokens will be added to each session before calculations.\n:param use_geo_mean: bool. If True, the likelihoods of the sliding windows will be raised to the power of (1/window_len).\n:return: None. The function updates the rarest windows and corresponding likelihoods in the Model instance."}, "tests": ["tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows"], "indent": 8, "domain": "Security", "code": "    def compute_rarest_windows(\n        self,\n        window_len: int,\n        use_start_end_tokens: bool = True,\n        use_geo_mean: bool = False,\n    ):\n        \"\"\"\n        Find the rarest window and corresponding likelihood for each session.\n\n        In particular, uses a sliding window approach to find the rarest window\n        and corresponding likelihood for that window for each session.\n\n        If we have a long session filled with benign activity except for a small\n        window of suspicious behaviour, then this approach should be able to\n        identity the session as anomalous. This approach should be more\n        effective than simply taking the geometric mean of the full session\n        likelihood. This is because the small window of suspicious behaviour\n        might get averaged out by the majority benign behaviour in the session\n        when using the geometric mean approach.\n\n        Note that if we have a session of length k, and we use a sliding window\n        of length k+1, then we will end up with np.nan for the rarest window\n        likelihood metric for that session. However, if `use_start_end_tokens`\n        is set to True, then because we will be appending self.end_token to the\n        session, the session will be treated as a session of length k+1,\n        therefore, we will end up with a non np.nan value.\n\n        Parameters\n        ----------\n        window_len: int\n            length of sliding window for likelihood calculations\n        use_start_end_tokens: bool\n            if True, then `start_token` and `end_token` will be prepended\n            and appended to each\n            session respectively before the calculations are done\n        use_geo_mean: bool\n            if True, then each of the likelihoods of the sliding windows\n            will be raised to the power\n            of (1/`window_len`)\n\n        \"\"\"\n        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if self.session_type == SessionType.cmds_only:\n            rare_tuples = [\n                cmds_only.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n        elif self.session_type == SessionType.cmds_params_only:\n            rare_tuples = [\n                cmds_params_only.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n        else:\n            rare_tuples = [\n                cmds_params_values.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n\n        if use_geo_mean:\n            self.rare_windows_geo[window_len] = [rare[0] for rare in rare_tuples]\n            self.rare_window_likelihoods_geo[window_len] = [\n                rare[1] for rare in rare_tuples\n            ]\n        else:\n            self.rare_windows[window_len] = [rare[0] for rare in rare_tuples]\n            self.rare_window_likelihoods[window_len] = [rare[1] for rare in rare_tuples]\n", "intra_context": "# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\"\"\"Module for Model class for modelling sessions data.\"\"\"\n\nfrom collections import defaultdict\nfrom typing import Dict, List, Union\n\nfrom ...common.exceptions import MsticpyException\nfrom .utils import cmds_only, cmds_params_only, cmds_params_values, probabilities\nfrom .utils.data_structures import Cmd\n\n\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-few-public-methods\nclass Model:\n    \"\"\"Class for modelling sessions data.\"\"\"\n\n    def __init__(\n        self, sessions: List[List[Union[str, Cmd]]], modellable_params: set = None\n    ):\n        \"\"\"\n        Instantiate the Model class.\n\n        This Model class can be used to model sessions, where each\n        session is a sequence of commands. We use a sliding window\n        approach to calculate the rarest part of each session. We\n        can view the sessions in ascending order of this metric to\n        see if the top sessions are anomalous/malicious.\n\n        Parameters\n        ----------\n        sessions: List[List[Union[str, Cmd]]]\n            list of sessions, where each session is a list of either\n            strings or a list of the Cmd datatype.\n\n            The Cmd datatype should have \"name\" and \"params\" as attributes\n            where \"name\" is the name of the command (string) and \"params\"\n            is either a set of accompanying params or a dict of\n            accompanying params and values.\n\n            examples formats of a session:\n            1) ['Set-User', 'Set-Mailbox']\n            2) [Cmd(name='Set-User', params={'Identity', 'Force'}),\n            Cmd(name='Set-Mailbox', params={'Identity', 'AuditEnabled'})]\n            3) [Cmd(name='Set-User',\n            params={'Identity': 'blahblah', 'Force': 'true'}),\n            Cmd(name='Set-Mailbox',\n            params={'Identity': 'blahblah', 'AuditEnabled': 'false'})]\n\n        modellable_params: set, optional\n            set of params which you deem to have categorical values which are suitable\n            for modelling.\n            Note this argument will only have an effect if your sessions include commands,\n            params and values. If your sessions include commands, params and values and\n            this argument is not set, then some rough heuristics will be used to determine\n            which params have values which are suitable for modelling.\n\n        \"\"\"\n        if not isinstance(sessions, list):\n            raise MsticpyException(\"`sessions` should be a list\")\n        if not sessions:\n            raise MsticpyException(\"`sessions` should not be an empty list\")\n        for i, ses in enumerate(sessions):\n            if not isinstance(ses, list):\n                raise MsticpyException(\"each session in `sessions` should be a list\")\n            if len(ses) == 0:\n                raise MsticpyException(\n                    f\"session at index {i} of `sessions` is empty. Each session \"\n                    \"should contain at least one command\"\n                )\n\n        self.start_token = \"##START##\"  # nosec B105\n        self.end_token = \"##END##\"  # nosec B105\n        self.unk_token = \"##UNK##\"  # nosec B105\n\n        self.sessions = sessions\n        self.session_type = None\n        self._asses_input()\n\n        # non laplace smoothed counts\n        self._seq1_counts = None\n        self._seq2_counts = None\n        self._param_counts = None\n        self._cmd_param_counts = None\n        self._value_counts = None\n        self._param_value_counts = None\n\n        # laplace smoothed counts\n        self.seq1_counts = None\n        self.seq2_counts = None\n        self.param_counts = None\n        self.cmd_param_counts = None\n        self.value_counts = None\n        self.param_value_counts = None\n\n        self.modellable_params = modellable_params\n\n        self.prior_probs = None\n        self.trans_probs = None\n        self.param_probs = None\n        self.param_cond_cmd_probs = None\n        self.value_probs = None\n        self.value_cond_param_probs = None\n\n        self.set_params_cond_cmd_probs: Dict[str, Dict[str, float]] = {}\n\n        self.session_likelihoods = None\n        self.session_geomean_likelihoods = None\n\n        self.rare_windows: Dict[int, list] = {}\n        self.rare_window_likelihoods: Dict[int, list] = {}\n\n        self.rare_windows_geo: Dict[int, list] = {}\n        self.rare_window_likelihoods_geo: Dict[int, list] = {}\n\n    def train(self):\n        \"\"\"\n        Train the model by computing counts and probabilities.\n\n        In particular, computes the counts and probabilities of the commands\n        (and possibly the params if provided, and possibly the values if provided)\n\n        \"\"\"\n        self._compute_counts()\n        self._laplace_smooth_counts()\n        self._compute_probs()\n\n    def compute_scores(self, use_start_end_tokens: bool):\n        \"\"\"\n        Compute some likelihood based scores/metrics for each of the sessions.\n\n        In particular, computes the likelihoods and geometric mean of\n        the likelihoods for each of the sessions. Also, uses the sliding\n        window approach to compute the rarest window likelihoods for each\n        of the sessions. It does this for windows of length 2 and 3.\n\n        Note that if we have a session of length k, and we use a sliding\n        window of length k+1, then we will end up with np.nan for the\n        rarest window likelihood metric for that session.\n        However, if `use_start_end_tokens` is set to True, then\n        because we will be appending self.end_token to the session,\n        the session will be treated as a session of length k+1,\n        therefore, we will end up with a non np.nan value for that session.\n\n        Parameters\n        ----------\n        use_start_end_tokens: bool\n            if True, then self.start_token and self.end_token will be\n            prepended and appended to each\n            of the sessions respectively before the calculations are done.\n\n        \"\"\"\n        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n        self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(\n            window_len=2, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n        self.compute_rarest_windows(\n            window_len=3, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n\n    def _compute_counts(self):\n        \"\"\"\n        Compute all the counts for the model.\n\n        The items we will count depend on the the `session_type` attribute.\n        We will compute the individual command and transition command counts.\n\n        If params are provided with the commands, then, in addition,\n        we will compute the individual param counts and param conditional\n        on the command counts.\n\n        If values are provided with the params, then in addition, we\n        will compute the individual value counts and value conditional\n        on the param counts. Also, we will use rough heuristics\n        to determine which params take categorical values, and hence\n        have modellable values.\n\n        \"\"\"\n        if self.session_type is None:\n            raise MsticpyException(\"session_type attribute should not be None\")\n\n        if self.session_type == SessionType.cmds_only:\n            seq1_counts, seq2_counts = cmds_only.compute_counts(\n                sessions=self.sessions,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n            self._seq1_counts = seq1_counts\n            self._seq2_counts = seq2_counts\n\n        elif self.session_type == SessionType.cmds_params_only:\n            (\n                seq1_counts,\n                seq2_counts,\n                param_counts,\n                cmd_param_counts,\n            ) = cmds_params_only.compute_counts(\n                sessions=self.sessions,\n                start_token=self.start_token,\n                end_token=self.end_token,\n            )\n\n            self._seq1_counts = seq1_counts\n            self._seq2_counts = seq2_counts\n            self._param_counts = param_counts\n            self._cmd_param_counts = cmd_param_counts\n\n        elif self.session_type == SessionType.cmds_params_values:\n            (\n                seq1_counts,\n                seq2_counts,\n                param_counts,\n                cmd_param_counts,\n                value_counts,\n                param_value_counts,\n            ) = cmds_params_values.compute_counts(\n                sessions=self.sessions,\n                start_token=self.start_token,\n                end_token=self.end_token,\n            )\n\n            if self.modellable_params is None:\n                modellable_params = cmds_params_values.get_params_to_model_values(\n                    param_counts=param_counts, param_value_counts=param_value_counts\n                )\n                self.modellable_params = modellable_params\n\n            self._seq1_counts = seq1_counts\n            self._seq2_counts = seq2_counts\n            self._param_counts = param_counts\n            self._cmd_param_counts = cmd_param_counts\n            self._value_counts = value_counts\n            self._param_value_counts = param_value_counts\n\n    def _laplace_smooth_counts(self):\n        \"\"\"\n        Laplace smooth all the counts for the model.\n\n        We do this by adding 1 to all the counts. This is so we shift\n        some of the probability mass from the very probable\n        commands/params/values to the unseen and very unlikely\n        commands/params/values. The `unk_token` means we can handle\n        unseen commands, params, values, sequences of commands.\n\n        \"\"\"\n        if self._seq1_counts is None:\n            raise MsticpyException(\"Please run the _compute_counts method first.\")\n\n        if self.session_type == SessionType.cmds_only:\n            seq1_counts_ls, seq2_counts_ls = cmds_only.laplace_smooth_counts(\n                seq1_counts=self._seq1_counts,\n                seq2_counts=self._seq2_counts,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n            self.seq1_counts = seq1_counts_ls\n            self.seq2_counts = seq2_counts_ls\n\n        elif self.session_type == SessionType.cmds_params_only:\n            (\n                seq1_counts_ls,\n                seq2_counts_ls,\n                param_counts_ls,\n                cmd_param_counts_ls,\n            ) = cmds_params_only.laplace_smooth_counts(\n                seq1_counts=self._seq1_counts,\n                seq2_counts=self._seq2_counts,\n                param_counts=self._param_counts,\n                cmd_param_counts=self._cmd_param_counts,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n\n            self.seq1_counts = seq1_counts_ls\n            self.seq2_counts = seq2_counts_ls\n            self.param_counts = param_counts_ls\n            self.cmd_param_counts = cmd_param_counts_ls\n\n        elif self.session_type == SessionType.cmds_params_values:\n            (\n                seq1_counts_ls,\n                seq2_counts_ls,\n                param_counts_ls,\n                cmd_param_counts_ls,\n                value_counts_ls,\n                param_value_counts_ls,\n            ) = cmds_params_values.laplace_smooth_counts(\n                seq1_counts=self._seq1_counts,\n                seq2_counts=self._seq2_counts,\n                param_counts=self._param_counts,\n                cmd_param_counts=self._cmd_param_counts,\n                value_counts=self._value_counts,\n                param_value_counts=self._param_value_counts,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n            self.seq1_counts = seq1_counts_ls\n            self.seq2_counts = seq2_counts_ls\n            self.param_counts = param_counts_ls\n            self.cmd_param_counts = cmd_param_counts_ls\n            self.value_counts = value_counts_ls\n            self.param_value_counts = param_value_counts_ls\n\n    def _compute_probs(self):\n        \"\"\"\n        Compute all the probabilities for the model.\n\n        The probabilities we compute depends on the `session_type` attribute.\n        We will compute the individual command and transition\n        command probabilities.\n\n        If params are provided with the commands, then, in addition,\n        we will compute the individual param probabilities and param\n        conditional on the command probabilities.\n\n        If values are provided with the params, then in addition,\n        we will compute the individual value probabilities and\n        value conditional on the param probabilities.\n\n        \"\"\"\n        self._compute_probs_cmds()\n        if self.session_type in [\n            SessionType.cmds_params_only,\n            SessionType.cmds_params_values,\n        ]:\n            self._compute_probs_params()\n        if self.session_type == SessionType.cmds_params_values:\n            self._compute_probs_values()\n\n    def compute_setof_params_cond_cmd(self, use_geo_mean: bool):  # noqa: MC0001\n        \"\"\"\n        Compute likelihood of combinations of params conditional on the cmd.\n\n        In particular, go through each command from each session and\n        compute the probability of that set of params (and values if provided)\n        appearing conditional on the command.\n\n        This can help us to identify unlikely combinations of params\n        (and values if provided) for each distinct command.\n\n        Note, this method is only available if each session is a list\n        of the Cmd datatype. It will result in an Exception if you\n        try and use it when each session is a list of strings.\n\n        Parameters\n        ----------\n        use_geo_mean: bool\n            if True, then the probabilities will be raised to\n            the power of (1/K)\n\n            case1: we have only params:\n                Then K is the number of distinct params which appeared\n                for the given cmd across all the sessions.\n            case2: we have params and values:\n                Then K is the number of distinct params which appeared\n                for the given cmd across all the sessions + the number\n                of values which we included in the modelling for this cmd.\n\n        \"\"\"\n        if self.param_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if self.session_type is None:\n            raise MsticpyException(\"session_type attribute should not be None\")\n\n        if self.session_type == SessionType.cmds_only:\n            raise MsticpyException(\n                'this method is not available for your type of input data \"sessions\"'\n            )\n        if self.session_type == SessionType.cmds_params_only:\n            result = defaultdict(lambda: defaultdict(lambda: 0))\n            for ses in self.sessions:\n                for cmd in ses:\n                    c_name = cmd.name\n                    params = cmd.params\n                    prob = cmds_params_only.compute_prob_setofparams_given_cmd(\n                        cmd=c_name,\n                        params=params,\n                        param_cond_cmd_probs=self.param_cond_cmd_probs,\n                        use_geo_mean=use_geo_mean,\n                    )\n                    result[c_name][tuple(params)] = prob\n            self.set_params_cond_cmd_probs = result\n        else:\n            result = defaultdict(lambda: defaultdict(lambda: 0))\n            for ses in self.sessions:\n                for cmd in ses:\n                    c_name = cmd.name\n                    params = cmd.params\n                    pars = set(cmd.params.keys())\n                    intersection_pars = pars.intersection(self.modellable_params)\n                    key = set()\n                    for par in pars:\n                        if par in intersection_pars:\n                            key.add(f\"{par} --- {params[par]}\")\n                        else:\n                            key.add(par)\n                    prob = cmds_params_values.compute_prob_setofparams_given_cmd(\n                        cmd=c_name,\n                        params_with_vals=params,\n                        param_cond_cmd_probs=self.param_cond_cmd_probs,\n                        value_cond_param_probs=self.value_cond_param_probs,\n                        modellable_params=self.modellable_params,\n                        use_geo_mean=use_geo_mean,\n                    )\n                    result[c_name][tuple(key)] = prob\n            self.set_params_cond_cmd_probs = result\n\n    def compute_likelihoods_of_sessions(self, use_start_end_tokens: bool = True):\n        \"\"\"\n        Compute the likelihoods for each of the sessions.\n\n        Note: If the lengths (number of commands) of the sessions vary a lot,\n        then you may not be able to fairly compare the likelihoods between a\n        long session and a short session. This is because longer sessions\n        involve multiplying more numbers together which are between 0 and 1.\n        Therefore the length of the session will be negatively correlated with\n        the likelihoods. If you take the geometric mean of the likelihood, then\n        you can compare the likelihoods more fairly across different session\n        lengths\n\n        Parameters\n        ----------\n        use_start_end_tokens: bool\n            if True, then `start_token` and `end_token` will be prepended\n            and appended to the session respectively before the calculations\n            are done\n\n        \"\"\"\n        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        result = []\n\n        for sess in self.sessions:\n            if self.session_type == SessionType.cmds_only:\n                tmp = cmds_only.compute_likelihood_window(\n                    window=sess,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            elif self.session_type == SessionType.cmds_params_only:\n                tmp = cmds_params_only.compute_likelihood_window(\n                    window=sess,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:\n                tmp = cmds_params_values.compute_likelihood_window(\n                    window=sess,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n\n            result.append(tmp)\n\n        self.session_likelihoods = result\n\n    def compute_geomean_lik_of_sessions(self):\n        \"\"\"\n        Compute the geometric mean of the likelihood for each of the sessions.\n\n        This is done by raising the likelihood of the session to the power of\n        (1 / k) where k is the length of the session.\n\n        Note: If the lengths (number of commands) of the sessions vary a lot,\n        then you may not be able to fairly compare the likelihoods between a\n        long session and a short session. This is because longer sessions\n        involve multiplying more numbers together which are between 0 and 1.\n        Therefore the length of the session will be negatively correlated with\n        the likelihoods. If you take the geometric mean of the likelihood, then\n        you can compare the likelihoods more fairly across different session\n        lengths.\n\n        \"\"\"\n        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions()\n        result = [\n            self.session_likelihoods[idx] ** (1 / len(session))\n            for idx, session in enumerate(self.sessions)\n        ]\n\n        self.session_geomean_likelihoods = result\n\n###The function: compute_rarest_windows###\n    def _compute_probs_cmds(self):\n        \"\"\"Compute the individual and transition command probabilties.\"\"\"\n        if self.seq1_counts is None:\n            raise MsticpyException(\"seq1_counts attribute should not be None\")\n        if self.seq2_counts is None:\n            raise MsticpyException(\"seq2_counts attribute should not be None\")\n\n        prior_probs, trans_probs = probabilities.compute_cmds_probs(\n            seq1_counts=self.seq1_counts,\n            seq2_counts=self.seq2_counts,\n            unk_token=self.unk_token,\n        )\n\n        self.prior_probs = prior_probs\n        self.trans_probs = trans_probs\n\n    def _compute_probs_params(self):\n        \"\"\"Compute the individual param probs and param conditional on command probs.\"\"\"\n        if self.param_counts is None:\n            raise MsticpyException(\"param_counts attribute should not be None\")\n        if self.cmd_param_counts is None:\n            raise MsticpyException(\"cmd_param_counts attribute should not be None\")\n\n        param_probs, param_cond_cmd_probs = probabilities.compute_params_probs(\n            param_counts=self.param_counts,\n            cmd_param_counts=self.cmd_param_counts,\n            seq1_counts=self.seq1_counts,\n            unk_token=self.unk_token,\n        )\n\n        self.param_probs = param_probs\n        self.param_cond_cmd_probs = param_cond_cmd_probs\n\n    def _compute_probs_values(self):\n        \"\"\"Compute the individual value probs and value conditional on param probs.\"\"\"\n        if self.value_counts is None:\n            raise MsticpyException(\"value_counts attribute should not be None\")\n        if self.param_value_counts is None:\n            raise MsticpyException(\"param_value_counts attribute should not be None\")\n\n        value_probs, value_cond_param_probs = probabilities.compute_values_probs(\n            value_counts=self.value_counts,\n            param_value_counts=self.param_value_counts,\n            unk_token=self.unk_token,\n        )\n\n        self.value_probs = value_probs\n        self.value_cond_param_probs = value_cond_param_probs\n\n    def _asses_input(self):\n        \"\"\"\n        Determine what type of sessions we have.\n\n        In particular, assess the input `self.sessions` to see whether each\n        session is a list of strings, or list of the Cmd datatype. And if each\n        session is a list of the Cmd datatype, it will assess whether the params\n        attribute of the Cmd datatype is a set or a dict.\n\n        \"\"\"\n        session = self.sessions[0]\n        cmd = session[0]\n        if isinstance(cmd, str):\n            self.session_type = SessionType.cmds_only\n        elif self._check_cmd_type():\n            if isinstance(cmd.params, set):\n                self.session_type = SessionType.cmds_params_only\n            elif isinstance(cmd.params, dict):\n                self.session_type = SessionType.cmds_params_values\n            else:\n                raise MsticpyException(\n                    \"Params attribute of Cmd data structure should \"\n                    + \"be either a set or a dict\"\n                )\n        else:\n            raise MsticpyException(\n                \"Each element of 'sessions' should be a list of either \"\n                + \"strings, or Cmd data types\"\n            )\n\n    def _check_cmd_type(self):\n        \"\"\"Check whether the Cmd datatype has the expected attributes.\"\"\"\n        session = self.sessions[0]\n        cmd = session[0]\n        if \"name\" in dir(cmd) and \"params\" in dir(cmd):\n            return True\n        return False\n\n\nclass SessionType:\n    \"\"\"Class for storing the types of accepted sessions.\"\"\"\n\n    cmds_only = \"cmds_only\"\n    cmds_params_only = \"cmds_params_only\"\n    cmds_params_values = \"cmds_params_values\"\n", "cross_context": [{"msticpy.common.exceptions.MsticpyException": "# -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\"\"\"Miscellaneous helper methods for Jupyter Notebooks.\"\"\"\nimport contextlib\nimport sys\nimport traceback\nfrom typing import List, Tuple, Union\n\nfrom IPython.display import display\n\nfrom .._version import VERSION\nfrom .utility import is_ipython\n\n__version__ = VERSION\n__author__ = \"Ian Hellen\"\n\n\n# placeholder for pkg_config.get_config - this function is\n# overwritten by msticpy.common.pkg_config\ndef _get_config(setting_path: str):\n    del setting_path\n    return True\n\n\n# Standard exception types\nclass MsticpyException(Exception):  # noqa: N818\n    \"\"\"Default exception class for msticpy.\"\"\"\n\n\nclass MsticpyConfigError(MsticpyException):\n    \"\"\"Configuration exception class for msticpy.\"\"\"\n\n\nclass MsticpyResourceError(MsticpyException):\n    \"\"\"Exception class for resource errors.\"\"\"\n\n\n######################################\n# User-friendly displayable exceptions\n# ------------------------------------\n# Note: for ease of distinguishing the two exception types\n# name any classes derived from MsticpyUserError with an \"Error\"\n# suffix. Name classes derived from MsticpyException with an\n# \"Exception\" suffix\nclass MsticpyUserError(MsticpyException):\n    \"\"\"Msticpy User exception displaying friendly message.\"\"\"\n\n    _display_exceptions = True\n\n    DEF_HELP_URI = (\"msticpy documentation\", \"https://msticpy.readthedocs.org\")\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create an instance of the MsticpyUserError class.\n\n        Parameters\n        ----------\n        args : Iterable of strings\n            Args will be printed as text of the exception.\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Primary URL, by default \"https://msticpy.readthedocs.org\"\n\n        Other Parameters\n        ----------------\n        title : str, optional\n            If a `title` keyword argument is supplied it will be used\n            to create the title line.\n        *_uri : str, optional\n            Additional keyword arguments who's names end in \"_uri\"\n            will be used to create a list of references in addition to\n            the primary `help_uri`\n        display : bool, optional\n            Display the exception when created. By default, False\n\n        Notes\n        -----\n        The exception text is displayed when the exception is created\n        and *not* when it is raised. We recommend creating the exception\n        within the `raise` statement. E.g.\n\n        `raise MsticpyUserException(arg1, arg2...)`\n\n        Developer note:\n        Any classes derived from MsticpyUserError should be named with\n        an \"Error\" suffix to distinguish these from standard exception types.\n\n        \"\"\"\n        # This nasty-looking thing just means that this is a list that\n        # holds:\n        # just strings - for simple args strings\n        # tuples(str, str) - if the item is annotated as a uri or title\n        # tuple(tuple(str, str), str) - if the URI is a tuple of display_name, URI\n        self._output: List[\n            Union[str, Tuple[str, str], Tuple[Tuple[str, str], str]]\n        ] = []\n        title = kwargs.pop(\"title\", \"we've hit an error while running\")\n        disp_exception = kwargs.pop(\"display\", False)\n        self._has_displayed = False\n        self._output.append((f\"{self.__class__.__name__} - {title}\", \"title\"))\n        self._output.extend(args)\n\n        self._output.append(\"\\nFor more help on fixing this error see:\")\n        if not help_uri:\n            help_uri = self.DEF_HELP_URI\n        self._output.append((help_uri, \"uri\"))  # type: ignore\n\n        help_args = [\n            kw_val for kw_arg, kw_val in kwargs.items() if kw_arg.endswith(\"_uri\")\n        ]\n        if help_args:\n            self._output.append(\"You can find other related help here:\")\n            self._output.extend((uri, \"uri\") for uri in help_args)\n        self._context = self._format_exception_context(\n            stack=traceback.format_stack(limit=5),\n            frame_locals=sys._getframe(1).f_locals,\n        )\n        if _get_config(\"msticpy.FriendlyExceptions\") and disp_exception:\n            self.display_exception()\n\n        # add the extra elements to the the exception standard args.\n        ex_args = [title, *args, help_uri, *help_args]\n        super().__init__(*ex_args)\n\n    @classmethod\n    @contextlib.contextmanager\n    def no_display_exceptions(cls):\n        \"\"\"Context manager to block exception display to IPython/stdout.\"\"\"\n        cls._display_exceptions = False\n        yield\n        cls._display_exceptions = True\n\n    @property\n    def help_uri(self) -> Union[Tuple[str, str], str]:\n        \"\"\"Get the default help URI.\"\"\"\n        return self.DEF_HELP_URI\n\n    def display_exception(self):\n        \"\"\"Output the exception HTML or text friendly exception.\"\"\"\n        if not self._display_exceptions or self._has_displayed:\n            return\n        if is_ipython(notebook=True):\n            display(self)\n        else:\n            self._display_txt_exception()\n        self._has_displayed = True\n\n    def _repr_html_(self):\n        \"\"\"Return HTML-formatted exception text.\"\"\"\n        ex_style = \"\"\"\n        <style>\n            div.solid {border: thin solid black; padding:10px;}\n            p.title {background-color:Tomato; padding:5px;}\n            ul.circle {list-style-type: circle;}\n            div.indent {text-indent: 20px; padding: 0px}\n        </style>\n        \"\"\"\n        div_tmplt = \"<div class='solid'>{content}</div>\"\n        about_blank = \"target='_blank' rel='noopener noreferrer'\"\n        content = []\n        for line in self._output:\n            if isinstance(line, tuple):\n                l_content, l_type = line\n                if l_type == \"title\":\n                    content.append(f\"<h3><p class='title'>{l_content}</p></h3>\")\n                elif l_type == \"uri\":\n                    if isinstance(l_content, tuple):\n                        name, uri = l_content\n                    else:\n                        name = uri = l_content\n                    content.append(\n                        f\"<ul class='circle'><li><a href='{uri}' {about_blank}>\"\n                        f\"{name}</a></li></ul>\"\n                    )\n            else:\n                text_line = line.replace(\"\\n\", \"<br>\")\n                content.append(f\"{text_line}<br>\")\n\n        if self._context:\n            context = [f\"<div class='indent'>{line}</div>\" for line in self._context]\n            content.extend(\n                (\n                    \"<summary>Additional context<details>\",\n                    *context,\n                    \"</details></summary>\",\n                )\n            )\n\n        return \"\".join((ex_style, div_tmplt.format(content=\"\".join(content))))\n\n    def _display_txt_exception(self):\n        \"\"\"Display text-only version of the exception text.\"\"\"\n        print(self._get_exception_text())\n\n    def _get_exception_text(self) -> str:\n        out_lines: List[str] = []\n        for line in self._output:\n            if isinstance(line, tuple):\n                l_content, l_type = line\n                if isinstance(l_content, tuple):\n                    l_content = l_content[0]\n                if l_type == \"title\":\n                    out_lines.extend(\n                        (\"-\" * len(l_content), l_content, \"-\" * len(l_content))\n                    )\n                elif l_type == \"uri\":\n                    if isinstance(l_content, tuple):\n                        out_lines.append(f\" - {': '.join(l_content)}\")\n                    else:\n                        out_lines.append(f\" - {l_content}\")\n            else:\n                out_lines.append(line)\n        if self._context:\n            out_lines.extend([\"\", \"Exception context:\", *self._context])\n        return \"\\n\".join(out_lines)\n\n    @staticmethod\n    def _format_exception_context(stack, frame_locals):\n        context_lines: List[str] = [\"Stack:\", *stack, \"---\", \"Locals:\"]\n        context_lines.extend(\n            f\"{var} ({type(val).__name__}) = {val}\" for var, val in frame_locals.items()\n        )\n        ex_type, ex_value, ex_traceback = sys.exc_info()\n        if ex_traceback:\n            context_lines.extend(\n                (\n                    \"---\",\n                    \"Exception was raised by:\",\n                    *(traceback.format_exception(ex_type, ex_value, ex_traceback)),\n                )\n            )\n        return context_lines\n\n\nclass MsticpyUserConfigError(MsticpyUserError):\n    \"\"\"Configuration user exception class for msticpy.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Configuring msticpy\",\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/msticpyconfig.html\",\n    )\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create generic user configuration exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n\n        \"\"\"\n        def_mssg = \"There is a problem with configuration in your msticpyconfig.yaml.\"\n        mp_loc_mssg = [\n            \"Ensure that the path to your msticpyconfig.yaml is specified with\"\n            + \" the MSTICPYCONFIG environment variable.\",\n            \"Or ensure that a copy of this file is in the current directory.\",\n        ]\n        add_args = [*args, *mp_loc_mssg] if args else [def_mssg, *mp_loc_mssg]\n        if help_uri:\n            uri: Union[Tuple[str, str], str] = help_uri\n            add_uris = {\"basehelp_uri\": self.DEF_HELP_URI}\n        else:\n            uri = self.DEF_HELP_URI\n            add_uris = {}\n        super().__init__(*add_args, help_uri=uri, **add_uris, **kwargs)\n\n\nclass MsticpyKeyVaultConfigError(MsticpyUserConfigError):\n    \"\"\"Key Vault configuration exception.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Using keyvault to store msticpy secrets\",\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/msticpyconfig.html\"\n        + \"#specifying-secrets-as-key-vault-secrets\",\n    )\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create Key Vault configuration exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n\n        \"\"\"\n        mssg = (\n            \"Please verify that a valid KeyVault section has been configured\"\n            + \"in your msticpyconfig.yaml.\"\n        )\n        add_args = [*args, mssg]\n        uri = help_uri or self.DEF_HELP_URI\n        super().__init__(*add_args, help_uri=uri, **kwargs)\n\n\nclass MsticpyKeyVaultMissingSecretError(MsticpyKeyVaultConfigError):\n    \"\"\"Missing secret exception.\"\"\"\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create Key Vault missing key exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n\n        \"\"\"\n        mssg = (\n            \"Please verify that the item using this secret is properly\"\n            + \" configured in in your msticpyconfig.yaml.\"\n        )\n        add_args = [*args, mssg]\n        uri = help_uri or self.DEF_HELP_URI\n        super().__init__(*add_args, help_uri=uri, **kwargs)\n\n\nclass MsticpyAzureConfigError(MsticpyUserConfigError):\n    \"\"\"Exception class for AzureData.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Using the Azure API connector\",\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureData.html\"\n        + \"#instantiating-and-connecting-with-an-azure-data-connector\",\n    )\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create Azure data missing configuration exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n\n        \"\"\"\n        uri = help_uri or self.DEF_HELP_URI\n        super().__init__(*args, help_uri=uri, **kwargs)\n\n\nclass MsticpyNotConnectedError(MsticpyUserError):\n    \"\"\"Exception class for NotConnected errors.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Querying and importing data\",\n        \"https://msticpy.readthedocs.io/en/latest/DataAcquisition.html\"\n        + \"#querying-and-importing-data\",\n    )\n\n\nclass MsticpyNoDataSourceError(MsticpyUserError):\n    \"\"\"Exception class for missing data source errors.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Querying and importing data\",\n        \"https://msticpy.readthedocs.io/en/latest/DataAcquisition.html\"\n        + \"#querying-and-importing-data\",\n    )\n\n\nclass MsticpyDataQueryError(MsticpyUserError):\n    \"\"\"Exception class for data query errors.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Query failed\",\n        \"https://msticpy.readthedocs.io/en/latest/DataAcquisition.html\"\n        + \"#querying-and-importing-data\",\n    )\n\n\nclass MsticpyConnectionError(MsticpyUserError):\n    \"\"\"Exception class for KqlConnection errors.\"\"\"\n\n    DEF_HELP_URI = (\n        \"DataProviders\",\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/DataProviders.html\",\n    )\n\n\nclass MsticpyKqlConnectionError(MsticpyUserError):\n    \"\"\"Exception class for KqlConnection errors.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Connecting to Microsoft Sentinel\",\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/DataProviders.html\"\n        + \"#connecting-to-an-azure-sentinel-workspace\",\n    )\n\n\nclass MsticpyImportExtraError(MsticpyUserError, ImportError):\n    \"\"\"Exception class for Imports that need an extra.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Installing msticpy\",\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/Installing.html\",\n    )\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create import missing extra exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n        extra : str\n            The name of the setup extra that needs to be installed.\n\n        \"\"\"\n        extra = kwargs.pop(\"extra\", None)\n        if not extra:\n            raise AttributeError(\"Keyword argument 'extra' must be supplied\")\n        mssg = \"\".join(\n            [\n                \"This feature requires one or more additional packages\",\n                \" to be installed.\\n\",\n                \"To do this run the command:\\n\",\n                f\"pip install msticpy[{extra}]\",\n            ]\n        )\n        add_args = [*args, mssg]\n        uri = help_uri or self.DEF_HELP_URI\n        super().__init__(*add_args, help_uri=uri, **kwargs)\n\n\nclass MsticpyMissingDependencyError(MsticpyUserError, ImportError):\n    \"\"\"Exception class for Imports that are not installed.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Installing msticpy\",\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/Installing.html\",\n    )\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create import missing extra exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n        packages : Union[str, List[str]]\n            The name of the packages or list of packages that need(s)\n            to be installed.\n\n        \"\"\"\n        packages = kwargs.pop(\"packages\", None)\n        if not packages:\n            raise AttributeError(\"Keyword argument 'packages' must be supplied\")\n        packages = [packages] if isinstance(packages, str) else packages\n        mssg = \"\".join(\n            [\n                \"This feature requires one or more additional packages\",\n                \" to be installed.\\n\",\n                \"To do this run the command:\\n\",\n                f\"pip install {' '.join(packages)}\\n\",\n                \"In a notebook run:\\n\",\n                f\"%pip install {' '.join(packages)}\",\n            ]\n        )\n        add_args = [*args, mssg]\n        uri = help_uri or self.DEF_HELP_URI\n        super().__init__(*add_args, help_uri=uri, **kwargs)\n\n\nclass MsticpyAzureConnectionError(MsticpyUserError):\n    \"\"\"Exception class for Azure Connection errors.\"\"\"\n\n    DEF_HELP_URI = (\n        \"Connecting to Microsoft Sentinel\",\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureData.html\"\n        + \"#instantiating-and-connecting-with-an-azure-data-connector\",\n    )\n\n\nclass MsticpyParameterError(MsticpyUserError):\n    \"\"\"Exception class for missing/incorrect parameters.\"\"\"\n\n    DEF_HELP_URI = (\"MSTICPy documentation\", \"https://msticpy.readthedocs.io\")\n\n    def __init__(\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\n    ):\n        \"\"\"\n        Create parameter exception.\n\n        Parameters\n        ----------\n        help_uri : Union[Tuple[str, str], str, None], optional\n            Override the default help URI.\n        parameters : Union[str, List[str]\n            The name of the bad parameter(s).\n\n        \"\"\"\n        parameter = kwargs.pop(\"parameter\", None)\n        if not parameter:\n            raise AttributeError(\"Keyword argument 'parameter' must be supplied\")\n        mssg = \"One or more parameters were incorrect.\"\n        if isinstance(parameter, str):\n            parameter = [parameter]\n        add_args = [*args, mssg, \", \".join(parameter)]\n        uri = help_uri or self.DEF_HELP_URI\n        super().__init__(*add_args, help_uri=uri, **kwargs)\n"}], "prompt": "Please write a python function called 'compute_rarest_windows' base the context. This function computes the rarest windows and corresponding likelihood for each session. It uses a sliding window approach to identify the rarest window and its likelihood in each session. The function takes into account the length of the sliding window, whether to use start and end tokens, and whether to use the geometric mean for likelihood calculations.:param self: Model. An instance of the Model class.\n:param window_len: int. The length of the sliding window for likelihood calculations.\n:param use_start_end_tokens: bool. If True, start and end tokens will be added to each session before calculations.\n:param use_geo_mean: bool. If True, the likelihoods of the sliding windows will be raised to the power of (1/window_len).\n:return: None. The function updates the rarest windows and corresponding likelihoods in the Model instance..\n        The context you need to refer to is as follows:\n        ####intra_file_context:\n        # -------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n# --------------------------------------------------------------------------\n\"\"\"Module for Model class for modelling sessions data.\"\"\"\n\nfrom collections import defaultdict\nfrom typing import Dict, List, Union\n\nfrom ...common.exceptions import MsticpyException\nfrom .utils import cmds_only, cmds_params_only, cmds_params_values, probabilities\nfrom .utils.data_structures import Cmd\n\n\n# pylint: disable=too-many-instance-attributes\n# pylint: disable=too-few-public-methods\nclass Model:\n    \"\"\"Class for modelling sessions data.\"\"\"\n\n    def __init__(\n        self, sessions: List[List[Union[str, Cmd]]], modellable_params: set = None\n    ):\n        \"\"\"\n        Instantiate the Model class.\n\n        This Model class can be used to model sessions, where each\n        session is a sequence of commands. We use a sliding window\n        approach to calculate the rarest part of each session. We\n        can view the sessions in ascending order of this metric to\n        see if the top sessions are anomalous/malicious.\n\n        Parameters\n        ----------\n        sessions: List[List[Union[str, Cmd]]]\n            list of sessions, where each session is a list of either\n            strings or a list of the Cmd datatype.\n\n            The Cmd datatype should have \"name\" and \"params\" as attributes\n            where \"name\" is the name of the command (string) and \"params\"\n            is either a set of accompanying params or a dict of\n            accompanying params and values.\n\n            examples formats of a session:\n            1) ['Set-User', 'Set-Mailbox']\n            2) [Cmd(name='Set-User', params={'Identity', 'Force'}),\n            Cmd(name='Set-Mailbox', params={'Identity', 'AuditEnabled'})]\n            3) [Cmd(name='Set-User',\n            params={'Identity': 'blahblah', 'Force': 'true'}),\n            Cmd(name='Set-Mailbox',\n            params={'Identity': 'blahblah', 'AuditEnabled': 'false'})]\n\n        modellable_params: set, optional\n            set of params which you deem to have categorical values which are suitable\n            for modelling.\n            Note this argument will only have an effect if your sessions include commands,\n            params and values. If your sessions include commands, params and values and\n            this argument is not set, then some rough heuristics will be used to determine\n            which params have values which are suitable for modelling.\n\n        \"\"\"\n        if not isinstance(sessions, list):\n            raise MsticpyException(\"`sessions` should be a list\")\n        if not sessions:\n            raise MsticpyException(\"`sessions` should not be an empty list\")\n        for i, ses in enumerate(sessions):\n            if not isinstance(ses, list):\n                raise MsticpyException(\"each session in `sessions` should be a list\")\n            if len(ses) == 0:\n                raise MsticpyException(\n                    f\"session at index {i} of `sessions` is empty. Each session \"\n                    \"should contain at least one command\"\n                )\n\n        self.start_token = \"##START##\"  # nosec B105\n        self.end_token = \"##END##\"  # nosec B105\n        self.unk_token = \"##UNK##\"  # nosec B105\n\n        self.sessions = sessions\n        self.session_type = None\n        self._asses_input()\n\n        # non laplace smoothed counts\n        self._seq1_counts = None\n        self._seq2_counts = None\n        self._param_counts = None\n        self._cmd_param_counts = None\n        self._value_counts = None\n        self._param_value_counts = None\n\n        # laplace smoothed counts\n        self.seq1_counts = None\n        self.seq2_counts = None\n        self.param_counts = None\n        self.cmd_param_counts = None\n        self.value_counts = None\n        self.param_value_counts = None\n\n        self.modellable_params = modellable_params\n\n        self.prior_probs = None\n        self.trans_probs = None\n        self.param_probs = None\n        self.param_cond_cmd_probs = None\n        self.value_probs = None\n        self.value_cond_param_probs = None\n\n        self.set_params_cond_cmd_probs: Dict[str, Dict[str, float]] = {}\n\n        self.session_likelihoods = None\n        self.session_geomean_likelihoods = None\n\n        self.rare_windows: Dict[int, list] = {}\n        self.rare_window_likelihoods: Dict[int, list] = {}\n\n        self.rare_windows_geo: Dict[int, list] = {}\n        self.rare_window_likelihoods_geo: Dict[int, list] = {}\n\n    def train(self):\n        \"\"\"\n        Train the model by computing counts and probabilities.\n\n        In particular, computes the counts and probabilities of the commands\n        (and possibly the params if provided, and possibly the values if provided)\n\n        \"\"\"\n        self._compute_counts()\n        self._laplace_smooth_counts()\n        self._compute_probs()\n\n    def compute_scores(self, use_start_end_tokens: bool):\n        \"\"\"\n        Compute some likelihood based scores/metrics for each of the sessions.\n\n        In particular, computes the likelihoods and geometric mean of\n        the likelihoods for each of the sessions. Also, uses the sliding\n        window approach to compute the rarest window likelihoods for each\n        of the sessions. It does this for windows of length 2 and 3.\n\n        Note that if we have a session of length k, and we use a sliding\n        window of length k+1, then we will end up with np.nan for the\n        rarest window likelihood metric for that session.\n        However, if `use_start_end_tokens` is set to True, then\n        because we will be appending self.end_token to the session,\n        the session will be treated as a session of length k+1,\n        therefore, we will end up with a non np.nan value for that session.\n\n        Parameters\n        ----------\n        use_start_end_tokens: bool\n            if True, then self.start_token and self.end_token will be\n            prepended and appended to each\n            of the sessions respectively before the calculations are done.\n\n        \"\"\"\n        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n        self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(\n            window_len=2, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n        self.compute_rarest_windows(\n            window_len=3, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n\n    def _compute_counts(self):\n        \"\"\"\n        Compute all the counts for the model.\n\n        The items we will count depend on the the `session_type` attribute.\n        We will compute the individual command and transition command counts.\n\n        If params are provided with the commands, then, in addition,\n        we will compute the individual param counts and param conditional\n        on the command counts.\n\n        If values are provided with the params, then in addition, we\n        will compute the individual value counts and value conditional\n        on the param counts. Also, we will use rough heuristics\n        to determine which params take categorical values, and hence\n        have modellable values.\n\n        \"\"\"\n        if self.session_type is None:\n            raise MsticpyException(\"session_type attribute should not be None\")\n\n        if self.session_type == SessionType.cmds_only:\n            seq1_counts, seq2_counts = cmds_only.compute_counts(\n                sessions=self.sessions,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n            self._seq1_counts = seq1_counts\n            self._seq2_counts = seq2_counts\n\n        elif self.session_type == SessionType.cmds_params_only:\n            (\n                seq1_counts,\n                seq2_counts,\n                param_counts,\n                cmd_param_counts,\n            ) = cmds_params_only.compute_counts(\n                sessions=self.sessions,\n                start_token=self.start_token,\n                end_token=self.end_token,\n            )\n\n            self._seq1_counts = seq1_counts\n            self._seq2_counts = seq2_counts\n            self._param_counts = param_counts\n            self._cmd_param_counts = cmd_param_counts\n\n        elif self.session_type == SessionType.cmds_params_values:\n            (\n                seq1_counts,\n                seq2_counts,\n                param_counts,\n                cmd_param_counts,\n                value_counts,\n                param_value_counts,\n            ) = cmds_params_values.compute_counts(\n                sessions=self.sessions,\n                start_token=self.start_token,\n                end_token=self.end_token,\n            )\n\n            if self.modellable_params is None:\n                modellable_params = cmds_params_values.get_params_to_model_values(\n                    param_counts=param_counts, param_value_counts=param_value_counts\n                )\n                self.modellable_params = modellable_params\n\n            self._seq1_counts = seq1_counts\n            self._seq2_counts = seq2_counts\n            self._param_counts = param_counts\n            self._cmd_param_counts = cmd_param_counts\n            self._value_counts = value_counts\n            self._param_value_counts = param_value_counts\n\n    def _laplace_smooth_counts(self):\n        \"\"\"\n        Laplace smooth all the counts for the model.\n\n        We do this by adding 1 to all the counts. This is so we shift\n        some of the probability mass from the very probable\n        commands/params/values to the unseen and very unlikely\n        commands/params/values. The `unk_token` means we can handle\n        unseen commands, params, values, sequences of commands.\n\n        \"\"\"\n        if self._seq1_counts is None:\n            raise MsticpyException(\"Please run the _compute_counts method first.\")\n\n        if self.session_type == SessionType.cmds_only:\n            seq1_counts_ls, seq2_counts_ls = cmds_only.laplace_smooth_counts(\n                seq1_counts=self._seq1_counts,\n                seq2_counts=self._seq2_counts,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n            self.seq1_counts = seq1_counts_ls\n            self.seq2_counts = seq2_counts_ls\n\n        elif self.session_type == SessionType.cmds_params_only:\n            (\n                seq1_counts_ls,\n                seq2_counts_ls,\n                param_counts_ls,\n                cmd_param_counts_ls,\n            ) = cmds_params_only.laplace_smooth_counts(\n                seq1_counts=self._seq1_counts,\n                seq2_counts=self._seq2_counts,\n                param_counts=self._param_counts,\n                cmd_param_counts=self._cmd_param_counts,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n\n            self.seq1_counts = seq1_counts_ls\n            self.seq2_counts = seq2_counts_ls\n            self.param_counts = param_counts_ls\n            self.cmd_param_counts = cmd_param_counts_ls\n\n        elif self.session_type == SessionType.cmds_params_values:\n            (\n                seq1_counts_ls,\n                seq2_counts_ls,\n                param_counts_ls,\n                cmd_param_counts_ls,\n                value_counts_ls,\n                param_value_counts_ls,\n            ) = cmds_params_values.laplace_smooth_counts(\n                seq1_counts=self._seq1_counts,\n                seq2_counts=self._seq2_counts,\n                param_counts=self._param_counts,\n                cmd_param_counts=self._cmd_param_counts,\n                value_counts=self._value_counts,\n                param_value_counts=self._param_value_counts,\n                start_token=self.start_token,\n                end_token=self.end_token,\n                unk_token=self.unk_token,\n            )\n            self.seq1_counts = seq1_counts_ls\n            self.seq2_counts = seq2_counts_ls\n            self.param_counts = param_counts_ls\n            self.cmd_param_counts = cmd_param_counts_ls\n            self.value_counts = value_counts_ls\n            self.param_value_counts = param_value_counts_ls\n\n    def _compute_probs(self):\n        \"\"\"\n        Compute all the probabilities for the model.\n\n        The probabilities we compute depends on the `session_type` attribute.\n        We will compute the individual command and transition\n        command probabilities.\n\n        If params are provided with the commands, then, in addition,\n        we will compute the individual param probabilities and param\n        conditional on the command probabilities.\n\n        If values are provided with the params, then in addition,\n        we will compute the individual value probabilities and\n        value conditional on the param probabilities.\n\n        \"\"\"\n        self._compute_probs_cmds()\n        if self.session_type in [\n            SessionType.cmds_params_only,\n            SessionType.cmds_params_values,\n        ]:\n            self._compute_probs_params()\n        if self.session_type == SessionType.cmds_params_values:\n            self._compute_probs_values()\n\n    def compute_setof_params_cond_cmd(self, use_geo_mean: bool):  # noqa: MC0001\n        \"\"\"\n        Compute likelihood of combinations of params conditional on the cmd.\n\n        In particular, go through each command from each session and\n        compute the probability of that set of params (and values if provided)\n        appearing conditional on the command.\n\n        This can help us to identify unlikely combinations of params\n        (and values if provided) for each distinct command.\n\n        Note, this method is only available if each session is a list\n        of the Cmd datatype. It will result in an Exception if you\n        try and use it when each session is a list of strings.\n\n        Parameters\n        ----------\n        use_geo_mean: bool\n            if True, then the probabilities will be raised to\n            the power of (1/K)\n\n            case1: we have only params:\n                Then K is the number of distinct params which appeared\n                for the given cmd across all the sessions.\n            case2: we have params and values:\n                Then K is the number of distinct params which appeared\n                for the given cmd across all the sessions + the number\n                of values which we included in the modelling for this cmd.\n\n        \"\"\"\n        if self.param_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if self.session_type is None:\n            raise MsticpyException(\"session_type attribute should not be None\")\n\n        if self.session_type == SessionType.cmds_only:\n            raise MsticpyException(\n                'this method is not available for your type of input data \"sessions\"'\n            )\n        if self.session_type == SessionType.cmds_params_only:\n            result = defaultdict(lambda: defaultdict(lambda: 0))\n            for ses in self.sessions:\n                for cmd in ses:\n                    c_name = cmd.name\n                    params = cmd.params\n                    prob = cmds_params_only.compute_prob_setofparams_given_cmd(\n                        cmd=c_name,\n                        params=params,\n                        param_cond_cmd_probs=self.param_cond_cmd_probs,\n                        use_geo_mean=use_geo_mean,\n                    )\n                    result[c_name][tuple(params)] = prob\n            self.set_params_cond_cmd_probs = result\n        else:\n            result = defaultdict(lambda: defaultdict(lambda: 0))\n            for ses in self.sessions:\n                for cmd in ses:\n                    c_name = cmd.name\n                    params = cmd.params\n                    pars = set(cmd.params.keys())\n                    intersection_pars = pars.intersection(self.modellable_params)\n                    key = set()\n                    for par in pars:\n                        if par in intersection_pars:\n                            key.add(f\"{par} --- {params[par]}\")\n                        else:\n                            key.add(par)\n                    prob = cmds_params_values.compute_prob_setofparams_given_cmd(\n                        cmd=c_name,\n                        params_with_vals=params,\n                        param_cond_cmd_probs=self.param_cond_cmd_probs,\n                        value_cond_param_probs=self.value_cond_param_probs,\n                        modellable_params=self.modellable_params,\n                        use_geo_mean=use_geo_mean,\n                    )\n                    result[c_name][tuple(key)] = prob\n            self.set_params_cond_cmd_probs = result\n\n    def compute_likelihoods_of_sessions(self, use_start_end_tokens: bool = True):\n        \"\"\"\n        Compute the likelihoods for each of the sessions.\n\n        Note: If the lengths (number of commands) of the sessions vary a lot,\n        then you may not be able to fairly compare the likelihoods between a\n        long session and a short session. This is because longer sessions\n        involve multiplying more numbers together which are between 0 and 1.\n        Therefore the length of the session will be negatively correlated with\n        the likelihoods. If you take the geometric mean of the likelihood, then\n        you can compare the likelihoods more fairly across different session\n        lengths\n\n        Parameters\n        ----------\n        use_start_end_tokens: bool\n            if True, then `start_token` and `end_token` will be prepended\n            and appended to the session respectively before the calculations\n            are done\n\n        \"\"\"\n        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        result = []\n\n        for sess in self.sessions:\n            if self.session_type == SessionType.cmds_only:\n                tmp = cmds_only.compute_likelihood_window(\n                    window=sess,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            elif self.session_type == SessionType.cmds_params_only:\n                tmp = cmds_params_only.compute_likelihood_window(\n                    window=sess,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:\n                tmp = cmds_params_values.compute_likelihood_window(\n                    window=sess,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n\n            result.append(tmp)\n\n        self.session_likelihoods = result\n\n    def compute_geomean_lik_of_sessions(self):\n        \"\"\"\n        Compute the geometric mean of the likelihood for each of the sessions.\n\n        This is done by raising the likelihood of the session to the power of\n        (1 / k) where k is the length of the session.\n\n        Note: If the lengths (number of commands) of the sessions vary a lot,\n        then you may not be able to fairly compare the likelihoods between a\n        long session and a short session. This is because longer sessions\n        involve multiplying more numbers together which are between 0 and 1.\n        Therefore the length of the session will be negatively correlated with\n        the likelihoods. If you take the geometric mean of the likelihood, then\n        you can compare the likelihoods more fairly across different session\n        lengths.\n\n        \"\"\"\n        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions()\n        result = [\n            self.session_likelihoods[idx] ** (1 / len(session))\n            for idx, session in enumerate(self.sessions)\n        ]\n\n        self.session_geomean_likelihoods = result\n\n###The function: compute_rarest_windows###\n    def _compute_probs_cmds(self):\n        \"\"\"Compute the individual and transition command probabilties.\"\"\"\n        if self.seq1_counts is None:\n            raise MsticpyException(\"seq1_counts attribute should not be None\")\n        if self.seq2_counts is None:\n            raise MsticpyException(\"seq2_counts attribute should not be None\")\n\n        prior_probs, trans_probs = probabilities.compute_cmds_probs(\n            seq1_counts=self.seq1_counts,\n            seq2_counts=self.seq2_counts,\n            unk_token=self.unk_token,\n        )\n\n        self.prior_probs = prior_probs\n        self.trans_probs = trans_probs\n\n    def _compute_probs_params(self):\n        \"\"\"Compute the individual param probs and param conditional on command probs.\"\"\"\n        if self.param_counts is None:\n            raise MsticpyException(\"param_counts attribute should not be None\")\n        if self.cmd_param_counts is None:\n            raise MsticpyException(\"cmd_param_counts attribute should not be None\")\n\n        param_probs, param_cond_cmd_probs = probabilities.compute_params_probs(\n            param_counts=self.param_counts,\n            cmd_param_counts=self.cmd_param_counts,\n            seq1_counts=self.seq1_counts,\n            unk_token=self.unk_token,\n        )\n\n        self.param_probs = param_probs\n        self.param_cond_cmd_probs = param_cond_cmd_probs\n\n    def _compute_probs_values(self):\n        \"\"\"Compute the individual value probs and value conditional on param probs.\"\"\"\n        if self.value_counts is None:\n            raise MsticpyException(\"value_counts attribute should not be None\")\n        if self.param_value_counts is None:\n            raise MsticpyException(\"param_value_counts attribute should not be None\")\n\n        value_probs, value_cond_param_probs = probabilities.compute_values_probs(\n            value_counts=self.value_counts,\n            param_value_counts=self.param_value_counts,\n            unk_token=self.unk_token,\n        )\n\n        self.value_probs = value_probs\n        self.value_cond_param_probs = value_cond_param_probs\n\n    def _asses_input(self):\n        \"\"\"\n        Determine what type of sessions we have.\n\n        In particular, assess the input `self.sessions` to see whether each\n        session is a list of strings, or list of the Cmd datatype. And if each\n        session is a list of the Cmd datatype, it will assess whether the params\n        attribute of the Cmd datatype is a set or a dict.\n\n        \"\"\"\n        session = self.sessions[0]\n        cmd = session[0]\n        if isinstance(cmd, str):\n            self.session_type = SessionType.cmds_only\n        elif self._check_cmd_type():\n            if isinstance(cmd.params, set):\n                self.session_type = SessionType.cmds_params_only\n            elif isinstance(cmd.params, dict):\n                self.session_type = SessionType.cmds_params_values\n            else:\n                raise MsticpyException(\n                    \"Params attribute of Cmd data structure should \"\n                    + \"be either a set or a dict\"\n                )\n        else:\n            raise MsticpyException(\n                \"Each element of 'sessions' should be a list of either \"\n                + \"strings, or Cmd data types\"\n            )\n\n    def _check_cmd_type(self):\n        \"\"\"Check whether the Cmd datatype has the expected attributes.\"\"\"\n        session = self.sessions[0]\n        cmd = session[0]\n        if \"name\" in dir(cmd) and \"params\" in dir(cmd):\n            return True\n        return False\n\n\nclass SessionType:\n    \"\"\"Class for storing the types of accepted sessions.\"\"\"\n\n    cmds_only = \"cmds_only\"\n    cmds_params_only = \"cmds_params_only\"\n    cmds_params_values = \"cmds_params_values\"\n\n        ####cross_file_context:\n        [{'msticpy.common.exceptions.MsticpyException': '# -------------------------------------------------------------------------\\n# Copyright (c) Microsoft Corporation. All rights reserved.\\n# Licensed under the MIT License. See License.txt in the project root for\\n# license information.\\n# --------------------------------------------------------------------------\\n\"\"\"Miscellaneous helper methods for Jupyter Notebooks.\"\"\"\\nimport contextlib\\nimport sys\\nimport traceback\\nfrom typing import List, Tuple, Union\\n\\nfrom IPython.display import display\\n\\nfrom .._version import VERSION\\nfrom .utility import is_ipython\\n\\n__version__ = VERSION\\n__author__ = \"Ian Hellen\"\\n\\n\\n# placeholder for pkg_config.get_config - this function is\\n# overwritten by msticpy.common.pkg_config\\ndef _get_config(setting_path: str):\\n    del setting_path\\n    return True\\n\\n\\n# Standard exception types\\nclass MsticpyException(Exception):  # noqa: N818\\n    \"\"\"Default exception class for msticpy.\"\"\"\\n\\n\\nclass MsticpyConfigError(MsticpyException):\\n    \"\"\"Configuration exception class for msticpy.\"\"\"\\n\\n\\nclass MsticpyResourceError(MsticpyException):\\n    \"\"\"Exception class for resource errors.\"\"\"\\n\\n\\n######################################\\n# User-friendly displayable exceptions\\n# ------------------------------------\\n# Note: for ease of distinguishing the two exception types\\n# name any classes derived from MsticpyUserError with an \"Error\"\\n# suffix. Name classes derived from MsticpyException with an\\n# \"Exception\" suffix\\nclass MsticpyUserError(MsticpyException):\\n    \"\"\"Msticpy User exception displaying friendly message.\"\"\"\\n\\n    _display_exceptions = True\\n\\n    DEF_HELP_URI = (\"msticpy documentation\", \"https://msticpy.readthedocs.org\")\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create an instance of the MsticpyUserError class.\\n\\n        Parameters\\n        ----------\\n        args : Iterable of strings\\n            Args will be printed as text of the exception.\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Primary URL, by default \"https://msticpy.readthedocs.org\"\\n\\n        Other Parameters\\n        ----------------\\n        title : str, optional\\n            If a `title` keyword argument is supplied it will be used\\n            to create the title line.\\n        *_uri : str, optional\\n            Additional keyword arguments who\\'s names end in \"_uri\"\\n            will be used to create a list of references in addition to\\n            the primary `help_uri`\\n        display : bool, optional\\n            Display the exception when created. By default, False\\n\\n        Notes\\n        -----\\n        The exception text is displayed when the exception is created\\n        and *not* when it is raised. We recommend creating the exception\\n        within the `raise` statement. E.g.\\n\\n        `raise MsticpyUserException(arg1, arg2...)`\\n\\n        Developer note:\\n        Any classes derived from MsticpyUserError should be named with\\n        an \"Error\" suffix to distinguish these from standard exception types.\\n\\n        \"\"\"\\n        # This nasty-looking thing just means that this is a list that\\n        # holds:\\n        # just strings - for simple args strings\\n        # tuples(str, str) - if the item is annotated as a uri or title\\n        # tuple(tuple(str, str), str) - if the URI is a tuple of display_name, URI\\n        self._output: List[\\n            Union[str, Tuple[str, str], Tuple[Tuple[str, str], str]]\\n        ] = []\\n        title = kwargs.pop(\"title\", \"we\\'ve hit an error while running\")\\n        disp_exception = kwargs.pop(\"display\", False)\\n        self._has_displayed = False\\n        self._output.append((f\"{self.__class__.__name__} - {title}\", \"title\"))\\n        self._output.extend(args)\\n\\n        self._output.append(\"\\\\nFor more help on fixing this error see:\")\\n        if not help_uri:\\n            help_uri = self.DEF_HELP_URI\\n        self._output.append((help_uri, \"uri\"))  # type: ignore\\n\\n        help_args = [\\n            kw_val for kw_arg, kw_val in kwargs.items() if kw_arg.endswith(\"_uri\")\\n        ]\\n        if help_args:\\n            self._output.append(\"You can find other related help here:\")\\n            self._output.extend((uri, \"uri\") for uri in help_args)\\n        self._context = self._format_exception_context(\\n            stack=traceback.format_stack(limit=5),\\n            frame_locals=sys._getframe(1).f_locals,\\n        )\\n        if _get_config(\"msticpy.FriendlyExceptions\") and disp_exception:\\n            self.display_exception()\\n\\n        # add the extra elements to the the exception standard args.\\n        ex_args = [title, *args, help_uri, *help_args]\\n        super().__init__(*ex_args)\\n\\n    @classmethod\\n    @contextlib.contextmanager\\n    def no_display_exceptions(cls):\\n        \"\"\"Context manager to block exception display to IPython/stdout.\"\"\"\\n        cls._display_exceptions = False\\n        yield\\n        cls._display_exceptions = True\\n\\n    @property\\n    def help_uri(self) -> Union[Tuple[str, str], str]:\\n        \"\"\"Get the default help URI.\"\"\"\\n        return self.DEF_HELP_URI\\n\\n    def display_exception(self):\\n        \"\"\"Output the exception HTML or text friendly exception.\"\"\"\\n        if not self._display_exceptions or self._has_displayed:\\n            return\\n        if is_ipython(notebook=True):\\n            display(self)\\n        else:\\n            self._display_txt_exception()\\n        self._has_displayed = True\\n\\n    def _repr_html_(self):\\n        \"\"\"Return HTML-formatted exception text.\"\"\"\\n        ex_style = \"\"\"\\n        <style>\\n            div.solid {border: thin solid black; padding:10px;}\\n            p.title {background-color:Tomato; padding:5px;}\\n            ul.circle {list-style-type: circle;}\\n            div.indent {text-indent: 20px; padding: 0px}\\n        </style>\\n        \"\"\"\\n        div_tmplt = \"<div class=\\'solid\\'>{content}</div>\"\\n        about_blank = \"target=\\'_blank\\' rel=\\'noopener noreferrer\\'\"\\n        content = []\\n        for line in self._output:\\n            if isinstance(line, tuple):\\n                l_content, l_type = line\\n                if l_type == \"title\":\\n                    content.append(f\"<h3><p class=\\'title\\'>{l_content}</p></h3>\")\\n                elif l_type == \"uri\":\\n                    if isinstance(l_content, tuple):\\n                        name, uri = l_content\\n                    else:\\n                        name = uri = l_content\\n                    content.append(\\n                        f\"<ul class=\\'circle\\'><li><a href=\\'{uri}\\' {about_blank}>\"\\n                        f\"{name}</a></li></ul>\"\\n                    )\\n            else:\\n                text_line = line.replace(\"\\\\n\", \"<br>\")\\n                content.append(f\"{text_line}<br>\")\\n\\n        if self._context:\\n            context = [f\"<div class=\\'indent\\'>{line}</div>\" for line in self._context]\\n            content.extend(\\n                (\\n                    \"<summary>Additional context<details>\",\\n                    *context,\\n                    \"</details></summary>\",\\n                )\\n            )\\n\\n        return \"\".join((ex_style, div_tmplt.format(content=\"\".join(content))))\\n\\n    def _display_txt_exception(self):\\n        \"\"\"Display text-only version of the exception text.\"\"\"\\n        print(self._get_exception_text())\\n\\n    def _get_exception_text(self) -> str:\\n        out_lines: List[str] = []\\n        for line in self._output:\\n            if isinstance(line, tuple):\\n                l_content, l_type = line\\n                if isinstance(l_content, tuple):\\n                    l_content = l_content[0]\\n                if l_type == \"title\":\\n                    out_lines.extend(\\n                        (\"-\" * len(l_content), l_content, \"-\" * len(l_content))\\n                    )\\n                elif l_type == \"uri\":\\n                    if isinstance(l_content, tuple):\\n                        out_lines.append(f\" - {\\': \\'.join(l_content)}\")\\n                    else:\\n                        out_lines.append(f\" - {l_content}\")\\n            else:\\n                out_lines.append(line)\\n        if self._context:\\n            out_lines.extend([\"\", \"Exception context:\", *self._context])\\n        return \"\\\\n\".join(out_lines)\\n\\n    @staticmethod\\n    def _format_exception_context(stack, frame_locals):\\n        context_lines: List[str] = [\"Stack:\", *stack, \"---\", \"Locals:\"]\\n        context_lines.extend(\\n            f\"{var} ({type(val).__name__}) = {val}\" for var, val in frame_locals.items()\\n        )\\n        ex_type, ex_value, ex_traceback = sys.exc_info()\\n        if ex_traceback:\\n            context_lines.extend(\\n                (\\n                    \"---\",\\n                    \"Exception was raised by:\",\\n                    *(traceback.format_exception(ex_type, ex_value, ex_traceback)),\\n                )\\n            )\\n        return context_lines\\n\\n\\nclass MsticpyUserConfigError(MsticpyUserError):\\n    \"\"\"Configuration user exception class for msticpy.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Configuring msticpy\",\\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/msticpyconfig.html\",\\n    )\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create generic user configuration exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n\\n        \"\"\"\\n        def_mssg = \"There is a problem with configuration in your msticpyconfig.yaml.\"\\n        mp_loc_mssg = [\\n            \"Ensure that the path to your msticpyconfig.yaml is specified with\"\\n            + \" the MSTICPYCONFIG environment variable.\",\\n            \"Or ensure that a copy of this file is in the current directory.\",\\n        ]\\n        add_args = [*args, *mp_loc_mssg] if args else [def_mssg, *mp_loc_mssg]\\n        if help_uri:\\n            uri: Union[Tuple[str, str], str] = help_uri\\n            add_uris = {\"basehelp_uri\": self.DEF_HELP_URI}\\n        else:\\n            uri = self.DEF_HELP_URI\\n            add_uris = {}\\n        super().__init__(*add_args, help_uri=uri, **add_uris, **kwargs)\\n\\n\\nclass MsticpyKeyVaultConfigError(MsticpyUserConfigError):\\n    \"\"\"Key Vault configuration exception.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Using keyvault to store msticpy secrets\",\\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/msticpyconfig.html\"\\n        + \"#specifying-secrets-as-key-vault-secrets\",\\n    )\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create Key Vault configuration exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n\\n        \"\"\"\\n        mssg = (\\n            \"Please verify that a valid KeyVault section has been configured\"\\n            + \"in your msticpyconfig.yaml.\"\\n        )\\n        add_args = [*args, mssg]\\n        uri = help_uri or self.DEF_HELP_URI\\n        super().__init__(*add_args, help_uri=uri, **kwargs)\\n\\n\\nclass MsticpyKeyVaultMissingSecretError(MsticpyKeyVaultConfigError):\\n    \"\"\"Missing secret exception.\"\"\"\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create Key Vault missing key exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n\\n        \"\"\"\\n        mssg = (\\n            \"Please verify that the item using this secret is properly\"\\n            + \" configured in in your msticpyconfig.yaml.\"\\n        )\\n        add_args = [*args, mssg]\\n        uri = help_uri or self.DEF_HELP_URI\\n        super().__init__(*add_args, help_uri=uri, **kwargs)\\n\\n\\nclass MsticpyAzureConfigError(MsticpyUserConfigError):\\n    \"\"\"Exception class for AzureData.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Using the Azure API connector\",\\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureData.html\"\\n        + \"#instantiating-and-connecting-with-an-azure-data-connector\",\\n    )\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create Azure data missing configuration exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n\\n        \"\"\"\\n        uri = help_uri or self.DEF_HELP_URI\\n        super().__init__(*args, help_uri=uri, **kwargs)\\n\\n\\nclass MsticpyNotConnectedError(MsticpyUserError):\\n    \"\"\"Exception class for NotConnected errors.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Querying and importing data\",\\n        \"https://msticpy.readthedocs.io/en/latest/DataAcquisition.html\"\\n        + \"#querying-and-importing-data\",\\n    )\\n\\n\\nclass MsticpyNoDataSourceError(MsticpyUserError):\\n    \"\"\"Exception class for missing data source errors.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Querying and importing data\",\\n        \"https://msticpy.readthedocs.io/en/latest/DataAcquisition.html\"\\n        + \"#querying-and-importing-data\",\\n    )\\n\\n\\nclass MsticpyDataQueryError(MsticpyUserError):\\n    \"\"\"Exception class for data query errors.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Query failed\",\\n        \"https://msticpy.readthedocs.io/en/latest/DataAcquisition.html\"\\n        + \"#querying-and-importing-data\",\\n    )\\n\\n\\nclass MsticpyConnectionError(MsticpyUserError):\\n    \"\"\"Exception class for KqlConnection errors.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"DataProviders\",\\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/DataProviders.html\",\\n    )\\n\\n\\nclass MsticpyKqlConnectionError(MsticpyUserError):\\n    \"\"\"Exception class for KqlConnection errors.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Connecting to Microsoft Sentinel\",\\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/DataProviders.html\"\\n        + \"#connecting-to-an-azure-sentinel-workspace\",\\n    )\\n\\n\\nclass MsticpyImportExtraError(MsticpyUserError, ImportError):\\n    \"\"\"Exception class for Imports that need an extra.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Installing msticpy\",\\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/Installing.html\",\\n    )\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create import missing extra exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n        extra : str\\n            The name of the setup extra that needs to be installed.\\n\\n        \"\"\"\\n        extra = kwargs.pop(\"extra\", None)\\n        if not extra:\\n            raise AttributeError(\"Keyword argument \\'extra\\' must be supplied\")\\n        mssg = \"\".join(\\n            [\\n                \"This feature requires one or more additional packages\",\\n                \" to be installed.\\\\n\",\\n                \"To do this run the command:\\\\n\",\\n                f\"pip install msticpy[{extra}]\",\\n            ]\\n        )\\n        add_args = [*args, mssg]\\n        uri = help_uri or self.DEF_HELP_URI\\n        super().__init__(*add_args, help_uri=uri, **kwargs)\\n\\n\\nclass MsticpyMissingDependencyError(MsticpyUserError, ImportError):\\n    \"\"\"Exception class for Imports that are not installed.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Installing msticpy\",\\n        \"https://msticpy.readthedocs.io/en/latest/getting_started/Installing.html\",\\n    )\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create import missing extra exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n        packages : Union[str, List[str]]\\n            The name of the packages or list of packages that need(s)\\n            to be installed.\\n\\n        \"\"\"\\n        packages = kwargs.pop(\"packages\", None)\\n        if not packages:\\n            raise AttributeError(\"Keyword argument \\'packages\\' must be supplied\")\\n        packages = [packages] if isinstance(packages, str) else packages\\n        mssg = \"\".join(\\n            [\\n                \"This feature requires one or more additional packages\",\\n                \" to be installed.\\\\n\",\\n                \"To do this run the command:\\\\n\",\\n                f\"pip install {\\' \\'.join(packages)}\\\\n\",\\n                \"In a notebook run:\\\\n\",\\n                f\"%pip install {\\' \\'.join(packages)}\",\\n            ]\\n        )\\n        add_args = [*args, mssg]\\n        uri = help_uri or self.DEF_HELP_URI\\n        super().__init__(*add_args, help_uri=uri, **kwargs)\\n\\n\\nclass MsticpyAzureConnectionError(MsticpyUserError):\\n    \"\"\"Exception class for Azure Connection errors.\"\"\"\\n\\n    DEF_HELP_URI = (\\n        \"Connecting to Microsoft Sentinel\",\\n        \"https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureData.html\"\\n        + \"#instantiating-and-connecting-with-an-azure-data-connector\",\\n    )\\n\\n\\nclass MsticpyParameterError(MsticpyUserError):\\n    \"\"\"Exception class for missing/incorrect parameters.\"\"\"\\n\\n    DEF_HELP_URI = (\"MSTICPy documentation\", \"https://msticpy.readthedocs.io\")\\n\\n    def __init__(\\n        self, *args, help_uri: Union[Tuple[str, str], str, None] = None, **kwargs\\n    ):\\n        \"\"\"\\n        Create parameter exception.\\n\\n        Parameters\\n        ----------\\n        help_uri : Union[Tuple[str, str], str, None], optional\\n            Override the default help URI.\\n        parameters : Union[str, List[str]\\n            The name of the bad parameter(s).\\n\\n        \"\"\"\\n        parameter = kwargs.pop(\"parameter\", None)\\n        if not parameter:\\n            raise AttributeError(\"Keyword argument \\'parameter\\' must be supplied\")\\n        mssg = \"One or more parameters were incorrect.\"\\n        if isinstance(parameter, str):\\n            parameter = [parameter]\\n        add_args = [*args, mssg, \", \".join(parameter)]\\n        uri = help_uri or self.DEF_HELP_URI\\n        super().__init__(*add_args, help_uri=uri, **kwargs)\\n'}]", "test_list": ["def test_compute_rarest_windows(self):\n    model = Model(sessions=self.sessions2)\n    self.assertRaises(MsticpyException, lambda: model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=False))\n    model.train()\n    model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=False)\n    self.assertTrue(3 in model.rare_window_likelihoods)\n    self.assertTrue(3 in model.rare_windows)\n    model = Model(sessions=self.sessions2)\n    model.train()\n    model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=True)\n    self.assertTrue(3 in model.rare_window_likelihoods_geo)\n    self.assertTrue(3 in model.rare_windows_geo)"], "requirements": {"Input-Output Conditions": {"requirement": "The function 'compute_rarest_windows' should validate that 'window_len' is a positive integer and 'use_start_end_tokens' and 'use_geo_mean' are booleans. It should raise a MsticpyException if these conditions are not met.", "unit_test": ["def test_compute_rarest_windows_input_validation(self):\n    model = Model(sessions=self.sessions2)\n    with self.assertRaises(MsticpyException):\n        model.compute_rarest_windows(window_len=-1, use_start_end_tokens=True, use_geo_mean=False)\n    with self.assertRaises(MsticpyException):\n        model.compute_rarest_windows(window_len=3, use_start_end_tokens='yes', use_geo_mean=False)\n    with self.assertRaises(MsticpyException):\n        model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean='no')"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_input_validation"}, "Exception Handling": {"requirement": "The function should raise a MsticpyException with a descriptive error message if 'prior_probs' is None, indicating that the model has not been trained.", "unit_test": ["def test_compute_rarest_windows_exception_handling(self):\n    model = Model(sessions=self.sessions2)\n    with self.assertRaises(MsticpyException) as context:\n        model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=False)\n    self.assertIn('please train the model first', str(context.exception))"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_exception_handling"}, "Edge Case Handling": {"requirement": "The function should handle edge cases where the session length is less than the 'window_len' by returning an empty list for rare windows and likelihoods.", "unit_test": ["def test_compute_rarest_windows_edge_case_handling(self):\n    model = Model(sessions=[['Cmd1']])\n    model.train()\n    model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=False)\n    self.assertEqual(model.rare_windows.get(3), [])\n    self.assertEqual(model.rare_window_likelihoods.get(3), [])"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_edge_case_handling"}, "Functionality Extension": {"requirement": "Extend the function to allow computation of rarest windows for multiple window lengths in a single call by accepting a list of window lengths.", "unit_test": ["def test_compute_rarest_windows_multiple_lengths(self):\n    model = Model(sessions=self.sessions2)\n    model.train()\n    model.compute_rarest_windows(window_len=[2, 3], use_start_end_tokens=True, use_geo_mean=False)\n    self.assertTrue(2 in model.rare_window_likelihoods)\n    self.assertTrue(3 in model.rare_window_likelihoods)"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_multiple_lengths"}, "Annotation Coverage": {"requirement": "Ensure that all parameters and return types of the 'compute_rarest_windows' function are annotated with appropriate type hints.", "unit_test": ["def test_compute_rarest_windows_annotations(self):\n    from typing import get_type_hints\n    hints = get_type_hints(Model.compute_rarest_windows)\n    self.assertEqual(hints['window_len'], int)\n    self.assertEqual(hints['use_start_end_tokens'], bool)\n    self.assertEqual(hints['use_geo_mean'], bool)\n    self.assertIsNone(hints.get('return'))"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_annotations"}, "Code Complexity": {"requirement": "The cyclomatic complexity of the 'compute_rarest_windows' function should not exceed 12 to ensure maintainability.", "unit_test": ["def test_compute_rarest_windows_complexity(self):\n    import radon.complexity as rc\n    with open('path_to_model_file.py', 'r') as file:\n        code = file.read()\n    complexity = rc.cc_visit(code)\n    compute_rarest_windows_complexity = next((c for c in complexity if c.name == 'compute_rarest_windows'), None)\n    self.assertIsNotNone(compute_rarest_windows_complexity)\n    self.assertLessEqual(compute_rarest_windows_complexity.complexity, 10)"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_complexity"}, "Code Standard": {"requirement": "The 'compute_rarest_windows' function should adhere to PEP 8 standards, including proper indentation, line length, and naming conventions.", "unit_test": ["def test_compute_rarest_windows_pep8(self):\n    import pycodestyle\n    style = pycodestyle.StyleGuide(quiet=True)\n    result = style.check_files(['path_to_model_file.py'])\n    self.assertEqual(result.total_errors, 0, 'Found code style errors (and warnings).')"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_check_code_style"}, "Context Usage Verification": {"requirement": "The function should utilize the 'sessions', 'prior_probs', and 'session_type' attributes from the Model class to compute the rarest windows.", "unit_test": ["def test_compute_rarest_windows_context_usage(self):\n    model = Model(sessions=self.sessions2)\n    model.train()\n    model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=False)\n    self.assertIsNotNone(model.sessions)\n    self.assertIsNotNone(model.prior_probs)\n    self.assertIsNotNone(model.session_type)"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_context_usage"}, "Context Usage Correctness Verification": {"requirement": "The function should correctly use the 'prior_probs' to calculate the likelihoods of windows, ensuring that the probabilities are applied as intended.", "unit_test": ["def test_compute_rarest_windows_correctness(self):\n    model = Model(sessions=self.sessions2)\n    model.train()\n    model.compute_rarest_windows(window_len=3, use_start_end_tokens=True, use_geo_mean=False)\n    # Assuming a mock or known probability distribution\n    expected_likelihood = ... # some expected value based on known distribution\n    self.assertAlmostEqual(model.rare_window_likelihoods[3][0], expected_likelihood, places=5)"], "test": "tests/analysis/test_anom_seq_model.py::TestModel::test_compute_rarest_windows_probability_calculation"}}}
