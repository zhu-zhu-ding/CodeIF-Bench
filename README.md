## <font style="color:rgb(31, 35, 40);">About</font>
+ **CodeIF-Bench** is a benchmark for evaluating the instruction-following ability of LLM in interactive code generation tasks.
+ **CodeIF-Bench** contains 9 verifiable instruction strategies collected from code review tasks.
+ **CodeIF-Bench** contains 964 verifiable instructions with test cases that cover both **SA** and **Non-SA** programming tasks and support **Multi-Turn** dialogue.


## Dataset
L1_1_part_1 and L1_1_part2 are from the StandAlone programming tasks of MBPP and DevEval, respectively. L-2 and L-3 are intra-file and cross-file repository-level programming tasks. <font style="color:rgb(31, 35, 40);">The original repositories can be downloaded from </font>[link](https://figshare.com/s/aa2ec81006727d9ddb0c)<font style="color:rgb(31, 35, 40);">.</font>

### funcion
In the `L_1_part_1.jsonl` file, each line of metadata is a json object, which contains the following fields:

+ `prompt`：string, a sentence describing the intended function of the generated code.
+ `test`: list, a list of basic tests to verify whether the generated function meets the fundamental requirements.
+ `code`: string, correct code that meets the basic requirements.
+ `task_id`: number, distinguishing task numbers for different assignments.
+ `requirements`: dictionary, with at most seven categories: `Input-Output Conditions`, `Exception Handling`, `Edge Case Handling`, `Functionality Extension`, `Annotation Coverage`, `Code Complexity`, and `Code Standard`. Each function may have a missing category depending on its specific situation.Each category's key-value pair is also a dictionary, containing `requirement` and `unit_test`, which represent the extended requirements and the corresponding additional tests for those requirements.
+ `multi-turn`: list, the generation order in multi-turn code generation tasks, which is designed to ensure no conflicts.

### repo
In the `L_1_part_1.jsonl`, `L_2_part_1.jsonl`, `L_2_part_2.jsonl` and `L_3.jsonl`， each line of metadata is a json object, which contains the following fields:

+ `namespace`: string, the unique name of the code to be generated, e.g., `benedict.utils.type_util.is_bool`.
+ `type`: string, the type of the code to be generated. `method` means the code is a member function in a class, and `function` means the code is a individual function.
+ `project_path`: string, the path of the project, e.g., `Text Processing/python-benedict`.
+ `completion_path`: string, the path of the file where the code to be generated is located, e.g., `Text Processing/python-benedict/benedict/utils/type_util.py`.
+ `signature_position`: list, the start and end line number of the signature within completion files e.g., `[238, 238]`. The line number starts from 1.
+ `body_position`: list, the start and end line number of the reference code within completion files e.g., `[239, 254]`. The line number starts from 1.
+ `dependency`: dict, the reference dependency. The keys include `intra_class`, `intra_file`, and `cross_file`. Each key stores a list of strings, which are namespaces of reference dependencies.
+ `requirement`: dict, the requirement of the code to be generated. The keys include `Functionality` and `Arguments`. `Functionality` is a string describing the functionality of the code to be generated. `Arguments` is a string explaining the input and output parameters.
+ `test`: list, a list of basic tests to verify whether the generated function meets the fundamental requirements.
+ `indent`: int, the indent of the code to be generated.
+ `domain`:string, the category of the project e.g., `Communications`.
+ `prompt`:string, generated by GPT, provided for reference.
+ `test_list`: list, the specific code content of each test in the test list.
+ `code`: string, correct code that meets the basic requirements.
+ `requirements`: dictionary, with at most seven categories: `Input-Output Conditions`, `Exception Handling`, `Edge Case Handling`, `Functionality Extension`, `Annotation Coverage`, `Code Complexity`, `Code Standard`, `Context Usage Verification` and `Context Usage Correctness Verification`. Each function may have a missing category depending on its specific situation.Each category's key-value pair is also a dictionary, containing `requirement` and `unit_test`, which represent the extended requirements and the corresponding additional tests for those requirements.
+ `multi-turn`: list, the generation order in multi-turn code generation tasks which is designed to ensure no conflicts.

### Setup
Before running the evaluation, researchers need to download the repositories.

#### Repositories
The new repositories can be downloaded from link. Users need to uncompressed the repositories and put them in the root directory (e.g., `repo/SourceCode`).  
The project contexts are stored in `Source_Code`. `Source_Code` contains 10 subfolders, each of which corresponds to a programming topic, e.g., `Text Processing`. Each topic contains multiple repositories. For each sample in metadata, we can find its repository based on the key `project_path`. Please do not modify the file structure of the repositories. Otherwise, the evaluation script can not work properly.

#### Dependency Data
The dependency data of repositories is used to evaluate the recall of reference dependencies and is available in [link](https://huggingface.co/datasets/LJ0815/DevEval/blob/main/Dependency_Data.tar.gz). Users need to uncompressed the dependency data and put them in the root directory (e.g., `repo/Dependency_Data`). Please do not modify the file name of the dependency data. Otherwise, the evaluation script can not load the cached dependency data.

### Evaluation
#### Environment Setup
Create a virtual conda environment and install the required packages.

```plain
conda create --name xxx --file environment.txt
conda activate xxx
pip install -r requirement.txt
# replace the path with your own path
echo "export NLTK_DATA=/home/user/xxx/nltk_data" >> ~/.bashrc
source ~/.bashrc
```

### Static Conversation

Goal: To evaluate the instruction following ability of LLM under predefined dialogue sequences.

#### Inference

Run `Inference/inferencet.sh` to perform inference using the LLM API interface.The script is shown below.

```python
ROOT=/home/user/benchmark/repo
python inference_gpt.py \
    --data_file $ROOT/L_1_part_2.jsonl \
    --output_file $ROOT/result.jsonl \
    --level 1 \
    --way 1 \
    --model QWEN
```

The arguments are explained as follows.

+ `data_file`: the metadata file.
+ `output_file`: the model's predictions.
+ `level`: the level of the dataset e.g., `1`, `2`, `3`, or `4`.
+ `way`: the inference ways, there are four ways: `way 1`: Basic generation, using the original question for generation without any additional requirements. `way 2`: Two-round generation, where the content from the basic generation is passed as the assistant, and the prompt is based on the new requirement. `else`: N-round generation, following the sequence of `multi_turn` in the dataset. For each requirement, the content generated from the previous requirement is passed as the assistant, and the prompt is a combination of the previous requirement's prompt and the new requirement.

#### Evaluation

1. Run `function/test/run_test.sh` to test L_1_part_1 or `repo/test/run_pass@_k_IFR and run_pass@_k_IACA `to test L_1_part_2, L2, and L3.
2. Run  `IA_CA.py` and `IFR.py` to get metrics.

### Dynamic Conversation

Goal: To evaluate the instruction following ability of LLM with test feedback and dynamic scenarios.

#### Inference

Run `inference_function.sh` and `inference_repo.sh`.

#### Evaluation

Run `report.py`
