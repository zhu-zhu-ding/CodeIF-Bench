## <font style="color:rgb(31, 35, 40);">About</font>
+ **CodeIF-Bench** is a benchmark for evaluating the instruction-following ability of LLM in both code generation tasks.
+ **CodeIF-Bench** contains 9 verifiable instruction strategies collected from code review tasks.
+ **CodeIF-Bench** contains 879 verifiable instructions with test cases that cover both **SA** and **Non-SA **programming tasks and support **Multi-Turn** dialogue.

## Leaderboard
![](https://cdn.nlark.com/yuque/0/2025/png/38861830/1738848430606-4b1254ea-4322-438e-82e7-f5b52a56f946.png)

## Dataset
<font style="color:rgb(31, 35, 40);">The original repositories can be downloaded from </font>[link](https://huggingface.co/datasets/zzzzd/CodeIF-Bench)<font style="color:rgb(31, 35, 40);">.</font>

### funcion
In the `L_1_part_1.jsonl` file, each line of metadata is a json object, which contains the following fields:

+ `prompt`：string, a sentence describing the intended function of the generated code.
+ `test`: list, a list of basic tests to verify whether the generated function meets the fundamental requirements.
+ `code`: string, correct code that meets the basic requirements.
+ `task_id`: number, distinguishing task numbers for different assignments.
+ `requirements`: dictionary, with at most seven categories: `Input-Output Conditions`, `Exception Handling`, `Edge Case Handling`, `Functionality Extension`, `Annotation Coverage`, `Code Complexity`, and `Code Standard`. Each function may have a missing category depending on its specific situation.Each category's key-value pair is also a dictionary, containing `requirement` and `unit_test`, which represent the extended requirements and the corresponding additional tests for those requirements.
+ `multi-turn`: list, the generation order in multi-turn code generation tasks which is designed to ensure no conflicts.

### repo
In the `L_1_part_1.jsonl`, `L_2_part_1.jsonl`, `L_2_part_2.jsonl` and `L_3.jsonl`， each line of metadata is a json object, which contains the following fields:

+ `namespace`: string, the unique name of the code to be generated, e.g., `benedict.utils.type_util.is_bool`.
+ `type`: string, the type of the code to be generated. `method` means the code is a member function in a class, and `function` means the code is a individual function.
+ `project_path`: string, the path of the project, e.g., `Text Processing/python-benedict`.
+ `completion_path`: string, the path of the file where the code to be generated is located, e.g., `Text Processing/python-benedict/benedict/utils/type_util.py`.
+ `signature_position`: list, the start and end line number of the signature within completion files e.g., `[238, 238]`. The line number starts from 1.
+ `body_position`: list, the start and end line number of the reference code within completion files e.g., `[239, 254]`. The line number starts from 1.
+ `dependency`: dict, the reference dependency. The keys include `intra_class`, `intra_file`, and `cross_file`. Each key stores a list of strings, which are namespaces of reference dependencies.
+ `requirement`: dict, the requirement of the code to be generated. The keys include `Functionality` and `Arguments`. `Functionality` is a string describing the functionality of the code to be generated. `Arguments` is a string explaining the input and output parameters.
+ `test`: list, a list of basic tests to verify whether the generated function meets the fundamental requirements.
+ `indent`: int, the indent of the code to be generated.
+ `domain`:string, the category of the project e.g., `Communications`.
+ `prompt`:string, generated by GPT, provided for reference.
+ `test_list`: list, the specific code content of each test in the test list.
+ `code`: string, correct code that meets the basic requirements.
+ `requirements`: dictionary, with at most seven categories: `Input-Output Conditions`, `Exception Handling`, `Edge Case Handling`, `Functionality Extension`, `Annotation Coverage`, `Code Complexity`, `Code Standard`, `Context Usage Verification` and `Context Usage Correctness Verification`. Each function may have a missing category depending on its specific situation.Each category's key-value pair is also a dictionary, containing `requirement` and `unit_test`, which represent the extended requirements and the corresponding additional tests for those requirements.
+ `multi-turn`: list, the generation order in multi-turn code generation tasks which is designed to ensure no conflicts.

### Setup
Before running the evaluation, researchers need to download the repositories.

#### Repositories
The new repositories can be downloaded from link. Users need to uncompressed the repositories and put them in the root directory (e.g., `repo/SourceCode`).  
The project contexts are stored in `Source_Code`. `Source_Code` contains 10 subfolders, each of which corresponds to a programming topic, e.g., `Text Processing`. Each topic contains multiple repositories. For each sample in metadata, we can find its repository based on the key `project_path`. Please do not modify the file structure of the repositories. Otherwise, the evaluation script can not work properly.

#### Dependency Data
The dependency data of repositories is used to evaluate the recall of reference dependencies and is available in [link](https://huggingface.co/datasets/LJ0815/DevEval/blob/main/Dependency_Data.tar.gz). Users need to uncompressed the dependency data and put them in the root directory (e.g., `repo/Dependency_Data`). Please do not modify the file name of the dependency data. Otherwise, the evaluation script can not load the cached dependency data.

### Evaluation
#### Environment Setup
Create a virtual conda environment and install the required packages.

```plain
conda create --name xxx --file environment.txt
conda activate xxx
pip install -r requirement.txt
# replace the path with your own path
echo "export NLTK_DATA=/home/user/xxx/nltk_data" >> ~/.bashrc
source ~/.bashrc
```

#### Completion Format
User need to convert models' predictions into a jsonl file. Each line of the jsonl file is a json object storing a completion for a requirement.   
An example of a funtion json object is shown below.

```json
{
    "task_id": "11",
    "completion": "I'll help you create a function to remove the first and last occurrence of a specified character from a string. Here's the solution:\n\n```python\ndef remove_occ(string, char):\n    # Handle empty string or character\n    if not string or not char:\n        return string\n    \n    # Find first and last occurrence```"
}
```

An example of a repo json of level_1 object is shown below.

```json
{
    "namespace": "benedict.utils.type_util.is_json_serializable",
    "completion": "I'll help you create a function to remove the first and last occurrence of a specified character from a string. Here's the solution:\n\n```python\ndef remove_occ(string, char):\n    # Handle empty string or character\n    if not string or not char:\n        return string\n    \n    # Find first and last occurrence```"
}
```

An example of a repo json of level_2, level_3 object is shown below.  
The `test` part is the list of tests corresponding to the type.

```json
{
    "namespace": "benedict.utils.type_util.is_json_serializable",
    "type": "Input-Output Conditions",
    "test":
"tests/utils/test_type_util.py::type_util_test_case::test_input_output_conditions",
    "completion": "I'll help you create a function to remove the first and last occurrence of a specified character from a string. Here's the solution:\n\n```python\ndef remove_occ(string, char):\n    # Handle empty string or character\n    if not string or not char:\n        return string\n    \n    # Find first and last occurrence```"
}
```

We provide processed completion files of a few LLMs in `Experiments`.

### Inference
Users can run `Inference/inference.sh` to perform local model inference. The script is shown below.

```python
ROOT=/home/user/benchmark/repo
python inference.py \
    --data_file $ROOT/L_1_part_2.jsonl \
    --output_file $ROOT/result.jsonl \
    --model_path /home/user/model/Qwen2.5-Coder-7B-Instruct \
    --level 1 \
    --way 1
    # --lora_path 
    # --base_file $ROOT/pre_result.jsonl \
```

The arguments are explained as follows.

+ `data_file`: the metadata file.
+ `output_file`: the model's predictions.
+ `model_path`: the location where the model is stored locally.
+ `level`: the level of the dataset e.g., `1`, `2`, `3`, or `4`.
+ `way`: the inference ways, there are four ways: `way 1`: Basic generation, using the original question for generation without any additional requirements. `way 2`: Single-round generation, using the basic question and each additional requirement for generation. `way 3`: Multi-round generation, where the content from the basic generation is passed as the assistant, and the prompt is based on the new requirement. `way 4`: N-round generation, following the sequence of `multi_turn` in the dataset. For each requirement, the content generated from the previous requirement is passed as the assistant, and the prompt is a combination of the previous requirement's prompt and the new requirement.
+ `lora_path`: the storage location of the fine-tuned model parameters.
+ `base_file`: The location of the results from the base generation that need to be utilized in multi-round and n-round generation.

Users can run `Inference/inference_gpt.sh` to perform inference using the LLM API interface.The script is shown below.

```python
ROOT=/home/user/benchmark/repo
python inference_gpt.py \
    --data_file $ROOT/L_1_part_2.jsonl \
    --output_file $ROOT/result.jsonl \
    --level 1 \
    --way 1 \
    --model QWEN
    # --base_file $ROOT/pre_result.jsonl
```

The arguments are explained as follows.

+ `data_file`: the metadata file.
+ `output_file`: the model's predictions.
+ `level`: the level of the dataset e.g., `1`, `2`, `3`, or `4`.
+ `way`: the inference ways, there are four ways: `way 1`: Basic generation, using the original question for generation without any additional requirements. `way 2`: Single-round generation, using the basic question and each additional requirement for generation. `way 3`: Multi-round generation, where the content from the basic generation is passed as the assistant, and the prompt is based on the new requirement. `way 4`: N-round generation, following the sequence of `multi_turn` in the dataset. For each requirement, the content generated from the previous requirement is passed as the assistant, and the prompt is a combination of the previous requirement's prompt and the new requirement.
+ `model`: the types of LLM interfaces to call, such as `GPT`, `QWEN`, or `DEEP`. Users need to replace the key in `inference_gpt.py` when using them.
+ `base_file`: The location of the results from the base generation that need to be utilized in multi-round and n-round generation.

#### Pass@k
Users can run `funtion/test/run_test.sh` to compute the Pass@k of funtion. The script is shown below.

```python
ROOT=/home/user/benchmark/function
python evaluate_functional_correctness.py \
    --sample_file $ROOT/result.jsonl \
    --n_workers 4 \
    --timeout 3.0 \
    --problem_file $ROOT/L_1_part_1.jsonl \
    --way 1
```

The arguments are explained as follows.

+ `sample_file`: the model's predictions.
+ `n_workers`: the number of parallel threads.
+ `timeout`: the time limit for generating code.
+ `problem_file`: the metadata file.
+ `way`: the test ways, there are three ways: `way 1`: Base generation, using the base's tests for testing. `way 2`: Use base generation along with a separate test for each requirement. `way 3`: N-round testing, following the sequence in multi-turn. For each `task_id`, the tests for the current requirement include both the base tests and the `unit_tests` of all previous requirements in the test list.

Users can run `repo/test/run_pass_k.sh` to compute the Pass@k of repo. The script is shown below.

```python
ROOT=/home/user/benchmark/repo
python $ROOT/check_source_code.py $ROOT/Source_Code
# Compute Pass@1
python run_pass@k.py \
    --output_file $ROOT/result.jsonl \
    --log_file $ROOT/result_result.jsonl \
    --source_code_root $ROOT/Source_Code \
    --data_file $ROOT/L_1_part_2.jsonl \
    --n 1 \
    --k 1
```

The arguments are explained as follows.

+ `output_file`: the model's predictions.
+ `log_file`: the output file that stores the evaluation results.
+ `source_code_root`: the path of repositories.
+ `data_file`: the metadata file.
+ `n`: number of completions per task, e.g., `1`
+ `k`:the k value in Pass@k, e.g., `1`

### Citation
